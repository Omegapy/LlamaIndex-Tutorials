{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Baseline LLM : LlamaIndex Intro. Tutorial\n",
    "Alejandro Ricciardi (Omegapy)  \n",
    "created date: 12/26/2023 \n",
    "GitHub: https://github.com/Omegapy\n",
    "\n",
    "Projects Description:\n",
    "\n",
    "Using LlamaIndex to evaluate an LLM query with the following two main evaluations:\n",
    "- ResponseSourceEvaluator:\n",
    "    - uses an LLM to decide if the response is similar enough to the sources \n",
    "        - a good measure for hallucination detection!\n",
    "- QueryResponseEvaluator:\n",
    "    - uses an LLM to decide if a response is similar enough to the original query \n",
    "        - a good measure for checking if the query was answered!\n",
    "\n",
    "Project Map:\n",
    "\n",
    "- API Key\n",
    "- Loading Docs\n",
    "    - load_markdown_docs() Function\n",
    "    - Load documents from each folder.\n",
    "- Create the indices\n",
    "   - Create a vector store index for each folder \n",
    "- Create Query Engine Tools\n",
    "    - Create a Unified Query Engine\n",
    "- Test the Query Engine\n",
    "    - Evaluate the Baseline\n",
    "        - Generate the Dataset\n",
    "        - Evaluate with the Dataset\n",
    "        - Investigating Hallucinations\n",
    "        - Generating response from a known hallucinated_questions\n",
    "    \n",
    "Credit: LlamaIndex https://www.youtube.com/watch?v=2c64G-iDJKQ"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "248a2e9410f3878d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### API KEY"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfa57417d257be98"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables API Keys\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv()) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:08.010215500Z",
     "start_time": "2023-12-29T16:05:07.974134900Z"
    }
   },
   "id": "872a371b7a515598",
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Docs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "401cd22381357f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import llama_index as li"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:08.023217600Z",
     "start_time": "2023-12-29T16:05:08.013376400Z"
    }
   },
   "id": "e186c4465f390b77",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_docs_bot.markdown_docs_reader import MarkdownDocsReader # See llama_docs_bot/markdown_docs_reader.py\n",
    "from llama_index import SimpleDirectoryReader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:08.024216600Z",
     "start_time": "2023-12-29T16:05:08.014883800Z"
    }
   },
   "id": "7d372c192350b642",
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### load_markdown_docs() Function\n",
    "Load markdown docs from a directory, excluding all other file types."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c5f99bbb1d53942"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_markdown_docs(filepath):\n",
    "    \n",
    "    loader = SimpleDirectoryReader(\n",
    "        input_dir=filepath, \n",
    "        required_exts=[\".md\"],\n",
    "        file_extractor={\".md\": MarkdownDocsReader()},\n",
    "        recursive=True\n",
    "    )\n",
    "\n",
    "    documents = loader.load_data()\n",
    "\n",
    "    # exclude some metadata from the LLM\n",
    "    for doc in documents:\n",
    "        doc.excluded_llm_metadata_keys = [\"file_name\", \"content_type\", \"header_path\"]\n",
    "\n",
    "    return documents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:08.049648200Z",
     "start_time": "2023-12-29T16:05:08.024216600Z"
    }
   },
   "id": "3307de3eb8a41756",
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load documents from each folder.\n",
    "The Docs are separate for now, in order to create separate indexes later on."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19f43e8aba90ed88"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "getting_started_docs = load_markdown_docs(\"docs/getting_started\")\n",
    "community_docs = load_markdown_docs(\"docs/community\")\n",
    "data_docs = load_markdown_docs(\"docs/core_modules/data_modules\")\n",
    "agent_docs = load_markdown_docs(\"docs/core_modules/agent_modules\")\n",
    "model_docs = load_markdown_docs(\"docs/core_modules/model_modules\")\n",
    "query_docs = load_markdown_docs(\"docs/core_modules/query_modules\")\n",
    "supporting_docs = load_markdown_docs(\"docs/core_modules/supporting_modules\")\n",
    "tutorials_docs = load_markdown_docs(\"docs/end_to_end_tutorials\")\n",
    "contributing_docs = load_markdown_docs(\"docs/development\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:08.117914100Z",
     "start_time": "2023-12-29T16:05:08.036636100Z"
    }
   },
   "id": "13fe910893b3f752",
   "execution_count": 96
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the indices\n",
    "The ServiceContext is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application. You can use it to set the global configuration, as well as local configurations at specific parts of the pipeline.\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context.html#setting-local-configuration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbc2c84b0b83c16c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, set_global_service_context\n",
    "from llama_index.llms import OpenAI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:08.117914100Z",
     "start_time": "2023-12-29T16:05:08.114455100Z"
    }
   },
   "id": "96bc204a06e8f9e3",
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# create a global service context\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0))\n",
    "set_global_service_context(service_context)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:08.126270500Z",
     "start_time": "2023-12-29T16:05:08.117914100Z"
    }
   },
   "id": "34d1c139d79c4f7b",
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create a vector store index for each folder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6059fc94f61594f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:08.130769900Z",
     "start_time": "2023-12-29T16:05:08.126270500Z"
    }
   },
   "id": "3ad44261df621d94",
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "try:\n",
    "    getting_started_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./getting_started_index\"))\n",
    "    community_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./community_index\"))\n",
    "    data_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./data_index\"))\n",
    "    agent_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./agent_index\"))\n",
    "    model_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./model_index\"))\n",
    "    query_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./query_index\"))\n",
    "    supporting_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./supporting_index\"))\n",
    "    tutorials_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./tutorials_index\"))\n",
    "    contributing_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./contributing_index\"))\n",
    "except:\n",
    "    getting_started_index = VectorStoreIndex.from_documents(getting_started_docs)\n",
    "    getting_started_index.storage_context.persist(persist_dir=\"./getting_started_index\")\n",
    "\n",
    "    community_index = VectorStoreIndex.from_documents(community_docs)\n",
    "    community_index.storage_context.persist(persist_dir=\"./community_index\")\n",
    "\n",
    "    data_index = VectorStoreIndex.from_documents(data_docs)\n",
    "    data_index.storage_context.persist(persist_dir=\"./data_index\")\n",
    "\n",
    "    agent_index = VectorStoreIndex.from_documents(agent_docs)\n",
    "    agent_index.storage_context.persist(persist_dir=\"./agent_index\")\n",
    "\n",
    "    model_index = VectorStoreIndex.from_documents(model_docs)\n",
    "    model_index.storage_context.persist(persist_dir=\"./model_index\")\n",
    "\n",
    "    query_index = VectorStoreIndex.from_documents(query_docs)\n",
    "    query_index.storage_context.persist(persist_dir=\"./query_index\")    \n",
    "\n",
    "    supporting_index = VectorStoreIndex.from_documents(supporting_docs)\n",
    "    supporting_index.storage_context.persist(persist_dir=\"./supporting_index\")\n",
    "\n",
    "    tutorials_index = VectorStoreIndex.from_documents(tutorials_docs)\n",
    "    tutorials_index.storage_context.persist(persist_dir=\"./tutorials_index\")\n",
    "\n",
    "    contributing_index = VectorStoreIndex.from_documents(contributing_docs)\n",
    "    contributing_index.storage_context.persist(persist_dir=\"./contributing_index\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:14.731562500Z",
     "start_time": "2023-12-29T16:05:08.132769800Z"
    }
   },
   "id": "17e4fb921dd5e61d",
   "execution_count": 100
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Query Engine Tools\n",
    "Since we have so many indices, we can create a query engine tool for each and then use them in a single query engine!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4839657c54bb76b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:14.739469600Z",
     "start_time": "2023-12-29T16:05:14.731562500Z"
    }
   },
   "id": "455d24a7373a9bcf",
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# create a query engine tool for each folder\n",
    "getting_started_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=getting_started_index.as_query_engine(), \n",
    "    name=\"Getting Started\", \n",
    "    description=\"Useful for answering questions about installing and running llama index, as well as basic explanations of how llama index works.\"\n",
    ")\n",
    "\n",
    "community_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=community_index.as_query_engine(),\n",
    "    name=\"Community\",\n",
    "    description=\"Useful for answering questions about integrations and other apps built by the community.\"\n",
    ")\n",
    "\n",
    "data_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=data_index.as_query_engine(),\n",
    "    name=\"Data Modules\",\n",
    "    description=\"Useful for answering questions about data loaders, documents, nodes, and index structures.\"\n",
    ")\n",
    "\n",
    "agent_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=agent_index.as_query_engine(),\n",
    "    name=\"Agent Modules\",\n",
    "    description=\"Useful for answering questions about data agents, agent configurations, and tools.\"\n",
    ")\n",
    "\n",
    "model_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=model_index.as_query_engine(),\n",
    "    name=\"Model Modules\",\n",
    "    description=\"Useful for answering questions about using and configuring LLMs, embedding modles, and prompts.\"\n",
    ")\n",
    "\n",
    "query_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_index.as_query_engine(),\n",
    "    name=\"Query Modules\",\n",
    "    description=\"Useful for answering questions about query engines, query configurations, and using various parts of the query engine pipeline.\"\n",
    ")\n",
    "\n",
    "supporting_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=supporting_index.as_query_engine(),\n",
    "    name=\"Supporting Modules\",\n",
    "    description=\"Useful for answering questions about supporting modules, such as callbacks, service context, and avaluation.\"\n",
    ")\n",
    "\n",
    "tutorials_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=tutorials_index.as_query_engine(),\n",
    "    name=\"Tutorials\",\n",
    "    description=\"Useful for answering questions about end-to-end tutorials and giving examples of specific use-cases.\"\n",
    ")\n",
    "\n",
    "contributing_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=contributing_index.as_query_engine(),\n",
    "    name=\"Contributing\",\n",
    "    description=\"Useful for answering questions about contributing to llama index, including how to contribute to the codebase and how to build documentation.\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:14.746040500Z",
     "start_time": "2023-12-29T16:05:14.734966500Z"
    }
   },
   "id": "92039233fbc5cf68",
   "execution_count": 102
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Unified Query Engine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c4c492a60456fd2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# needed for notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[\n",
    "        getting_started_tool,\n",
    "        community_tool,\n",
    "        data_tool,\n",
    "        agent_tool,\n",
    "        model_tool,\n",
    "        query_tool,\n",
    "        supporting_tool,\n",
    "        tutorials_tool,\n",
    "        contributing_tool\n",
    "    ],\n",
    "    # enable this for streaming\n",
    "    #response_synthesizer=get_response_synthesizer(streaming=True),\n",
    "    verbose=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:14.752797200Z",
     "start_time": "2023-12-29T16:05:14.743040100Z"
    }
   },
   "id": "10535bfd2f01c995",
   "execution_count": 103
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the Query Engine!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a13793ca61a01aec"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install Llama Index, you can use either of the following methods:\n",
      "\n",
      "1. Installation from Pip:\n",
      "   Run the following command in your terminal or command prompt:\n",
      "   ```\n",
      "   pip install llama-index\n",
      "   ```\n",
      "\n",
      "2. Installation from Source:\n",
      "   Follow these steps:\n",
      "   - Clone the repository by running the command:\n",
      "     ```\n",
      "     git clone https://github.com/jerryjliu/llama_index.git\n",
      "     ```\n",
      "   - After cloning, navigate to the cloned directory.\n",
      "   - If you want to do an editable install (where you can modify source files), run:\n",
      "     ```\n",
      "     pip install -e .\n",
      "     ```\n",
      "   - If you want to install optional dependencies and dependencies used for development (e.g., unit testing), run:\n",
      "     ```\n",
      "     pip install -r requirements.txt\n",
      "     ```\n",
      "\n",
      "Choose the method that suits your needs and follow the corresponding steps to install Llama Index.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do I install llama index?\")\n",
    "print(str(response))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:39.337512700Z",
     "start_time": "2023-12-29T16:05:14.751796500Z"
    }
   },
   "id": "3a58815ee2e3df9a",
   "execution_count": 104
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the Baseline!\n",
    "\n",
    "Now that we have our baseline query engine created, we can create a basic evaluation pipeline!\n",
    "\n",
    "Our pipeline will:\n",
    "\n",
    "- Generate a small dataset of questions\n",
    "- Save/cache these questions (so we can properly compare performance later!)\n",
    "- Evaluate both response quality and hallucination\n",
    "\n",
    "To do this reliably, we need to use an LLM smarter than `gpt-3.5-turbo`, so we will setup `gpt-4` for the evaluation process!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d57b3454bf429cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate the Dataset\n",
    "\n",
    "In order to make the question generation more efficient, we can remove small documents and combine all documents into a giant single document.\n",
    "\n",
    "I also modify the question generation prompt, to generate a single question for each chunk, along with extra context for what it is reading."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1a117507fd24fab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index import Document"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:39.341176400Z",
     "start_time": "2023-12-29T16:05:39.338509800Z"
    }
   },
   "id": "8c43852cee870f18",
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"docs\", recursive=True, required_exts=[\".md\"]).load_data()\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for doc in documents:\n",
    "    all_text += doc.text\n",
    "\n",
    "giant_document = Document(text=all_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:39.443333500Z",
     "start_time": "2023-12-29T16:05:39.341176400Z"
    }
   },
   "id": "6d4cde2a6530f4fc",
   "execution_count": 106
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.prompts import Prompt\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:39.446198600Z",
     "start_time": "2023-12-29T16:05:39.444333100Z"
    }
   },
   "id": "2221495459a0db7",
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gpt4_service_context = ServiceContext.from_defaults(llm=OpenAI(llm=\"gpt-4\", temperature=0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:39.453568600Z",
     "start_time": "2023-12-29T16:05:39.446198600Z"
    }
   },
   "id": "8362de13dd254dcd",
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "question_dataset = []\n",
    "if os.path.exists(\"./data/question_dataset.txt\"):\n",
    "    with open(\"data/question_dataset.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            question_dataset.append(line.strip())\n",
    "else:\n",
    "    # generate questions\n",
    "    data_generator = DatasetGenerator.from_documents(\n",
    "        [giant_document],\n",
    "        text_question_template=Prompt(\n",
    "            \"A sample from the LlamaIndex documentation is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Using the documentation sample, carefully follow the instructions below:\\n\"\n",
    "            \"{query_str}\"\n",
    "        ),\n",
    "        question_gen_query=(\n",
    "            \"You are an evaluator for a search pipeline. Your task is to write a single question \"\n",
    "            \"using the provided documentation sample above to test the search pipeline. The question should \"\n",
    "            \"reference specific names, functions, and terms. Restrict the question to the \"\n",
    "            \"context information provided.\\n\"\n",
    "            \"Question: \"\n",
    "        ),\n",
    "        # set this to be low, so we can generate more questions\n",
    "        service_context=gpt4_service_context\n",
    "    )\n",
    "    generated_questions = data_generator.generate_questions_from_nodes()\n",
    "\n",
    "    # randomly pick 40 questions from each dataset\n",
    "    generated_questions = random.sample(generated_questions, 40)\n",
    "    question_dataset.extend(generated_questions)\n",
    "\n",
    "    print(f\"Generated {len(question_dataset)} questions.\")\n",
    "\n",
    "    # save the questions!\n",
    "    with open(\"question_dataset.txt\", \"w\") as f:\n",
    "        for question in question_dataset:\n",
    "            f.write(f\"{question.strip()}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:39.459800500Z",
     "start_time": "2023-12-29T16:05:39.454572100Z"
    }
   },
   "id": "9093ae20d3ca23e3",
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the function used to specify the metadata visible to the embedding model and how can it be customized?', 'How can I convert tools to LangChain tools using the provided documentation sample?', 'What is the purpose of the \"router query engine\" in the LlamaIndex framework?', 'What are the different vector stores supported by LlamaIndex for use as the storage backend for `VectorStoreIndex`?', 'What is the default number of LLM calls required for the ListIndex?']\n",
      "Generated 40 questions.\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(question_dataset, 5))\n",
    "print(f\"Generated {len(question_dataset)} questions.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:39.467044400Z",
     "start_time": "2023-12-29T16:05:39.460304900Z"
    }
   },
   "id": "3bfd975fd6d09367",
   "execution_count": 110
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate with the Dataset\n",
    "\n",
    "Now that we have our dataset, let's measure performance!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "136a551cfd7890e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluating Response for Hallucination"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b341da8079b16e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index import Response\n",
    "\n",
    "def evaluate_query_engine(evaluator, query_engine, questions):\n",
    "    async def run_query(query_engine, q):\n",
    "        try:\n",
    "            return await query_engine.aquery(q)\n",
    "        except:\n",
    "            return Response(response=\"Error, query failed.\")\n",
    "\n",
    "    total_correct = 0\n",
    "    all_results = []\n",
    "    for batch_size in range(0, len(questions), 5):\n",
    "        batch_qs = questions[batch_size:batch_size+5]\n",
    "        \n",
    "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
    "        responses = asyncio.run(asyncio.gather(*tasks))\n",
    "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
    "        \n",
    "        for response in responses:\n",
    "            if evaluator.evaluate_response(response=response).passing: \n",
    "                eval_result = 1\n",
    "            else:\n",
    "                eval_result = 0\n",
    "            total_correct += eval_result\n",
    "            all_results.append(eval_result)\n",
    "        \n",
    "        # helps avoid rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return total_correct, all_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:05:39.472943900Z",
     "start_time": "2023-12-29T16:05:39.467044400Z"
    }
   },
   "id": "fa243310166ea49a",
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished batch 1 out of 8\n",
      "finished batch 2 out of 8\n",
      "finished batch 3 out of 8\n",
      "finished batch 4 out of 8\n",
      "finished batch 5 out of 8\n",
      "finished batch 6 out of 8\n",
      "finished batch 7 out of 8\n",
      "finished batch 8 out of 8\n",
      "Hallucination? Scored 33 out of 40 questions correctly.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "# gpt-4 evaluator!\n",
    "evaluator = FaithfulnessEvaluator(service_context=gpt4_service_context)\n",
    "\n",
    "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
    "\n",
    "print(f\"Hallucination? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:09:06.930054600Z",
     "start_time": "2023-12-29T16:05:39.473943900Z"
    }
   },
   "id": "340f1e2747652e3b",
   "execution_count": 112
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Investigating Hallucinations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1119a5e25e5fd7d8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How can I convert tools to LangChain tools using the provided documentation sample?'\n",
      " 'What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?'\n",
      " 'What are the available options for the storage backend of the index store in LlamaIndex?'\n",
      " \"What is the purpose of the `CollectionQueryConsumer` class in the Delphic application's WebSocket handling?\"\n",
      " \"What is the function used to retrieve the collections for the logged-in user in the Delphic project's frontend?\"\n",
      " 'What is the purpose of the Algovera tool built on top of LlamaIndex?'\n",
      " 'What are the three primary sections within the layout of the ChatView component?']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Incorrect answers, incorrect answer number = len(question_dataset) - total_correct\n",
    "hallucinated_questions = np.array(question_dataset)[np.array(all_results) == 0]\n",
    "print(hallucinated_questions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:09:06.931563Z",
     "start_time": "2023-12-29T16:09:06.926556Z"
    }
   },
   "id": "1cf3c57203420e9e",
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generating response from a known hallucinated_questions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ba3e577bc416ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inv_hallucinated_question = query_engine.query(hallucinated_questions[0]) # \"How can I convert tools to LangChain tools using the provided documentation sample?\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:09:26.676906900Z",
     "start_time": "2023-12-29T16:09:06.932066700Z"
    }
   },
   "id": "8322a41ae8f8c497",
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context information does not provide any information about converting tools to LangChain tools using the provided documentation sample.\n",
      "-----------------\n",
      "> Source (Doc id: e0069bdd-195d-40ed-a0ab-c92d6d1e60b3): Sub question: What are the installation requirements for llama index?\n",
      "Response: The installation requirements for llama index include the following:\n",
      "- Git clone the repository: `git clone https://github.com/jerryjliu/llama_index.git`\n",
      "- Install the packa...\n",
      "\n",
      "> Source (Doc id: b4c280af-af4f-4b5e-9281-cba8ecc139e7): Sub question: How do I download and install llama index?\n",
      "Response: To download and install Llama Index, you can use either of the following methods:\n",
      "\n",
      "1. Installation from Pip:\n",
      "   Run the following command in your terminal or command prompt:\n",
      "   ```\n",
      "   pi...\n",
      "\n",
      "> Source (Doc id: b9cd686a-5e90-4ffc-ab17-d58d9f5aefac): Sub question: What are the steps to set up llama index?\n",
      "Response: To set up LlamaIndex, you need to follow these steps:\n",
      "1. Install LlamaIndex by running the command \"pip install llama-index\".\n",
      "2. Familiarize yourself with the high-level concepts of Llama...\n",
      "\n",
      "> Source (Doc id: 0a970091-c205-455b-b7b3-0f94848666aa): pip install llama-index\n",
      "\n",
      "> Source (Doc id: 5feaad62-8280-43fb-a495-b20c122d2f39): Git clone this repository: `git clone https://github.com/jerryjliu/llama_index.git`. Then do:\n",
      "\n",
      "- `pip install -e .` if you want to do an editable install (you can modify source files) of just the package itself.\n",
      "- `pip install -r requirements.txt` if yo...\n",
      "\n",
      "> Source (Doc id: 0a970091-c205-455b-b7b3-0f94848666aa): pip install llama-index\n",
      "\n",
      "> Source (Doc id: 5feaad62-8280-43fb-a495-b20c122d2f39): Git clone this repository: `git clone https://github.com/jerryjliu/llama_index.git`. Then do:\n",
      "\n",
      "- `pip install -e .` if you want to do an editable install (you can modify source files) of just the package itself.\n",
      "- `pip install -r requirements.txt` if yo...\n",
      "\n",
      "> Source (Doc id: 0a970091-c205-455b-b7b3-0f94848666aa): pip install llama-index\n",
      "\n",
      "> Source (Doc id: 99c14b36-f5cc-4e01-a302-575c25cd0bcc): LlamaIndex helps you build LLM-powered applications (e.g. Q&A, chatbot, and agents) over custom data.\n",
      "\n",
      "In this high-level concepts guide, you will learn:\n",
      "* the retrieval augmented generation (RAG) paradigm for combining LLM with custom data,\n",
      "* key conce...\n"
     ]
    }
   ],
   "source": [
    "print(str(inv_hallucinated_question))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=256))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T16:09:26.684815200Z",
     "start_time": "2023-12-29T16:09:26.678416900Z"
    }
   },
   "id": "66615a15acce133b",
   "execution_count": 115
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "46a4fe2e1acd77a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we covered several key topics!\n",
    "\n",
    "- setting up a sub-question query engine\n",
    "- generating a dataset of evaluation questions\n",
    "- evaluating responses for hallucination\n",
    "- evaluating responses for answer quality"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88bfac52a93a88ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
