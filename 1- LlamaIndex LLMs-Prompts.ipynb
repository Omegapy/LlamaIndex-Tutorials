{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LlamaIndex LLMs-Prompts : LlamaIndex Intro. Tutorial\n",
    "Alejandro Ricciardi (Omegapy)  \n",
    "created date: 12/23/2023 \n",
    "GitHub: https://github.com/Omegapy\n",
    "\n",
    "Projects Description:\n",
    "Testing an LLM using the primary prompt templates used in LlamaIndex.\n",
    "LlamaIndex LLMs-Prompts tutorial base on LlamaIndex Bottoms-Up Development video series.\n",
    "\n",
    "- Initialization \n",
    "    - API Keys\n",
    "    - LLM Init.\n",
    "    - Load File\n",
    "\n",
    "- Templates\n",
    "    - Context\n",
    "    - Refined Context - More Context\n",
    "\n",
    "- Chat\n",
    "    - Simulate a ChatBot that can answer questions about llama-index.\n",
    "\n",
    "credit: LlamaIndex https://www.youtube.com/watch?v=p0jcvGiBKSA&t=201s"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcef53ed2feeb497"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### API Keys\n",
    "This project you require API keys from: OpenAI: https://openai.com/ "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b25039ec7017c7c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables API Keys\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv()) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T21:47:33.481111200Z",
     "start_time": "2023-12-28T21:47:33.461302900Z"
    }
   },
   "id": "90c5df019088e481",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LlmaIndex \n",
    "llm initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b3560e5cdd9cc3"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f68644c659fcd9cb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T22:03:48.313037400Z",
     "start_time": "2023-12-28T22:03:46.528455400Z"
    }
   },
   "id": "ac97eb8575e86bb2",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Document"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69d216f8ce96f8ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"docs/getting_started/starter_example.md\", \"r\") as f:\n",
    "    text = f.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T15:30:35.258283100Z",
     "start_time": "2023-12-26T15:30:35.255143600Z"
    }
   },
   "id": "a9a74d1a2f31d8f0",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Templates \n",
    "Usage Pattern - Defining a custom prompt: \n",
    "https://docs.llamaindex.ai/en/stable/module_guides/models/prompts/usage_pattern.html#template-variable-mappings\n",
    "\n",
    "`text_qa_template` -> initial answers. (context)\n",
    "`refine_template`  -> refining an existing answer when all the text does not fit into one LLM call. (more context)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cd44b13c03a1e3a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'version_short' from 'pydantic.version' (C:\\Users\\User\\anaconda3\\Lib\\site-packages\\pydantic\\version.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Prompt\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\__init__.py:13\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Callable, Optional\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# import global eval handler\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mglobal_handlers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m set_global_handler\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_structs\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstruct_type\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m IndexStructType\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# embeddings\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\callbacks\\__init__.py:7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mopen_inference_callback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenInferenceCallbackHandler\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mschema\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CBEvent, CBEventType, EventPayload\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtoken_counting\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TokenCountingHandler\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m trace_method\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwandb_callback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WandbCallbackHandler\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\callbacks\\token_counting.py:6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_handler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseCallbackHandler\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mschema\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CBEventType, EventPayload\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutilities\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtoken_counting\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TokenCounter\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_tokenizer\n\u001B[0;32m     10\u001B[0m \u001B[38;5;129m@dataclass\u001B[39m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mTokenCountingEvent\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\utilities\\token_counting.py:6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Modified from:\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# https://github.com/nyno-ai/openai-token-counter\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Callable, Dict, List, Optional\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChatMessage, MessageRole\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_tokenizer\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mTokenCounter\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\llms\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mai21\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AI21\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01manthropic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Anthropic\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01manyscale\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Anyscale\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mazure_openai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AzureOpenAI\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbedrock\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Bedrock\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\llms\\anyscale.py:9\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01manyscale_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      6\u001B[0m     anyscale_modelname_to_contextsize,\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeneric_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_from_param_or_env\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mopenai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenAI\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChatMessage, LLMMetadata\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mllama_index\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseOutputParser, PydanticProgramMode\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\llms\\openai.py:16\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mhttpx\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtiktoken\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopenai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AsyncOpenAI\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopenai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenAI \u001B[38;5;28;01mas\u001B[39;00m SyncOpenAI\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopenai\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchat_completion_chunk\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     19\u001B[0m     ChatCompletionChunk,\n\u001B[0;32m     20\u001B[0m     ChoiceDelta,\n\u001B[0;32m     21\u001B[0m     ChoiceDeltaToolCall,\n\u001B[0;32m     22\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\__init__.py:8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_os\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m override\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m types\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_types\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NoneType, Transport, ProxiesTypes\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m file_from_path\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\types\\__init__.py:5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# File generated from our OpenAPI spec by Stainless.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m annotations\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01medit\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Edit \u001B[38;5;28;01mas\u001B[39;00m Edit\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image \u001B[38;5;28;01mas\u001B[39;00m Image\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model \u001B[38;5;28;01mas\u001B[39;00m Model\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\types\\edit.py:6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_models\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseModel\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompletion_usage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CompletionUsage\n\u001B[0;32m      9\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEdit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChoice\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_models.py:19\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      7\u001B[0m     Unpack,\n\u001B[0;32m      8\u001B[0m     Literal,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m     runtime_checkable,\n\u001B[0;32m     16\u001B[0m )\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgenerics\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfields\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FieldInfo\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_types\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     23\u001B[0m     Body,\n\u001B[0;32m     24\u001B[0m     IncEx,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     31\u001B[0m     HttpxRequestFiles,\n\u001B[0;32m     32\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\generics.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"The `generics` module is a backport module from V1.\"\"\"\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_migration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m getattr_migration\n\u001B[0;32m      4\u001B[0m \u001B[38;5;21m__getattr__\u001B[39m \u001B[38;5;241m=\u001B[39m getattr_migration(\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\_migration.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Callable, Dict\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m version_short\n\u001B[0;32m      6\u001B[0m MOVED_IN_V2 \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.utils:version_info\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.version:version_info\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.error_wrappers:ValidationError\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic:ValidationError\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.generics:GenericModel\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.BaseModel\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     14\u001B[0m }\n\u001B[0;32m     16\u001B[0m DEPRECATED_MOVED_IN_V2 \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.tools:schema_of\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.deprecated.tools:schema_of\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.tools:parse_obj_as\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.deprecated.tools:parse_obj_as\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.config:Extra\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpydantic.deprecated.config:Extra\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     29\u001B[0m }\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'version_short' from 'pydantic.version' (C:\\Users\\User\\anaconda3\\Lib\\site-packages\\pydantic\\version.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "from llama_index import Prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T21:42:25.569116100Z",
     "start_time": "2023-12-28T21:42:25.425727500Z"
    }
   },
   "id": "30929fb2af1cbab3",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text_qa_template = Prompt(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "refine_template = Prompt(\n",
    "    \"We have the opportunity to refine the original answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better \"\n",
    "    \"answer the question: {query_str}. \"\n",
    "    \"If the context isn't useful, output the original answer again.\\n\"\n",
    "    \"Original Answer: {existing_answer}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T15:19:26.065991900Z",
     "start_time": "2023-12-26T15:19:26.056973100Z"
    }
   },
   "id": "db7c61cb65541a1",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Context\n",
    "Prompted Questions - AI Generated answers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "950ef05880e604b5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install LlamaIndex, you need to follow the installation steps provided in the \"installation.md\" file.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I install llama-index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T15:31:35.731927300Z",
     "start_time": "2023-12-26T15:31:33.905461Z"
    }
   },
   "id": "cf9a31a275843f7d",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an index using LlamaIndex, you need to follow these steps:\n",
      "\n",
      "1. Download the LlamaIndex repository by cloning it from GitHub using the command: `$ git clone https://github.com/jerryjliu/llama_index.git`\n",
      "\n",
      "2. Navigate to the downloaded repository using the command: `$ cd llama_index`\n",
      "\n",
      "3. Go to the `examples/paul_graham_essay` folder using the command: `$ cd examples/paul_graham_essay`\n",
      "\n",
      "4. Create a new Python file and import the necessary classes from LlamaIndex:\n",
      "\n",
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "```\n",
      "\n",
      "5. Load the documents that you want to index using the `SimpleDirectoryReader` class:\n",
      "\n",
      "```python\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "```\n",
      "\n",
      "6. Build the index using the `VectorStoreIndex` class:\n",
      "\n",
      "```python\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "```\n",
      "\n",
      "Now you have created an index over the documents in the specified folder.\n",
      "\n",
      "Note: The example assumes that the documents are stored in the `data` folder. You can modify the code accordingly to point to your desired folder.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T15:32:26.607104100Z",
     "start_time": "2023-12-26T15:32:16.795565200Z"
    }
   },
   "id": "4e3e4617750da9c",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "```"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response_gen = llm.stream_complete(prompt)\n",
    "for response in response_gen:\n",
    "    print(response.delta, end=\"\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T15:32:50.921277800Z",
     "start_time": "2023-12-26T15:32:48.837606900Z"
    }
   },
   "id": "f431bb638932ce06",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More Context - Refine Template "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f6fb866ffafee9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an index using LlamaIndex, you can follow these steps:\n",
      "\n",
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "# Step 1: Load the documents\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "\n",
      "# Step 2: Build the index\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "\n",
      "# Step 3: Persist the index to disk\n",
      "index.storage_context.persist()\n",
      "\n",
      "# Step 4: Reload the index from disk\n",
      "from llama_index import StorageContext, load_index_from_storage\n",
      "\n",
      "# rebuild storage context\n",
      "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
      "# load index\n",
      "index = load_index_from_storage(storage_context)\n",
      "```\n",
      "\n",
      "Make sure you have already installed LlamaIndex and have the necessary dependencies.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "existing_answer = \"\"\"To create an index using LlamaIndex, you need to follow these steps:\n",
    "\n",
    "1. Download the LlamaIndex repository by cloning it from GitHub.\n",
    "2. Navigate to the `examples/paul_graham_essay` folder in the cloned repository.\n",
    "3. Create a new Python file and import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.\n",
    "4. Load the documents from the `data` folder using `SimpleDirectoryReader('data').load_data()`.\n",
    "5. Build the index using `VectorStoreIndex.from_documents(documents)`.\n",
    "6. To persist the index to disk, use `index.storage_context.persist()`.\n",
    "7. To reload the index from disk, use the `StorageContext` and `load_index_from_storage` functions.\n",
    "\n",
    "Note: This answer assumes that you have already installed LlamaIndex and have the necessary dependencies.\"\"\"\n",
    "prompt = refine_template.format(context_msg=text, query_str=question, existing_answer=existing_answer)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T15:35:55.046036200Z",
     "start_time": "2023-12-26T15:35:48.817904100Z"
    }
   },
   "id": "d16ab95f313c24d2",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat\n",
    " The LLM also has a `chat` method that takes in a list of messages, to simulate a chat session."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d3a33942b493a29"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "27eef851b80db8b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.llms import ChatMessage"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T16:01:01.310468500Z",
     "start_time": "2023-12-26T16:01:01.305456400Z"
    }
   },
   "id": "1023edd47c481653",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: To create an index, you can follow these general steps:\n",
      "\n",
      "1. Determine the purpose and scope of your index: Decide what information you want to include in your index and what it will be used for. This will help you define the scope and structure of your index.\n",
      "\n",
      "2. Identify the items to be indexed: Determine the specific items or topics that you want to include in your index. For example, if you are creating an index for a book, you might want to index chapters, sections, and important concepts.\n",
      "\n",
      "3. Create a list of index terms: Compile a list of terms or keywords that represent the items you identified in the previous step. These terms should be concise and specific to help users find the information they need.\n",
      "\n",
      "4. Organize the index terms: Group the index terms into logical categories or sections. This will help users navigate the index more easily and find relevant information faster.\n",
      "\n",
      "5. Assign page numbers or locations: For each index term, determine the page number or location where the term can be found. This is typically done by scanning the document or source material and noting the relevant page numbers or locations.\n",
      "\n",
      "6. Format and design the index: Decide on the format and design of your index. You can use software tools like Microsoft Word or dedicated indexing software to create a professional-looking index. Consider factors like font size, layout, and any additional formatting elements.\n",
      "\n",
      "7. Proofread and revise: Review your index for accuracy, consistency, and completeness. Make sure all index terms are correctly assigned to the appropriate page numbers or locations.\n",
      "\n",
      "8. Include the index in your document: Once you are satisfied with your index, insert it into your document or publication. Ensure that it is placed in a prominent location, such as at the end of a book or at the beginning of a report.\n",
      "\n",
      "Remember, the specific steps may vary depending on the type of index you are creating and the tools you are using.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful QA chatbot that can answer questions about llama-index.\"),\n",
    "    ChatMessage(role=\"user\", content=\"How do I create an index?\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(chat_history)\n",
    "print(response.message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T16:03:51.300615100Z",
     "start_time": "2023-12-26T16:03:35.255566800Z"
    }
   },
   "id": "2b857f57c97eb73e",
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
