{"docstore/data": {"d9281baa-c7fb-427b-b9a6-2871fde16930": {"__data__": {"id_": "d9281baa-c7fb-427b-b9a6-2871fde16930", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b", "node_type": "1", "metadata": {}, "hash": "4520438bbb63add46db8092c10706b2a7f98bbecf23496be1d3866be12bf1b92", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "caa3d881-2b6a-4a21-897c-9040f14abbd0", "node_type": "1", "metadata": {}, "hash": "74ac7475481c990ffe046c550864909cb409d022b732827a4c9dcf2efedc567b", "class_name": "RelatedNodeInfo"}, {"node_id": "9a6b808b-c800-4276-bb45-e411144750c8", "node_type": "1", "metadata": {}, "hash": "385546f8839bcd57982e313be5ef2559532a3a74a5be12eedc313d3125cd74de", "class_name": "RelatedNodeInfo"}, {"node_id": "c0b79f38-f54d-49a8-b5a3-5827fb3d638b", "node_type": "1", "metadata": {}, "hash": "862f7ca88c65c3a4572e372531f6d618ee9de4f208912d303ddbde4838815099", "class_name": "RelatedNodeInfo"}, {"node_id": "129e80a0-343f-4e02-ad24-7da18d5945c5", "node_type": "1", "metadata": {}, "hash": "ecb1412810d8511072d1ea14642472dd639349713592c4c2b6af2eaa5a4db3ce", "class_name": "RelatedNodeInfo"}]}, "hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "text": "File Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere is a sample of some of the incredible applications and tools built on top of LlamaIndex!\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Meru - Dense Data Retrieval API\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHosted API service. Includes a \"Dense Data Retrieval\" API built on top of LlamaIndex where users can upload their documents and query them.\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Algovera\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBuild AI workflows using building blocks. Many workflows built on top of LlamaIndex.\n\n[Website].\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/ChatGPT LlamaIndex\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nInterface that allows users to upload long docs and chat with the bot.\n[Tweet thread]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/AgentHQ\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA web tool to build agents, interacting with LlamaIndex data structures.[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/PapersGPT\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFeed any of the following content into GPT to give it deep customized knowledge:\n- Scientific Papers\n- Substack Articles\n- Podcasts\n- Github Repos\nand more.\n\n[Tweet thread]\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/VideoQues + DocsQues\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**VideoQues**: A tool that answers your queries on YouTube videos. \n[LinkedIn post here].\n\n**DocsQues**: A tool that answers your questions on longer documents (including .pdfs!)\n[LinkedIn post here].\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/PaperBrain\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA platform to access/understand research papers.\n\n[Tweet thread].\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/CACTUS\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nContextual search on top of LinkedIn search results. \n[LinkedIn post here].\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Personal Note Chatbot\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA chatbot that can answer questions over a directory of Obsidian notes. \n[Tweet thread].\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/RHOBH AMA\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAsk questions about the Real Housewives of Beverly Hills.\n[Tweet thread]\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Mynd\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA journaling app that uses AI to uncover insights and patterns over time.", "start_char_idx": 0, "end_char_idx": 4865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "115b0331-8aa7-44bf-a5c5-2fbc49584b6b": {"__data__": {"id_": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9281baa-c7fb-427b-b9a6-2871fde16930", "node_type": "1", "metadata": {}, "hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236", "node_type": "1", "metadata": {}, "hash": "ef9c76e6c0c10e120aa1805a956c3532e019b02b7a02de48252084024805b4ba", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "60b5bfaf-706e-42d7-a723-9df702496b39", "node_type": "1", "metadata": {}, "hash": "9ec5c9e0e5fe3be8845573ca4c79c3390b03cc2020700b23a225c4bc26d32b92", "class_name": "RelatedNodeInfo"}, {"node_id": "85cd9d1c-f2ed-4826-98e8-8f352c020431", "node_type": "1", "metadata": {}, "hash": "c0821ea1a5be7ff15030d1d1ad49a477e91ce5add07577d7bbfae6a1760d4ced", "class_name": "RelatedNodeInfo"}, {"node_id": "5fa49686-6f5f-4a1a-89b0-93ffe41006a4", "node_type": "1", "metadata": {}, "hash": "6676bbcc56ba85b210a3a8fe824e508ad4e2262447f8538c665a6b93f2a6b2bf", "class_name": "RelatedNodeInfo"}]}, "hash": "4520438bbb63add46db8092c10706b2a7f98bbecf23496be1d3866be12bf1b92", "text": "[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/CoFounder\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe First AI Co-Founder for Your Start-up \ud83d\ude4c\n\nCoFounder is a platform to revolutionize the start-up ecosystem by providing founders with unparalleled tools, resources, and support. We are changing how founders build their companies from 0-1\u2014productizing the accelerator/incubator programs using AI.\n\nCurrent features:\n\n* AI Investor Matching and Introduction and Tracking\n* AI Pitch Deck creation\n* Real-time Pitch Deck practice/feedback\n* Automatic Competitive Analysis / Watchlist\n* More coming soon...\n\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Al-X by OpenExO\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYour Digital Transformation Co-Pilot\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/AnySummary\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSummarize any document, audio or video with AI\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Blackmaria\nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nPython package for webscraping in Natural language.\n[Tweet thread]\n[Github]\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**NOTE**: This is a work-in-progress, stay tuned for more exciting updates on this front!\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe OpenAI ChatGPT Retrieval Plugin\noffers a centralized API specification for any document storage system to interact \nwith ChatGPT. Since this can be deployed on any service, this means that more and more\ndocument retrieval services will implement this spec; this allows them to not only\ninteract with ChatGPT, but also interact with any LLM toolkit that may use \na retrieval service.\n\nLlamaIndex provides a variety of integrations with the ChatGPT Retrieval Plugin.\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/Loading Data from LlamaHub into the ChatGPT Retrieval Plugin\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe ChatGPT Retrieval Plugin defines an `/upsert` endpoint for users to load\ndocuments. This offers a natural integration point with LlamaHub, which offers\nover 65 data loaders from various API's and document formats.", "start_char_idx": 4866, "end_char_idx": 8676, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "193c1e07-03ac-45f6-bd70-5bb0b54f1236": {"__data__": {"id_": "193c1e07-03ac-45f6-bd70-5bb0b54f1236", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b", "node_type": "1", "metadata": {}, "hash": "4520438bbb63add46db8092c10706b2a7f98bbecf23496be1d3866be12bf1b92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2495892e-f217-42d3-b47c-f13cc073d7ac", "node_type": "1", "metadata": {}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0ed32ab1-3669-4106-ac1d-2f66c3bdb29d", "node_type": "1", "metadata": {}, "hash": "44b50cb9c25d6163f3ff7d64eb124f97295ae175c296072257efed15974ca1f3", "class_name": "RelatedNodeInfo"}, {"node_id": "b30ebacc-9abf-42ad-9f40-9e5e88833711", "node_type": "1", "metadata": {}, "hash": "35f9c836cad41909017022efaf9e93d1cb5d76157b6b708618ee0188e8e6b536", "class_name": "RelatedNodeInfo"}, {"node_id": "6910eea8-072f-42ba-8c11-43f5781d409e", "node_type": "1", "metadata": {}, "hash": "6ae082112a9a3bce10ab77503d8dcdb7ef3eb8fd7b6f0940a368d54fccc94f55", "class_name": "RelatedNodeInfo"}]}, "hash": "ef9c76e6c0c10e120aa1805a956c3532e019b02b7a02de48252084024805b4ba", "text": "Here is a sample code snippet of showing how to load a document from LlamaHub\ninto the JSON format that `/upsert` expects:\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/Loading Data from LlamaHub into the ChatGPT Retrieval Plugin\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import download_loader, Document\nfrom typing import Dict, List\nimport json\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/download loader, load documents\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\nloader = SimpleWebPageReader(html_to_text=True)\nurl = \"http://www.paulgraham.com/worked.html\"\ndocuments = loader.load_data(urls=[url])\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/Convert LlamaIndex Documents to JSON format\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndef dump_docs_to_json(documents: List[Document], out_path: str) -> Dict:\n    \"\"\"Convert LlamaIndex Documents to JSON format and save it.\"\"\"\n    result_json = []\n    for doc in documents:\n        cur_dict = {\n            \"text\": doc.get_text(),\n            \"id\": doc.get_doc_id(),\n            # NOTE: feel free to customize the other fields as you wish\n            # fields taken from https://github.com/openai/chatgpt-retrieval-plugin/tree/main/scripts/process_json#usage\n            # \"source\": ...,\n            # \"source_id\": ...,\n            # \"url\": url,\n            # \"created_at\": ...,\n            # \"author\": \"Paul Graham\",\n        }\n        result_json.append(cur_dict)\n    \n    json.dump(result_json, open(out_path, 'w'))\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/Convert LlamaIndex Documents to JSON format\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor more details, check out the full example notebook.\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/ChatGPT Retrieval Plugin Data Loader\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe ChatGPT Retrieval Plugin data loader can be accessed on LlamaHub.\n\nIt allows you to easily load data from any docstore that implements the plugin API, into a LlamaIndex data structure.\n\nExample code:\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/ChatGPT Retrieval Plugin Data Loader\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.readers import ChatGPTRetrievalPluginReader\nimport os\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/load documents\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nbearer_token = os.getenv(\"BEARER_TOKEN\")\nreader = ChatGPTRetrievalPluginReader(\n    endpoint_url=\"http://localhost:8000\",\n    bearer_token=bearer_token\n)\ndocuments = reader.load_data(\"What did the author do growing up?\")", "start_char_idx": 8678, "end_char_idx": 13258, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2495892e-f217-42d3-b47c-f13cc073d7ac": {"__data__": {"id_": "2495892e-f217-42d3-b47c-f13cc073d7ac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236", "node_type": "1", "metadata": {}, "hash": "ef9c76e6c0c10e120aa1805a956c3532e019b02b7a02de48252084024805b4ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8877944a-5895-49cd-8397-2777043dd11b", "node_type": "1", "metadata": {}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b784149f-4d79-4495-baf7-04894159cac1", "node_type": "1", "metadata": {}, "hash": "03391ff7326c6a47d68f6b0c6235851852408757c5d83ec827d09ca2ef60e4b7", "class_name": "RelatedNodeInfo"}, {"node_id": "b3527ee3-d586-45cf-ac35-6ed9dae27802", "node_type": "1", "metadata": {}, "hash": "5e99c994f1c48176472a14344b48a5c517ea0234284b62c9a8d298414e2cff33", "class_name": "RelatedNodeInfo"}, {"node_id": "e9ad55ed-47aa-4c1b-b760-c898291c8c33", "node_type": "1", "metadata": {}, "hash": "783ddefd6c786fb0f1f3cfa759ab5414439ee105ae04b2917ab9021c2aaec554", "class_name": "RelatedNodeInfo"}, {"node_id": "037a136f-42d9-4099-8139-a859c61c9690", "node_type": "1", "metadata": {}, "hash": "555ee5bc897781941f99b545d7d3ed76c4148d6261a238cdcc2a746c7c772454", "class_name": "RelatedNodeInfo"}]}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "text": "File Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/build and query index\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ListIndex\nindex = ListIndex(documents)\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/set Logging to DEBUG for more detailed outputs\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine(\n    response_mode=\"compact\"\n)\nresponse = query_engine.query(\n    \"Summarize the retrieved content and describe what the author did growing up\",\n)\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/set Logging to DEBUG for more detailed outputs\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor more details, check out the full example notebook.\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/ChatGPT Retrieval Plugin Index\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe ChatGPT Retrieval Plugin Index allows you to easily build a vector index over any documents, with storage backed by a document store implementing the \nChatGPT endpoint.\n\nNote: this index is a vector index, allowing top-k retrieval.\n\nExample code:\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/ChatGPT Retrieval Plugin Index\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.vector_store import ChatGPTRetrievalPluginIndex\nfrom llama_index import SimpleDirectoryReader\nimport os\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/load documents\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/build index\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nbearer_token = os.getenv(\"BEARER_TOKEN\")\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/initialize without metadata filter\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = ChatGPTRetrievalPluginIndex(\n    documents, \n    endpoint_url=\"http://localhost:8000\",\n    bearer_token=bearer_token,\n)\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/query index\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine(\n    similarity_top_k=3,\n    response_mode=\"compact\",\n)\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/query index\nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor more details, check out the full example notebook.", "start_char_idx": 13260, "end_char_idx": 18410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8877944a-5895-49cd-8397-2777043dd11b": {"__data__": {"id_": "8877944a-5895-49cd-8397-2777043dd11b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2495892e-f217-42d3-b47c-f13cc073d7ac", "node_type": "1", "metadata": {}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634", "node_type": "1", "metadata": {}, "hash": "c219a0cf1ce7ea6fb6019c24f8f6ab08129b4f50db9a4c46f7828629906eeff3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "c58a5832-c6e1-4660-bb8b-0b305cf97f2c", "node_type": "1", "metadata": {}, "hash": "84d5bf069d69d7757dd6e3296db165f4e5d02d11325fc891ba01ff366e7dc709", "class_name": "RelatedNodeInfo"}, {"node_id": "84c56aae-2e36-40d8-949c-d689fdcd64f0", "node_type": "1", "metadata": {}, "hash": "9da881b544e6c4afa4a9a46baf05da1cbf41557ffb592006034474e8ce81718b", "class_name": "RelatedNodeInfo"}, {"node_id": "e6c79f20-6e57-4dbc-922a-e3074073104f", "node_type": "1", "metadata": {}, "hash": "b0171cc53f83dbca35c5013ec2c3799f6c482ad822bc310936588d975c517cbd", "class_name": "RelatedNodeInfo"}, {"node_id": "2cb5bde9-973e-4fd5-a8cd-b012b5e49ed4", "node_type": "1", "metadata": {}, "hash": "6a00c494ef4dec7389d5d6188c9e86e88187bae9aafd47bbcd073d7a3b9ac493", "class_name": "RelatedNodeInfo"}]}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "text": "File Name: Docs\\community\\integrations\\graph_stores.md\nContent Type: text\nHeader Path: Using Graph Stores/`NebulaGraphStore`\nLinks: \nfile_path: Docs\\community\\integrations\\graph_stores.md\nfile_name: graph_stores.md\nfile_type: None\nfile_size: 541\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support a `NebulaGraphStore` integration, for persisting graphs directly in Nebula! Furthermore, you can generate cypher queries and return natural language responses for your Nebula graphs using the `KnowledgeGraphQueryEngine`.\n\nSee the associated guides below:\n\nFile Name: Docs\\community\\integrations\\graph_stores.md\nContent Type: text\nHeader Path: Using Graph Stores/`NebulaGraphStore`\nLinks: \nfile_path: Docs\\community\\integrations\\graph_stores.md\nfile_name: graph_stores.md\nfile_type: None\nfile_size: 541\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nNebula Graph Store </examples/index_structs/knowledge_graph/NebulaGraphKGIndexDemo.ipynb>\nKnowledge Graph Query Engine </examples/query_engine/knowledge_graph_query_engine.ipynb>\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGraphsignal provides observability for AI agents and LLM-powered applications. It helps developers ensure AI applications run as expected and users have the best experience.\n\nGraphsignal **automatically** traces and monitors LlamaIndex. Traces and metrics provide execution details for query, retrieval, and index operations. These insights include **prompts**, **completions**, **embedding statistics**, **retrieved nodes**, **parameters**, **latency**, and **exceptions**.\n\nWhen OpenAI APIs are used, Graphsignal provides additional insights such as **token counts** and **costs** per deployment, model or any context.\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Installation and Setup\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAdding Graphsignal tracer is simple, just install and configure it:\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Installation and Setup\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\npip install graphsignal\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: code\nHeader Path: Tracing with Graphsignal/Provide an API key directly or via GRAPHSIGNAL_API_KEY environment variable\nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nimport graphsignal\n\ngraphsignal.configure(api_key='my-api-key', deployment='my-llama-index-app-prod')\n```\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Provide an API key directly or via GRAPHSIGNAL_API_KEY environment variable\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can get an API key here.\n\nSee the Quick Start guide, Integration guide, and an example app for more information.\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Tracing Other Functions\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo additionally trace any function or code, you can use a decorator or a context manager:\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Tracing Other Functions\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwith graphsignal.start_trace('load-external-data'):\n    reader.load_data()\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Tracing Other Functions\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee Python API Reference for complete instructions.", "start_char_idx": 18412, "end_char_idx": 23540, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f95c61d-1036-4ace-93d4-24d8d50f6634": {"__data__": {"id_": "5f95c61d-1036-4ace-93d4-24d8d50f6634", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8877944a-5895-49cd-8397-2777043dd11b", "node_type": "1", "metadata": {}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "node_type": "1", "metadata": {}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b38d046d-ae64-4c9b-8144-06d6834a1819", "node_type": "1", "metadata": {}, "hash": "c7eb3dc0b8ff4c1bea48efce4f34e9dd3169fe54566e8e46aaed115cc383ed9e", "class_name": "RelatedNodeInfo"}, {"node_id": "a1b126a3-f651-4ec8-8868-9bed2e78f7b1", "node_type": "1", "metadata": {}, "hash": "16dc86e3511ab3361a1aa7f2b1e94c6ca292ceaf5c59ae716ccfb92fbd14427b", "class_name": "RelatedNodeInfo"}, {"node_id": "63f07a12-74d3-4d22-ae66-9d3d5ab4399e", "node_type": "1", "metadata": {}, "hash": "55ec3913793bef189844a6d5a771c99af8cf8b8e83bec4eb0ab57e9e6dbdd9a5", "class_name": "RelatedNodeInfo"}]}, "hash": "c219a0cf1ce7ea6fb6019c24f8f6ab08129b4f50db9a4c46f7828629906eeff3", "text": "File Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Useful Links\nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* Tracing and Monitoring LlamaIndex Applications\n* Monitor OpenAI API Latency, Tokens, Rate Limits, and More\n* OpenAI API Cost Tracking: Analyzing Expenses by Model, Deployment, and Context\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGuidance is a guidance language for controlling large language models developed by Microsoft.\n\nGuidance programs allow you to interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOne particularly exciting aspect of guidance is the ability to output structured objects (think JSON following a specific schema, or a pydantic object). Instead of just \"suggesting\" the desired output structure to the LLM, guidance can actually \"force\" the LLM output to follow the desired schema. This allows the LLM to focus on the content rather than the syntax, and completely eliminate the possibility of output parsing issues.\n\nThis is particularly powerful for weaker LLMs which be smaller in parameter count, and not trained on sufficient source code data to be able to reliably produce well-formed, hierarchical structured output.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn LlamaIndex, we provide an initial integration with guidance, to make it super easy for generating structured output (more specifically pydantic objects).\n\nFor example, if we want to generate an album of songs, with the following schema:\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nclass Song(BaseModel):\n    title: str\n    length_seconds: int\n    \nclass Album(BaseModel):\n    name: str\n    artist: str\n    songs: List[Song]\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIt's as simple as creating a `GuidancePydanticProgram`, specifying our desired pydantic class `Album`, \nand supplying a suitable prompt template.\n\n> Note: guidance uses handlebars-style templates, which uses double braces for variable substitution, and single braces for literal braces. This is the opposite convention of Python format strings. \n\n> Note: We provide an utility function `from llama_index.prompts.guidance_utils import convert_to_handlebars` that can convert from the Python format string style template to guidance handlebars-style template.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprogram = GuidancePydanticProgram(\n    output_cls=Album, \n    prompt_template_str=\"Generate an example album, with an artist and a list of songs. Using the movie {{movie_name}} as inspiration\",\n    guidance_llm=OpenAI('text-davinci-003'),\n    verbose=True,\n)\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow we can run the program by calling it with additional user input. \nHere let's go for something spooky and create an album inspired by the Shining.", "start_char_idx": 23542, "end_char_idx": 28782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42cb046b-75bc-4943-b5f6-5b179a24a6d9": {"__data__": {"id_": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634", "node_type": "1", "metadata": {}, "hash": "c219a0cf1ce7ea6fb6019c24f8f6ab08129b4f50db9a4c46f7828629906eeff3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "node_type": "1", "metadata": {}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f8284800-a627-47ba-a595-2bf621e5bfcc", "node_type": "1", "metadata": {}, "hash": "dabff93b9974353d35367350c249525c653c74d731a2a10e886725addc7f780a", "class_name": "RelatedNodeInfo"}, {"node_id": "d676ddeb-3565-4c6a-985b-c1161668436c", "node_type": "1", "metadata": {}, "hash": "209904f1692154051c100b4049308b5c191b4ac3f8391cf6d218a0a016c61b72", "class_name": "RelatedNodeInfo"}, {"node_id": "4077c693-e2f5-4093-ac5a-85f8435a7afc", "node_type": "1", "metadata": {}, "hash": "69f4124539afdbe1eca9ede5a912589325fa6c4005a9412433e1f9896856b485", "class_name": "RelatedNodeInfo"}, {"node_id": "ff83f3c5-dab5-4b42-9742-1341cd52b835", "node_type": "1", "metadata": {}, "hash": "478aa8a2805f16cc6c6d04051b73f315293aae6b956e3f03a0e4883bb87f8236", "class_name": "RelatedNodeInfo"}]}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "text": "Here let's go for something spooky and create an album inspired by the Shining.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\noutput = program(movie_name='The Shining')\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe have our pydantic object:\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAlbum(name='The Shining', artist='Jack Torrance', songs=[Song(title='All Work and No Play', length_seconds=180), Song(title='The Overlook Hotel', length_seconds=240), Song(title='The Shining', length_seconds=210)])\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can play with this notebook for more details.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Using guidance to improve the robustness of our sub-question query engine.\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides a toolkit of advanced query engines for tackling different use-cases.\nSeveral relies on structured output in intermediate steps.\nWe can use guidance to improve the robustness of these query engines, by making sure the\nintermediate response has the expected structure (so that they can be parsed correctly to a structured object).\n\nAs an example, we implement a `GuidanceQuestionGenerator` that can be plugged into a `SubQuestionQueryEngine` to make it more robust than using the default setting.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Using guidance to improve the robustness of our sub-question query engine.\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.question_gen.guidance_generator import GuidanceQuestionGenerator\nfrom guidance.llms import OpenAI as GuidanceOpenAI\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/define guidance based question generator\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquestion_gen = GuidanceQuestionGenerator.from_defaults(guidance_llm=GuidanceOpenAI('text-davinci-003'), verbose=False)\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/define query engine tools\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine_tools = ...\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/construct sub-question query engine\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ns_engine = SubQuestionQueryEngine.from_defaults(\n    question_gen=question_gen  # use guidance based question_gen defined above\n    query_engine_tools=query_engine_tools, \n)\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/construct sub-question query engine\nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee this notebook for more details.\n\nFile Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens\nLinks: \nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis page covers how to use TruLens to evaluate and track LLM apps built on Llama-Index.", "start_char_idx": 28703, "end_char_idx": 34181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c986777-e62e-473a-92dc-acdb96c9c9d2": {"__data__": {"id_": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "node_type": "1", "metadata": {}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10810e34-526c-413d-98e3-255cac0f8ee1", "node_type": "1", "metadata": {}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d8210398-936d-4da0-afaa-cafbda9d4703", "node_type": "1", "metadata": {}, "hash": "ea0702b0772383062e345b5a8ac82c92e944a0c12fb15148ac644c2ecc055a56", "class_name": "RelatedNodeInfo"}, {"node_id": "8aad2f35-e698-41fa-87ed-7173b63d324d", "node_type": "1", "metadata": {}, "hash": "9d82e3b9b8509be9bd173a82634199b269e2b67ddf26abd596dfa190bb76d086", "class_name": "RelatedNodeInfo"}, {"node_id": "29f19384-7403-42eb-b681-19b889439afb", "node_type": "1", "metadata": {}, "hash": "ea3727cffc782d6ab1f454e1740abddc701d3b2520184b9bf18b5f6597c30a45", "class_name": "RelatedNodeInfo"}, {"node_id": "5302d647-66eb-4181-83fb-4b6b2a0f3ffd", "node_type": "1", "metadata": {}, "hash": "60ae2fcfacb8a40b02946a7d4e114ab0e8caae9b73246fd17f912a7c52dc1327", "class_name": "RelatedNodeInfo"}]}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "text": "File Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?\nLinks: \nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTruLens is an opensource package that provides instrumentation and evaluation tools for large language model (LLM) based applications. This includes feedback function evaluations of relevance, sentiment and more, plus in-depth tracing including cost and latency.\n\n!TruLens Architecture\n\nAs you iterate on new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up. You'll also be able to view evaluations at a record level, and explore the app metadata for each record.\n\nFile Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?/Installation and Setup\nLinks: \nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAdding TruLens is simple, just install it from pypi!\n\nFile Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?/Installation and Setup\nLinks: \nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\npip install trulens-eval\n\nFile Name: Docs\\community\\integrations\\trulens.md\nContent Type: code\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?/Installation and Setup\nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom trulens_eval import TruLlama\n\n```\n\nFile Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?/Try it out!\nLinks: (link_text: Open In Colab, link_url: fhttps://colab.research.google.com/github/truera/trulens/blob/google-colab/trulens_eval/examples/colab/quickstarts/llama_index_quickstart_colab.ipynb)\nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllama_index_quickstart.ipynb\n![Open In Colab](https://colab.research.google.com/github/truera/trulens/blob/google-colab/trulens_eval/examples/colab/quickstarts/llama_index_quickstart_colab.ipynb)\n\nFile Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?/Read more\nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* Build and Evaluate LLM Apps with LlamaIndex and TruLens\n\n* trulens.org\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides both Tool abstractions for a Langchain agent as well as a memory module.\n\nThe API reference of the Tool abstractions + memory modules are here.\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use any data loader as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex allows you to use any data loader within the LlamaIndex core repo or in LlamaHub as an \"on-demand\" data query Tool within a LangChain agent.\n\nThe Tool will 1) load data using the data loader, 2) index the data, and 3) query the data and return the response in an ad-hoc manner.\n\n**Resources**\n- OnDemandLoaderTool Tutorial\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides Tool abstractions so that you can use a LlamaIndex query engine along with a Langchain agent.", "start_char_idx": 34183, "end_char_idx": 39140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10810e34-526c-413d-98e3-255cac0f8ee1": {"__data__": {"id_": "10810e34-526c-413d-98e3-255cac0f8ee1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "node_type": "1", "metadata": {}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "node_type": "1", "metadata": {}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2f911b85-4221-47f6-a216-26ec753f3446", "node_type": "1", "metadata": {}, "hash": "1db0515f685e963440fac925e697ffefedce17c950798c649fa11fb98c411f92", "class_name": "RelatedNodeInfo"}, {"node_id": "436fd2be-6597-4d79-896d-58c726d0f4f0", "node_type": "1", "metadata": {}, "hash": "d480d39d5ef1c6bf263d11bfda71f9478f5bcc6de211998f99b14413cf6d14e8", "class_name": "RelatedNodeInfo"}, {"node_id": "4ed0193f-b2bd-4f01-9a63-53e5d3fba802", "node_type": "1", "metadata": {}, "hash": "67cedb04a562b962d95412a5c76ba82773ec4ce271c9935b7a0b81e6cdd130f8", "class_name": "RelatedNodeInfo"}, {"node_id": "18ed6626-69f0-4378-be4f-1280d22459c6", "node_type": "1", "metadata": {}, "hash": "0494715f4b948fddb927c9b18fba19b6c186761e2c2b996ebfe6175c2bfb431b", "class_name": "RelatedNodeInfo"}]}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "text": "For instance, you can choose to create a \"Tool\" from an `QueryEngine` directly as follows:\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool\n\ntool_config = IndexToolConfig(\n    query_engine=query_engine, \n    name=f\"Vector Index\",\n    description=f\"useful for when you want to answer queries about X\",\n    tool_kwargs={\"return_direct\": True}\n)\n\ntool = LlamaIndexTool.from_tool_config(tool_config)\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to provide a `LlamaToolkit`:\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntoolkit = LlamaToolkit(\n    index_configs=index_configs,\n)\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSuch a toolkit can be used to create a downstream Langchain-based chat agent through\nour `create_llama_agent` and `create_llama_chat_agent` commands:\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.langchain_helpers.agents import create_llama_chat_agent\n\nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n    verbose=True\n)\n\nagent_chain.run(input=\"Query about X\")\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can take a look at the full tutorial notebook here.\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Llama Demo Notebook: Tool + Memory module\nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe provide another demo notebook showing how you can build a chat agent with the following components.\n- Using LlamaIndex as a generic callable tool with a Langchain agent\n- Using LlamaIndex as a memory module; this allows you to insert arbitrary amounts of conversation history with a Langchain chatbot!\n\nPlease see the notebook here.\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex offers multiple integration points with vector stores / vector databases:\n\n1. LlamaIndex can use a vector store itself as an index. Like any other index, this index can store documents and be used to answer queries.\n2. LlamaIndex can load data from vector stores, similar to any other data connector. This data can then be used within LlamaIndex data structures.\n\n(vector-store-index)=\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Using a Vector Store as an Index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex also supports different vector stores\nas the storage backend for `VectorStoreIndex`.\n\n- Chroma (`ChromaVectorStore`) Installation\n- DeepLake (`DeepLakeVectorStore`) Installation\n- Qdrant (`QdrantVectorStore`) Installation Python Client\n- Weaviate (`WeaviateVectorStore`). Installation. Python Client.\n- Pinecone (`PineconeVectorStore`). Installation/Quickstart.\n- Faiss (`FaissVectorStore`). Installation.", "start_char_idx": 39143, "end_char_idx": 44623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa": {"__data__": {"id_": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10810e34-526c-413d-98e3-255cac0f8ee1", "node_type": "1", "metadata": {}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "node_type": "1", "metadata": {}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "45e3ce5a-8dbe-4e8a-a391-cc2c29bd9f96", "node_type": "1", "metadata": {}, "hash": "d1e15cbcc7cdb74330f528818cec49772b771c22c2fdf70028d0096cb17716e8", "class_name": "RelatedNodeInfo"}, {"node_id": "b84b4357-5200-409c-bf2d-0be1fab0441f", "node_type": "1", "metadata": {}, "hash": "bb521dcd0e7d7a6bcbe8e0480877537c2fed32d07d2fc63cf25f2bda6f2f6a03", "class_name": "RelatedNodeInfo"}, {"node_id": "e84b17a5-8c32-44dc-8fe0-fdb1ad963e08", "node_type": "1", "metadata": {}, "hash": "8654b763b7d9d11aad3c1cf4eef968c822082fe350af0fd6c147e9b6c8e67c7f", "class_name": "RelatedNodeInfo"}, {"node_id": "37dc6775-88ee-4f77-9ee4-e4932ea9bdec", "node_type": "1", "metadata": {}, "hash": "a4ef6e7fd00288e9fdc3848af49cdd18638687d46d761eb8ec487e15ad3f8672", "class_name": "RelatedNodeInfo"}]}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "text": "Installation/Quickstart.\n- Faiss (`FaissVectorStore`). Installation.\n- Milvus (`MilvusVectorStore`). Installation\n- Zilliz (`MilvusVectorStore`). Quickstart\n- MyScale (`MyScaleVectorStore`). Quickstart. Installation/Python Client.\n- Supabase (`SupabaseVectorStore`). Quickstart.\n- DocArray (`DocArrayHnswVectorStore`, `DocArrayInMemoryVectorStore`). Installation/Python Client.\n- MongoDB Atlas (`MongoDBAtlasVectorSearch`). [Installation/Quickstart] (https://www.mongodb.com/atlas/database).\n- Redis (`RedisVectorStore`). Installation.\n\nA detailed API reference is found here.\n\nSimilar to any other index within LlamaIndex (tree, keyword table, list), `VectorStoreIndex` can be constructed upon any collection\nof documents. We use the vector store within the index to store embeddings for the input text chunks.\n\nOnce constructed, the index can be used for querying.\n\n**Default Vector Store Index Construction/Querying**\n\nBy default, `VectorStoreIndex` uses a in-memory `SimpleVectorStore`\nthat's initialized as part of the default storage context.\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Using a Vector Store as an Index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Load documents and build index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Custom Vector Store Index Construction/Querying**\n\nWe can query over a custom vector store as follows:\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\nfrom llama_index.vector_stores import DeepLakeVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store and customize storage context\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(\n    vector_store = DeepLakeVectorStore(dataset_path=\"<dataset_path>\")\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Load documents and build index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBelow we show more examples of how to construct various vector stores we support.", "start_char_idx": 44555, "end_char_idx": 49726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd509e3e-640c-40f0-99e9-4bb7dcfd378a": {"__data__": {"id_": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "node_type": "1", "metadata": {}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "392fdffd-9b49-404d-888c-bba04b0af715", "node_type": "1", "metadata": {}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b8d60a4c-25cf-43e9-a388-952342591041", "node_type": "1", "metadata": {}, "hash": "2feb79328dacdf8cdee63520441762c4f0e821971d6f4082aa9c91cbabfa0bc1", "class_name": "RelatedNodeInfo"}, {"node_id": "277d968a-88eb-491c-a9e5-85bf55f39f11", "node_type": "1", "metadata": {}, "hash": "993ccdf411673d8eebe7b0ff5446842942c2e08c9ad02c9a7275710bcd727156", "class_name": "RelatedNodeInfo"}, {"node_id": "f0cf3648-44c2-4393-8319-4f5a36faed00", "node_type": "1", "metadata": {}, "hash": "66086896f7512f4eaa09458e6b2643bcdadff9f6c2687c2fec37a62ddb44addc", "class_name": "RelatedNodeInfo"}, {"node_id": "9886d89a-600e-4752-b244-e82a88999cf6", "node_type": "1", "metadata": {}, "hash": "308eaec48ee03d3d79475876f0fd39956265ee9ec5d99a94ec3ee7ee76e12727", "class_name": "RelatedNodeInfo"}]}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "text": "**Redis**\n\nFirst, start Redis-Stack (or get url from Redis provider)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocker run --name redis-vecdb -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen connect and use Redis as a vector database with LlamaIndex\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.vector_stores import RedisVectorStore\nvector_store = RedisVectorStore(\n    index_name=\"llm-project\",\n    redis_url=\"redis://localhost:6379\",\n    overwrite=True\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis can be used with the `VectorStoreIndex` to provide a query interface for retrieval, querying, deleting, persisting the index, and more.\n\n**DeepLake**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport os\nimport getpath\nfrom llama_index.vector_stores import DeepLakeVectorStore\n\nos.environ[\"OPENAI_API_KEY\"] = getpath.getpath(\"OPENAI_API_KEY: \")\nos.environ[\"ACTIVELOOP_TOKEN\"] = getpath.getpath(\"ACTIVELOOP_TOKEN: \")\ndataset_path = \"hub://adilkhan/paul_graham_essay\"\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Faiss**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport faiss\nfrom llama_index.vector_stores import FaissVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/create faiss index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nd = 1536\nfaiss_index = faiss.IndexFlatL2(d)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = FaissVectorStore(faiss_index)\n\n...\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/persist() takes in optional arg persist_path. If none give, will use default paths.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context.persist()\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/persist() takes in optional arg persist_path. If none give, will use default paths.", "start_char_idx": 49728, "end_char_idx": 54827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "392fdffd-9b49-404d-888c-bba04b0af715": {"__data__": {"id_": "392fdffd-9b49-404d-888c-bba04b0af715", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "node_type": "1", "metadata": {}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64e62721-99d2-4653-89a4-327bce7cb75b", "node_type": "1", "metadata": {}, "hash": "01313ab8cdfdfc9475fb755f41c81aee633f4976305f0aff0bdf06376688116c", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d056ad7a-d759-48e5-9bb4-fe59e7230f79", "node_type": "1", "metadata": {}, "hash": "e459f789e94432cb2c21d094b88585fc1a98b730ff6a8c8b129045c10f1d8ce5", "class_name": "RelatedNodeInfo"}, {"node_id": "032b4796-6f12-4c57-95de-5e6287b9b309", "node_type": "1", "metadata": {}, "hash": "7227483d02e6c60057be6156405b43444290a5d765ed62e471a3a5bba0434ffc", "class_name": "RelatedNodeInfo"}, {"node_id": "726bab5c-341d-4653-a1b7-568794b28cdb", "node_type": "1", "metadata": {}, "hash": "ca57768a24dc6e495f4ab4cec3ea76f0eff05255c9cea742a950784bdee771de", "class_name": "RelatedNodeInfo"}, {"node_id": "b703bcbb-235e-4a9a-adae-ae27ffc8b7b2", "node_type": "1", "metadata": {}, "hash": "377da729cc7e5233dc3c06140aed8e949384c1f3893202413c3615cad981555b", "class_name": "RelatedNodeInfo"}]}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "text": "If none give, will use default paths.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Weaviate**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/persist() takes in optional arg persist_path. If none give, will use default paths.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport weaviate\nfrom llama_index.vector_stores import WeaviateVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/creating a Weaviate client\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresource_owner_config = weaviate.AuthClientPassword(\n    username=\"<username>\",\n    password=\"<password>\",\n)\nclient = weaviate.Client(\n    \"https://<cluster-id>.semi.network/\", auth_client_secret=resource_owner_config\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = WeaviateVectorStore(weaviate_client=client)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Pinecone**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport pinecone\nfrom llama_index.vector_stores import PineconeVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Creating a Pinecone index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\napi_key = \"api_key\"\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\npinecone.create_index(\n    \"quickstart\",\n    dimension=1536,\n    metric=\"euclidean\",\n    pod_type=\"p1\"\n)\nindex = pinecone.Index(\"quickstart\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/reuse pinecone indexes)\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmetadata_filters = {\"title\": \"paul_graham_essay\"}\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = PineconeVectorStore(\n    pinecone_index=index,\n    metadata_filters=metadata_filters\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Qdrant**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport qdrant_client\nfrom llama_index.vector_stores import QdrantVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Creating a Qdrant vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.", "start_char_idx": 54790, "end_char_idx": 59714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64e62721-99d2-4653-89a4-327bce7cb75b": {"__data__": {"id_": "64e62721-99d2-4653-89a4-327bce7cb75b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "392fdffd-9b49-404d-888c-bba04b0af715", "node_type": "1", "metadata": {}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "node_type": "1", "metadata": {}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "96f54233-b16b-40fe-aae0-385b9a91fc34", "node_type": "1", "metadata": {}, "hash": "ab2b63d08d97ca1d6781cd5ef4b71cd33b5920136e6f3eeb7ae34950942cc1cc", "class_name": "RelatedNodeInfo"}, {"node_id": "03ab4346-2c32-4920-a14a-79b02cfe75dd", "node_type": "1", "metadata": {}, "hash": "7a588769e2e28e14af6c27d05e05268fc1546b42e95dc24d62eb8fae6ee40581", "class_name": "RelatedNodeInfo"}, {"node_id": "fdb7123d-fc53-41b2-8a65-ef26675c6dfc", "node_type": "1", "metadata": {}, "hash": "23ed4bee7a44176a65d2eab7aab5ff6df16efe12d5a92e651e8b84f1624af6c3", "class_name": "RelatedNodeInfo"}]}, "hash": "01313ab8cdfdfc9475fb755f41c81aee633f4976305f0aff0bdf06376688116c", "text": "md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nclient = qdrant_client.QdrantClient(\n    host=\"<qdrant-host>\",\n    api_key=\"<qdrant-api-key>\",\n    https=True\n)\ncollection_name = \"paul_graham\"\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = QdrantVectorStore(\n    client=client,\n    collection_name=collection_name,\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Chroma**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport chromadb\nfrom llama_index.vector_stores import ChromaVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/By default, Chroma will operate purely in-memory.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchroma_client = chromadb.Client()\nchroma_collection = chroma_client.create_collection(\"quickstart\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = ChromaVectorStore(\n    chroma_collection=chroma_collection,\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Milvus**\n\n- Milvus Index offers the ability to store both Documents and their embeddings. Documents are limited to the predefined Document attributes and does not include metadata.\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport pymilvus\nfrom llama_index.vector_stores import MilvusVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = MilvusVectorStore(\n    host='localhost',\n    port=19530,\n    overwrite='True'\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Note**: `MilvusVectorStore` depends on the `pymilvus` library.\nUse `pip install pymilvus` if not already installed.\nIf you get stuck at building wheel for `grpcio`, check if you are using python 3.11\n(there's a known issue: https://github.com/milvus-io/pymilvus/issues/1308)\nand try downgrading.\n\n**Zilliz**\n\n- Zilliz Cloud (hosted version of Milvus) uses the Milvus Index with some extra arguments.", "start_char_idx": 59686, "end_char_idx": 64227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83735c20-50e7-493a-a9a3-af0c3d909a4e": {"__data__": {"id_": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64e62721-99d2-4653-89a4-327bce7cb75b", "node_type": "1", "metadata": {}, "hash": "01313ab8cdfdfc9475fb755f41c81aee633f4976305f0aff0bdf06376688116c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "node_type": "1", "metadata": {}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "fe988892-00ed-40d6-bd55-5575565ad14c", "node_type": "1", "metadata": {}, "hash": "bbb06337933b0800e1f3708b8aacdb048bf08a0ac1d4cfb1d45f4da020160740", "class_name": "RelatedNodeInfo"}, {"node_id": "cb240758-46ff-40f9-99ef-1f67ac5345fc", "node_type": "1", "metadata": {}, "hash": "1c200eeb8fdccb8e1bdde273e532d69ec5dbd0ab3c48217656fa5e22e2fe05e2", "class_name": "RelatedNodeInfo"}, {"node_id": "a96841ca-0274-4186-bdd7-2dd8aecf02a9", "node_type": "1", "metadata": {}, "hash": "33abbfd1c0bbede017f316316b87aefa30dea2198479bc1945783c243ce44033", "class_name": "RelatedNodeInfo"}, {"node_id": "d0f157a3-5025-4f7a-9fd0-f00ef7b15858", "node_type": "1", "metadata": {}, "hash": "d544b353d7a6da68e88caae5e9e37397508cd71695b34b0ce380081e97672de9", "class_name": "RelatedNodeInfo"}]}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "text": "File Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport pymilvus\nfrom llama_index.vector_stores import MilvusVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = MilvusVectorStore(\n    host='foo.vectordb.zillizcloud.com',\n    port=403,\n    user=\"db_admin\",\n    password=\"foo\",\n    use_secure=True,\n    overwrite='True'\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Note**: `MilvusVectorStore` depends on the `pymilvus` library.\nUse `pip install pymilvus` if not already installed.\nIf you get stuck at building wheel for `grpcio`, check if you are using python 3.11\n(there's a known issue: https://github.com/milvus-io/pymilvus/issues/1308)\nand try downgrading.\n\n**MyScale**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport clickhouse_connect\nfrom llama_index.vector_stores import MyScaleVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Creating a MyScale client\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nclient = clickhouse_connect.get_client(\n    host='YOUR_CLUSTER_HOST',\n    port=8443,\n    username='YOUR_USERNAME',\n    password='YOUR_CLUSTER_PASSWORD'\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = MyScaleVectorStore(\n    myscale_client=client\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**DocArray**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.vector_stores import (\n    DocArrayHnswVectorStore, \n    DocArrayInMemoryVectorStore,\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = DocArrayHnswVectorStore(work_dir='hnsw_index')\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/alternatively, construct the in-memory vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = DocArrayInMemoryVectorStore()\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/alternatively, construct the in-memory vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**MongoDBAtlas**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Provide URI to constructor,", "start_char_idx": 64229, "end_char_idx": 69330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1af9863-e7a3-460a-b4c2-6ded2595f8df": {"__data__": {"id_": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "node_type": "1", "metadata": {}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73c5cb41-444a-497b-9712-8883e99a63c9", "node_type": "1", "metadata": {}, "hash": "76ee8df3cb51834a2cb3f3a660c7e9a907fb1b8a92a406410183bc869c10f0c7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "775d675f-dbdf-42ee-9d5e-c725e9b7fdc5", "node_type": "1", "metadata": {}, "hash": "cb0d6d45f66737149a5ca1b44d7888c1dccf1a147417926039429a7710899974", "class_name": "RelatedNodeInfo"}, {"node_id": "336c0ae4-4ee7-43a9-b2d5-94924f8947ed", "node_type": "1", "metadata": {}, "hash": "21b88c3cb34c57a0a4b39919578bff95dbf4e366148aa5263342c1bf05566864", "class_name": "RelatedNodeInfo"}, {"node_id": "5fac2a33-b3b9-43a6-85e5-5557ebc23269", "node_type": "1", "metadata": {}, "hash": "c2df17712f95abf8988588650f524e344eb8c11077a6964ad19f1c3ad038e77f", "class_name": "RelatedNodeInfo"}, {"node_id": "86135e2e-a847-4de5-b87e-868ac0f17e97", "node_type": "1", "metadata": {}, "hash": "cc60ce768021769d3476b0c39d499435d23ca6e6bdcddc0405dca4b94951e774", "class_name": "RelatedNodeInfo"}]}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "text": "md\nContent Type: text\nHeader Path: Using Vector Stores/Provide URI to constructor, or use environment variable\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport pymongo\nfrom llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\nfrom llama_index.indices.vector_store.base import VectorStoreIndex\nfrom llama_index.storage.storage_context import StorageContext\nfrom llama_index.readers.file.base import SimpleDirectoryReader\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/mongo_uri = os.environ[\"MONGO_URI\"]\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmongo_uri = \"mongodb+srv://<username>:<password>@<host>?retryWrites=true&w=majority\"\nmongodb_client = pymongo.MongoClient(mongo_uri)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstore = MongoDBAtlasVectorSearch(mongodb_client)\nstorage_context = StorageContext.from_defaults(vector_store=store)\nuber_docs = SimpleDirectoryReader(input_files=[\"./data/10k/uber_2021.pdf\"]).load_data()\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(uber_docs, storage_context=storage_context)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nExample notebooks can be found here.\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Loading Data from Vector Stores using Data Connector\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports loading data from the following sources. See Data Connectors for more details and API documentation.\n\nChroma stores both documents and vectors. This is an example of how to use Chroma:\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Loading Data from Vector Stores using Data Connector\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.readers.chroma import ChromaReader\nfrom llama_index.indices import ListIndex\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/This requires a collection name and a persist directory.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nreader = ChromaReader(\n    collection_name=\"chroma_collection\",\n    persist_directory=\"examples/data_connectors/chroma_collection\"\n)\n\nquery_vector=[n1, n2, n3, ...]\n\ndocuments = reader.load_data(collection_name=\"demo\", query_vector=query_vector, limit=5)\nindex = ListIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"<query_text>\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/This requires a collection name and a persist directory.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQdrant also stores both documents and vectors. This is an example of how to use Qdrant:\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/This requires a collection name and a persist directory.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73c5cb41-444a-497b-9712-8883e99a63c9": {"__data__": {"id_": "73c5cb41-444a-497b-9712-8883e99a63c9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "node_type": "1", "metadata": {}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bf84f43-dde5-4866-89d0-bcab22576f63", "node_type": "1", "metadata": {}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1f631b18-8411-4438-9307-c9038c6fc816", "node_type": "1", "metadata": {}, "hash": "f85704546583957a5f9de63218c93038650fcb0e457b7dff43147ca345277183", "class_name": "RelatedNodeInfo"}, {"node_id": "5a662638-6ad4-494d-8676-f6274686a19f", "node_type": "1", "metadata": {}, "hash": "48a489cab05ed0363eb88169192662b95af93622b8e708d16da60f6ad9371d32", "class_name": "RelatedNodeInfo"}, {"node_id": "851aefec-48d1-4bdc-b4ed-ff3f727f0f0a", "node_type": "1", "metadata": {}, "hash": "15948ee16243e8fc86e52c250ac2e877fef28351ccfcccde4f1a9f468b48b96b", "class_name": "RelatedNodeInfo"}]}, "hash": "76ee8df3cb51834a2cb3f3a660c7e9a907fb1b8a92a406410183bc869c10f0c7", "text": "Links: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.readers.qdrant import QdrantReader\n\nreader = QdrantReader(host=\"localhost\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/query_vector = [0.3, 0.3, 0.3, 0.3, ...]\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_vector = [n1, n2, n3, ...]\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/for more details\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = reader.load_data(collection_name=\"demo\", query_vector=query_vector, limit=5)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/for more details\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNOTE: Since Weaviate can store a hybrid of document and vector objects, the user may either choose to explicitly specify `class_name` and `properties` in order to query documents, or they may choose to specify a raw GraphQL query. See below for usage.\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/1) load data using class_name and properties\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = reader.load_data(\n    class_name=\"<class_name>\",\n    properties=[\"property1\", \"property2\", \"...\"],\n    separate_documents=True\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/2) example GraphQL query\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery = \"\"\"\n{\n    Get {\n        <class_name> {\n            <property1>\n            <property2>\n        }\n    }\n}\n\"\"\"\n\ndocuments = reader.load_data(graphql_query=query, separate_documents=True)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/2) example GraphQL query\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNOTE: Both Pinecone and Faiss data loaders assume that the respective data sources only store vectors; text content is stored elsewhere. Therefore, both data loaders require that the user specifies an `id_to_text_map` in the load_data call.\n\nFor instance, this is an example usage of the Pinecone data loader `PineconeReader`:\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/2) example GraphQL query\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.readers.pinecone import PineconeReader\n\nreader = PineconeReader(api_key=api_key, environment=\"us-west1-gcp\")\n\nid_to_text_map = {\n    \"id1\": \"text blob 1\",\n    \"id2\": \"text blob 2\",\n}\n\nquery_vector=[n1, n2, n3, ..]\n\ndocuments = reader.load_data(\n    index_name=\"quickstart\", id_to_text_map=id_to_text_map, top_k=3, vector=query_vector, separate_documents=True\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/2) example GraphQL query\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nExample notebooks can be found here.", "start_char_idx": 74178, "end_char_idx": 78738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bf84f43-dde5-4866-89d0-bcab22576f63": {"__data__": {"id_": "6bf84f43-dde5-4866-89d0-bcab22576f63", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73c5cb41-444a-497b-9712-8883e99a63c9", "node_type": "1", "metadata": {}, "hash": "76ee8df3cb51834a2cb3f3a660c7e9a907fb1b8a92a406410183bc869c10f0c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0", "node_type": "1", "metadata": {}, "hash": "6319320dc240344335701df12777a3160bcb93f3f0e66382a213fd9d52a94a16", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7f92ac4c-c144-4662-bd84-33d3ffb867bc", "node_type": "1", "metadata": {}, "hash": "1fea7a8d80596b5f2834d1b8b3827badce28d0bbb919f35aa82af867ebd24ffd", "class_name": "RelatedNodeInfo"}, {"node_id": "a06a4f97-9432-4d71-9126-26efe752e422", "node_type": "1", "metadata": {}, "hash": "31b86f35b4100cd3b32e710fa26c9755f1d3be2401002d2f9b131d40ad218f9f", "class_name": "RelatedNodeInfo"}, {"node_id": "ad4eaa11-b7d7-4892-bc18-6db5c9b48769", "node_type": "1", "metadata": {}, "hash": "5696f37eca9a1654661fa85075c265d797bca1445783c1a061719f666a0daf97", "class_name": "RelatedNodeInfo"}, {"node_id": "04394911-77be-4d35-b5e7-431af7087cc3", "node_type": "1", "metadata": {}, "hash": "ce465f270ed2c87a78408aec316aab6f0773661aaf387dbada713661e45b0182", "class_name": "RelatedNodeInfo"}]}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "text": "File Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/2) example GraphQL query\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\ncaption: Examples\nmaxdepth: 1\n---\n../../examples/vector_stores/SimpleIndexDemo.ipynb\n../../examples/vector_stores/SimpleIndexDemoMMR.ipynb\n../../examples/vector_stores/RedisIndexDemo.ipynb\n../../examples/vector_stores/QdrantIndexDemo.ipynb\n../../examples/vector_stores/FaissIndexDemo.ipynb\n../../examples/vector_stores/DeepLakeIndexDemo.ipynb\n../../examples/vector_stores/MyScaleIndexDemo.ipynb\n../../examples/vector_stores/MetalIndexDemo.ipynb\n../../examples/vector_stores/WeaviateIndexDemo.ipynb\n../../examples/vector_stores/OpensearchDemo.ipynb\n../../examples/vector_stores/PineconeIndexDemo.ipynb\n../../examples/vector_stores/ChromaIndexDemo.ipynb\n../../examples/vector_stores/LanceDBIndexDemo.ipynb\n../../examples/vector_stores/MilvusIndexDemo.ipynb\n../../examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb\n../../examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb\n../../examples/vector_stores/AsyncIndexCreationDemo.ipynb\n../../examples/vector_stores/SupabaseVectorIndexDemo.ipynb\n../../examples/vector_stores/DocArrayHnswIndexDemo.ipynb\n../../examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb\n../../examples/vector_stores/MongoDBAtlasVectorSearch.ipynb\n../../examples/vector_stores/postgres.ipynb\n\nFile Name: Docs\\community\\integrations.md\nContent Type: text\nHeader Path: Integrations\nLinks: \nfile_path: Docs\\community\\integrations.md\nfile_name: integrations.md\nfile_type: None\nfile_size: 383\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex has a number of community integrations, from vector stores, to prompt trackers, tracers, and more!\n\nFile Name: Docs\\community\\integrations.md\nContent Type: text\nHeader Path: Integrations\nLinks: \nfile_path: Docs\\community\\integrations.md\nfile_name: integrations.md\nfile_type: None\nfile_size: 383\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nintegrations/graphsignal.md\nintegrations/guidance.md\nintegrations/trulens.md\nintegrations/chatgpt_plugins.md\nintegrations/using_with_langchain.md\nintegrations/graph_stores.md\nintegrations/vector_stores.md\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 646\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThese guide provide an overview of how to use our agent classes.\n\nFor more detailed guides on how to use specific tools, check out our tools module guides.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\modules.md\nContent Type: code\nHeader Path: Module Guides/OpenAI Agent\nfile_path: Docs\\core_modules\\agent_modules\\agents\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 646\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/agent/openai_agent.ipynb\n/examples/agent/openai_agent_with_query_engine.ipynb\n/examples/agent/openai_agent_retrieval.ipynb\n/examples/agent/openai_agent_query_cookbook.ipynb\n/examples/agent/openai_agent_query_plan.ipynb\n/examples/agent/openai_agent_context_retrieval.ipynb\n```\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\modules.md\nContent Type: code\nHeader Path: Module Guides/ReAct Agent\nfile_path: Docs\\core_modules\\agent_modules\\agents\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 646\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/agent/react_agent_with_query_engine.ipynb\n```\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nData Agents are LLM-powered knowledge workers in LlamaIndex that can intelligently perform various tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d function. They are capable of the following:\n\n- Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured.\n- Calling any external service API in a structured fashion, and processing the response + storing it for later.\n\nIn that sense, agents are a step beyond our query engines in that they can not only \"read\" from a static source of data, but can dynamically ingest and modify data from a variety of different tools.\n\nBuilding a data agent requires the following core components:\n\n- A reasoning loop\n- Tool abstractions\n\nA data agent is initialized with set of APIs, or Tools, to interact with; these APIs can be called by the agent to return information or modify state. Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool.", "start_char_idx": 78740, "end_char_idx": 84083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "887ff996-3d20-466f-8a43-7ccf4043ebb0": {"__data__": {"id_": "887ff996-3d20-466f-8a43-7ccf4043ebb0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bf84f43-dde5-4866-89d0-bcab22576f63", "node_type": "1", "metadata": {}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b7c415a-d0cb-4a25-8639-1d943d367102", "node_type": "1", "metadata": {}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "469b13f6-1879-4e2d-86f4-58608a2fe361", "node_type": "1", "metadata": {}, "hash": "77d7838cd18dbbc272c15ececc81aa73819ab4d239dc179450e6a6fd960e88f9", "class_name": "RelatedNodeInfo"}, {"node_id": "16fce20f-be0d-4dc9-bc9c-44e1842c6e06", "node_type": "1", "metadata": {}, "hash": "9bf20cb927d30f415046eeadb4fc3dac18b2cd24298f3ae856d675cfbc2d6159", "class_name": "RelatedNodeInfo"}, {"node_id": "5a46f800-df81-4eda-b5a0-a2244ca97e03", "node_type": "1", "metadata": {}, "hash": "6d671394c3aca276eb52e60329e4a92df1aa087a19c0e5445d401e3fc2f33a3f", "class_name": "RelatedNodeInfo"}]}, "hash": "6319320dc240344335701df12777a3160bcb93f3f0e66382a213fd9d52a94a16", "text": "File Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Reasoning Loop\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe reasoning loop depends on the type of agent. We have support for the following agents: \n- OpenAI Function agent (built on top of the OpenAI Function API)\n- a ReAct agent (which works across any chat/text completion endpoint).\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Tool Abstractions\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can learn more about our Tool abstractions in our Tools section.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Blog Post\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor full details, please check out our detailed blog post.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nData agents can be used in the following manner (the example uses the OpenAI Function API)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.agent import OpenAIAgent\nfrom llama_index.llms import OpenAI\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/import and define tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/initialize llm\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/initialize openai agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/initialize openai agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee our usage pattern guide for more details.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/initialize openai agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Modules\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLearn more about our different agent types in our module guides below.\n\nAlso take a look at our tools section!", "start_char_idx": 84085, "end_char_idx": 88587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b7c415a-d0cb-4a25-8639-1d943d367102": {"__data__": {"id_": "9b7c415a-d0cb-4a25-8639-1d943d367102", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0", "node_type": "1", "metadata": {}, "hash": "6319320dc240344335701df12777a3160bcb93f3f0e66382a213fd9d52a94a16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3399da4-ee87-4514-84fb-805c1728e658", "node_type": "1", "metadata": {}, "hash": "b184212a41d9892e8649c3391792c691507b74d39e6d6e949a723976c63ae5cf", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "bbce3433-c5aa-4437-918a-55b061bcef40", "node_type": "1", "metadata": {}, "hash": "98ee78e9f0350368ac64a604e0f64831becfb77be72d48a1d19d7ce56afd1170", "class_name": "RelatedNodeInfo"}, {"node_id": "f64c7dde-ee0f-44fe-9359-6c2291604a98", "node_type": "1", "metadata": {}, "hash": "ca0b8dab7c699c4f023cdd8850df82c1f4efd6ed913f527342ee05614dfe07d1", "class_name": "RelatedNodeInfo"}, {"node_id": "72cc8a69-5259-4f17-9dab-ef0f12c531b3", "node_type": "1", "metadata": {}, "hash": "aad49d7eb54d918d43ccb503c7af3edad1972476b0a1ae513a7ec482e3578a85", "class_name": "RelatedNodeInfo"}, {"node_id": "3050f0c7-639e-433c-a51e-360048a44846", "node_type": "1", "metadata": {}, "hash": "d3247939dcdf59c20c3b15ee78268f356c9c5bd14660b0cd9a3b0d13e5872755", "class_name": "RelatedNodeInfo"}]}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "text": "Also take a look at our tools section!\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Modules\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn agent is initialized from a set of Tools. Here's an example of instantiating a ReAct\nagent from a set of Tools.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.tools import FunctionTool\nfrom llama_index.llms import OpenAI\nfrom llama_index.agent import ReActAgent\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/define sample Tool\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return a * b\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/initialize llm\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/initialize ReAct agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent = ReActAgent.from_tools([multiply_tool], llm=llm, verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/initialize ReAct agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn agent supports both `chat` and `query` endpoints, inheriting from our `ChatEngine` and `QueryEngine` respectively.\n\nExample usage:\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/initialize ReAct agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent.chat(\"What is 2123 * 215123\")\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Query Engine Tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIt is easy to wrap query engines as tools for an agent as well. Simply do the following:\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Query Engine Tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.agent import ReActAgent\nfrom llama_index.tools import QueryEngineTool\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/NOTE: lyft_index and uber_index are both SimpleVectorIndex instances\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nlyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\nuber_engine = uber_index.as_query_engine(similarity_top_k=3)\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=lyft_engine,\n        metadata=ToolMetadata(\n            name=\"lyft_10k\",\n            description=\"Provides information about Lyft financials for year 2021. \"\n            \"Use a detailed plain text question as input to the tool.", "start_char_idx": 88549, "end_char_idx": 93839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3399da4-ee87-4514-84fb-805c1728e658": {"__data__": {"id_": "b3399da4-ee87-4514-84fb-805c1728e658", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b7c415a-d0cb-4a25-8639-1d943d367102", "node_type": "1", "metadata": {}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f326b656-9b2d-4837-abd3-67e6877c7439", "node_type": "1", "metadata": {}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1f701998-d2dc-40f8-95ef-eed75c2be17a", "node_type": "1", "metadata": {}, "hash": "e24e2ab95a9e2284ae4fd2a9f33514fce87bf701d475ff83b71a1a0829cc79da", "class_name": "RelatedNodeInfo"}, {"node_id": "56fba29e-2ab2-4832-87cd-c149d6a32fe6", "node_type": "1", "metadata": {}, "hash": "0075b8d4fa6e3a5fbbbbd1d2e9ce1f0981f7fde32d7738483f40b4fd148572b8", "class_name": "RelatedNodeInfo"}]}, "hash": "b184212a41d9892e8649c3391792c691507b74d39e6d6e949a723976c63ae5cf", "text": "\"Use a detailed plain text question as input to the tool.\",\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=uber_engine,\n        metadata=ToolMetadata(\n            name=\"uber_10k\",\n            description=\"Provides information about Uber financials for year 2021. \"\n            \"Use a detailed plain text question as input to the tool.\",\n        ),\n    ),\n]\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/initialize ReAct agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Use other agents as Tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA nifty feature of our agents is that since they inherit from `BaseQueryEngine`, you can easily define other agents as tools\nthrough our `QueryEngineTool`.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Use other agents as Tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.tools import QueryEngineTool\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sql_agent,\n        metadata=ToolMetadata(\n            name=\"sql_agent\",\n            description=\"Agent that can execute SQL queries.\"\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=gmail_agent,\n        metadata=ToolMetadata(\n            name=\"gmail_agent\",\n            description=\"Tool that can send emails on Gmail.\"\n        ),\n    ),\n]\n\nouter_agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also use agents in more advanced settings. For instance, being able to retrieve tools from an index during query-time, and\nbeing able to perform query planning over an existing set of Tools.\n\nThese are largely implemented with our `OpenAIAgent` classes (which depend on the OpenAI Function API). Support\nfor our more general `ReActAgent` is something we're actively investigating.\n\nNOTE: these are largely still in beta. The abstractions may change and become more general over time.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/Function Retrieval Agents\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf the set of Tools is very large, you can create an `ObjectIndex` to index the tools, and then pass in an `ObjectRetriever` to the agent during query-time, to first dynamically retrieve the relevant tools before having the agent pick from the candidate tools.\n\nWe first build an `ObjectIndex` over an existing set of Tools.", "start_char_idx": 93782, "end_char_idx": 97626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f326b656-9b2d-4837-abd3-67e6877c7439": {"__data__": {"id_": "f326b656-9b2d-4837-abd3-67e6877c7439", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3399da4-ee87-4514-84fb-805c1728e658", "node_type": "1", "metadata": {}, "hash": "b184212a41d9892e8649c3391792c691507b74d39e6d6e949a723976c63ae5cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07802572-8cc3-4488-a4fd-6135f0cc3045", "node_type": "1", "metadata": {}, "hash": "f44f2532ad9efd4160a02b65b972bea2c66a323ebc0e40e94781c65978c6bd85", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "db5a729d-92c2-412b-a5a1-a622ee0f629d", "node_type": "1", "metadata": {}, "hash": "7b3c6398de10eedb5e0eafb7b891fb7a75b9e63e2164add8238655e59a0d90f1", "class_name": "RelatedNodeInfo"}, {"node_id": "07ef6ba2-1212-4620-9a64-d97e427b2584", "node_type": "1", "metadata": {}, "hash": "f3247709ab321d44b6b7a07d17f82dfee0b9c97b4bb76c56fb711efb52ba079b", "class_name": "RelatedNodeInfo"}, {"node_id": "cce549ec-b777-4de0-a696-011295bbe324", "node_type": "1", "metadata": {}, "hash": "d415f1114829514bf9b5b631c9faefe43a68d68d2bd31cb54f04fffc75cd0638", "class_name": "RelatedNodeInfo"}, {"node_id": "64124226-5ec9-4b0e-a4cf-82c44b8a7e08", "node_type": "1", "metadata": {}, "hash": "3f21d108f04f55a7efc7f5793956639e8f609d7e0126052c1f830c85221771bd", "class_name": "RelatedNodeInfo"}]}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "text": "We first build an `ObjectIndex` over an existing set of Tools.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/define an \"object\" index over these tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\nfrom llama_index.objects import ObjectIndex, SimpleToolNodeMapping\n\ntool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\nobj_index = ObjectIndex.from_objects(\n    all_tools,\n    tool_mapping,\n    VectorStoreIndex,\n)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/define an \"object\" index over these tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe then define our `FnRetrieverOpenAIAgent`:\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/define an \"object\" index over these tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.agent import FnRetrieverOpenAIAgent\n\nagent = FnRetrieverOpenAIAgent.from_retriever(obj_index.as_retriever(), verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/Context Retrieval Agents\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOur context-augmented OpenAI Agent will always perform retrieval before calling any tools.\n\nThis helps to provide additional context that can help the agent better pick Tools, versus\njust trying to make a decision without any context.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/Context Retrieval Agents\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import Document\nfrom llama_index.agent import ContextRetrieverOpenAIAgent\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/toy index - stores a list of abbreviations\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntexts = [\n    \"Abbrevation: X = Revenue\",\n    \"Abbrevation: YZ = Risk Factors\",\n    \"Abbreviation: Z = Costs\",\n]\ndocs = [Document(text=t) for t in texts]\ncontext_index = VectorStoreIndex.from_documents(docs)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/add context agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncontext_agent = ContextRetrieverOpenAIAgent.from_tools_and_retriever(\n    query_engine_tools, context_index.as_retriever(similarity_top_k=1), verbose=True\n)\nresponse = context_agent.chat(\"What is the YZ of March 2022?\")\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/Query Planning\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOpenAI Function Agents can be capable of advanced query planning. The trick is to provide the agent\nwith a `QueryPlanTool` - if the agent calls the QueryPlanTool, it is forced to infer a full Pydantic schema representing a query\nplan over a set of subtools.", "start_char_idx": 97564, "end_char_idx": 102427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07802572-8cc3-4488-a4fd-6135f0cc3045": {"__data__": {"id_": "07802572-8cc3-4488-a4fd-6135f0cc3045", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f326b656-9b2d-4837-abd3-67e6877c7439", "node_type": "1", "metadata": {}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd", "node_type": "1", "metadata": {}, "hash": "a7c902e17e614210a0e6aa290102eca75a25b2cdf0f7753c249ee78c27c37520", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "19811015-7b6e-4227-a8e1-71300febfdec", "node_type": "1", "metadata": {}, "hash": "bf8a006929ca2e2595a32510816dc65c87d32e2efcf7d27485fbf6188c51fe6b", "class_name": "RelatedNodeInfo"}, {"node_id": "e42e97d4-89d8-42fb-aab2-a01085e26abf", "node_type": "1", "metadata": {}, "hash": "7dfd0da7f19f501659c295cd94cb24b9a1f714967c9334fd48c182851faa1c2a", "class_name": "RelatedNodeInfo"}, {"node_id": "82277ba7-e9d4-451e-8936-bdf713e2781f", "node_type": "1", "metadata": {}, "hash": "ecb0ea18a34cc3f2a2ca1f227e32caa4c65585a1b607fa66070bccc5dd631d04", "class_name": "RelatedNodeInfo"}]}, "hash": "f44f2532ad9efd4160a02b65b972bea2c66a323ebc0e40e94781c65978c6bd85", "text": "File Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/define query plan tool\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.tools import QueryPlanTool\nfrom llama_index import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(service_context=service_context)\nquery_plan_tool = QueryPlanTool.from_defaults(\n    query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march],\n    response_synthesizer=response_synthesizer,\n)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/initialize agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent = OpenAIAgent.from_tools(\n    [query_plan_tool],\n    max_function_calls=10,\n    llm=OpenAI(temperature=0, model=\"gpt-4-0613\"),\n    verbose=True,\n)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/should output a query plan to call march, june, and september tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = agent.query(\"Analyze Uber revenue growth in March, June, and September\")\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe offer a rich set of Tool Specs that are offered through LlamaHub \ud83e\udd99. \n!\n\nThese tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions. \n\nWe also provide a list of **utility tools** that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Tool Specs\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nComing soon!\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using). \n\nTo tackle this, we\u2019ve provided an initial set of \u201cutility tools\u201d in LlamaHub Tools - utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that\u2019s returned from any API request.\n\nLet\u2019s walk through our two main utility tools below.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/OnDemandLoaderTool\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis tool turns any existing LlamaIndex data loader ( `BaseReader` class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it \u201con-demand\u201d. All three of these steps happen in a single tool call.\n\nOftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call.", "start_char_idx": 102429, "end_char_idx": 107549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd": {"__data__": {"id_": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07802572-8cc3-4488-a4fd-6135f0cc3045", "node_type": "1", "metadata": {}, "hash": "f44f2532ad9efd4160a02b65b972bea2c66a323ebc0e40e94781c65978c6bd85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "652a770b-edf5-4968-bf1c-1e7289800dce", "node_type": "1", "metadata": {}, "hash": "76ed119e7fb6d90417a3ec226117314d7d2d067ad641aaf7b2fe72d61e421851", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "44b5d802-dbc2-48bb-9f6a-3ceb0c3dcc93", "node_type": "1", "metadata": {}, "hash": "cd4aa6a6bfd3808825408c4eb726b068daf9e59adbec4d641f63ba48adba4245", "class_name": "RelatedNodeInfo"}, {"node_id": "ce343639-2638-4fc0-8fc4-b83afb10564a", "node_type": "1", "metadata": {}, "hash": "beaa9f5d8b34d3227c0b6d6d46094e3b10b43dc76e253be1cadbe14a8ba49624", "class_name": "RelatedNodeInfo"}, {"node_id": "6e49b319-be00-40e6-af4b-5c0a8d2ad0f3", "node_type": "1", "metadata": {}, "hash": "5bba92a70333a3659924a1231f0b2a742999cfbd79adacbc238959294e21e26f", "class_name": "RelatedNodeInfo"}]}, "hash": "a7c902e17e614210a0e6aa290102eca75a25b2cdf0f7753c249ee78c27c37520", "text": "A usage example is given below:\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/OnDemandLoaderTool\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_hub.wikipedia.base import WikipediaReader\nfrom llama_index.tools.on_demand_loader_tool import OnDemandLoaderTool\n\ntool = OnDemandLoaderTool.from_defaults(\n\treader,\n\tname=\"Wikipedia Tool\",\n\tdescription=\"A tool for loading data and querying articles from Wikipedia\"\n)\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/LoadAndSearchToolSpec\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements `to_tool_list` , and when that function is called, two tools are returned: a `load` tool and then a `search` tool.\n\nThe `load` Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The `search` Tool execution would take in a query string as input and call the underlying index.\n\nThis is helpful for any API endpoint that will by default return large volumes of data - for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows.\n\nExample usage is shown below:\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/LoadAndSearchToolSpec\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_hub.tools.wikipedia.base import WikipediaToolSpec\nfrom llama_index.tools.tool_spec.load_and_search import LoadAndSearchToolSpec\n\nwiki_spec = WikipediaToolSpec()\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/Get the search wikipedia tool\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntool = wiki_spec.to_tool_list()[1]\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/Create the Agent with load/search tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent = OpenAIAgent.from_tools(\n LoadAndSearchToolSpec.from_defaults(\n    tool\n ).to_tool_list(), verbose=True\n)\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHaving proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a **Tool** as well as a **ToolSpec** containing a series of functions under the hood. \n\nA Tool implements a very generic interface - simply define `__call__` and also return some basic metadata (name, description, function schema).\n\nA Tool Spec defines a full API specification of any service that can be converted into a list of Tools.\n\nWe offer a few different types of Tools:\n- `FunctionTool`: A function tool allows users to easily convert any user-defined function into a Tool. It can also auto-infer the function schema.\n- `QueryEngineTool`: A tool that wraps an existing query engine. Note: since our agent abstractions inherit from `BaseQueryEngine`, these tools can also wrap other agents.\n\nWe offer a rich set of Tools and Tool Specs through LlamaHub \ud83e\udd99.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Blog Post\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor full details, please check out our detailed blog post.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOur Tool Specs and Tools can be imported from the `llama-hub` package.", "start_char_idx": 107552, "end_char_idx": 112974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "652a770b-edf5-4968-bf1c-1e7289800dce": {"__data__": {"id_": "652a770b-edf5-4968-bf1c-1e7289800dce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd", "node_type": "1", "metadata": {}, "hash": "a7c902e17e614210a0e6aa290102eca75a25b2cdf0f7753c249ee78c27c37520", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "29988d7a-5ee7-4ad8-81c9-c83f972d0697", "node_type": "1", "metadata": {}, "hash": "cd4e9c5bf64d00f506f5b73b75977590c3face787bd666ee217b7bc677cabe12", "class_name": "RelatedNodeInfo"}, {"node_id": "e24cf0ac-8186-4727-84ed-6c07832eab80", "node_type": "1", "metadata": {}, "hash": "63b75e57436987c21b6ccbe1b8b0e89cda2fc39a9528584f85994083a5a60e10", "class_name": "RelatedNodeInfo"}, {"node_id": "0c2c25f4-13ca-43ed-8f97-c793f711ea4b", "node_type": "1", "metadata": {}, "hash": "633e28bd176c37a75988779638f42aa5c2463cec129fb99a8f1bba1ff36e1e32", "class_name": "RelatedNodeInfo"}]}, "hash": "76ed119e7fb6d90417a3ec226117314d7d2d067ad641aaf7b2fe72d61e421851", "text": "To use with our agent,\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.agent import OpenAIAgent\nfrom llama_hub.tools.gmail.base import GmailToolSpec\n\ntool_spec = GmailToolSpec()\nagent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee our Usage Pattern Guide for more details.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/LlamaHub Tools Guide \ud83d\udee0\ufe0f\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCheck out our guide for a full overview of the Tools/Tool Specs in LlamaHub!\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/LlamaHub Tools Guide \ud83d\udee0\ufe0f\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nllamahub_tools_guide.md\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/LlamaHub Tools Guide \ud83d\udee0\ufe0f\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n<!-- We offer a rich set of Tool Specs that are offered through LlamaHub \ud83e\udd99. \nThese tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions. \n\n! -->\n\n\n<!-- ## Module Guides\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/LlamaHub Tools Guide \ud83d\udee0\ufe0f\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Tool Example Notebooks\nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nComing soon!  -->\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaHub Tool Specs and Tools can be imported from the `llama-hub` package. They can be plugged into our native agents, or LangChain agents.", "start_char_idx": 112976, "end_char_idx": 116808, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae": {"__data__": {"id_": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "652a770b-edf5-4968-bf1c-1e7289800dce", "node_type": "1", "metadata": {}, "hash": "76ed119e7fb6d90417a3ec226117314d7d2d067ad641aaf7b2fe72d61e421851", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "node_type": "1", "metadata": {}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "35952454-60da-4b15-868b-24884a0f8a0d", "node_type": "1", "metadata": {}, "hash": "d8535206f8b6117400bd9773dfc7ef06929be30946b773af97b3c641902bcde9", "class_name": "RelatedNodeInfo"}, {"node_id": "d1330cb2-d00f-4890-857d-9b0aac7ead05", "node_type": "1", "metadata": {}, "hash": "2428f1e6dd08348a2f2253b78efd023bb2dfd2d686d4b7066949412c888c3322", "class_name": "RelatedNodeInfo"}, {"node_id": "1c5741a9-c6fc-47a9-a2cd-3c45be6dc465", "node_type": "1", "metadata": {}, "hash": "b63d31f2813e0fc9d4f5d2467a38a7ffd119a7f15f22483f872bb4fb6521be10", "class_name": "RelatedNodeInfo"}, {"node_id": "ffe26393-c9d4-4612-bbfc-391cdb7af831", "node_type": "1", "metadata": {}, "hash": "5aacfcd9628a7441d56a1a5bd09a744a45f53154a38d603a024097dc65ace382", "class_name": "RelatedNodeInfo"}, {"node_id": "b3d6a034-cbd5-43c5-9022-1715c05ef1ff", "node_type": "1", "metadata": {}, "hash": "6e1ee1923377b895a9ad3d3a7eab6c9dd9052d228b4f5e6817a4c5c7cd5acac5", "class_name": "RelatedNodeInfo"}]}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "text": "They can be plugged into our native agents, or LangChain agents.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with our Agents\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo use with our OpenAIAgent,\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with our Agents\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.agent import OpenAIAgent\nfrom llama_hub.tools.gmail.base import GmailToolSpec\n\ntool_spec = GmailToolSpec()\nagent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/use agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent.chat(\"Can you create a new email to helpdesk and support @example.com about a service outage\")\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/use agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFull Tool details can be found on our LlamaHub page. Each tool contains a \"Usage\" section showing how that tool can be used.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with LangChain\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo use with a LangChain agent, simply convert tools to LangChain tools with `to_langchain_tool()`.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with LangChain\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntools = tool_spec.to_tool_list()\nlangchain_tools = [t.to_langchain_tool() for t in tools]\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/plug into LangChain agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom langchain.agents import initialize_agent\n\nagent_executor = initialize_agent(\n    langchain_tools, llm, agent=\"conversational-react-description\", memory=memory\n)\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\modules.md\nContent Type: code\nHeader Path: Module Guides\nfile_path: Docs\\core_modules\\data_modules\\connector\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1278\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n../../../examples/data_connectors/PsychicDemo.ipynb\n../../../examples/data_connectors/DeepLakeReader.ipynb\n../../../examples/data_connectors/QdrantDemo.ipynb\n../../../examples/data_connectors/DiscordDemo.ipynb\n../../../examples/data_connectors/MongoDemo.ipynb\n../../../examples/data_connectors/ChromaDemo.ipynb\n../../../examples/data_connectors/MyScaleReaderDemo.ipynb\n../../../examples/data_connectors/FaissDemo.ipynb\n../../../examples/data_connectors/ObsidianReaderDemo.ipynb\n../../../examples/data_connectors/SlackDemo.ipynb\n../../../examples/data_connectors/WebPageDemo.ipynb\n../../../examples/data_connectors/PineconeDemo.ipynb\n../../../examples/data_connectors/MboxReaderDemo.ipynb\n../../../examples/data_connectors/MilvusReaderDemo.ipynb\n../../../examples/data_connectors/NotionDemo.ipynb\n../../../examples/data_connectors/GithubRepositoryReaderDemo.ipynb\n../../../examples/data_connectors/GoogleDocsDemo.ipynb\n../../../examples/data_connectors/DatabaseReaderDemo.ipynb\n../../../examples/data_connectors/TwitterDemo.ipynb\n../../../examples/data_connectors/WeaviateDemo.ipynb\n../../../examples/data_connectors/MakeDemo.ipynb\n../../../examples/data_connectors/deplot/DeplotReader.ipynb\n```\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).", "start_char_idx": 116744, "end_char_idx": 122135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a": {"__data__": {"id_": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "node_type": "1", "metadata": {}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f4263475-b3a1-4f60-84a0-ca4ccdc7482c", "node_type": "1", "metadata": {}, "hash": "d85b8f3d96526ad0bc43cc481b1e148eb65ca97d6ee5aa519b550adb32144e60", "class_name": "RelatedNodeInfo"}, {"node_id": "a54d98eb-d4ad-4125-90ec-4a38c95483e7", "node_type": "1", "metadata": {}, "hash": "c016d9c06ffa72224ac57a9c0d227b433f0debadf9fb172394981c26aa4025aa", "class_name": "RelatedNodeInfo"}, {"node_id": "4acfe805-ad5c-4dd3-86e4-3ed23d752c4e", "node_type": "1", "metadata": {}, "hash": "c84ac99be091b050e2a31a0e234a8896ded86084f8f3a4fb3c438ea43c8a1bcd", "class_name": "RelatedNodeInfo"}, {"node_id": "6af4dded-1e29-4b14-9289-f2cf8af37bbf", "node_type": "1", "metadata": {}, "hash": "e2b6cd9e2a9e3821b58ae7fec74132b4e5d446fb0e44bc0200439e4deb4fe304", "class_name": "RelatedNodeInfo"}]}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "text": "File Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOnce you've ingested your data, you can build an Index on top, ask questions using a Query Engine, and have a conversation using a Chat Engine.\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/LlamaHub\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOur data connectors are offered through LlamaHub \ud83e\udd99. \nLlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet started with:\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import download_loader\n\nGoogleDocsReader = download_loader('GoogleDocsReader')\nloader = GoogleDocsReader()\ndocuments = loader.load_data(document_ids=[...])\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: code\nHeader Path: Data Connectors (LlamaHub)/Usage Pattern\nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Modules\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSome sample data connectors:\n- local file directory (`SimpleDirectoryReader`). Can support parsing a wide range of file types: `.pdf`, `.jpg`, `.png`, `.docx`, etc.\n- Notion (`NotionPageReader`)\n- Google Docs (`GoogleDocsReader`)\n- Slack (`SlackReader`)\n- Discord (`DiscordReader`)\n- Apify Actors (`ApifyActor`). Can crawl the web, scrape webpages, extract text content, download files including `.pdf`, `.jpg`, `.png`, `.docx`, etc.\n\nSee below for detailed guides.\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Modules\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nmodules.rst\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 720\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEach data loader contains a \"Usage\" section showing how that loader can be used. At the core of using each loader is a `download_loader` function, which\ndownloads the loader file into a module that you can use within your application.\n\nExample usage:\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 720\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, download_loader\n\nGoogleDocsReader = download_loader('GoogleDocsReader')\n\ngdoc_ids = ['1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec']\nloader = GoogleDocsReader()\ndocuments = loader.load_data(document_ids=gdoc_ids)\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nquery_engine.query('Where did the author go to school?')\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: text\nHeader Path: Documents / Nodes/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDocument and Node objects are core abstractions within LlamaIndex.\n\nA **Document** is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. They can be constructed manually, or created automatically via our data loaders. By default, a Document stores text along with some other attributes. Some of these are listed below.", "start_char_idx": 122137, "end_char_idx": 127660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b61dd542-0ed4-4a31-9612-cd7b9fa77581": {"__data__": {"id_": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "node_type": "1", "metadata": {}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97474367-637c-42cb-bdbe-82a982b8ddac", "node_type": "1", "metadata": {}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "467759b4-efac-438a-9c9c-d79a54126a79", "node_type": "1", "metadata": {}, "hash": "2d79dd0295430e816caaf76780d3f5704ed0ad3e97757cfcfb92b58dd78f7de1", "class_name": "RelatedNodeInfo"}, {"node_id": "32f45229-cca8-4dd5-a6e0-7ebf62badeaf", "node_type": "1", "metadata": {}, "hash": "e09cbf435c77025b37993dae0915f6ccb23dd2664a3c19cf17fb173c0f4376b2", "class_name": "RelatedNodeInfo"}, {"node_id": "9f014092-2ab1-476d-ba96-e5fd449c45e3", "node_type": "1", "metadata": {}, "hash": "9ce7b3078731baccbf62890878b2c451660f59cd355584fb461921298ed1ad14", "class_name": "RelatedNodeInfo"}, {"node_id": "c1fc7dc8-1fe2-4476-b2e9-7e2134603e7c", "node_type": "1", "metadata": {}, "hash": "240837e08b10a0b91de30f4e5eb52d9a9192ac27c1bf76968e6dd9e20b033c17", "class_name": "RelatedNodeInfo"}]}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "text": "Some of these are listed below.\n- `metadata` - a dictionary of annotations that can be appended to the text.\n- `relationships` - a dictionary containing relationships to other Documents/Nodes.\n\n*Note*: We have beta support for allowing Documents to store images, and are actively working on improving its multimodal capabilities.\n\nA **Node** represents a \"chunk\" of a source Document, whether that is a text chunk, an image, or other. Similar to Documents, they contain metadata and relationship information with other nodes.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes. By default every Node derived from a Document will inherit the same metadata from that Document (e.g. a \"file_name\" filed in the Document is propagated to every Node).\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: text\nHeader Path: Documents / Nodes/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere are some simple snippets to get started with Documents and Nodes.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: code\nHeader Path: Documents / Nodes/Usage Pattern/build index\nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index import Document, VectorStoreIndex\n\ntext_list = [text1, text2, ...]\ndocuments = [Document(text=t) for t in text_list]\n\nindex = VectorStoreIndex.from_documents(documents)\n\n```\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: code\nHeader Path: Documents / Nodes/Usage Pattern/build index\nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\n\nfrom llama_index.node_parser import SimpleNodeParser\n\n...\n\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(documents)\n\nindex = VectorStoreIndex(nodes)\n\n```\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: text\nHeader Path: Documents / Nodes/Usage Pattern/Document/Node Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTake a look at our in-depth guides for more details on how to use Documents/Nodes.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: text\nHeader Path: Documents / Nodes/Usage Pattern/Document/Node Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_documents.md\nusage_nodes.md\nusage_metadata_extractor.md\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDocuments can either be created automatically via data loaders, or constructed manually.\n\nBy default, all of our data loaders (including those offered on LlamaHub) return `Document` objects through the `load_data` function.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('./data').load_data()\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to construct documents manually. LlamaIndex exposes the `Document` struct.", "start_char_idx": 127629, "end_char_idx": 132610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97474367-637c-42cb-bdbe-82a982b8ddac": {"__data__": {"id_": "97474367-637c-42cb-bdbe-82a982b8ddac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "node_type": "1", "metadata": {}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad6dd30e-799b-4385-b420-d59372953192", "node_type": "1", "metadata": {}, "hash": "a7e433526e10b0bd8ddf7cc047e74cb5f2dd09378f9f522c2414f4b3f52cfe0e", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "bdebdcf2-d10d-48f8-bed0-afdc55ddbffa", "node_type": "1", "metadata": {}, "hash": "1485ccea8bec3f956bf5be3314acc728117260cdf85080d7b0c7bb57167e2563", "class_name": "RelatedNodeInfo"}, {"node_id": "7554fb30-8d54-4acd-a4de-dbbe5da79084", "node_type": "1", "metadata": {}, "hash": "c7f2b5a8787b41210ee5d2ef3d7f647c81f04aa93c5b76a8cb9c48c14a5cf714", "class_name": "RelatedNodeInfo"}, {"node_id": "898e0035-4dc6-4af1-87c5-b5a3f8436aaf", "node_type": "1", "metadata": {}, "hash": "aa092b08e564d7a8dc2a9d8a26d6680b96789d907c12c7cb1de28fe65203f2f5", "class_name": "RelatedNodeInfo"}, {"node_id": "47a8ccc3-e483-403c-87ab-2fd898383d67", "node_type": "1", "metadata": {}, "hash": "aa6f7d4cabd4656fba93bfda137b64338a03e65487b2a3f0ac2a51bfece825c7", "class_name": "RelatedNodeInfo"}]}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "text": "LlamaIndex exposes the `Document` struct.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Document\n\ntext_list = [text1, text2, ...]\ndocuments = [Document(text=t) for t in text_list]\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo speed up prototyping and development, you can also quickly create a document using some default text:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument = Document.example()\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis section covers various ways to customize `Document` objects. Since the `Document` object is a subclass of our `TextNode` object, all these settings and details apply to the `TextNode` object class as well.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDocuments also offer the chance to include useful metadata. Using the `metadata` dictionary on each document, additional information can be included to help inform responses and track down sources for query responses. This information can be anything, such as filenames or categories. If you are intergrating with a vector database, keep in mind that some vector databases require that the keys must be strings, and the values must be flat (either `str`, `float`, or `int`).\n\nAny information set in the `metadata` dictionary of each document will show up in the `metadata` of each source node created from the document. Additionaly, this information is included in the nodes, enabling the index to utilize it on queries and responses. By default, the metadata is injected into the text for both embedding and LLM model calls.\n\nThere are a few ways to set up this dictionary:\n\n1. In the document constructor:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument = Document(\n    text='text', \n    metadata={\n        'filename': '<doc_file_name>', \n        'category': '<category>'\n    }\n)\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n2. After the document is created:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument.metadata = {'filename': '<doc_file_name>'}\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n3. Set the filename automatically using the `SimpleDirectoryReader` and `file_metadata` hook.", "start_char_idx": 132569, "end_char_idx": 138030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad6dd30e-799b-4385-b420-d59372953192": {"__data__": {"id_": "ad6dd30e-799b-4385-b420-d59372953192", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97474367-637c-42cb-bdbe-82a982b8ddac", "node_type": "1", "metadata": {}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9", "node_type": "1", "metadata": {}, "hash": "cf0dcf293c895e4b6aae5368bf837f89c99af86f74ebf04689f0a2f6b57b923e", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3dec0a73-1cc9-4602-a065-f9142880083e", "node_type": "1", "metadata": {}, "hash": "bb0200e54267e5cfa2b7d8f9d3eabe762ecd6d476575ca05f30fdb9cdab06957", "class_name": "RelatedNodeInfo"}, {"node_id": "47ef4582-31ec-46fe-8d19-566262c354e5", "node_type": "1", "metadata": {}, "hash": "b2fb198eac297fdf122a23477e11cc31011859148a9ce71a1b94ea1925376ac9", "class_name": "RelatedNodeInfo"}, {"node_id": "9322d884-94e3-45db-a0e1-ff6d3602ccf6", "node_type": "1", "metadata": {}, "hash": "f85e4db56f2f5d713240df54682e831531dcd4101ed8160ddea1bb7f7d293f05", "class_name": "RelatedNodeInfo"}]}, "hash": "a7e433526e10b0bd8ddf7cc047e74cb5f2dd09378f9f522c2414f4b3f52cfe0e", "text": "Set the filename automatically using the `SimpleDirectoryReader` and `file_metadata` hook. This will automatically run the hook on each document to set the `metadata` field:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\nfilename_fn = lambda filename: {'file_name': filename}\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/automatically sets the metadata of each document according to filename_fn\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('./data', file_metadata=filename_fn)\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Customizing the id\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAs detailed in the section Document Management, the doc `id_` is used to enable effecient refreshing of documents in the index. When using the `SimpleDirectoryReader`, you can automatically set the doc `id_` to be the full path to each document:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Customizing the id\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data\", filename_as_id=True).load_data()\nprint([x.doc_id for x in documents])\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Customizing the id\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also set the `id_` of any `Document` or `TextNode` directly!\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Customizing the id\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument.id_ = \"My new document id!\"\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA key detail mentioned above is that by default, any metadata you set is included in the embeddings generation and LLM.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing LLM Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTypically, a document might have many metadata keys, but you might not want all of them visibile to the LLM during response synthesis. In the above examples, we may not want the LLM to read the `file_name` of our document. However, the `file_name` might include information that will help generate better embeddings. A key advantage of doing this is to bias the embeddings for retrieval without changing what the LLM ends up reading.", "start_char_idx": 137940, "end_char_idx": 142897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e223e35-663d-45b7-bcfb-566a4b26d9b9": {"__data__": {"id_": "2e223e35-663d-45b7-bcfb-566a4b26d9b9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad6dd30e-799b-4385-b420-d59372953192", "node_type": "1", "metadata": {}, "hash": "a7e433526e10b0bd8ddf7cc047e74cb5f2dd09378f9f522c2414f4b3f52cfe0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa8855d7-51de-40ee-842f-c3acdf2ce809", "node_type": "1", "metadata": {}, "hash": "ec60c4d7677f32291466d5d8fb00d539015b65a4ccfc4e5fd9c7003ff4dba02a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "de6ba78a-53e3-4297-967c-9d0490004a20", "node_type": "1", "metadata": {}, "hash": "886a8f54155e24d7d6ad38fd59a990663c6bb2d2b4b28a8baccb6a039db5b2b4", "class_name": "RelatedNodeInfo"}, {"node_id": "e853cb59-60ac-4f2a-99d1-828b877e5ab3", "node_type": "1", "metadata": {}, "hash": "9cf0d2ea3ee005f558102927c271090077993d7170f9c93690c6dcc58a14e720", "class_name": "RelatedNodeInfo"}, {"node_id": "ecfd65a4-f974-4554-ba86-4805cab833b1", "node_type": "1", "metadata": {}, "hash": "f602a0d021d6c79350be9a199e05aa6772eca1228b6be3da61a53466900b8faf", "class_name": "RelatedNodeInfo"}]}, "hash": "cf0dcf293c895e4b6aae5368bf837f89c99af86f74ebf04689f0a2f6b57b923e", "text": "We can exclude it like so:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing LLM Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument.excluded_llm_metadata_keys = ['file_name']\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing LLM Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen, we can test what the LLM will actually end up reading using the `get_content()` function and specifying `MetadataMode.LLM`:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing LLM Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import MetadataMode\nprint(document.get_content(metadata_mode=MetadataMode.LLM))\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing Embedding Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSimilar to customing the metadata visibile to the LLM, we can also customize the metadata visible to emebddings. In this case, you can specifically exclude metadata visible to the embedding model, in case you DON'T want particular text to bias the embeddings.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing Embedding Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument.excluded_embed_metadata_keys = ['file_name']\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing Embedding Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen, we can test what the embedding model will actually end up reading using the `get_content()` function and specifying `MetadataMode.EMBED`:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing Embedding Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import MetadataMode\nprint(document.get_content(metadata_mode=MetadataMode.EMBED))\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing Metadata Format\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAs you know by now, metadata is injected into the actual text of each document/node when sent to the LLM or embedding model. By default, the format of this metadata is controlled by three attributes:\n\n1. `Document.metadata_seperator` -> default = `\"\\n\"`\n\nWhen concatenating all key/value fields of your metadata, this field controls the seperator bewtween each key/value pair.\n\n2. `Document.metadata_template` -> default = `\"{key}: {value}\"`\n\nThis attribute controls how each key/value pair in your metadata is formatted. The two variables `key` and `value` string keys are required.\n\n3. `Document.text_template` -> default = `{metadata_str}\\n\\n{content}`\n\nOnce your metadata is converted into a string using `metadata_seperator` and `metadata_template`, this templates controls what that metadata looks like when joined with the text content of your document/node. The `metadata` and `content` string keys are required.", "start_char_idx": 142900, "end_char_idx": 148519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa8855d7-51de-40ee-842f-c3acdf2ce809": {"__data__": {"id_": "fa8855d7-51de-40ee-842f-c3acdf2ce809", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9", "node_type": "1", "metadata": {}, "hash": "cf0dcf293c895e4b6aae5368bf837f89c99af86f74ebf04689f0a2f6b57b923e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7", "node_type": "1", "metadata": {}, "hash": "26f84ce32d42a77c94f10e0aec56543bdd5b5aaf79f3d51e97b5103a638f71d6", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e846f65e-f37d-46c8-868e-ea192deecbb1", "node_type": "1", "metadata": {}, "hash": "3170ea2caffc5b4fe183246f09635145b055b4a41960a0f4f67bdf1e29dd0590", "class_name": "RelatedNodeInfo"}, {"node_id": "64b66bb3-6599-463d-b7d6-5913638e26e0", "node_type": "1", "metadata": {}, "hash": "c3e2a8a853b3f5232854222fec32b8db36afa3012336710aeb0aa8be3aaeb754", "class_name": "RelatedNodeInfo"}]}, "hash": "ec60c4d7677f32291466d5d8fb00d539015b65a4ccfc4e5fd9c7003ff4dba02a", "text": "The `metadata` and `content` string keys are required.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Summary\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nKnowing all this, let's create a short example using all this power:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Summary\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Document\nfrom llama_index.schema import MetadataMode\n\ndocument = Document(\n    text=\"This is a super-customized document\",\n    metadata={\n        \"file_name\": \"super_secret_document.txt\",\n        \"category\": \"finance\",\n        \"author\": \"LlamaIndex\"    \n    },\n    excluded_llm_metadata_keys=['file_name'],\n    metadata_seperator=\"::\",\n    metadata_template=\"{key}=>{value}\",\n    text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n)\n\nprint(\"The LLM sees this: \\n\", document.get_content(metadata_mode=MetadataMode.LLM))\nprint(\"The Embedding model sees this: \\n\", document.get_content(metadata_mode=MetadataMode.EMBED))\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Advanced - Automatic Metadata Extraction\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe have initial examples of using LLMs themselves to perform metadata extraction.\n\nTake a look here!\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Advanced - Automatic Metadata Extraction\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\n/examples/metadata_extraction/MetadataExtractionSEC.ipynb\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nContent Type: text\nHeader Path: Automated Metadata Extraction for Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nfile_name: usage_metadata_extractor.md\nfile_type: None\nfile_size: 1424\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use LLMs to automate metadata extraction with our `MetadataExtractor` modules.\n\nOur metadata extractor modules include the following \"feature extractors\":\n- `SummaryExtractor` - automatically extracts a summary over a set of Nodes\n- `QuestionsAnsweredExtractor` - extracts a set of questions that each Node can answer\n- `TitleExtractor` - extracts a title over the context of each Node\n\nYou can use these feature extractors within our overall `MetadataExtractor` class.", "start_char_idx": 148465, "end_char_idx": 152204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f5fdbba-1016-4d60-aeca-e98227fe9ea7": {"__data__": {"id_": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa8855d7-51de-40ee-842f-c3acdf2ce809", "node_type": "1", "metadata": {}, "hash": "ec60c4d7677f32291466d5d8fb00d539015b65a4ccfc4e5fd9c7003ff4dba02a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b07194c-2acc-4baa-8e31-a367741156c1", "node_type": "1", "metadata": {}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "4e87c606-e67e-4afb-b738-5a26b6e05584", "node_type": "1", "metadata": {}, "hash": "5c0bc19293900a5a1b909a87af247bf297325d27f8bf0d34120bde066c022819", "class_name": "RelatedNodeInfo"}, {"node_id": "00d0fc93-663f-4feb-8c25-03e5d2998334", "node_type": "1", "metadata": {}, "hash": "3dd9daf956dec3cfddd70c33c78ee2de61360f4021bf80a9ce06513df20bbbed", "class_name": "RelatedNodeInfo"}, {"node_id": "bd4513ae-9145-41ff-8297-a26e911309ff", "node_type": "1", "metadata": {}, "hash": "b028581e5325709c8bd179b89d320db983aba6306d5583330c12f1da2c1a60cf", "class_name": "RelatedNodeInfo"}]}, "hash": "26f84ce32d42a77c94f10e0aec56543bdd5b5aaf79f3d51e97b5103a638f71d6", "text": "Then you can plug in the `MetadataExtractor` into our node parser:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nContent Type: text\nHeader Path: Automated Metadata Extraction for Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nfile_name: usage_metadata_extractor.md\nfile_type: None\nfile_size: 1424\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser.extractors import (\n    MetadataExtractor,\n    TitleExtractor,\n    QuestionsAnsweredExtractor\n)\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)\nmetadata_extractor = MetadataExtractor(\n    extractors=[\n        TitleExtractor(nodes=5),\n        QuestionsAnsweredExtractor(questions=3),\n    ],\n)\n\nnode_parser = SimpleNodeParser(\n    text_splitter=text_splitter,\n    metadata_extractor=metadata_extractor,\n)\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nContent Type: text\nHeader Path: assume documents are defined -> extract nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nfile_name: usage_metadata_extractor.md\nfile_type: None\nfile_size: 1424\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnodes = node_parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nContent Type: code\nHeader Path: assume documents are defined -> extract nodes\nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nfile_name: usage_metadata_extractor.md\nfile_type: None\nfile_size: 1424\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\ncaption: Metadata Extraction Guides\nmaxdepth: 1\n---\n/examples/metadata_extraction/MetadataExtractionSEC.ipynb\n```\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: Defining and Customizing Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNodes represent \"chunks\" of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information\nwith other nodes and index structures.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes.\n\nFor instance, you can do\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: Defining and Customizing Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser import SimpleNodeParser\n\nparser = SimpleNodeParser()\n\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: Defining and Customizing Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to construct Node objects manually and skip the first section.", "start_char_idx": 152205, "end_char_idx": 156025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b07194c-2acc-4baa-8e31-a367741156c1": {"__data__": {"id_": "0b07194c-2acc-4baa-8e31-a367741156c1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7", "node_type": "1", "metadata": {}, "hash": "26f84ce32d42a77c94f10e0aec56543bdd5b5aaf79f3d51e97b5103a638f71d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d8aa81e5-9c1a-4994-9576-f420e4c763a4", "node_type": "1", "metadata": {}, "hash": "b1645a4fab6a91a09f8e9ad50a08dff831181affbab7b249dc5143e739f78a20", "class_name": "RelatedNodeInfo"}, {"node_id": "889388cb-178c-4046-9c58-3b9e2a38b89c", "node_type": "1", "metadata": {}, "hash": "ae6e486b71a0f5eb295639db51f837d5e1d0cc7ada0b4e792edc5df69d82c3ef", "class_name": "RelatedNodeInfo"}, {"node_id": "f2f975fc-d497-40fc-83df-27ee09d7dc64", "node_type": "1", "metadata": {}, "hash": "e685d78d2f666b77e2c958ccfd8832729c84efafeecc43e0115f34fa44074ce4", "class_name": "RelatedNodeInfo"}, {"node_id": "1335bd5a-3236-4891-9db8-379da4c1ca05", "node_type": "1", "metadata": {}, "hash": "7190196b248df9825743b9d1aea45c516bc4b8a483cecd2d819d934eb10831c6", "class_name": "RelatedNodeInfo"}]}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "text": "For instance,\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: Defining and Customizing Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n\nnode1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\nnode2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: set relationships\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnode1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id)\nnode2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id)\nnodes = [node1, node2]\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: set relationships\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `RelatedNodeInfo` class can also store additional `metadata` if needed:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: set relationships\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnode2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=node1.node_id, metadata={\"key\": \"val\"})\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex offers **composability** of your indices, meaning that you can build indices on top of other indices. This allows you to more effectively index your entire document tree in order to feed custom knowledge to GPT.\n\nComposability allows you to to define lower-level indices for each document, and higher-order indices over a collection of documents. To see how this works, imagine defining 1) a tree index for the text within each document, and 2) a list index over each tree index (one document) within your collection.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Subindices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo see how this works, imagine you have 3 documents: `doc1`, `doc2`, and `doc3`.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Subindices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\n\ndoc1 = SimpleDirectoryReader('data1').load_data()\ndoc2 = SimpleDirectoryReader('data2').load_data()\ndoc3 = SimpleDirectoryReader('data3').load_data()\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Subindices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nNow let's define a tree index for each document. In order to persist the graph later, each index should share the same storage context.\n\nIn Python, we have:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Subindices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import TreeIndex\n\nstorage_context = storage_context.from_defaults()\n\nindex1 = TreeIndex.from_documents(doc1, storage_context=storage_context)\nindex2 = TreeIndex.from_documents(doc2, storage_context=storage_context)\nindex3 = TreeIndex.from_documents(doc3, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Subindices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!", "start_char_idx": 156026, "end_char_idx": 161438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2dcb8839-8d3f-474c-8f9b-724ab0904611": {"__data__": {"id_": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b07194c-2acc-4baa-8e31-a367741156c1", "node_type": "1", "metadata": {}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "node_type": "1", "metadata": {}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "fd274941-455a-48b8-b720-cee0ae61075d", "node_type": "1", "metadata": {}, "hash": "326cb317ee85019efd15db2f6c1c97ab31a14a4cf8737fa0af2a17c9a1161b5c", "class_name": "RelatedNodeInfo"}, {"node_id": "9504f2b8-a7b0-4ab2-960a-0c704dcf9bee", "node_type": "1", "metadata": {}, "hash": "3f94fa8b77888623c16a8e1bd1b4a48100e8b448942e5e88cdb37e7b3c54941f", "class_name": "RelatedNodeInfo"}, {"node_id": "bfed0e6f-d2bd-4f6d-8cb6-f200ee70a83a", "node_type": "1", "metadata": {}, "hash": "5f701fa1370144d71e782ff0921f921a7173268acca95e8b083f5f13d3871e63", "class_name": "RelatedNodeInfo"}, {"node_id": "0321e342-6af1-4315-a0e3-a299b091a7e1", "node_type": "1", "metadata": {}, "hash": "cdd6be7e959b5f7c33012810258816f86b7f1175a87e66d8b69afe665148b9cd", "class_name": "RelatedNodeInfo"}, {"node_id": "686fb86d-ffbb-46f0-819e-3495e7ff4547", "node_type": "1", "metadata": {}, "hash": "71d602335d61014dd592ed125e398a0bf56eb87e2cf789c13e28c0f17f7568f8", "class_name": "RelatedNodeInfo"}]}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "text": "File Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Summary Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou then need to explicitly define *summary text* for each subindex. This allows  \nthe subindices to be used as Documents for higher-level indices.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Summary Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex1_summary = \"<summary1>\"\nindex2_summary = \"<summary2>\"\nindex3_summary = \"<summary3>\"\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Summary Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou may choose to manually specify the summary text, or use LlamaIndex itself to generate\na summary, for instance with the following:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Summary Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nsummary = index1.query(\n    \"What is a summary of this document?\", retriever_mode=\"all_leaf\"\n)\nindex1_summary = str(summary)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Summary Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**If specified**, this summary text for each subindex can be used to refine the answer during query-time.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Creating a Graph with a Top-Level Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can then create a graph with a list index on top of these 3 tree indices:\nWe can query, save, and load the graph to/from disk as any other index.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Creating a Graph with a Top-Level Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.composability import ComposableGraph\n\ngraph = ComposableGraph.from_indices(\n    ListIndex,\n    [index1, index2, index3],\n    index_summaries=[index1_summary, index2_summary, index3_summary],\n    storage_context=storage_context,\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Creating a Graph with a Top-Level Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Querying the Graph\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDuring a query, we would start with the top-level list index. Each node in the list corresponds to an underlying tree index. \nThe query will be executed recursively, starting from the root index, then the sub-indices.\nThe default query engine for each index is called under the hood (i.e. `index.as_query_engine()`), unless otherwise configured by passing `custom_query_engines` to the `ComposableGraphQueryEngine`.\nBelow we show an example that configure the tree index retrievers to use `child_branch_factor=2` (instead of the default `child_branch_factor=1`).\n\n\nMore detail on how to configure `ComposableGraphQueryEngine` can be found here.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/set custom retrievers.", "start_char_idx": 161440, "end_char_idx": 166482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "767d8353-d5b7-4dda-ab2b-13adb4d57fc3": {"__data__": {"id_": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f", "node_type": "1", "metadata": {}, "hash": "61a26eec879d891a2a323e46275b58c8696878d3ddbcc8c9c858d6f0055e7ff0", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1843d26d-92bb-4f9c-ba21-bfb76b1ed4b0", "node_type": "1", "metadata": {}, "hash": "65aaf6ff6fdc8312600f6dbb238a14543558917669b03787fde1bc83765b179b", "class_name": "RelatedNodeInfo"}, {"node_id": "3d49d111-1bfc-4311-bc44-8a6c151f6156", "node_type": "1", "metadata": {}, "hash": "149efcb818412eb826620a2b3b86afee8e7af8be25b984d482201d9cec1e3ba5", "class_name": "RelatedNodeInfo"}, {"node_id": "1abf506b-8336-41bc-b55b-f13581d2a7da", "node_type": "1", "metadata": {}, "hash": "72c321a9cbf19b5de7c8f00ea84c98cb073d1d871f8cdff28bfa0574c73b3090", "class_name": "RelatedNodeInfo"}, {"node_id": "406cd354-54c5-4304-bbfd-b3d99e3b37a1", "node_type": "1", "metadata": {}, "hash": "43c525080a6f4dc102501ee14e1544b395e0d0b5aa10ca87d19bb24c4ff42324", "class_name": "RelatedNodeInfo"}]}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "text": "An example is provided below\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncustom_query_engines = {\n    index.index_id: index.as_query_engine(\n        child_branch_factor=2\n    ) \n    for index in [index1, index2, index3]\n}\nquery_engine = graph.as_query_engine(\n    custom_query_engines=custom_query_engines\n)\nresponse = query_engine.query(\"Where did the author grow up?\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/set custom retrievers. An example is provided below\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note that specifying custom retriever for index by id\n> might require you to inspect e.g., `index1.index_id`.\n> Alternatively, you can explicitly set it as follows:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/set custom retrievers. An example is provided below\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex1.set_index_id(\"<index_id_1>\")\nindex2.set_index_id(\"<index_id_2>\")\nindex3.set_index_id(\"<index_id_3>\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/set custom retrievers. An example is provided below\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nSo within a node, instead of fetching the text, we would recursively query the stored tree index to retrieve our answer.\n\n!\n\nNOTE: You can stack indices as many times as you want, depending on the hierarchies of your knowledge base!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/[Optional] Persisting the Graph\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe graph can also be persisted to storage, and then loaded again when needed. Note that you'll need to set the \nID of the root index, or keep track of the default.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/set the ID\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph.root_index.set_index_id(\"my_id\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/persist to storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph.root_index.storage_context.persist(persist_dir=\"./storage\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/load\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import StorageContext, load_graph_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\ngraph = load_graph_from_storage(storage_context, root_id=\"my_id\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/load\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can take a look at a code example below as well. We first build two tree indices, one over the Wikipedia NYC page, and the other over Paul Graham's essay. We then define a keyword extractor index over the two tree indices.\n\nHere is an example notebook.", "start_char_idx": 166483, "end_char_idx": 171171, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f": {"__data__": {"id_": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "node_type": "1", "metadata": {}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "node_type": "1", "metadata": {}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ffbf1723-e016-411a-86f7-05f2442a7283", "node_type": "1", "metadata": {}, "hash": "2971877059a96b230ab45d2853d131c473d842e80fc9b1c560a0cf5173fb48ac", "class_name": "RelatedNodeInfo"}, {"node_id": "f03a32ac-46e0-444a-8581-738e7d5e040b", "node_type": "1", "metadata": {}, "hash": "f10c217862d994157a2c8675c5052ca6fdf1c1226adfc88b0c63cb3d9978f215", "class_name": "RelatedNodeInfo"}, {"node_id": "b9146699-a47a-4169-b3fc-d029991dec71", "node_type": "1", "metadata": {}, "hash": "0a55c006619d879cb395c69932aee7785b537c954c21543732368ca881810e99", "class_name": "RelatedNodeInfo"}]}, "hash": "61a26eec879d891a2a323e46275b58c8696878d3ddbcc8c9c858d6f0055e7ff0", "text": "We then define a keyword extractor index over the two tree indices.\n\nHere is an example notebook.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/load\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\ncaption: Examples\nmaxdepth: 1\n---\n../../../../examples/composable_indices/ComposableIndices-Prior.ipynb\n../../../../examples/composable_indices/ComposableIndices-Weaviate.ipynb\n../../../../examples/composable_indices/ComposableIndices.ipynb\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMost LlamaIndex index structures allow for **insertion**, **deletion**, **update**, and **refresh** operations.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Insertion\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can \"insert\" a new Document into any index data structure, after building the index initially. This document will be broken down into nodes and ingested into the index.\n\nThe underlying mechanism behind insertion depends on the index structure. For instance, for the list index, a new Document is inserted as additional node(s) in the list.\nFor the vector store index, a new Document (and embeddings) is inserted into the underlying document/embedding store.\n\nAn example notebook showcasing our insert capabilities is given here.\nIn this notebook we showcase how to construct an empty index, manually create Document objects, and add those to our index data structures.\n\nAn example code snippet is given below:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Insertion\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ListIndex, Document\n\nindex = ListIndex([])\ntext_chunks = ['text_chunk_1', 'text_chunk_2', 'text_chunk_3']\n\ndoc_chunks = []\nfor i, text in enumerate(text_chunks):\n    doc = Document(text=text, id_=f\"doc_id_{i}\")\n    doc_chunks.append(doc)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/insert\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfor doc_chunk in doc_chunks:\n    index.insert(doc_chunk)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Deletion\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can \"delete\" a Document from most index data structures by specifying a document_id. (**NOTE**: the tree index currently does not support deletion). All nodes corresponding to the document will be deleted.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Deletion\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.delete_ref_doc(\"doc_id_0\", delete_from_docstore=True)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Deletion\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n`delete_from_docstore` will default to `False` in case you are sharing nodes betweeen indexes using the same docstore. However, these nodes will not be used when querying when this is set to `False` as they will be deleted from the `index_struct` of the index, which keeps track of which nodes can be used for querying.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Update\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf a Document is already present within an index, you can \"update\" a Document with the same doc `id_` (for instance, if the information in the Document has changed).", "start_char_idx": 171074, "end_char_idx": 176581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5cc1e57-4660-4db9-97e9-9c60e9e7c159": {"__data__": {"id_": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f", "node_type": "1", "metadata": {}, "hash": "61a26eec879d891a2a323e46275b58c8696878d3ddbcc8c9c858d6f0055e7ff0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "node_type": "1", "metadata": {}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2d3e1f2a-7c5e-4655-8fb6-f3e37c9f09a4", "node_type": "1", "metadata": {}, "hash": "9dc5064226ecdb9b6da472fcd2fc87e53124c0b4a9682a8cfb1ce9ed28bb1fd7", "class_name": "RelatedNodeInfo"}, {"node_id": "88e956ba-bb12-4885-8d75-71185ec887e6", "node_type": "1", "metadata": {}, "hash": "795dd27439477e0564863c08bd786dd04dc1c4ead75dc1cb64157ec47b3d5a27", "class_name": "RelatedNodeInfo"}, {"node_id": "96f11461-5762-44d2-b517-888dcc2de650", "node_type": "1", "metadata": {}, "hash": "d0a43e75fb20a32de7b78439400a06c47bafc145d32d2726678a7c5aac0aaa45", "class_name": "RelatedNodeInfo"}, {"node_id": "35cb2b8d-cc7f-49ca-b85b-6ce401dc2336", "node_type": "1", "metadata": {}, "hash": "204f109aa3b3b0f49a1703fdcced28733ba2381b5341b7d4cb86376f509d9227", "class_name": "RelatedNodeInfo"}]}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "text": "File Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/NOTE: the document has a `doc_id` specified\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndoc_chunks[0].text = \"Brand new document text\"\nindex.update_ref_doc(\n    doc_chunks[0], \n    update_kwargs={\"delete_kwargs\": {'delete_from_docstore': True}}\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/NOTE: the document has a `doc_id` specified\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Refresh\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you set the doc `id_` of each document when loading your data, you can also automatically refresh the index.\n\nThe `refresh()` function will only update documents who have the same doc `id_`, but different text contents. Any documents not present in the index at all will also be inserted.\n\n`refresh()` also returns a boolean list, indicating which documents in the input have been refreshed in the index.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/modify first document, with the same doc_id\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndoc_chunks[0] = Document(text='Super new document text', id_=\"doc_id_0\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/add a new document\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndoc_chunks.append(Document(text=\"This isn't in the index yet, but it will be soon!\", id_=\"doc_id_3\"))\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/refresh the index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nrefreshed_docs = index.refresh_ref_docs(\n    doc_chunks,\n    update_kwargs={\"delete_kwargs\": {'delete_from_docstore': True}}\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: code\nHeader Path: Document Management/refreshed_docs[0] and refreshed_docs[-1] should be true\nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```\n\nAgain, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.\n\nIf you `print()` the output of `refresh()`, you would see which input documents were refreshed:\n\n```python\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/refreshed_docs[0] and refreshed_docs[-1] should be true\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprint(refreshed_docs)\n> [True, False, False, True]\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/refreshed_docs[0] and refreshed_docs[-1] should be true\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis is most useful when you are reading from a directory that is constantly updating with new information.\n\nTo autmatically set the doc `id_` when using the `SimpleDirectoryReader`, you can set the `filename_as_id` flag. More details can be found here.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Document Tracking\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAny index that uses the docstore (i.e. all indexes except for most vector store integrations), you can also see which documents you have inserted into the docstore.", "start_char_idx": 176583, "end_char_idx": 182199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4542dbfd-0d9c-4e95-8501-681dc1e404af": {"__data__": {"id_": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "node_type": "1", "metadata": {}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "node_type": "1", "metadata": {}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "6f083448-5542-4404-acee-af96835a9db8", "node_type": "1", "metadata": {}, "hash": "899357273069354443b6dfd7f76c9b99e692a24d0b5266f75dd79d21b6bd9a4c", "class_name": "RelatedNodeInfo"}, {"node_id": "48a96332-cc2f-4081-8f7d-c36963ea928b", "node_type": "1", "metadata": {}, "hash": "df28d3af2a029afb525d37c57809c4ce64933699a89686c95996c177058acca6", "class_name": "RelatedNodeInfo"}, {"node_id": "1227e1e5-4a48-4130-9f78-43623f4c1c86", "node_type": "1", "metadata": {}, "hash": "adaec6e390e15c9c88b6aae2c11bd52f64cac5030268bb3c0324a3cf1162755b", "class_name": "RelatedNodeInfo"}, {"node_id": "e16341dd-63c3-4997-81b2-e69224f7c731", "node_type": "1", "metadata": {}, "hash": "3abfc045f326987c76114346f2bbf82b90b06dabad4ca3924a3571e5665428af", "class_name": "RelatedNodeInfo"}]}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "text": "File Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Document Tracking\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprint(index.ref_doc_info)\n> {'doc_id_1': RefDocInfo(node_ids=['071a66a8-3c47-49ad-84fa-7010c6277479'], metadata={}), \n   'doc_id_2': RefDocInfo(node_ids=['9563e84b-f934-41c3-acfd-22e88492c869'], metadata={}), \n   'doc_id_0': RefDocInfo(node_ids=['b53e6c2f-16f7-4024-af4c-42890e945f36'], metadata={}), \n   'doc_id_3': RefDocInfo(node_ids=['6bedb29f-15db-4c7c-9885-7490e10aa33f'], metadata={})}\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Document Tracking\nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEach entry in the output shows the ingested doc `id_`s as keys, and their associated `node_ids` of the nodes they were split into. \n\nLastly, the orignal `metadata` dictionary of each input document is also tracked. You can read more about the `metadata` attribute in Customizing Documents.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis guide describes how each index works with diagrams. \n\nSome terminology:\n- **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects.\n- **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to \n    specify different response modes here.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe list index simply stores Nodes as a sequential chain.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Querying\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDuring query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\nour Response Synthesis module.\n\n!\n\nThe list index does offer numerous ways of querying a list index, from an embedding-based query which \nwill fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Vector Store Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe vector store index stores each Node and a corresponding embedding in a Vector Store.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Querying\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuerying a vector store index involves fetching the top-k most similar Nodes, and passing\nthose into our Response Synthesis module.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Tree Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Querying\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuerying a tree index involves traversing from root nodes down \nto leaf nodes. By default, (`child_branch_factor=1`), a query\nchooses one child node given a parent node. If `child_branch_factor=2`, a query\nchooses two child nodes per level.\n\n!", "start_char_idx": 182201, "end_char_idx": 187457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dd123d3-c5d6-475a-b90b-2cd8e959aeac": {"__data__": {"id_": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "node_type": "1", "metadata": {}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "156f0050-4cf2-4295-8684-c1d16f36c709", "node_type": "1", "metadata": {}, "hash": "39e6a05c133d33dae399ddb62ba9101934294dab2a9578a00b26ba8107b37ec4", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "663fb4ad-8fa4-4850-b686-345f93efcd36", "node_type": "1", "metadata": {}, "hash": "dc7e700360a7f0d3a00b1183494e05abcc251c8c67f4f04369258d8bef6e3348", "class_name": "RelatedNodeInfo"}, {"node_id": "5cc27566-ccb0-4f57-a305-15c89e85721d", "node_type": "1", "metadata": {}, "hash": "7cee225c44e41cd18bcd5c577026c328ef378dc6a294e0e2add55adf428bf410", "class_name": "RelatedNodeInfo"}, {"node_id": "842b2d1f-a046-4f59-9141-9f44f22b7120", "node_type": "1", "metadata": {}, "hash": "f51c1f2c0b5d82cb92870c81aa9c2450061c8ebf56adb8594a1595238d82cad1", "class_name": "RelatedNodeInfo"}, {"node_id": "72843296-d34d-46e7-afc0-7dfd104a6dc5", "node_type": "1", "metadata": {}, "hash": "81e62fe5696cc1e43d8bdadf02519521ad9ea2632944153e74037cae7ebaaa1b", "class_name": "RelatedNodeInfo"}]}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "text": "If `child_branch_factor=2`, a query\nchooses two child nodes per level.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Keyword Table Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe keyword table index extracts keywords from each Node and builds a mapping from \neach keyword to the corresponding Nodes of that keyword.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Querying\nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDuring query time, we extract relevant keywords from the query, and match those with pre-extracted\nNode keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our \nResponse Synthesis module.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Introduction\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn many cases, especially with long documents, a chunk of text may lack the context necessary to disambiguate the chunk from other similar chunks of text. \n\nTo combat this, we use LLMs to extract certain contextual information relevant to the document to better help the retrieval and language models disambiguate similar-looking passages.\n\nWe show this in an example notebook and demonstrate its effectiveness in processing long documents.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFirst, we define a metadata extractor that takes in a list of feature extractors that will be processed in sequence.\n\nWe then feed this to the node parser, which will add the additional metadata to each node.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index.node_parser.extractors import (\n    MetadataExtractor,\n    SummaryExtractor,\n    QuestionsAnsweredExtractor,\n    TitleExtractor,\n    KeywordExtractor,\n)\n\nmetadata_extractor = MetadataExtractor(\n    extractors=[\n        TitleExtractor(nodes=5),\n        QuestionsAnsweredExtractor(questions=3),\n        SummaryExtractor(summaries=[\"prev\", \"self\"]),\n        KeywordExtractor(keywords=10),\n    ],\n)\n\nnode_parser = SimpleNodeParser(\n    metadata_extractor=metadata_extractor,\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere is an sample of extracted metadata:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n{'page_label': '2',\n 'file_name': '10k-132.pdf',\n 'document_title': 'Uber Technologies, Inc. 2019 Annual Report: Revolutionizing Mobility and Logistics Across 69 Countries and 111 Million MAPCs with $65 Billion in Gross Bookings',\n 'questions_this_excerpt_can_answer': '\\n\\n1. How many countries does Uber Technologies, Inc. operate in?\\n2. What is the total number of MAPCs served by Uber Technologies, Inc.?\\n3. How much gross bookings did Uber Technologies, Inc. generate in 2019?',\n 'prev_section_summary': \"\\n\\nThe 2019 Annual Report provides an overview of the key topics and entities that have been important to the organization over the past year. These include financial performance, operational highlights, customer satisfaction, employee engagement, and sustainability initiatives. It also provides an overview of the organization's strategic objectives and goals for the upcoming year.\",\n 'section_summary': '\\nThis section discusses a global tech platform that serves multiple multi-trillion dollar markets with products leveraging core technology and infrastructure. It enables consumers and drivers to tap a button and get a ride or work. The platform has revolutionized personal mobility with ridesharing and is now leveraging its platform to redefine the massive meal delivery and logistics industries. The foundation of the platform is its massive network, leading technology, operational excellence, and product expertise.", "start_char_idx": 187384, "end_char_idx": 193033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "156f0050-4cf2-4295-8684-c1d16f36c709": {"__data__": {"id_": "156f0050-4cf2-4295-8684-c1d16f36c709", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "node_type": "1", "metadata": {}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93d40a64-cd24-4423-9b76-38270c080399", "node_type": "1", "metadata": {}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "32e90751-5bfc-459b-8414-aa67fa4a00c1", "node_type": "1", "metadata": {}, "hash": "de244f1bb4a5f5e43bd48ef8d9ac0fbac11ab4526a5b7e0c221581b4dd58a504", "class_name": "RelatedNodeInfo"}, {"node_id": "21da08f4-4d26-4ac5-9798-5c10f3b8333c", "node_type": "1", "metadata": {}, "hash": "266f1b1a4687ac0541d1b3e7707efaebe8bf37a974ff73f35f83b1c45d2b7190", "class_name": "RelatedNodeInfo"}]}, "hash": "39e6a05c133d33dae399ddb62ba9101934294dab2a9578a00b26ba8107b37ec4", "text": "The foundation of the platform is its massive network, leading technology, operational excellence, and product expertise.',\n 'excerpt_keywords': '\\nRidesharing, Mobility, Meal Delivery, Logistics, Network, Technology, Operational Excellence, Product Expertise, Point A, Point B'}\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Custom Extractors\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf the provided extractors do not fit your needs, you can also define a custom extractor like so:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Custom Extractors\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser.extractors import MetadataFeatureExtractor\n\nclass CustomExtractor(MetadataFeatureExtractor):\n    def extract(self, nodes) -> List[Dict]:\n        metadata_list = [\n            {\n                \"custom\": node.metadata[\"document_title\"]\n                + \"\\n\"\n                + node.metadata[\"excerpt_keywords\"]\n            }\n            for node in nodes\n        ]\n        return metadata_list\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Custom Extractors\nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn a more advanced example, it can also make use of an `llm_predictor` to extract features from the node content and the existing metadata. Refer to the source code of the provided metadata extractors for more details.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\modules.md\nContent Type: code\nHeader Path: Module Guides\nfile_path: Docs\\core_modules\\data_modules\\index\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 552\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\nvector_store_guide.ipynb\nList Index <./index_guide.md>\nTree Index <./index_guide.md>\nKeyword Table Index <./index_guide.md>\n/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.ipynb\n/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.ipynb\nSQL Index </examples/index_structs/struct_indices/SQLIndexDemo.ipynb>\n/examples/index_structs/struct_indices/duckdb_sql_query.ipynb\n/examples/index_structs/doc_summary/DocSummary.ipynb\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: text\nHeader Path: Indexes/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn `Index` is a data structure that allows us to quickly retrieve relevant context for a user query.\nFor LlamaIndex, it's the core foundation for retrieval-augmented generation (RAG) use-cases.\n\n\nAt a high-level, `Indices` are built from Documents.\nThey are used to build Query Engines and Chat Engines\nwhich enables question & answer and chat over your data.  \n\nUnder the hood, `Indices` store data in `Node` objects (which represent chunks of the original documents), and expose a Retriever interface that supports additional configuration and automation.", "start_char_idx": 192912, "end_char_idx": 196745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93d40a64-cd24-4423-9b76-38270c080399": {"__data__": {"id_": "93d40a64-cd24-4423-9b76-38270c080399", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "156f0050-4cf2-4295-8684-c1d16f36c709", "node_type": "1", "metadata": {}, "hash": "39e6a05c133d33dae399ddb62ba9101934294dab2a9578a00b26ba8107b37ec4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c521edf8-0e9f-40d7-a411-07c241fb6733", "node_type": "1", "metadata": {}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "fee806fd-9986-4970-aa40-62f2682f5601", "node_type": "1", "metadata": {}, "hash": "73bd043351bab60887420a6612dc1cd170f99f6845042d3d1ec993d81ed8d4d5", "class_name": "RelatedNodeInfo"}, {"node_id": "4f4d8270-900d-42eb-9090-f93304ecac87", "node_type": "1", "metadata": {}, "hash": "49bdd696a8dd831f0875c4b30a6ac779e0a4496c507fbbb06ecbc9de536c2d7c", "class_name": "RelatedNodeInfo"}, {"node_id": "b3a4aeb7-f12f-44fb-bb8b-206d05e237b0", "node_type": "1", "metadata": {}, "hash": "a86052b9a40eed5699529d04d8f3e61be99c3d8a84adc1c9db3e8f736efdd6f2", "class_name": "RelatedNodeInfo"}, {"node_id": "f7c25916-5a5c-4dff-8eb6-0e33191ae75e", "node_type": "1", "metadata": {}, "hash": "cf54d456d9c4a0a91438169886481c6c8b799e8c728bf8e82d956f3ba2eeee75", "class_name": "RelatedNodeInfo"}]}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "text": "For a more in-depth explanation, check out our guide below:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: text\nHeader Path: Indexes/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nindex_guide.md\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: text\nHeader Path: Indexes/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet started with:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: text\nHeader Path: Indexes/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(docs)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: code\nHeader Path: Indexes/Usage Pattern\nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: code\nHeader Path: Indexes/Modules\nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: code\nHeader Path: Indexes/Advanced Concepts\nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\ncomposability.md\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBuild an index from documents:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(docs)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nTo learn how to load documents, see Data Connectors\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/What is happening under the hood?\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n1. Documents are chunked up and parsed into `Node` objects (which are lightweight abstraction over text str that additional keep track of metadata and relationships).\n2. Additional computation is performed to add `Node` into index data structure\n   > Note: the computation is index-specific.\n   >\n   > - For a vector store index, this means calling an embedding model (via API or locally) to compute embedding for the `Node` objects\n   > - For a document summary index, this means calling an LLM to generate a summary\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Configuring Document Parsing\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe most common configuration you might want to change is how to parse document into `Node` objects.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can configure our service context to use the desired chunk size and set `show_progress` to display a progress bar during index construction.", "start_char_idx": 196747, "end_char_idx": 202054, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c521edf8-0e9f-40d7-a411-07c241fb6733": {"__data__": {"id_": "c521edf8-0e9f-40d7-a411-07c241fb6733", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93d40a64-cd24-4423-9b76-38270c080399", "node_type": "1", "metadata": {}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6", "node_type": "1", "metadata": {}, "hash": "0b63e68ac1162371428a3f1cad68e21b77b247d754a513b382d57787363ec247", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f6aeac0a-9ebc-4847-8c85-8cbbee3e663b", "node_type": "1", "metadata": {}, "hash": "eaebff7ca74ac43bdae8e016fef5595a602ffff7f234b033696d5a792138d5f6", "class_name": "RelatedNodeInfo"}, {"node_id": "31da94f6-90b8-4595-8f2e-88d702dac292", "node_type": "1", "metadata": {}, "hash": "1a579ecaba612f5beaf6b3b398740c015052f070368442c7f4a66db4c11749b4", "class_name": "RelatedNodeInfo"}, {"node_id": "b5cbe7b5-d45d-474b-bfcc-10fe061780a2", "node_type": "1", "metadata": {}, "hash": "1a833459ccf16f1e2c55e4f89267d4bf797f82165a6e1e2dc2c04afe8ddeb498", "class_name": "RelatedNodeInfo"}, {"node_id": "7053c6b9-a453-48d7-aefc-2353774c4bbc", "node_type": "1", "metadata": {}, "hash": "bc18fe21506dfd58ed0494542fac1314ba3acd82a8a84fdac905e3be7d4ea003", "class_name": "RelatedNodeInfo"}]}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "text": "File Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, VectorStoreIndex\n\nservice_context = ServiceContext.from_defaults(chunk_size=512)\nindex = VectorStoreIndex.from_documents(\n    docs,\n    service_context=service_context,\n    show_progress=True\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note: While the high-level API optimizes for ease-of-use, it does _NOT_ expose full range of configurability.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use the low-level composition API if you need more granular control.\n\nHere we show an example where you want to both modify the text chunk size, disable injecting metadata, and disable creating `Node` relationships.  \nThe steps are:\n\n1. Configure a node parser\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser import SimpleNodeParser\n\nparser = SimpleNodeParser.from_defaults(\n    chunk_size=512,\n    include_extra_info=False,\n    include_prev_next_rel=False,\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n2. Parse document into `Node` objects\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n3. build index from `Node` objects\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Handling Document Update\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRead more about how to deal with data sources that change over time with `Index` **insertion**, **deletion**, **update**, and **refresh** operations.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Handling Document Update\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nmetadata_extraction.md\ndocument_management.md\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nContent Type: text\nHeader Path: Node Parser/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1000\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNode parsers are a simple abstraction that take a list of documents, and chunk them into `Node` objects, such that each node is a specific size. When a document is broken into nodes, all of it's attributes are inherited to the children nodes (i.e. `metadata`, text and metadata templates, etc.). You can read more about `Node` and `Document` properies here.\n\nA node parser can configure the chunk size (in tokens) as well as any overlap between chunked nodes.", "start_char_idx": 202056, "end_char_idx": 207632, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8054fe31-12d5-45a2-93e6-2592350a9cc6": {"__data__": {"id_": "8054fe31-12d5-45a2-93e6-2592350a9cc6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c521edf8-0e9f-40d7-a411-07c241fb6733", "node_type": "1", "metadata": {}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "node_type": "1", "metadata": {}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "68cd4ecc-145c-4942-a673-54b67b2ca4c5", "node_type": "1", "metadata": {}, "hash": "ecc749583cacaa8dd70b6c69d326f70903f4cfa784adcba48d3474bfb22ce0a4", "class_name": "RelatedNodeInfo"}, {"node_id": "ad95958f-2a96-4b93-be43-2c1a8a62ca06", "node_type": "1", "metadata": {}, "hash": "842da23bd464499fcd5d82052e7c87bd3459254b09471826660624e91f224a8f", "class_name": "RelatedNodeInfo"}, {"node_id": "f0a628c5-0903-4d58-ab2f-171f2f0b930e", "node_type": "1", "metadata": {}, "hash": "04c885c289aff7aa1f418911466ca64120ec5c762d462bddb5b6fedf321eadaf", "class_name": "RelatedNodeInfo"}]}, "hash": "0b63e68ac1162371428a3f1cad68e21b77b247d754a513b382d57787363ec247", "text": "The chunking is done by using a `TokenTextSplitter`, which default to a chunk size of 1024 and a default chunk overlap of 20 tokens.\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nContent Type: code\nHeader Path: Node Parser/Usage Pattern\nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1000\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index.node_parser import SimpleNodeParser\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n```\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nContent Type: text\nHeader Path: Node Parser/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1000\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can find more usage details and availbale customization options below.\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nContent Type: text\nHeader Path: Node Parser/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1000\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNode parsers can be used on their own:\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Document\nfrom llama_index.node_parser import SimpleNodeParser\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n\nnodes = node_parser.get_nodes_from_documents([Document(text=\"long text\")], show_progress=False)\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOr set inside a `ServiceContext` to be used automatically when an index is constructed using `.from_documents()`:\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext\nfrom llama_index.node_parser import SimpleNodeParser\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\nservice_context = ServiceContext.from_defaults(node_parser=node_parser)\n\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThere are several options available to customize:\n\n- `text_spliiter` (defaults to `TokenTextSplitter`) - the text splitter used to split text into chunks.\n- `include_metadata` (defaults to `True`) - whether or not `Node`s should inherit the document metadata.\n- `include_prev_next_rel` (defaults to `True`) - whether or not to include previous/next relationships between chunked `Node`s\n- `metadata_extractor` (defaults to `None`) - extra processing to extract helpful metadata. See here for details.\n\nIf you don't want to change the `text_splitter`, you can use `SimpleNodeParser.from_defaults()` to easily change the chunk size and chunk overlap. The defaults are 1024 and 20 respectively.", "start_char_idx": 207633, "end_char_idx": 212371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03": {"__data__": {"id_": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6", "node_type": "1", "metadata": {}, "hash": "0b63e68ac1162371428a3f1cad68e21b77b247d754a513b382d57787363ec247", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "node_type": "1", "metadata": {}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0ce373c6-b769-4ae8-9dc6-74d32bee6687", "node_type": "1", "metadata": {}, "hash": "8b90438801cc9dbd84fb3e50a212b4650e2b07903df9e9402019bf589f49c4aa", "class_name": "RelatedNodeInfo"}, {"node_id": "89724f49-aa7d-4b16-a4ce-d074cdf4fb9d", "node_type": "1", "metadata": {}, "hash": "edbfd0cac5f3dc4426dfb6247ae65858b3697f7a65c02f34476bbad97fd52059", "class_name": "RelatedNodeInfo"}, {"node_id": "0ccbc9fc-beff-42f9-a50d-7ff02530331a", "node_type": "1", "metadata": {}, "hash": "9116082ff49e02072354f70761a66a65d3d24dc933cd82674fef4cfeb21b06e8", "class_name": "RelatedNodeInfo"}, {"node_id": "3629e574-e58e-476f-9354-2a8b1649d84b", "node_type": "1", "metadata": {}, "hash": "401b008930bd9609833297090654f8db126973a44ee380bb5cacc798d16b44fa", "class_name": "RelatedNodeInfo"}]}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "text": "The defaults are 1024 and 20 respectively.\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser import SimpleNodeParser\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Text Splitter Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you do customize the `text_splitter` from the default `TokenTextSplitter`, you can use any splitter from langchain, or optionally our `SentenceSplitter`. Each text splitter has options for the default seperator, as well as options for backup seperators. These are useful for languages that are sufficiently different from English.\n\n`TokenTextSplitter` configuration:\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Text Splitter Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(\n  seperator=\" \",\n  chunk_size=1024,\n  chunk_overlap=20,\n  backup_seperators=[\"\\n\"]\n)\n\nnode_parser = SimpleNodeParser(text_splitter=text_splitter)\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Text Splitter Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n`SentenceSplitter` configuration:\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Text Splitter Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.langchain_helpers.text_splitter import SentenceSplitter\n\ntext_splitter = SentenceSplitter(\n  seperator=\" \",\n  chunk_size=1024,\n  chunk_overlap=20,\n  backup_seperators=[\"\\n\"],\n  paragraph_seperator=\"\\n\\n\\n\"\n)\n\nnode_parser = SimpleNodeParser(text_splitter=text_splitter)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LlamaIndex hides away the complexities and let you query your data in under 5 lines of code:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Summarize the documents.\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUnder the hood, LlamaIndex also supports a swappable **storage layer** that allows you to customize where ingested documents (i.e., `Node` objects), embedding vectors, and index metadata are stored.\n\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo do this, instead of the high-level API,\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.", "start_char_idx": 212329, "end_char_idx": 218025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1": {"__data__": {"id_": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "node_type": "1", "metadata": {}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8779a8b9-760f-4de7-b49c-0df95d1bcd8b", "node_type": "1", "metadata": {}, "hash": "21215dc15b2e69800bb35d61983507f66418ee4c56d71e753d07733816b50eab", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "08c56a70-26b6-41d3-bee8-2e0e3e0f81de", "node_type": "1", "metadata": {}, "hash": "1609017a897c407245e9b4f340c0dbe47028394db382e54a486e2a563ed56276", "class_name": "RelatedNodeInfo"}, {"node_id": "bd3420d8-9621-49b1-acfc-52208e8049bf", "node_type": "1", "metadata": {}, "hash": "483a3ec6e425186e688ef85dbffff201b59c1f10119fc004fa3acac12c906aa2", "class_name": "RelatedNodeInfo"}, {"node_id": "5bcc33da-c757-4943-a190-3856c64392f1", "node_type": "1", "metadata": {}, "hash": "099c091dda7abb29b801b9b1aac6151d186fa4db49063e905c23b07b22027e77", "class_name": "RelatedNodeInfo"}, {"node_id": "f1e23c7e-3eb7-4888-a2b8-fe8975b68a8c", "node_type": "1", "metadata": {}, "hash": "0cd1b9efdace84bb7f5e30d274e06a0a4390855ce59b70e1df84c3dad331e0e1", "class_name": "RelatedNodeInfo"}]}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "text": "md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwe use a lower-level API that gives more granular control:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.docstore import SimpleDocumentStore\nfrom llama_index.storage.index_store import SimpleIndexStore\nfrom llama_index.vector_stores import SimpleVectorStore\nfrom llama_index.node_parser import SimpleNodeParser\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create parser and parse document into nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create storage context using default stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore(),\n    vector_store=SimpleVectorStore(),\n    index_store=SimpleIndexStore(),\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create (or load) docstore and add nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context.docstore.add_documents(nodes)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/save index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/can also set index_id to save multiple indexes to the same folder\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.set_index_id = \"<index_id>\"\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/this will loaded the persisted stores from persist_dir\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(\n    persist_dir=\"<persist_dir>\"\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/then load the index object\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import load_index_from_storage\nloaded_index = load_index_from_storage(storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/if loading an index from a persist_dir containing multiple indexes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nloaded_index = load_index_from_storage(storage_context, index_id=\"<index_id>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/if loading multiple indexes from a persist dir\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nloaded_indicies = load_index_from_storage(storage_context, index_ids=[\"<index_id>\", .])\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8779a8b9-760f-4de7-b49c-0df95d1bcd8b": {"__data__": {"id_": "8779a8b9-760f-4de7-b49c-0df95d1bcd8b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "node_type": "1", "metadata": {}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "node_type": "1", "metadata": {}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e805909e-8d09-43df-a9c6-dc53f9d770b6", "node_type": "1", "metadata": {}, "hash": "21215dc15b2e69800bb35d61983507f66418ee4c56d71e753d07733816b50eab", "class_name": "RelatedNodeInfo"}]}, "hash": "21215dc15b2e69800bb35d61983507f66418ee4c56d71e753d07733816b50eab", "text": ".])\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/if loading multiple indexes from a persist dir\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can customize the underlying storage with a one-line change to instantiate different document stores, index stores, and vector stores.\nSee Document Stores, Vector Stores, Index Stores guides for more details.\n\nFor saving and loading a graph/composable index, see the full guide here.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Vector Store Integrations and Storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMost of our vector store integrations store the entire index (vectors + text) in the vector store itself. This comes with the major benefit of not having to exlicitly persist the index as shown above, since the vector store is already hosted and persisting the data in our index.", "start_char_idx": 223503, "end_char_idx": 224847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c": {"__data__": {"id_": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8779a8b9-760f-4de7-b49c-0df95d1bcd8b", "node_type": "1", "metadata": {}, "hash": "21215dc15b2e69800bb35d61983507f66418ee4c56d71e753d07733816b50eab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd1ce237-0bf3-493e-b318-08c55024cba4", "node_type": "1", "metadata": {}, "hash": "f4c3c478c1f46a44b06ef0f2776cc3dc4bdd684b0676026672b09fdb6e79e7fc", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f89aa930-dca9-40e5-b5ef-8f084a3ac5e4", "node_type": "1", "metadata": {}, "hash": "c3d3c2035f941135406c79f196342970c8cdf29e09d9b663dad708de1de443c5", "class_name": "RelatedNodeInfo"}, {"node_id": "28fd1901-5401-42bb-8b1d-26ae73c1afad", "node_type": "1", "metadata": {}, "hash": "82d8b41e6aaa0ec8aa7d6c9b746e4804da099500ec01dc32de606c1a6a2c1724", "class_name": "RelatedNodeInfo"}, {"node_id": "2ed1fe30-5069-4a0e-89cf-347e756b4cdb", "node_type": "1", "metadata": {}, "hash": "145145b4260658c2dc929ff9eb6687246cbcad9c9120024d2d72246e4ee7fb2e", "class_name": "RelatedNodeInfo"}, {"node_id": "3ed20e50-d408-40db-b36b-8488d813aafd", "node_type": "1", "metadata": {}, "hash": "93697686d45eb7ea3455180d3dd74b356d0e2f13a39e92d4c01e8d6d4a22b450", "class_name": "RelatedNodeInfo"}]}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "text": "The vector stores that support this practice are:\n\n- ChatGPTRetrievalPluginClient\n- ChromaVectorStore\n- DocArrayHnswVectorStore\n- DocArrayInMemoryVectorStore\n- LanceDBVectorStore\n- MetalVectorStore\n- MilvusVectorStore\n- MyScaleVectorStore\n- OpensearchVectorStore\n- PineconeVectorStore\n- QdrantVectorStore\n- RedisVectorStore\n- WeaviateVectorStore\n\nA small example using Pinecone is below:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Vector Store Integrations and Storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport pinecone\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores import PineconeVectorStore\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Creating a Pinecone index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\napi_key = \"api_key\"\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\npinecone.create_index(\n    \"quickstart\",\n    dimension=1536,\n    metric=\"euclidean\",\n    pod_type=\"p1\"\n)\nindex = pinecone.Index(\"quickstart\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/construct vector store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = PineconeVectorStore(pinecone_index=index)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/load documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create index, which will insert documents/vectors to pinecone\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create index, which will insert documents/vectors to pinecone\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you have an existing vector store with data already loaded in, \nyou can connect to it and directly create a `VectorStoreIndex` as follows:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create index, which will insert documents/vectors to pinecone\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = pinecone.Index(\"quickstart\")\nvector_store = PineconeVectorStore(pinecone_index=index)\nloaded_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDocument stores contain ingested document chunks, which we call `Node` objects.\n\nSee the API Reference for more details.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/Simple Document Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, the `SimpleDocumentStore` stores `Node` objects in-memory. \nThey can be persisted to (and loaded from) disk by calling `docstore.persist()` (and `SimpleDocumentStore.from_persist_path(...)` respectively).", "start_char_idx": 224849, "end_char_idx": 230258, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd1ce237-0bf3-493e-b318-08c55024cba4": {"__data__": {"id_": "cd1ce237-0bf3-493e-b318-08c55024cba4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "node_type": "1", "metadata": {}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45256c7f-e83f-4f1b-9943-5d93df44beef", "node_type": "1", "metadata": {}, "hash": "c43ba2531e2c721dee02361971b76eac954ea5f5ce1a137be507d9353e9dbff2", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "20101a59-a024-4649-bac5-9059bfdd5587", "node_type": "1", "metadata": {}, "hash": "c9c3a3f9648edb351c0e1196a12a68322c4655efbbf17f1fbb6f833f6b0dc5d9", "class_name": "RelatedNodeInfo"}, {"node_id": "b9e67033-f226-4fd3-8b78-793a4ff61056", "node_type": "1", "metadata": {}, "hash": "50b422e96aa5564e04be9d5c3027f84ff7deb1b32cda192a744a140a95e103ac", "class_name": "RelatedNodeInfo"}, {"node_id": "aa47c07e-5519-415a-98b9-998b1eb33242", "node_type": "1", "metadata": {}, "hash": "08dc7f2bba0677767e8d5bede45f4dbb7f1f0a0c01bf5d802bb228e868adbaab", "class_name": "RelatedNodeInfo"}]}, "hash": "f4c3c478c1f46a44b06ef0f2776cc3dc4bdd684b0676026672b09fdb6e79e7fc", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/MongoDB Document Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support MongoDB as an alternative document store backend that persists data as `Node` objects are ingested.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/MongoDB Document Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.docstore import MongoDocumentStore\nfrom llama_index.node_parser import SimpleNodeParser\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create parser and parse document into nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create (or load) docstore and add nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocstore = MongoDocumentStore.from_uri(uri=\"<mongodb+srv://...>\")\ndocstore.add_documents(nodes)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUnder the hood, `MongoDocumentStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your nodes.\n> Note: You can configure the `db_name` and `namespace` when instantiating `MongoDocumentStore`, otherwise they default to `db_name=\"db_docstore\"` and `namespace=\"docstore\"`.\n\nNote that it's not necessary to call `storage_context.persist()` (or `docstore.persist()`) when using an `MongoDocumentStore`\nsince data is persisted by default. \n\nYou can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoDocumentStore` with an existing `db_name` and `collection_name`.\n\nA more complete example can be found here\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/Redis Document Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support Redis as an alternative document store backend that persists data as `Node` objects are ingested.", "start_char_idx": 230260, "end_char_idx": 234421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45256c7f-e83f-4f1b-9943-5d93df44beef": {"__data__": {"id_": "45256c7f-e83f-4f1b-9943-5d93df44beef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd1ce237-0bf3-493e-b318-08c55024cba4", "node_type": "1", "metadata": {}, "hash": "f4c3c478c1f46a44b06ef0f2776cc3dc4bdd684b0676026672b09fdb6e79e7fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538", "node_type": "1", "metadata": {}, "hash": "d86b971b5a48e95f197d11500c39125c644077c162add32d9ebf22631f3252da", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e4e81f8d-42e5-4864-b424-4c84f2be118a", "node_type": "1", "metadata": {}, "hash": "0c608f25c41cb97ed1dc3672bd65676fa06a1a7710e8cd62a25af0d2f5d2dc0c", "class_name": "RelatedNodeInfo"}, {"node_id": "cf6f149b-9c66-4a34-bdbd-10d460b9a92d", "node_type": "1", "metadata": {}, "hash": "3ff483d831c053b457f0da395f385f56345d1a3e5226f6ae1fd633b55e53ea7e", "class_name": "RelatedNodeInfo"}, {"node_id": "ab467314-fca3-4e77-9f26-7cb7378e39c7", "node_type": "1", "metadata": {}, "hash": "ea85eff7d1ae394d2af2b37bd26b6f9566410dab2de06dbdba5735864aa432c0", "class_name": "RelatedNodeInfo"}]}, "hash": "c43ba2531e2c721dee02361971b76eac954ea5f5ce1a137be507d9353e9dbff2", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/Redis Document Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.docstore import RedisDocumentStore\nfrom llama_index.node_parser import SimpleNodeParser\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create parser and parse document into nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create (or load) docstore and add nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocstore = RedisDocumentStore.from_host_and_port(\n  host=\"127.0.0.1\", \n  port=\"6379\", \n  namespace='llama_index'\n)\ndocstore.add_documents(nodes)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/build index\nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUnder the hood, `RedisDocumentStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/docs`.\n> Note: You can configure the `namespace` when instantiating `RedisDocumentStore`, otherwise it defaults `namespace=\"docstore\"`.\n\nYou can easily reconnect to your Redis client and reload the index by re-initializing a `RedisDocumentStore` with an existing `host`, `port`, and `namespace`.\n\nA more complete example can be found here\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIndex stores contains lightweight index metadata (i.e. additional state information created when building an index).\n\nSee the API Reference for more details.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/Simple Index Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LlamaIndex uses a simple index store backed by an in-memory key-value store.\nThey can be persisted to (and loaded from) disk by calling `index_store.persist()` (and `SimpleIndexStore.from_persist_path(...)` respectively).\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/MongoDB Index Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSimilarly to document stores, we can also use `MongoDB` as the storage backend of the index store.", "start_char_idx": 234423, "end_char_idx": 238999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1536d61c-50cb-49fa-97c4-0aae79f4e538": {"__data__": {"id_": "1536d61c-50cb-49fa-97c4-0aae79f4e538", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45256c7f-e83f-4f1b-9943-5d93df44beef", "node_type": "1", "metadata": {}, "hash": "c43ba2531e2c721dee02361971b76eac954ea5f5ce1a137be507d9353e9dbff2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1075c947-32af-4c99-8c75-4f87b980f909", "node_type": "1", "metadata": {}, "hash": "f143b93edef9fa1e87134984f4e10697b652de6b5dba7b3abdb8d8c8a0ec8224", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e66c707a-63e2-4e3d-9b6d-99e26513393e", "node_type": "1", "metadata": {}, "hash": "4496384e8762fd1d55a0f21749b63b5ceb62450032112f0f944aee7a09984405", "class_name": "RelatedNodeInfo"}, {"node_id": "7f07022f-8d04-4293-a052-3d0640606654", "node_type": "1", "metadata": {}, "hash": "0e9a7c16c2b7c49b360f12c6c69bc040388b7b31425491fc2a5a969ff0b5fa0b", "class_name": "RelatedNodeInfo"}, {"node_id": "11f1ac58-f0cb-4b48-a6bc-3a23d4f90a0f", "node_type": "1", "metadata": {}, "hash": "6d7af20b737f1fddf06b00b5da26923b741f5b7b89d3d9ede38dd89302720aee", "class_name": "RelatedNodeInfo"}]}, "hash": "d86b971b5a48e95f197d11500c39125c644077c162add32d9ebf22631f3252da", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/MongoDB Index Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.index_store import MongoIndexStore\nfrom llama_index import VectorStoreIndex\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/create (or load) index store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_store = MongoIndexStore.from_uri(uri=\"<mongodb+srv://...>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/create storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(index_store=index_store)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/or alternatively, load index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import load_index_from_storage\nindex = load_index_from_storage(storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/or alternatively, load index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUnder the hood, `MongoIndexStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your index metadata.\n> Note: You can configure the `db_name` and `namespace` when instantiating `MongoIndexStore`, otherwise they default to `db_name=\"db_docstore\"` and `namespace=\"docstore\"`.\n\nNote that it's not necessary to call `storage_context.persist()` (or `index_store.persist()`) when using an `MongoIndexStore`\nsince data is persisted by default. \n\nYou can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoIndexStore` with an existing `db_name` and `collection_name`.\n\nA more complete example can be found here\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/Redis Index Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support Redis as an alternative document store backend that persists data as `Node` objects are ingested.", "start_char_idx": 239001, "end_char_idx": 242709, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1075c947-32af-4c99-8c75-4f87b980f909": {"__data__": {"id_": "1075c947-32af-4c99-8c75-4f87b980f909", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538", "node_type": "1", "metadata": {}, "hash": "d86b971b5a48e95f197d11500c39125c644077c162add32d9ebf22631f3252da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cccb9e7-9a10-4487-a24a-550528d54562", "node_type": "1", "metadata": {}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "c5838bac-77c4-4377-b9ab-07ea8231d1fe", "node_type": "1", "metadata": {}, "hash": "221a064b4876faef97a9ce2e3adf707e7f3b00e8663733cc56195a2892ccee07", "class_name": "RelatedNodeInfo"}, {"node_id": "0f6091dc-cb54-4eb4-9763-bc6001f95072", "node_type": "1", "metadata": {}, "hash": "467cd5acd425894745fb0986d379da6fd98cd1da8036e9b703e38146dc3adf2d", "class_name": "RelatedNodeInfo"}, {"node_id": "8f721465-dba3-4b27-b3ea-531a2aedb1f4", "node_type": "1", "metadata": {}, "hash": "6b000d10e55e8419055786c0ce8a3099b218896e7ed3f03ec048824ce65aba75", "class_name": "RelatedNodeInfo"}]}, "hash": "f143b93edef9fa1e87134984f4e10697b652de6b5dba7b3abdb8d8c8a0ec8224", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/Redis Index Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.index_store import RedisIndexStore\nfrom llama_index import VectorStoreIndex\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/create (or load) docstore and add nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_store = RedisIndexStore.from_host_and_port(\n  host=\"127.0.0.1\", \n  port=\"6379\", \n  namespace='llama_index'\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/create storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(index_store=index_store)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/or alternatively, load index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import load_index_from_storage\nindex = load_index_from_storage(storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/or alternatively, load index\nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUnder the hood, `RedisIndexStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/index`.\n> Note: You can configure the `namespace` when instantiating `RedisIndexStore`, otherwise it defaults `namespace=\"index_store\"`.\n\nYou can easily reconnect to your Redis client and reload the index by re-initializing a `RedisIndexStore` with an existing `host`, `port`, and `namespace`.\n\nA more complete example can be found here\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\kv_stores.md\nContent Type: text\nHeader Path: Key-Value Stores\nfile_path: Docs\\core_modules\\data_modules\\storage\\kv_stores.md\nfile_name: kv_stores.md\nfile_type: None\nfile_size: 560\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nKey-Value stores are the underlying storage abstractions that power our Document Stores and Index Stores.\n\nWe provide the following key-value stores:\n- **Simple Key-Value Store**: An in-memory KV store. The user can choose to call `persist` on this kv store to persist data to disk.\n- **MongoDB Key-Value Store**: A MongoDB KV store.\n\nSee the API Reference for more details.\n\nNote: At the moment, these storage abstractions are not externally facing.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides a high-level interface for ingesting, indexing, and querying your external data.\n\nUnder the hood, LlamaIndex also supports swappable **storage components** that allows you to customize:\n\n- **Document stores**: where ingested documents (i.e., `Node` objects) are stored,\n- **Index stores**: where index metadata are stored,\n- **Vector stores**: where embedding vectors are stored.\n\nThe Document/Index stores rely on a common Key-Value store abstraction, which is also detailed below.\n\nLlamaIndex supports persisting data to any storage backend supported by fsspec. \nWe have confirmed support for the following storage backends:\n\n- Local filesystem\n- AWS S3\n- Cloudflare R2", "start_char_idx": 242711, "end_char_idx": 247543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cccb9e7-9a10-4487-a24a-550528d54562": {"__data__": {"id_": "2cccb9e7-9a10-4487-a24a-550528d54562", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1075c947-32af-4c99-8c75-4f87b980f909", "node_type": "1", "metadata": {}, "hash": "f143b93edef9fa1e87134984f4e10697b652de6b5dba7b3abdb8d8c8a0ec8224", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96b953dd-0f15-42fa-a00b-ecc0e554907c", "node_type": "1", "metadata": {}, "hash": "df94adb5aa2b2f90688857b542f2b2b10c554a3e169fc64642073cdeeb466041", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7fcf52bd-c5be-4e21-a4fb-93e2af5e06c9", "node_type": "1", "metadata": {}, "hash": "5ba4a14802321b14532588b0d18816fc4438723e1de064828bcf4d6f74c43539", "class_name": "RelatedNodeInfo"}, {"node_id": "f20e29cb-25e5-4d56-a2e2-e2d612ff07f1", "node_type": "1", "metadata": {}, "hash": "ca758cda474e739667ca84f065284d5a83e76b553a932e7aef72925e3776a72b", "class_name": "RelatedNodeInfo"}, {"node_id": "5a8f23cd-6466-4b33-97f8-d52f17345761", "node_type": "1", "metadata": {}, "hash": "673ef040f393b4588ff5efb750a6b54b27832c6565f969fd3b8c7ab59120222f", "class_name": "RelatedNodeInfo"}, {"node_id": "6e27bbfc-12a0-446d-b4e2-feaf0b9c1066", "node_type": "1", "metadata": {}, "hash": "7adbce3a0a26bb7cdb559a38b4f31a24f0839acae29d5a9ead5a84156f03e6b3", "class_name": "RelatedNodeInfo"}]}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "text": "!\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMany vector stores (except FAISS) will store both the data as well as the index (embeddings). This means that you will not need to use a separate document store or index store. This *also* means that you will not need to explicitly persist this data - this happens automatically. Usage would look something like the following to build a new index / reload an existing one.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/build a new index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, StorageContext\nfrom llama_index.vector_stores import DeepLakeVectorStore\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/construct vector store and customize storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = DeepLakeVectorStore(dataset_path=\"<dataset_path>\")\nstorage_context = StorageContext.from_defaults(\n    vector_store = vector_store\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/Load documents and build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/reload an existing one\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/reload an existing one\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee our Vector Store Module Guide below for more details.\n\n\nNote that in general to use storage abstractions, you need to define a `StorageContext` object:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/reload an existing one\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.docstore import SimpleDocumentStore\nfrom llama_index.storage.index_store import SimpleIndexStore\nfrom llama_index.vector_stores import SimpleVectorStore\nfrom llama_index.storage import StorageContext\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/create storage context using default stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore(),\n    vector_store=SimpleVectorStore(),\n    index_store=SimpleIndexStore(),\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/create storage context using default stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMore details on customization/persistence can be found in the guides below.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/create storage context using default stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\ncustomization.md\nsave_load.md\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/Modules\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe offer in-depth guides on the different storage components.", "start_char_idx": 247546, "end_char_idx": 252746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96b953dd-0f15-42fa-a00b-ecc0e554907c": {"__data__": {"id_": "96b953dd-0f15-42fa-a00b-ecc0e554907c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cccb9e7-9a10-4487-a24a-550528d54562", "node_type": "1", "metadata": {}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd7538f0-7953-4038-8b82-42031ff86fb1", "node_type": "1", "metadata": {}, "hash": "a137a1c4a08fb5334a8f0c6127e5140526090e2d54fb96acf6916bc5447361f1", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8a1d7807-aa41-47c8-9dd7-084d4097483a", "node_type": "1", "metadata": {}, "hash": "e5af6cc85b86529fa88691ce6a8899e69674b33a0776939b355b60d8f761bb3d", "class_name": "RelatedNodeInfo"}, {"node_id": "9344c360-d43a-4ece-a321-d6e98c311d4b", "node_type": "1", "metadata": {}, "hash": "751da0c2085d592c95fea605117ed1bef7a545b5ba3e4320496041c35eebe61a", "class_name": "RelatedNodeInfo"}]}, "hash": "df94adb5aa2b2f90688857b542f2b2b10c554a3e169fc64642073cdeeb466041", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/Modules\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nvector_stores.md\ndocstores.md\nindex_stores.md\nkv_stores.md\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Persisting Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LlamaIndex stores data in-memory, and this data can be explicitly persisted if desired:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Persisting Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context.persist(persist_dir=\"<persist_dir>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Persisting Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis will persist data to disk, under the specified `persist_dir` (or `./storage` by default).\n\nMultiple indexes can be persisted and loaded from the same directory, assuming you keep track of index ID's for loading.\n\nUser can also configure alternative storage backends (e.g. `MongoDB`) that persist data by default.\nIn this case, calling `storage_context.persist()` will do nothing.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Loading Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo load data, user simply needs to re-create the storage context using the same configuration (e.g. pass in the same `persist_dir` or vector store client).\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Loading Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n    vector_store=SimpleVectorStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Loading Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can then load specific indices from the `StorageContext` through some convenience functions below.", "start_char_idx": 252748, "end_char_idx": 256341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd7538f0-7953-4038-8b82-42031ff86fb1": {"__data__": {"id_": "fd7538f0-7953-4038-8b82-42031ff86fb1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96b953dd-0f15-42fa-a00b-ecc0e554907c", "node_type": "1", "metadata": {}, "hash": "df94adb5aa2b2f90688857b542f2b2b10c554a3e169fc64642073cdeeb466041", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537", "node_type": "1", "metadata": {}, "hash": "3c4e3f57d8d8eeb02c6f8c3bfc094d56481ac66edba4b1acee241552182c8444", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "5c0a5cf8-a554-4b45-8532-aa25ceb3d911", "node_type": "1", "metadata": {}, "hash": "8e32897bbc6a48ce40450a1c0b1f9070ce5fc9595c6332ac90cb1cd725bb834c", "class_name": "RelatedNodeInfo"}, {"node_id": "a11f277d-1983-4066-aace-321e61588680", "node_type": "1", "metadata": {}, "hash": "3d6ccc61dd186370a04136a0d91cd64600da5e16f14ee8d151ce165654aaf185", "class_name": "RelatedNodeInfo"}, {"node_id": "711f400f-f144-4aba-bbd7-f0ef888b36bf", "node_type": "1", "metadata": {}, "hash": "172771c06ba470d8001018cf84a886ba8c5ad955e1a4231210204044ab802636", "class_name": "RelatedNodeInfo"}]}, "hash": "a137a1c4a08fb5334a8f0c6127e5140526090e2d54fb96acf6916bc5447361f1", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Loading Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import load_index_from_storage, load_indices_from_storage, load_graph_from_storage\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/need to specify index_id if multiple indexes are persisted to the same directory\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = load_index_from_storage(storage_context, index_id=\"<index_id>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/don't need to specify index_id if there's only one index in storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = load_index_from_storage(storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load multiple indices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindices = load_indices_from_storage(storage_context) # loads all indices\nindices = load_indices_from_storage(storage_context, index_ids=[index_id1, ...]) # loads specific indices\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load composable graph\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph = load_graph_from_storage(storage_context, root_id=\"<root_id>\") # loads graph with the specified root_id\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load composable graph\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere's the full API Reference on saving and loading.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Using a remote backend\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LlamaIndex uses a local filesystem to load and save files. However, you can override this by passing a `fsspec.AbstractFileSystem` object.\n\nHere's a simple example, instantiating a vector store:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Using a remote backend\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport dotenv\nimport s3fs\nimport os\ndotenv.load_dotenv(\"../../../.env\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../../../examples/paul_graham_essay/data/').load_data()\nprint(len(documents))\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAt this point, everything has been the same. Now - let's instantiate a S3 filesystem and save / load from there.", "start_char_idx": 256343, "end_char_idx": 261153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54ef6afd-0d9a-4358-a1dc-d317c9225537": {"__data__": {"id_": "54ef6afd-0d9a-4358-a1dc-d317c9225537", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd7538f0-7953-4038-8b82-42031ff86fb1", "node_type": "1", "metadata": {}, "hash": "a137a1c4a08fb5334a8f0c6127e5140526090e2d54fb96acf6916bc5447361f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cae4deaf-d284-4a2f-a529-c51f448188a0", "node_type": "1", "metadata": {}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7165700b-0795-438a-9394-5943ce730da2", "node_type": "1", "metadata": {}, "hash": "6dc836a2c7f279ee3fa90b5adac3c30d676bcce909efb814fdff44b3f7e7dc82", "class_name": "RelatedNodeInfo"}, {"node_id": "4d438c52-7882-4870-b7d7-9786dad54f7f", "node_type": "1", "metadata": {}, "hash": "6bb05212b99c7498a2402e8bf260fc253186bf99ab38924b8b4cd83c714b381b", "class_name": "RelatedNodeInfo"}, {"node_id": "aa59b324-bced-42a5-bfc5-4d9e93f4e35d", "node_type": "1", "metadata": {}, "hash": "05f331d9ad376f3460e7c4795db651d3024011e60ec7d580a16401d97ede21a3", "class_name": "RelatedNodeInfo"}]}, "hash": "3c4e3f57d8d8eeb02c6f8c3bfc094d56481ac66edba4b1acee241552182c8444", "text": "Now - let's instantiate a S3 filesystem and save / load from there.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/set up s3fs\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAWS_KEY = os.environ['AWS_ACCESS_KEY_ID']\nAWS_SECRET = os.environ['AWS_SECRET_ACCESS_KEY']\nR2_ACCOUNT_ID = os.environ['R2_ACCOUNT_ID']\n\nassert AWS_KEY is not None and AWS_KEY != \"\"\n\ns3 = s3fs.S3FileSystem(\n   key=AWS_KEY,\n   secret=AWS_SECRET,\n   endpoint_url=f'https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com',\n   s3_additional_kwargs={'ACL': 'public-read'}\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/save index to remote blob storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.set_index_id(\"vector_index\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/this is {bucket_name}/{index_name}\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.storage_context.persist('llama-index/storage_demo', fs=s3)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load index from s3\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nsc = StorageContext.from_defaults(persist_dir='llama-index/storage_demo', fs=s3)\nindex2 = load_index_from_storage(sc, 'vector_index')\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load index from s3\nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, if you do not pass a filesystem, we will assume a local filesystem.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nVector stores contain embedding vectors of ingested document chunks \n(and sometimes the document chunks as well).\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Simple Vector Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LlamaIndex uses a simple in-memory vector store that's great for quick experimentation.\nThey can be persisted to (and loaded from) disk by calling `vector_store.persist()` (and `SimpleVectorStore.from_persist_path(...)` respectively).\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Third-Party Vector Store Integrations\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe also integrate with a wide range of vector store implementations. \nThey mainly differ in 2 aspects:\n1. in-memory vs. hosted\n2. stores only vector embeddings vs.", "start_char_idx": 261086, "end_char_idx": 265227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cae4deaf-d284-4a2f-a529-c51f448188a0": {"__data__": {"id_": "cae4deaf-d284-4a2f-a529-c51f448188a0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537", "node_type": "1", "metadata": {}, "hash": "3c4e3f57d8d8eeb02c6f8c3bfc094d56481ac66edba4b1acee241552182c8444", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "node_type": "1", "metadata": {}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "4e11970d-2737-408a-9880-82efb9f30500", "node_type": "1", "metadata": {}, "hash": "4420f98742af652388587db35a0cf3531de04a164b805fdd4ae86ca6fcbf5d70", "class_name": "RelatedNodeInfo"}, {"node_id": "b02504ef-9496-4b5d-9803-34e6bb4c4b20", "node_type": "1", "metadata": {}, "hash": "f7df85472e7839c85ffd3bfc64fd2fd59a5ec4e80b0ab781a50d1378e298025a", "class_name": "RelatedNodeInfo"}, {"node_id": "665b86e0-639d-4207-a483-3a874ea9d39a", "node_type": "1", "metadata": {}, "hash": "a2cac16372549a7ad03186064cd451e496bcdd13ba07a467b4df9c17dc3fb87b", "class_name": "RelatedNodeInfo"}, {"node_id": "3fc3f90d-1a37-403c-a257-0f84b3e46ea0", "node_type": "1", "metadata": {}, "hash": "8e601881c37eb7ae0a17b019926cd4e4f4fbff7c4e1223659c4ead5870598b1c", "class_name": "RelatedNodeInfo"}]}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "text": "hosted\n2. stores only vector embeddings vs. also stores documents\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Third-Party Vector Store Integrations/In-Memory Vector Stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* Faiss\n* Chroma\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Third-Party Vector Store Integrations/(Self) Hosted Vector Stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* Pinecone\n* Weaviate\n* Milvus/Zilliz\n* Qdrant\n* Chroma\n* Opensearch\n* DeepLake\n* MyScale\n* Tair\n* DocArray\n* MongoDB Atlas\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Third-Party Vector Store Integrations/Others\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* ChatGPTRetrievalPlugin\n\nFor more details, see Vector Store Integrations.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Third-Party Vector Store Integrations/Others\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\ncaption: Examples\nmaxdepth: 1\n---\n/examples/vector_stores/SimpleIndexDemo.ipynb\n/examples/vector_stores/QdrantIndexDemo.ipynb\n/examples/vector_stores/FaissIndexDemo.ipynb\n/examples/vector_stores/DeepLakeIndexDemo.ipynb\n/examples/vector_stores/MyScaleIndexDemo.ipynb\n/examples/vector_stores/MetalIndexDemo.ipynb\n/examples/vector_stores/WeaviateIndexDemo.ipynb\n/examples/vector_stores/OpensearchDemo.ipynb\n/examples/vector_stores/PineconeIndexDemo.ipynb\n/examples/vector_stores/ChromaIndexDemo.ipynb\n/examples/vector_stores/LanceDBIndexDemo.ipynb\n/examples/vector_stores/MilvusIndexDemo.ipynb\n/examples/vector_stores/RedisIndexDemo.ipynb\n/examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb\n/examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb\n/examples/vector_stores/AsyncIndexCreationDemo.ipynb\n/examples/vector_stores/TairIndexDemo.ipynb\n/examples/vector_stores/SupabaseVectorIndexDemo.ipynb\n/examples/vector_stores/DocArrayHnswIndexDemo.ipynb\n/examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb\n/examples/vector_stores/MongoDBAtlasVectorSearch.ipynb\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\modules.md\nContent Type: text\nHeader Path: Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 300\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support integrations with OpenAI, Azure, and anything LangChain offers.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\modules.md\nContent Type: text\nHeader Path: Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 300\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\n/examples/embeddings/OpenAI.ipynb\n/examples/embeddings/Langchain.ipynb\n/examples/customization/llms/AzureOpenAI.ipynb\n/examples/embeddings/custom_embeddings.ipynb\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Concept\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEmbeddings are used in LlamaIndex to represent your documents using a sophistacted numerical representation. Embedding models take text as input, and return a long list of numbers used to caputre the semantics of the text. These embedding models have been trained to represent text this way, and help enable many applications, including search!\n\nAt a high level, if a user asks a question about dogs, then the embedding for that question will be highly similar to text that talks about dogs.\n\nWhen calculating the similarity between embeddings, there are many methods to use (dot product, cosine similarity, etc.). By default, LlamaIndex uses cosine similarity when comparing embeddings.\n\nThere are many embedding models to pick from. By default, LlamaIndex uses `text-embedding-ada-002` from OpenAI. We also support any embedding model offered by Langchain here, as well as providing an easy to extend base class for implementing your own embeddings.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMost commonly in LlamaIndex, embedding models will be specified in the `ServiceContext` object, and then used in a vector index.", "start_char_idx": 265184, "end_char_idx": 270734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a707dfb3-b0ee-47e4-a50b-c8f43d560b91": {"__data__": {"id_": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cae4deaf-d284-4a2f-a529-c51f448188a0", "node_type": "1", "metadata": {}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf87247e-da28-4d97-827f-4beaef959aa9", "node_type": "1", "metadata": {}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "97faf9c1-4ef6-43f9-b68d-dc9eb12cace0", "node_type": "1", "metadata": {}, "hash": "784c063b454a5de45458286de31877fd6071cd7a3e02e187630c4568c274ea6d", "class_name": "RelatedNodeInfo"}, {"node_id": "6537a827-6b22-4ea0-909d-d40605457dc8", "node_type": "1", "metadata": {}, "hash": "e0e5afa98659fa8ab13de585b76ce8cb3eab601c2759f74736c2ad2ac9e19b0d", "class_name": "RelatedNodeInfo"}, {"node_id": "d74b1e06-4f5d-43f4-9481-acbad0dc7f1a", "node_type": "1", "metadata": {}, "hash": "725f97fcdc31ec7b0903c1e238efd0f669bc136291df750fdb1a5d1d2b295433", "class_name": "RelatedNodeInfo"}, {"node_id": "2b1b51ae-3158-40ed-8b5e-0d17adac50ec", "node_type": "1", "metadata": {}, "hash": "7d01f9c6ad9afdac055dc9bd736169a3df140c124903502169c356163f100df5", "class_name": "RelatedNodeInfo"}]}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "text": "The embedding model will be used to embed the documents used during index construction, as well as embedding any queries you make using the query engine later on.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext\nfrom llama_index.embeddings import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding()\nservice_context = serviceContext.from_defaults(embed_model=embed_model)\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can find more usage details and availbale customization options below.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support integrations with OpenAI, Azure, and anything LangChain offers. Details below.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe most common usage for an embedding model will be setting it in the service context object, and then using it to construct an index and query. The input documents will be broken into nodes, and the emedding model will generate an embedding for each node.\n\nBy default, LlamaIndex will use `text-embedding-ada-002`, which is what the example below manually sets up for you.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding()\nservice_context = serviceContext.from_defaults(embed_model=embed_model)\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/optionally set a global service context to avoid passing it into other objects every time\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import set_global_service_context\nset_global_service_context(service_context)\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/optionally set a global service context to avoid passing it into other objects every time\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen, at query time, the embedding model will be used again to embed the query text.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/optionally set a global service context to avoid passing it into other objects every time\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"query string\")\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Batch Size\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, embeddings requests are sent to OpenAI in batches of 10. For some users, this may (rarely) incur a rate limit.", "start_char_idx": 270735, "end_char_idx": 276475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf87247e-da28-4d97-827f-4beaef959aa9": {"__data__": {"id_": "cf87247e-da28-4d97-827f-4beaef959aa9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "node_type": "1", "metadata": {}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "node_type": "1", "metadata": {}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1c67d148-7cec-4ba2-8970-d6584100e4d9", "node_type": "1", "metadata": {}, "hash": "5191e763e3d59e63fe90a7e4481e9f32306062f82a27ed0f900ef9859324de59", "class_name": "RelatedNodeInfo"}, {"node_id": "b00879d8-1aa8-4cbb-ac15-51f6e436f2e6", "node_type": "1", "metadata": {}, "hash": "6a81ce7d9beaea315c3ef86b11acc06a0a3426535f56d1f01d631a4412e6c88e", "class_name": "RelatedNodeInfo"}, {"node_id": "7f14ce69-f3dd-4132-84b0-206a96d9de9b", "node_type": "1", "metadata": {}, "hash": "cd493143f16f2bd00085287f100bef08839972dc19881af1b5e715725a7591d3", "class_name": "RelatedNodeInfo"}, {"node_id": "d098b1ae-02c9-4f62-bb8d-f5ed9e288914", "node_type": "1", "metadata": {}, "hash": "7928ced9d521975bdecb52fdf8a980f27dca5e7400724d9e541457bf65a58a0a", "class_name": "RelatedNodeInfo"}]}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "text": "For some users, this may (rarely) incur a rate limit. For other users embedding many documents, this batch size may be too small.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/set the batch size to 42\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nembed_model = OpenAIEmbedding(embed_batch_size=42)\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Embedding Model Integrations\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe also support any embeddings offered by Langchain here, using our `LangchainEmbedding` wrapper class.\n\nThe example below loads a model from Hugging Face, using Langchain's embedding class.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Embedding Model Integrations\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom llama_index import LangchainEmbedding, ServiceContext\n\nembed_model = LangchainEmbedding(\n  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n)\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Custom Embedding Model\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you wanted to use embeddings not offered by LlamaIndex or Langchain, you can also extend our base embeddings class and implement your own!\n\nThe example below uses Instructor Embeddings (install/setup details here), and implements a custom embeddings class. Instructor embeddings work by providing text, as well as \"instructions\" on the domain of the text to embed. This is helpful when embedding text from a very specific and specialized topic.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Custom Embedding Model\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom typing import Any, List\nfrom InstructorEmbedding import INSTRUCTOR\nfrom llama_index.embeddings.base import BaseEmbedding\n\nclass InstructorEmbeddings(BaseEmbedding):\n  def __init__(\n    self, \n    instructor_model_name: str = \"hkunlp/instructor-large\",\n    instruction: str = \"Represent the Computer Science documentation or question:\",\n    **kwargs: Any,\n  ) -> None:\n    self._model = INSTRUCTOR(instructor_model_name)\n    self._instruction = instruction\n    super().__init__(**kwargs)\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n      embeddings = self._model.encode([[self._instruction, query]])\n      return embeddings[0]\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n      embeddings = self._model.encode([[self._instruction, text]])\n      return embeddings[0] \n\n    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n      embeddings = self._model.encode([[self._instruction, text] for text in texts])\n      return embeddings\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Standalone Usage\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also use embeddings as a standalone module for your project, existing application, or general testing and exploration.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Standalone Usage\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nembeddings = embed_model.get_text_embedding(\"It is raining cats and dogs here!\")\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\modules.md\nContent Type: text\nHeader Path: Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 677\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support integrations with OpenAI, Anthropic, Hugging Face, PaLM, and more.", "start_char_idx": 276422, "end_char_idx": 281825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e539d691-178a-42b3-a0a5-e415d86e6bf8": {"__data__": {"id_": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf87247e-da28-4d97-827f-4beaef959aa9", "node_type": "1", "metadata": {}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "node_type": "1", "metadata": {}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "60447b3a-237a-4a20-bdf4-02d73d9c5b70", "node_type": "1", "metadata": {}, "hash": "ffcdd0847d257ba804898c0b2fa7140e035f04baa3c896200d1326b932c69bef", "class_name": "RelatedNodeInfo"}, {"node_id": "8502831f-6b0c-4d66-ae71-6a5e4f0b9158", "node_type": "1", "metadata": {}, "hash": "a8be8269f220de0aab2524e4346305eb52b9ec9a31ef1a0494d5dfb7c4bf1b94", "class_name": "RelatedNodeInfo"}, {"node_id": "4ca2153a-bb41-4cd3-8eba-f9d3efec4352", "node_type": "1", "metadata": {}, "hash": "8d92950d7529071c6f77909e063c182be4240fe75bbbc65c1ee91dc3fc720e6d", "class_name": "RelatedNodeInfo"}, {"node_id": "877e0a46-7397-4bfc-a924-0ac24e0c8eb7", "node_type": "1", "metadata": {}, "hash": "e747b0eba6d307af451ff80eb13761f1ad94165fb7748b5c8237f970f80267b3", "class_name": "RelatedNodeInfo"}]}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "text": "File Name: Docs\\core_modules\\model_modules\\llms\\modules.md\nContent Type: code\nHeader Path: Modules/OpenAI\nfile_path: Docs\\core_modules\\model_modules\\llms\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 677\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/openai.ipynb\n/examples/llm/azure_openai.ipynb\n\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\modules.md\nContent Type: code\nHeader Path: Modules/Anthropic\nfile_path: Docs\\core_modules\\model_modules\\llms\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 677\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/anthropic.ipynb\n\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\modules.md\nContent Type: code\nHeader Path: Modules/Hugging Face\nfile_path: Docs\\core_modules\\model_modules\\llms\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 677\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/customization/llms/SimpleIndexDemo-Huggingface_camel.ipynb\n/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb\n\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\modules.md\nContent Type: code\nHeader Path: Modules/PaLM\nfile_path: Docs\\core_modules\\model_modules\\llms\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 677\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/palm.ipynb\n\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/Concept\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nPicking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.\n\nLLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\n\nLlamaIndex provides a unified interface for defining LLM modules, whether it's from OpenAI, Hugging Face, or LangChain, so that you \ndon't have to write the boilerplate code of defining the LLM interface yourself. This interface consists of the following (more details below):\n- Support for **text completion** and **chat** endpoints (details below)\n- Support for **streaming** and **non-streaming** endpoints\n- Support for **synchronous** and **asynchronous** endpoints\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe following code snippet shows how you can get started using LLMs.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.llms import OpenAI\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/non-streaming\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresp = OpenAI().complete('Paul Graham is ')\nprint(resp)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/non-streaming\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use the LLM as a standalone module or with other LlamaIndex abstractions. Check out our guide below.", "start_char_idx": 281827, "end_char_idx": 286275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe": {"__data__": {"id_": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "node_type": "1", "metadata": {}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "node_type": "1", "metadata": {}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f55e0d52-7922-46fc-adca-747d0dbdb6f5", "node_type": "1", "metadata": {}, "hash": "f361664e23d522a282e75224507b3b5c3aab078fd252e667e1955d86c6f28b29", "class_name": "RelatedNodeInfo"}, {"node_id": "066d5c34-faad-4abb-a00f-6a9ccdf982d7", "node_type": "1", "metadata": {}, "hash": "5d3b290f6b30b050356fe8f6f853d8acd6641f1d19d1ae65477ee6a1143e2d07", "class_name": "RelatedNodeInfo"}, {"node_id": "4eba65f0-8080-4260-8cdf-c806124d8292", "node_type": "1", "metadata": {}, "hash": "b89ec8b6f91dad05f2c0cd5c9a1cce0a1026a979c2b5146cc3a09000d50f3c7e", "class_name": "RelatedNodeInfo"}, {"node_id": "8dc13219-c223-4f21-9482-cdebddf4b7e3", "node_type": "1", "metadata": {}, "hash": "90417415a65efa77c4f9937a7056d6c1864f203c190b075b96e64035af8d6e9f", "class_name": "RelatedNodeInfo"}]}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "text": "Check out our guide below.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/non-streaming\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_standalone.md\nusage_custom.md\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support integrations with OpenAI, Hugging Face, PaLM, and more.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.\n\nBy default, we use OpenAI's `text-davinci-003` model. But you may choose to customize\nthe underlying LLM being used.\n\nBelow we show a few examples of LLM customization. This includes\n\n- changing the underlying LLM\n- changing the number of output tokens (for OpenAI, Cohere, or AI21)\n- having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Changing the underlying LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn example snippet of customizing the LLM being used is shown below.\nIn this example, we use `text-davinci-002` instead of `text-davinci-003`. Available models include `text-davinci-003`,`text-curie-001`,`text-babbage-001`,`text-ada-001`, `code-davinci-002`,`code-cushman-001`. \n\nNote that\nyou may also plug in any LLM shown on Langchain's\nLLM page.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Changing the underlying LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext\n)\nfrom llama_index.llms import OpenAI\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/from langchain.llms import ...\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/define LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(temperature=0, model=\"text-davinci-002\")\nservice_context = ServiceContext.from_defaults(llm=llm)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/build index\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = KeywordTableIndex.from_documents(documents, service_context=service_context)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/get response from query\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do after his time at Y Combinator?\")", "start_char_idx": 286249, "end_char_idx": 291505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b0400e2-b023-44f0-b993-a89f6e3903c6": {"__data__": {"id_": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "node_type": "1", "metadata": {}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a", "node_type": "1", "metadata": {}, "hash": "e993cae5ebc3cb0bd0f20104b76fc036c729a07a74ece8f4a7543bee076343b7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1364720d-f74c-454b-9505-be0425082e42", "node_type": "1", "metadata": {}, "hash": "19f415fe0cfd6406f3eaf29d2e4ecd1159855efe0b943ed3e1c37c6db23ef49f", "class_name": "RelatedNodeInfo"}, {"node_id": "a31672c0-029c-49ea-83ef-ebea6bf21349", "node_type": "1", "metadata": {}, "hash": "121dc53a9f2e5499ebeb227e5703fb5453c6bba4220c06e7c8ef739f6f6dfd8c", "class_name": "RelatedNodeInfo"}, {"node_id": "7ed14c17-a43b-4cbf-9b74-74e78cabdade", "node_type": "1", "metadata": {}, "hash": "1207e634db08727ad7d02e5a12a526f21a967a63fb61db3558eb377dbd0a7cdb", "class_name": "RelatedNodeInfo"}, {"node_id": "fb69fa0a-3c79-4a62-bcd7-ea2a95975d9d", "node_type": "1", "metadata": {}, "hash": "10a939e1204b7a00932dcaa4192e624413f25c91ba32beec24a0fb8bf4bc65f0", "class_name": "RelatedNodeInfo"}]}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "text": "File Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe number of output tokens is usually set to some low number by default (for instance,\nwith OpenAI the default is 256).\n\nFor OpenAI, Cohere, AI21, you just need to set the `max_tokens` parameter\n(or maxTokens for AI21). We will handle text chunking/calculations under the hood.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama_index.llms import OpenAI\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/define LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(temperature=0, model=\"text-davinci-002\", max_tokens=512)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Explicitly configure `context_window` and `num_output`\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you are using other LLM classes from langchain, you may need to explicitly configure the `context_window` and `num_output` via the `ServiceContext` since the information is not available by default.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Explicitly configure `context_window` and `num_output`\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama_index.llms import OpenAI\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/from langchain.llms import ...\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/set context window\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncontext_window = 4096\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/set number of output tokens\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnum_output = 256\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/define LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(\n    temperature=0, \n    model=\"text-davinci-002\", \n    max_tokens=num_output,\n)\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm,\n    context_window=context_window,\n    num_output=num_output,\n)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Using a HuggingFace LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a local embedding model (example here).", "start_char_idx": 291507, "end_char_idx": 296959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bcec9c19-8725-42e9-b91f-a355a6a0389a": {"__data__": {"id_": "bcec9c19-8725-42e9-b91f-a355a6a0389a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "node_type": "1", "metadata": {}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c", "node_type": "1", "metadata": {}, "hash": "c0641090de36ac0cf8d00d2ab57ef7bdbf433a45416289a3918828aa606a42a9", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d78599f9-78e8-4ebc-b7b7-b1d6d7307702", "node_type": "1", "metadata": {}, "hash": "34ccd23fc0d486dc28930baeac3fb8815a7ebdb016b6e058c515fdfd1ed9bdaa", "class_name": "RelatedNodeInfo"}, {"node_id": "47f69943-ccf3-4bf2-864b-b7ed32b85e9c", "node_type": "1", "metadata": {}, "hash": "3269ee61e7262ef3eb6b34fb5ca758b6e00cbdb3e6c64245abda640a0521cd58", "class_name": "RelatedNodeInfo"}, {"node_id": "9876e684-21c6-48a3-bb99-01f70cdb6908", "node_type": "1", "metadata": {}, "hash": "78daf13efc20062afa1ab83d46df286176801cb9177288cb31ed244858c0d8b5", "class_name": "RelatedNodeInfo"}]}, "hash": "e993cae5ebc3cb0bd0f20104b76fc036c729a07a74ece8f4a7543bee076343b7", "text": "Note that for a completely private experience, also setup a local embedding model (example here).\n\nMany open-source models from HuggingFace require either some preamble before before each prompt, which is a `system_prompt`. Additionally, queries themselves may need an additional wrapper around the `query_str` itself. All this information is usually available from the HuggingFace model card for the model you are using.\n\nBelow, this example uses both the `system_prompt` and `query_wrapper_prompt`, using specific prompts from the model card found here.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Using a HuggingFace LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.prompts.prompts import SimpleInputPrompt\n\nsystem_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/This will wrap the default prompts that are internal to llama-index\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n\nimport torch\nfrom llama_index.llms import HuggingFaceLLM\nllm = HuggingFaceLLM(\n    context_window=4096, \n    max_new_tokens=256,\n    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n    system_prompt=system_prompt,\n    query_wrapper_prompt=query_wrapper_prompt,\n    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    device_map=\"auto\",\n    stopping_ids=[50278, 50279, 50277, 1, 0],\n    tokenizer_kwargs={\"max_length\": 4096},\n    # uncomment this if using CUDA to reduce memory usage\n    # model_kwargs={\"torch_dtype\": torch.float16}\n)\nservice_context = ServiceContext.from_defaults(\n    chunk_size=1024, \n    llm=llm,\n)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/This will wrap the default prompts that are internal to llama-index\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSome models will raise errors if all the keys from the tokenizer are passed to the model. A common tokenizer output that causes issues is `token_type_ids`. Below is an example of configuring the predictor to remove this before passing the inputs to the model:\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/This will wrap the default prompts that are internal to llama-index\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHuggingFaceLLM(\n    ...\n    tokenizer_outputs_to_remove=[\"token_type_ids\"]\n)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/This will wrap the default prompts that are internal to llama-index\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full API reference can be found here.\n\nSeveral example notebooks are also listed below:\n\n- StableLM\n- Camel\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Using a Custom LLM Model - Advanced\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo use a custom LLM model, you only need to implement the `LLM` class (or `CustomLLM` for a simpler interface)\nYou will be responsible for passing the text to the model and returning the newly generated tokens.\n\nNote that for a completely private experience, also setup a local embedding model (example here).", "start_char_idx": 296862, "end_char_idx": 302025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c": {"__data__": {"id_": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a", "node_type": "1", "metadata": {}, "hash": "e993cae5ebc3cb0bd0f20104b76fc036c729a07a74ece8f4a7543bee076343b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0a12723-8a55-44cf-ac43-2688613ee018", "node_type": "1", "metadata": {}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8ddc107e-02a7-4d69-a976-a15b237b97f8", "node_type": "1", "metadata": {}, "hash": "60333aca05813aa542a9889f82d07a793cbf6e363466fe9efc3daf97ac3fe8ef", "class_name": "RelatedNodeInfo"}, {"node_id": "82aac71e-f8f6-41b5-86fc-cc647f68c6b6", "node_type": "1", "metadata": {}, "hash": "326faad75b59cd94ed683f37a7a54cda6f310134e282d7356791cbabbdb58768", "class_name": "RelatedNodeInfo"}, {"node_id": "fecefe8c-f135-4ecd-b1f4-bcf27763cbce", "node_type": "1", "metadata": {}, "hash": "bf319d317f552e2c6f9c72efa14daad3e5c627c8e92b2a30184e2d0b3c782053", "class_name": "RelatedNodeInfo"}]}, "hash": "c0641090de36ac0cf8d00d2ab57ef7bdbf433a45416289a3918828aa606a42a9", "text": "Note that for a completely private experience, also setup a local embedding model (example here).\n\nHere is a small example using locally running facebook/OPT model and Huggingface's pipeline abstraction:\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Using a Custom LLM Model - Advanced\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport torch\nfrom transformers import pipeline\nfrom typing import Optional, List, Mapping, Any\n\nfrom llama_index import (\n    ServiceContext, \n    SimpleDirectoryReader, \n    LangchainEmbedding, \n    ListIndex\n)\nfrom llama_index.llms import CustomLLM, CompletionResponse, LLMMetadata\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/set context window size\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncontext_window = 2048\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/set number of output tokens\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnum_output = 256\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/store the pipeline/model outisde of the LLM class to avoid memory issues\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmodel_name = \"facebook/opt-iml-max-30b\"\npipeline = pipeline(\"text-generation\", model=model_name, device=\"cuda:0\", model_kwargs={\"torch_dtype\":torch.bfloat16})\n\nclass OurLLM(CustomLLM):\n\n    @property\n    def metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        return LLMMetadata(\n            context_window=context_window, num_output=num_output\n        )\n\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        prompt_length = len(prompt)\n        response = pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n\n        # only return newly generated tokens\n        text = response[prompt_length:]\n        return CompletionResponse(text=text)\n    \n    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n        raise NotImplementedError()\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/define our LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OurLLM()\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm, \n    context_window=context_window, \n    num_output=num_output\n)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Load the your data\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('./data').load_data()\nindex = ListIndex.from_documents(documents, service_context=service_context)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Query and print response\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"<query_text>\")\nprint(response)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Query and print response\nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUsing this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\n\nNote that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it's capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.\n\nA list of all default internal prompts is available here, and chat-specific prompts are listed here. You can also implement your own custom prompts, as described here.", "start_char_idx": 301928, "end_char_idx": 307575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0a12723-8a55-44cf-ac43-2688613ee018": {"__data__": {"id_": "e0a12723-8a55-44cf-ac43-2688613ee018", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c", "node_type": "1", "metadata": {}, "hash": "c0641090de36ac0cf8d00d2ab57ef7bdbf433a45416289a3918828aa606a42a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8eefb2ba-49eb-4091-a4a8-5bc8a0765803", "node_type": "1", "metadata": {}, "hash": "200ad72b96b0d94fc6de636ea312e3bf1a893561ec921b4c30bab66ccaf688e4", "class_name": "RelatedNodeInfo"}, {"node_id": "a96e2d52-a0a9-4f0b-b7e8-7cbefefa2ff4", "node_type": "1", "metadata": {}, "hash": "dc021e4c0aa0f17c8de2246ff4d4f1fad7123de8c725e7b631f60b3683ef4eb2", "class_name": "RelatedNodeInfo"}, {"node_id": "5ce6af19-6c4e-49aa-8530-527c1776d181", "node_type": "1", "metadata": {}, "hash": "ffb4574b5c3ce9df058b25c71bd2fd49e4a61a3f6d6c80b95a174ca2cf32e030", "class_name": "RelatedNodeInfo"}, {"node_id": "053a299d-174e-4691-be85-64399ab269a1", "node_type": "1", "metadata": {}, "hash": "f81c33c64247c60b01fb32f55770ed732aa5750df6b77c1f7370ab92caa5e4c2", "class_name": "RelatedNodeInfo"}]}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "text": "You can also implement your own custom prompts, as described here.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nContent Type: text\nHeader Path: Using LLMs as standalone modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nfile_name: usage_standalone.md\nfile_type: None\nfile_size: 797\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use our LLM modules on their own.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nContent Type: code\nHeader Path: Using LLMs as standalone modules/using streaming endpoint\nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nfile_name: usage_standalone.md\nfile_type: None\nfile_size: 797\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index.llms import OpenAI\n\nresp = OpenAI().complete('Paul Graham is ')\nprint(resp)\n\nfrom llama_index.llms import OpenAI\nllm = OpenAI()\nresp = llm.stream_complete('Paul Graham is ')\nfor delta in resp:\n    print(delta, end='')\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nContent Type: code\nHeader Path: Using LLMs as standalone modules/Chat Example\nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nfile_name: usage_standalone.md\nfile_type: None\nfile_size: 797\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index.llms import ChatMessage, OpenAI\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a pirate with a colorful personality\"),\n    ChatMessage(role=\"user\", content=\"What is your name\"),\n]\nresp = OpenAI().chat(messages)\nprint(resp)\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nContent Type: text\nHeader Path: Using LLMs as standalone modules/Chat Example\nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nfile_name: usage_standalone.md\nfile_type: None\nfile_size: 797\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCheck out our modules section for usage guides for each LLM.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Concept\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nPrompting is the fundamental input that gives LLMs their expressive power. LlamaIndex uses prompts to build the index, do insertion, \nperform traversal during querying, and to synthesize the final answer.\n\nLlamaIndex uses a set of default prompt templates that work well out of the box.\n\nIn addition, there are some prompts written and used specifically for chat models like `gpt-3.5-turbo` here.\n\nUsers may also provide their own prompt templates to further customize the behavior of the framework. The best method for customizing is copying the default prompt from the link above, and using that as the base for any modifications.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Defining a custom prompt\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDefining a custom prompt is as simple as creating a format string\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Defining a custom prompt\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Prompt\n\ntemplate = (\n    \"We have provided context information below. \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given this information, please answer the question: {query_str}\\n\"\n)\nqa_template = Prompt(template)\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Defining a custom prompt\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note: you may see references to legacy prompt subclasses such as `QuestionAnswerPrompt`, `RefinePrompt`. These have been deprecated (and now are type aliases of `Prompt`). Now you can directly specify `Prompt(template)` to construct custom prompts. But you still have to make sure the template string contains the expected parameters (e.g. `{context_str}` and `{query_str}`) when replacing a default question answer prompt.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSince LlamaIndex is a multi-step pipeline, it's important to identify the operation that you want to modify and pass in the custom prompt at the right place.\n\nAt a high-level, prompts are used in 1) index construction, and 2) query engine execution\n\nThe most commonly used prompts will be the `text_qa_template` and the `refine_template`.", "start_char_idx": 307509, "end_char_idx": 313096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2": {"__data__": {"id_": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0a12723-8a55-44cf-ac43-2688613ee018", "node_type": "1", "metadata": {}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4", "node_type": "1", "metadata": {}, "hash": "1f2969c151e8e590987b04579d8802598ae3d17b17ded6ea334bec5bf6e71f5e", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d0f715d0-e1de-4622-b474-01547e080bf3", "node_type": "1", "metadata": {}, "hash": "06823781c2c32ab0bc68aa74ba87757fa38f888916aeb09364f84e38508844cd", "class_name": "RelatedNodeInfo"}, {"node_id": "0f3972c2-9ca0-4dc0-9c58-481c82e443d3", "node_type": "1", "metadata": {}, "hash": "a9380cb22cffcc8982cbd6077d9973b3bd04dba78ba07fa37357b1d82d85defb", "class_name": "RelatedNodeInfo"}, {"node_id": "19e99400-c7d2-40fb-8687-098392f49704", "node_type": "1", "metadata": {}, "hash": "6223020496945b6321e7968eea6262a58425b2fbae844f1b9fef9cf59f809429", "class_name": "RelatedNodeInfo"}, {"node_id": "1f9963d1-6a38-4220-8617-93a5cb9789bd", "node_type": "1", "metadata": {}, "hash": "6bd034b2de3d298909ba1c958e277076b0f01652d067513d4721b399af3fcd27", "class_name": "RelatedNodeInfo"}, {"node_id": "ecc26c36-b4b2-4a93-80b2-61fb57afdbd0", "node_type": "1", "metadata": {}, "hash": "2ebeecc3087fabe235965196530db325b8ba31cf13dab190b5fae1cd41590f59", "class_name": "RelatedNodeInfo"}]}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "text": "- `text_qa_template` - used to get an initial answer to a query using retrieved nodes\n- `refine_tempalate` - used when the retrieved text does not fit into a single LLM call with `response_mode=\"compact\"` (the default), or when more than one node is retrieved using `response_mode=\"refine\"`. The answer from the first query is inserted as an `existing_answer`, and the LLM must update or repeat the existing answer based on the new context.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in index construction\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDifferent indices use different types of prompts during construction (some don't use prompts at all). \nFor instance, `TreeIndex` uses a `SummaryPrompt` to hierarchically\nsummarize the nodes, and `KeywordTableIndex` uses a `KeywordExtractPrompt` to extract keywords.\n\nThere are two equivalent ways to override the prompts:\n\n1. via the default nodes constructor\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in index construction\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = TreeIndex(nodes, summary_template=<custom_prompt>)\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in index construction\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n2. via the documents constructor.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in index construction\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = TreeIndex.from_documents(docs, summary_template=<custom_prompt>)\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in index construction\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor more details on which index uses which prompts, please visit\nIndex class references.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in query engine\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMore commonly, prompts are used at query-time (i.e. for executing a query against an index and synthesizing the final response). \n\nThere are also two equivalent ways to override the prompts:\n\n1. via the high-level API\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in query engine\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    text_qa_template=<custom_qa_prompt>,\n    refine_template=<custom_refine_prompt>\n)\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in query engine\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n2. via the low-level composition API\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in query engine\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = index.as_retriever()\nsynth = get_response_synthesizer(\n    text_qa_template=<custom_qa_prompt>,\n    refine_template=<custom_refine_prompt>\n)\nquery_engine = RetrieverQueryEngine(retriever, response_synthesizer)\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in query engine\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe two approaches above are equivalent, where 1 is essentially syntactic sugar for 2 and hides away the underlying complexity.", "start_char_idx": 313099, "end_char_idx": 318823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3807ce52-317a-494a-9b60-f0cc6ec111e4": {"__data__": {"id_": "3807ce52-317a-494a-9b60-f0cc6ec111e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec", "node_type": "1", "metadata": {}, "hash": "63458a56aa3fc79bf06244416fc0fdd25738e6a24ac2ffe262277c44e61576bb", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b857577b-3e79-4099-b044-63d4274fbb69", "node_type": "1", "metadata": {}, "hash": "4d7b8806b7407221c2f7901dcfddd52637567150fef24a77cbaeefe87249eefa", "class_name": "RelatedNodeInfo"}, {"node_id": "c0490741-1793-41f1-a0cf-e480098af30b", "node_type": "1", "metadata": {}, "hash": "960effeef9bc1135375a5bb9a31aa500f85ca2ca983d377e12603a5112e49e83", "class_name": "RelatedNodeInfo"}, {"node_id": "618c9782-52ac-4364-b4e9-75de2b313e4f", "node_type": "1", "metadata": {}, "hash": "583a0cfe950528b8bee03b0768b85375db6b699b13f6d153e982f1b331e54dbb", "class_name": "RelatedNodeInfo"}]}, "hash": "1f2969c151e8e590987b04579d8802598ae3d17b17ded6ea334bec5bf6e71f5e", "text": "You might want to use 1 to quickly modify some common parameters, and use 2 to have more granular control.\n\n\nFor more details on which classes use which prompts, please visit\nQuery class references.\n\nCheck out the reference documentation for a full set of all prompts.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: code\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modules\nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/customization/prompts/completion_prompts.ipynb\n/examples/customization/prompts/chat_prompts.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 644\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe provide a few simple implementations to start, with more sophisticated modes coming soon!  \n\nMore specifically, the `SimpleChatEngine` does not make use of a knowledge base, \nwhereas `CondenseQuestionChatEngine` and `ReActChatEngine` make use of a query engine over knowledge base.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 644\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nSimple Chat Engine </examples/chat_engine/chat_engine_repl.ipynb>\nReAct Chat Engine </examples/chat_engine/chat_engine_react.ipynb>\nOpenAI Chat Engine </examples/chat_engine/chat_engine_openai.ipynb>\nCondense Question Chat Engine </examples/chat_engine/chat_engine_condense_question.ipynb>\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nChat engine is a high-level interface for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer).\nThink ChatGPT, but augmented with your knowledge base.  \n\nConceptually, it is a **stateful** analogy of a Query Engine. \nBy keeping track of the conversation history, it can answer questions with past context in mind.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you want to ask standalone question over your data (i.e. without keeping track of conversation history), use Query Engine instead.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet started with:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine = index.as_chat_engine()\nresponse = chat_engine.chat(\"Tell me a joke.\")\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo stream response:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.\")", "start_char_idx": 318824, "end_char_idx": 323566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec": {"__data__": {"id_": "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4", "node_type": "1", "metadata": {}, "hash": "1f2969c151e8e590987b04579d8802598ae3d17b17ded6ea334bec5bf6e71f5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8ea7021-af99-4bc8-b505-811562e86599", "node_type": "1", "metadata": {}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "cad62ac2-f538-411f-8ab6-d6a0c7dc1afd", "node_type": "1", "metadata": {}, "hash": "0a19811f3c89236becd6d45947cb0510b2581e4f2f2d452c2ef2010b8dcaae7f", "class_name": "RelatedNodeInfo"}, {"node_id": "c969a149-819d-44e5-a53e-fe3b9c7bf3b2", "node_type": "1", "metadata": {}, "hash": "713892e8ad7e4ff6fbdb38aab5caa1547496e599a5d0a65afa716e331926027a", "class_name": "RelatedNodeInfo"}]}, "hash": "63458a56aa3fc79bf06244416fc0fdd25738e6a24ac2ffe262277c44e61576bb", "text": "for token in streaming_response.response_gen:\n    print(token, end=\"\")\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: code\nHeader Path: Chat Engine/Usage Pattern\nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBelow you can find corresponding tutorials to see the available chat engines in action.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBuild a chat engine from index:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine = index.as_chat_engine()\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHave a conversation with your data:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = chat_engine.chat(\"Tell me a joke.\")", "start_char_idx": 323567, "end_char_idx": 326804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8ea7021-af99-4bc8-b505-811562e86599": {"__data__": {"id_": "e8ea7021-af99-4bc8-b505-811562e86599", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec", "node_type": "1", "metadata": {}, "hash": "63458a56aa3fc79bf06244416fc0fdd25738e6a24ac2ffe262277c44e61576bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0d05493-b438-49a8-8171-2da3c170a2a4", "node_type": "1", "metadata": {}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8a7ea6ca-717b-4b09-a88b-6def15e14eef", "node_type": "1", "metadata": {}, "hash": "a9d90ab8757e484cf8365d1070837c35750f46c40bf5ea0ed545bc7f7cdc37ae", "class_name": "RelatedNodeInfo"}, {"node_id": "ba35fd49-1e5b-487c-9605-98df0fbd0e46", "node_type": "1", "metadata": {}, "hash": "e665f6e6619f2a34f78d3a1eb9b34e38364adfe3bed1282c8583ef01e8db046d", "class_name": "RelatedNodeInfo"}, {"node_id": "5ba4363f-c41e-4e89-8b36-96ef866d2c2b", "node_type": "1", "metadata": {}, "hash": "24c42c11803af77b67e461a70f092954e86cd52aaa741d245f6a9003f8f44e35", "class_name": "RelatedNodeInfo"}, {"node_id": "457677b5-d8c1-420f-8ea1-83b56cf3b153", "node_type": "1", "metadata": {}, "hash": "277b8262cc061b136630fc8a9650ddcc287d7be7677c6d6d50d42204f7d6397f", "class_name": "RelatedNodeInfo"}]}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "text": "File Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nReset chat history to start a new conversation:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine.reset()\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEnter an interactive chat REPL:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine.chat_repl()\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nConfiguring a chat engine is very similar to configuring a query engine.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can directly build and configure a chat engine from an index in 1 line of code:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine = index.as_chat_engine(\n    chat_mode='condense_question', \n    verbose=True\n)\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note: you can access different chat engines by specifying the `chat_mode` as a kwarg. `condense_question` corresponds to `CondenseQuestionChatEngine`, `react` corresponds to `ReActChatEngine`.\n\n> Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use the low-level composition API if you need more granular control.\nConcretely speaking, you would explicitly construct `ChatEngine` object instead of calling `index.as_chat_engine(...)`.\n> Note: You may need to look at API references or example notebooks.\n\nHere's an example where we configure the following:\n* configure the condense question prompt, \n* initialize the conversation with some existing history,\n* print verbose debug message.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.prompts  import Prompt\n\ncustom_prompt = Prompt(\"\"\"\\\nGiven a conversation (between Human and Assistant) and a follow up message from Human, \\\nrewrite the message to be a standalone question that captures all relevant context \\\nfrom the conversation.", "start_char_idx": 326806, "end_char_idx": 332002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0d05493-b438-49a8-8171-2da3c170a2a4": {"__data__": {"id_": "d0d05493-b438-49a8-8171-2da3c170a2a4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8ea7021-af99-4bc8-b505-811562e86599", "node_type": "1", "metadata": {}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "node_type": "1", "metadata": {}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "52b153c9-e550-4a6d-8325-254c3a866d9a", "node_type": "1", "metadata": {}, "hash": "4764f3bad01cf2dcf1d52954345eab86671d018fc97f4d578f7aef76c81c1e36", "class_name": "RelatedNodeInfo"}, {"node_id": "e502ed84-b3d9-4265-af70-95d5738e1807", "node_type": "1", "metadata": {}, "hash": "d92e3f49269f701d59464f14f013005ffa4f2b551313e358046282db5adf353b", "class_name": "RelatedNodeInfo"}, {"node_id": "055491d4-1246-43bf-8f94-54c2a555705f", "node_type": "1", "metadata": {}, "hash": "584f5f1a814d1e354db54aabcd54a3e1c2511a4536abe63fcbe7f96f6cf96032", "class_name": "RelatedNodeInfo"}, {"node_id": "3aae0939-41d6-4a37-a417-c6da7c0fb284", "node_type": "1", "metadata": {}, "hash": "697fe89f3b35e0f5f8df427d6a7d1b17ecb55ffcb861baeae4140c3495589721", "class_name": "RelatedNodeInfo"}]}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "text": "<Chat History> \n{chat_history}\n\n<Follow Up Message>\n{question}\n\n<Standalone question>\n\"\"\")\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/list of (human_message, ai_message) tuples\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncustom_chat_history = [\n    (\n        'Hello assistant, we are having a insightful discussion about Paul Graham today.', \n        'Okay, sounds good.'\n    )\n]\n\nquery_engine = index.as_query_engine()\nchat_engine = CondenseQuestionChatEngine.from_defaults(\n    query_engine=query_engine, \n    condense_question_prompt=custom_prompt,\n    chat_history=custom_chat_history,\n    verbose=True\n)\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo enable streaming, you simply need to call the `stream_chat` endpoint instead of the `chat` endpoint.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis somewhat inconsistent with query engine (where you pass in a `streaming=True` flag). We are working on making the behavior more consistent!\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.\")\nfor token in streaming_response.response_gen:\n    print(token, end=\"\")\n```\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee an end-to-end tutorial\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/SimilarityPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUsed to remove nodes that are below a similarity score threshold.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/SimilarityPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\npostprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/KeywordNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUsed to ensure certain keywords are either excluded or included.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/KeywordNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import KeywordNodePostprocessor\n\npostprocessor = KeywordNodePostprocessor(\n  required_keywords=[\"word1\", \"word2\"],\n  exclude_keywords=[\"word3\", \"word4\"]\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis postprocessor optimizes token usage by removing sentences that are not relevant to the query (this is done using embeddings).\n\nThe percentile cutoff is a measure for using the top percentage of relevant sentences.\n\nThe threshold cutoff can be specified instead, which uses a raw similarity cutoff for picking which sentences to keep.", "start_char_idx": 332004, "end_char_idx": 337594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b": {"__data__": {"id_": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0d05493-b438-49a8-8171-2da3c170a2a4", "node_type": "1", "metadata": {}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "178e7a4c-422c-4c79-8e54-119e2e05c739", "node_type": "1", "metadata": {}, "hash": "f3542e49f8d2cb1d95f9b322b0e3fcf3894852f6e22c5bf1ac10faf515f29913", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8919ae46-fb0f-4130-bc8d-2cddb2a74f73", "node_type": "1", "metadata": {}, "hash": "9517ba1eb27e4ce5ecceedee38d93312863fb8654c44dc0be0ef4a13fa7b1c11", "class_name": "RelatedNodeInfo"}, {"node_id": "e9e43751-02d0-4621-b39d-2e44418f4455", "node_type": "1", "metadata": {}, "hash": "42fb258fbba7e9ccca9ae276e70dd4aeccbbd2aebdfc4b4115d711b679d5f8fc", "class_name": "RelatedNodeInfo"}, {"node_id": "abf16a18-0c5e-4efd-8898-60b1c0832f83", "node_type": "1", "metadata": {}, "hash": "13105deaa880b1567e2d8c04a7738dff9bbaad99ec9e552cb3ec550f6ba4f660", "class_name": "RelatedNodeInfo"}, {"node_id": "05040174-34f9-4d79-a1be-b1cba73de84e", "node_type": "1", "metadata": {}, "hash": "fc339a165d14c367721680b6baa6008a350a854b045e9d18d3fa45f8edf82996", "class_name": "RelatedNodeInfo"}]}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import SentenceEmbeddingOptimizer\n\npostprocessor = SentenceEmbeddingOptimizer(\n  embed_model=service_context.embed_model,\n  percentile_cutoff=0.5,\n  # threshold_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full notebook guide can be found here\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUses the \"Cohere ReRank\" functionality to re-order nodes, and returns the top N nodes.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import CohereRerank\n\npostprocessor = CohereRerank(\n  top_n=2\n  model=\"rerank-english-v2.0\",\n  api_key=\"YOUR COHERE API KEY\"\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFull notebook guide is available here.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUses a LLM to re-order nodes by asking the LLM to return the relevant documents and a score of how relevant they are. Returns the top N ranked nodes.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import LLMRerank\n\npostprocessor = LLMRerank(\n  top_n=2\n  service_context=service_context,\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFull notebook guide is available her for Gatsby and here for Lyft 10K documents.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis postproccesor returns the top K nodes sorted by date. This assumes there is a `date` field to parse in the metadata of each node.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import FixedRecencyPostprocessor\n\npostprocessor = FixedRecencyPostprocessor(\n  tok_k=1,\n  date_key=\"date\"  # the key in the metadata to find the date\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nA full notebook guide is available here.", "start_char_idx": 337596, "end_char_idx": 342974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "178e7a4c-422c-4c79-8e54-119e2e05c739": {"__data__": {"id_": "178e7a4c-422c-4c79-8e54-119e2e05c739", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "node_type": "1", "metadata": {}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7f4e18c-f12c-4440-bec3-960141c48bea", "node_type": "1", "metadata": {}, "hash": "ca492d2c5c08edadd2435c94f6ac4de35cf5745049a75e1ecfbacf2adb4de0e1", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ae435429-cf0c-42e8-8050-c9ac1b8c91df", "node_type": "1", "metadata": {}, "hash": "fb01da5b0c7bdc1c1a1f4d8457662c13a07cfae247b720d88d272c1376372593", "class_name": "RelatedNodeInfo"}, {"node_id": "30c86132-8708-43b2-b30b-5fa311f3bac5", "node_type": "1", "metadata": {}, "hash": "077ec10394eabac712658efa27c47b486e21b084aa29a85ae7d398955fb97781", "class_name": "RelatedNodeInfo"}, {"node_id": "1462f6cf-d6f6-42b4-bd56-1bcc7ab4db0d", "node_type": "1", "metadata": {}, "hash": "a76892c88fd471842c16da77cb4f7c47112f02e620fdd4f18d5d7bc984c21cfe", "class_name": "RelatedNodeInfo"}]}, "hash": "f3542e49f8d2cb1d95f9b322b0e3fcf3894852f6e22c5bf1ac10faf515f29913", "text": "A full notebook guide is available here.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis postproccesor returns the top K nodes after sorting by date and removing older nodes that are too similar after measuring embedding similarity.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import EmbeddingRecencyPostprocessor\n\npostprocessor = EmbeddingRecencyPostprocessor(\n  service_context=service_context,\n  date_key=\"date\",\n  similarity_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full notebook guide is available here.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis postproccesor returns the top K nodes applying a time-weighted rerank to each node. Each time a node is retrieved, the time it was retrieved is recorded. This biases search to favor information that has not be returned in a query yet.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import TimeWeightedPostprocessor\n\npostprocessor = TimeWeightedPostprocessor(\n  time_decay=0.99,\n  top_k=1\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full notebook guide is available here.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe PII (Personal Identifiable Information) postprocssor removes information that might be a security risk. It does this by using NER (either with a dedicated NER model, or with a local LLM model).\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: code\nHeader Path: Modules/(Beta) PIINodePostprocessor/LLM Version\nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index.indices.postprocessor import PIINodePostprocessor\n\npostprocessor = PIINodePostprocessor(\n  service_context=service_context,  # this should be setup with an LLM you trust\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis version uses the default local model from Hugging Face that is loaded when you run `pipline(\"ner\")`.", "start_char_idx": 342934, "end_char_idx": 347751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7f4e18c-f12c-4440-bec3-960141c48bea": {"__data__": {"id_": "d7f4e18c-f12c-4440-bec3-960141c48bea", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "178e7a4c-422c-4c79-8e54-119e2e05c739", "node_type": "1", "metadata": {}, "hash": "f3542e49f8d2cb1d95f9b322b0e3fcf3894852f6e22c5bf1ac10faf515f29913", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d1ffae8-1523-447e-9879-92deafe8423c", "node_type": "1", "metadata": {}, "hash": "69405ea491b03796819a1192f7395d251090e6c9723db4dd4e9d2a53f3c1cd6d", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "717e1455-26c2-4e94-8828-0aafc65fe714", "node_type": "1", "metadata": {}, "hash": "420fc35f633728fbce99d3072a5c858d6397acbd1cd9307f1eba5f48ad2cdbac", "class_name": "RelatedNodeInfo"}, {"node_id": "43d90206-228a-42f7-8038-dec0902d6fae", "node_type": "1", "metadata": {}, "hash": "28043d4ffd660547c1ecbd08bd6877564c4d823f972fef478904dbcf3a134348", "class_name": "RelatedNodeInfo"}, {"node_id": "fe3b84fa-7467-48fa-969e-b9fba40b30c2", "node_type": "1", "metadata": {}, "hash": "fd51068037a1ecce0a10b24a41c49e9111fe355461036292b366aa3bffd4a874", "class_name": "RelatedNodeInfo"}]}, "hash": "ca492d2c5c08edadd2435c94f6ac4de35cf5745049a75e1ecfbacf2adb4de0e1", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import NERPIINodePostprocessor\n\npostprocessor = NERPIINodePostprocessor()\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full notebook guide for both can be found here.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUses pre-defined settings to read the `Node` relationships and fetch either all nodes that come previously, next, or both.\n\nThis is useful when you know the relationships point to important data (either before, after, or both) that should be sent to the LLM if that node is retrieved.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import PrevNextNodePostprocessor\n\npostprocessor = PrevNextNodePostprocessor(\n  docstore=index.docstore,\n  num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards\n  mode=\"next\"   # can be either 'next', 'previous', or 'both'\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe same as PrevNextNodePostprocessor, but lets the LLM decide the mode (next, previous, or both).\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import AutoPrevNextNodePostprocessor\n\npostprocessor = AutoPrevNextNodePostprocessor(\n  docstore=index.docstore,\n  service_context=service_context\n  num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full example notebook is available here.", "start_char_idx": 347753, "end_char_idx": 352190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d1ffae8-1523-447e-9879-92deafe8423c": {"__data__": {"id_": "8d1ffae8-1523-447e-9879-92deafe8423c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7f4e18c-f12c-4440-bec3-960141c48bea", "node_type": "1", "metadata": {}, "hash": "ca492d2c5c08edadd2435c94f6ac4de35cf5745049a75e1ecfbacf2adb4de0e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88a26da0-22e2-444b-9df6-8f01c0e24203", "node_type": "1", "metadata": {}, "hash": "9e26995e97574a5611b450946caa3a36830ba558282f15189073f61e799d13c5", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "c485a396-c235-463c-b215-ebde1980ff28", "node_type": "1", "metadata": {}, "hash": "a96c2f4a771b2d1b091e0c8df0413c6d4c1d2aeaaf82989ddda97714c0999640", "class_name": "RelatedNodeInfo"}, {"node_id": "139ad006-31e5-407f-ac67-e31c4c44b392", "node_type": "1", "metadata": {}, "hash": "ef6d8e81675850480a30ada9bdf5098db266f4dfde9e3d0fa1003fbfeb9819f0", "class_name": "RelatedNodeInfo"}, {"node_id": "9ec911a2-ad2e-4b5d-acc2-5da8be39c8e5", "node_type": "1", "metadata": {}, "hash": "b6603d3edeb94c85b70ac8f113d6390bc89a1f821bad111e9baa0a1830a5c37c", "class_name": "RelatedNodeInfo"}]}, "hash": "69405ea491b03796819a1192f7395d251090e6c9723db4dd4e9d2a53f3c1cd6d", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: code\nHeader Path: Modules/(Beta) PIINodePostprocessor/All Notebooks\nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/node_postprocessor/OptimizerDemo.ipynb\n/examples/node_postprocessor/CohereRerank.ipynb\n/examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb\n/examples/node_postprocessor/LLMReranker-Gatsby.ipynb\n/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb\n/examples/node_postprocessor/TimeWeightedPostprocessorDemo.ipynb\n/examples/node_postprocessor/PII.ipynb\n/examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNode postprocessors are a set of modules that take a set of nodes, and apply some kind of transformation or filtering before returning them.\n\nIn LlamaIndex, node postprocessors are most commonly applied within a query engine, after the node retrieval step and before the response synthesis step.\n\nLlamaIndex offers several node postprocessors for immediate use, while also providing a simple API for adding your own custom postprocessors.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nConfused about where node postprocessor fits in the pipeline? Read about high-level concepts\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn example of using a node postprocessors is below:\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.schema import Node, NodeWithScore\n\nnodes = [\n  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n]\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can find more details using post processors and how to build your own below.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBelow you can find guides for each node postprocessor.", "start_char_idx": 352192, "end_char_idx": 357050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88a26da0-22e2-444b-9df6-8f01c0e24203": {"__data__": {"id_": "88a26da0-22e2-444b-9df6-8f01c0e24203", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d1ffae8-1523-447e-9879-92deafe8423c", "node_type": "1", "metadata": {}, "hash": "69405ea491b03796819a1192f7395d251090e6c9723db4dd4e9d2a53f3c1cd6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "node_type": "1", "metadata": {}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "006ee14e-9068-48ac-83c7-dcc4b99c69c9", "node_type": "1", "metadata": {}, "hash": "3b0b47d35b9193c2b7ca0adc4f8097be111bfb896dd4cf536bc54a25e7089dbe", "class_name": "RelatedNodeInfo"}, {"node_id": "2f217501-fdde-4300-8766-590ca19de1e3", "node_type": "1", "metadata": {}, "hash": "aefacc9987210ffa4d9e25744ee6bd560abc011c2efbe9588fa4d47b5bda72b6", "class_name": "RelatedNodeInfo"}, {"node_id": "38317e39-4b36-458a-bd0a-afe652b6a453", "node_type": "1", "metadata": {}, "hash": "4a6f72cf81eeb5e90d1955af55c20c8603241ce9511c98d05350353c5839b9aa", "class_name": "RelatedNodeInfo"}]}, "hash": "9e26995e97574a5611b450946caa3a36830ba558282f15189073f61e799d13c5", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMost commonly, node-postprocessors will be used in a query engine, where they are applied to the nodes returned from a retriever, and before the response synthesis step.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/all node post-processors will be applied during each query\nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.indices.postprocessor import TimeWeightedPostprocessor\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(\n  node_postprocessors=[\n    TimeWeightedPostprocessor(\n        time_decay=0.5, time_access_refresh=False, top_k=1\n    )\n  ]\n)\n\nresponse = query_engine.query(\"query string\")\n```\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with Retrieved Nodes\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOr used as a standalone object for filtering retrieved nodes:\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with Retrieved Nodes\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\nnodes = index.as_retriever().query(\"query string\")\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/filter nodes below 0.75 similarity score\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with your own nodes\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAs you may have noticed, the postprocessors take `NodeWithScore` objects as inputs, which is just a wrapper class with a `Node` and a `score` value.", "start_char_idx": 357052, "end_char_idx": 360873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c7a0d7b-2e71-4380-8cd7-706ee035b617": {"__data__": {"id_": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88a26da0-22e2-444b-9df6-8f01c0e24203", "node_type": "1", "metadata": {}, "hash": "9e26995e97574a5611b450946caa3a36830ba558282f15189073f61e799d13c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bbacaf0-99f4-4ae6-9740-b7774c835b80", "node_type": "1", "metadata": {}, "hash": "09c72f3249a95e391f5cb9ba0f0f49ec5645ee86e151ece31a7f14f141158518", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f0e1077f-7df1-4be1-a15e-cf519bc7e007", "node_type": "1", "metadata": {}, "hash": "0f4976fa48845f8fd1cb25744ba0f34ff7ae63dbea25eaa7c1bd61ebe5341bd4", "class_name": "RelatedNodeInfo"}, {"node_id": "6e3c8471-87b5-4e0b-9653-9ec23b2e6cad", "node_type": "1", "metadata": {}, "hash": "9867512f3ac55930e3a1494cef0ad9984b97e2484586c74c5f1663686a6f3cd0", "class_name": "RelatedNodeInfo"}, {"node_id": "4c338303-12d9-4c81-b8de-e12c64d598d3", "node_type": "1", "metadata": {}, "hash": "a13fcd75e7966fa6660dae984b810eb2581ea1f1faa490bb3e47b74de1864940", "class_name": "RelatedNodeInfo"}, {"node_id": "886e3f81-c5dc-4b0f-98ef-26b73704602f", "node_type": "1", "metadata": {}, "hash": "e4c25e6e24fdcc151ce355864c2efcf16c46e3474592e613b73faf0a5062023b", "class_name": "RelatedNodeInfo"}]}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with your own nodes\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.schema import Node, NodeWithScore\n\nnodes = [\n  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n]\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/filter nodes below 0.75 similarity score\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe base class is `BaseNodePostprocessor`, and the API interface is very simple:\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nclass BaseNodePostprocessor:\n    \"\"\"Node postprocessor.\"\"\"\n\n    @abstractmethod\n    def postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA dummy node-postprocessor can be implemented in just a few lines of code:\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import QueryBundle\nfrom llama_index.indices.postprocessor.base import BaseNodePostprocessor\nfrom llama_index.schema import NodeWithScore\n\nclass DummyNodePostprocessor:\n\n    def postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \n        # subtracts 1 from the score\n        for n in nodes:\n            n.score -= 1\n\n        return nodes\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex allows you to perform *query transformations* over your index structures.\nQuery transformations are modules that will convert a query into another query. They can be **single-step**, as in the transformation is run once before the query is executed against an index. \n\nThey can also be **multi-step**, as in: \n1. The query is transformed, executed against an index, \n2. The response is retrieved.\n3. Subsequent queries are transformed/executed in a sequential fashion.\n\nWe list some of our query transformations in more detail below.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Use Cases\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuery transformations have multiple use cases:\n- Transforming an initial query into a form that can be more easily embedded (e.g. HyDE)\n- Transforming an initial query into a subquestion that can be more easily answered from the data (single-step query decomposition)\n- Breaking an initial query into multiple subquestions that can be more easily answered on their own. (multi-step query decomposition)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/HyDE (Hypothetical Document Embeddings)\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHyDE is a technique where given a natural language query, a hypothetical document/answer is generated first. This hypothetical document is then used for embedding lookup rather than the raw query.\n\nTo use HyDE, an example code snippet is shown below.", "start_char_idx": 360875, "end_char_idx": 366868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bbacaf0-99f4-4ae6-9740-b7774c835b80": {"__data__": {"id_": "7bbacaf0-99f4-4ae6-9740-b7774c835b80", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "node_type": "1", "metadata": {}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf", "node_type": "1", "metadata": {}, "hash": "a8e47cc50e6e4a90dc3ecf90a80320bdc25455d894dc0095d9bac38bac7ec2dc", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ecb74226-c19d-4a62-940b-a190429915a2", "node_type": "1", "metadata": {}, "hash": "599ed6488ad4438f9d64afe6f9be1661fb6bec7b2904629b20c9e84f757598af", "class_name": "RelatedNodeInfo"}, {"node_id": "bc3772b8-cd45-489d-9821-b1e072034617", "node_type": "1", "metadata": {}, "hash": "d865974a25914e5cd9ff3931e738fb2f6c22323759608ec5e873689d133f88a3", "class_name": "RelatedNodeInfo"}]}, "hash": "09c72f3249a95e391f5cb9ba0f0f49ec5645ee86e151ece31a7f14f141158518", "text": "To use HyDE, an example code snippet is shown below.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/HyDE (Hypothetical Document Embeddings)\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.indices.query.query_transform.base import HyDEQueryTransform\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/load documents, build index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex(documents)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/run query with HyDE query transform\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_str = \"what did paul graham do after going to RISD\"\nhyde = HyDEQueryTransform(include_original=True)\nquery_engine = index.as_query_engine()\nquery_engine = TransformQueryEngine(query_engine, query_transform=hyde)\nresponse = query_engine.query(query_str)\nprint(response)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/run query with HyDE query transform\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCheck out our example notebook for a full walkthrough.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Single-Step Query Decomposition\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSome recent approaches (e.g. self-ask, ReAct) have suggested that LLM's \nperform better at answering complex questions when they break the question into smaller steps. We have found that this is true for queries that require knowledge augmentation as well.\n\nIf your query is complex, different parts of your knowledge base may answer different \"subqueries\" around the overall query.\n\nOur single-step query decomposition feature transforms a **complicated** question into a simpler one over the data collection to help provide a sub-answer to the original question.\n\nThis is especially helpful over a composed graph. Within a composed graph, a query can be routed to multiple subindexes, each representing a subset of the overall knowledge corpus. Query decomposition allows us to transform the query into a more suitable question over any given index.\n\nAn example image is shown below.\n\n!", "start_char_idx": 366816, "end_char_idx": 370593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46c525df-09b7-4fe1-a4df-c7f1676daedf": {"__data__": {"id_": "46c525df-09b7-4fe1-a4df-c7f1676daedf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7bbacaf0-99f4-4ae6-9740-b7774c835b80", "node_type": "1", "metadata": {}, "hash": "09c72f3249a95e391f5cb9ba0f0f49ec5645ee86e151ece31a7f14f141158518", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cedbeb90-5626-4faf-ac24-de412008c78c", "node_type": "1", "metadata": {}, "hash": "331b266488459b207e1643ed788f071467f6f531f195502932f3391d94a00ce5", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "4c2568d9-9d74-46f8-9494-0f6c1208b1e7", "node_type": "1", "metadata": {}, "hash": "3686e6638541a01366d76c7c6d1b0bcfa75b59f897b4b892c74835bcc2438604", "class_name": "RelatedNodeInfo"}, {"node_id": "ff288ad3-f67e-453f-80a2-36e8df06acf8", "node_type": "1", "metadata": {}, "hash": "5477578229295ebd519be0616cac87fef33038a765db12d17856e7af3c3463cf", "class_name": "RelatedNodeInfo"}, {"node_id": "7b8d6b4d-877f-4f5a-bc3c-d15e7eaae96f", "node_type": "1", "metadata": {}, "hash": "ad8d8b948996e894668f9b4c1a9ad7c4ffacd2a9b16b5db1324055a4dbd6c0c8", "class_name": "RelatedNodeInfo"}]}, "hash": "a8e47cc50e6e4a90dc3ecf90a80320bdc25455d894dc0095d9bac38bac7ec2dc", "text": "An example image is shown below.\n\n!\n\n\nHere's a corresponding example code snippet over a composed graph.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/llm_predictor_chatgpt corresponds to the ChatGPT LLM interface\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor_chatgpt, verbose=True\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/initialize indexes and graph\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/configure retrievers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_query_engine = vector_index.as_query_engine()\nvector_query_engine = TransformQueryEngine(\n    vector_query_engine, \n    query_transform=decompose_transform\n    transform_extra_info={'index_summary': vector_index.index_struct.summary}\n)\ncustom_query_engines = {\n    vector_index.index_id: vector_query_engine\n}\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/query\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_str = (\n    \"Compare and contrast the airports in Seattle, Houston, and Toronto. \"\n)\nquery_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\nresponse = query_engine.query(query_str)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/query\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCheck out our example notebook for a full walkthrough.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Multi-Step Query Transformations\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMulti-step query transformations are a generalization on top of existing single-step query transformation approaches.\n\nGiven an initial, complex query, the query is transformed and executed against an index. The response is retrieved from the query. \nGiven the response (along with prior responses) and the query, followup questions may be asked against the index as well. This technique allows a query to be run against a single knowledge source until that query has satisfied all questions.\n\nAn example image is shown below.\n\n!\n\n\nHere's a corresponding example code snippet.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Multi-Step Query Transformations\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.query.query_transform.base import StepDecomposeQueryTransform\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstep_decompose_transform = StepDecomposeQueryTransform(\n    llm_predictor, verbose=True\n)\n\nquery_engine = index.as_query_engine()\nquery_engine = MultiStepQueryEngine(query_engine, query_transform=step_decompose_transform)\n\nresponse = query_engine.query(\n    \"Who was in the first batch of the accelerator program the author started?\",\n)\nprint(str(response))\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCheck out our example notebook for a full walkthrough.", "start_char_idx": 370558, "end_char_idx": 376302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cedbeb90-5626-4faf-ac24-de412008c78c": {"__data__": {"id_": "cedbeb90-5626-4faf-ac24-de412008c78c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf", "node_type": "1", "metadata": {}, "hash": "a8e47cc50e6e4a90dc3ecf90a80320bdc25455d894dc0095d9bac38bac7ec2dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "node_type": "1", "metadata": {}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "dc004b59-b2ee-4071-aa1d-845bddf28a4d", "node_type": "1", "metadata": {}, "hash": "8079ebce2d5c235539a4e5f1c2b19f61b99c01cace03a0830928398b1cf74887", "class_name": "RelatedNodeInfo"}, {"node_id": "4f5d255e-1c3c-45e5-afe0-4bd6b947188d", "node_type": "1", "metadata": {}, "hash": "d62608b9243ec1dc4e382915753d7a7bb4e978f4b4ed076362073018197d1a5f", "class_name": "RelatedNodeInfo"}, {"node_id": "818cb288-4336-4115-846e-24de9ebecb88", "node_type": "1", "metadata": {}, "hash": "5ff509539b3a2f8b13ad463bc090103df9ad7ce953fed8a059514bee97d2bfcf", "class_name": "RelatedNodeInfo"}]}, "hash": "331b266488459b207e1643ed788f071467f6f531f195502932f3391d94a00ce5", "text": "File Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\ncaption: Examples\nmaxdepth: 1\n---\n/examples/query_transformations/HyDEQueryTransformDemo.ipynb\n/examples/query_transformations/SimpleIndexDemo-multistep.ipynb\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nContent Type: code\nHeader Path: Module Guides/Basic\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1348\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\nRetriever Query Engine </examples/query_engine/CustomRetrievers.ipynb>\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nContent Type: code\nHeader Path: Module Guides/Structured & Semi-Structured Data\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1348\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/json_query_engine.ipynb\n/examples/query_engine/pandas_query_engine.ipynb\n/examples/query_engine/knowledge_graph_query_engine.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nContent Type: code\nHeader Path: Module Guides/Advanced\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1348\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/RouterQueryEngine.ipynb\n/examples/query_engine/RetrieverRouterQueryEngine.ipynb\n/examples/query_engine/JointQASummary.ipynb\n/examples/query_engine/sub_question_query_engine.ipynb\n/examples/query_transformations/SimpleIndexDemo-multistep.ipynb\n/examples/query_engine/SQLRouterQueryEngine.ipynb\n/examples/query_engine/SQLAutoVectorQueryEngine.ipynb\n/examples/query_engine/SQLJoinQueryEngine.ipynb\n/examples/index_structs/struct_indices/duckdb_sql_query.ipynb\nRetry Query Engine </examples/evaluation/RetryQuery.ipynb>\nRetry Source Query Engine </examples/evaluation/RetryQuery.ipynb>\nRetry Guideline Query Engine </examples/evaluation/RetryQuery.ipynb>\n/examples/query_engine/citation_query_engine.ipynb\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nContent Type: code\nHeader Path: Module Guides/Experimental\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1348\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/flare_query_engine.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\response_modes.md\nContent Type: text\nHeader Path: Response Modes\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\response_modes.md\nfile_name: response_modes.md\nfile_type: None\nfile_size: 1347\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRight now, we support the following options:\n- `default`: \"create and refine\" an answer by sequentially going through each retrieved `Node`; \n    This makes a separate LLM call per Node. Good for more detailed answers.\n- `compact`: \"compact\" the prompt during each LLM call by stuffing as \n    many `Node` text chunks that can fit within the maximum prompt size. If there are \n    too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n    multiple prompts.\n- `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree \n    and return the root node as the response. Good for summarization purposes.\n- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n    without actually sending them. Then can be inspected by checking `response.source_nodes`.\n    The response object is covered in more detail in Section 5.\n- `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text\n    chunk while accumulating the responses into an array. Returns a concatenated string of all\n    responses. Good for when you need to run the same query separately against each text\n    chunk.\n\nSee Response Synthesizer to learn more.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuery engine is a generic interface that allows you to ask question over your data.\n\nA query engine takes in a natural language query, and returns a rich response.\nIt is most often (but not always) built on one or many Indices via Retrievers.\nYou can compose multiple query engines to achieve more advanced capability.", "start_char_idx": 376304, "end_char_idx": 381680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e66155c-ea3c-4435-9d90-ef6444c9f514": {"__data__": {"id_": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cedbeb90-5626-4faf-ac24-de412008c78c", "node_type": "1", "metadata": {}, "hash": "331b266488459b207e1643ed788f071467f6f531f195502932f3391d94a00ce5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f99b44ac-ef51-4064-b11f-376000a8d227", "node_type": "1", "metadata": {}, "hash": "56da6f91d81b42c0bcdd08a6fbb6456a7f243f2ae32d5b910461d8ccb5a6c72e", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2172d386-222e-4632-979d-c43cf53eef8a", "node_type": "1", "metadata": {}, "hash": "ba2cd19dc540177ae35f4b61e5c80240ad0f1acf2ab6618b0c3d325317fee17b", "class_name": "RelatedNodeInfo"}, {"node_id": "496f6e3d-6497-4e4d-8f64-7bc846728080", "node_type": "1", "metadata": {}, "hash": "51b79ba348a14ad62ad28b7fcff4d62ce1fe52ecf0c3bd2aabb96f23edf7f687", "class_name": "RelatedNodeInfo"}, {"node_id": "64c581be-ffaf-4820-9cbe-60e7faa0d31d", "node_type": "1", "metadata": {}, "hash": "4043b1d3cdb38733215bd235a9e96ce0381ccaa897b1211803816e6c6976efd6", "class_name": "RelatedNodeInfo"}, {"node_id": "60564498-bab5-47c1-9a8c-b991f672b4e4", "node_type": "1", "metadata": {}, "hash": "6a9f3c15c9a1072d0c9e066aa62427dd0b7992d765cde196e62b64521a5dda2d", "class_name": "RelatedNodeInfo"}]}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "text": "You can compose multiple query engines to achieve more advanced capability.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you want to have a conversation with your data (multiple back-and-forth instead of a single question & answer), take a look at Chat Engine\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet started with:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Who is Paul Graham.\")\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo stream response:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(streaming=True)\nstreaming_response = query_engine.query(\"Who is Paul Graham.\")\nstreaming_response.print_response_stream()\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: code\nHeader Path: Query Engine/Usage Pattern\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: code\nHeader Path: Query Engine/Modules\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 3\n---\nmodules.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: code\nHeader Path: Query Engine/Supporting Modules\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nsupporting_modules.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports streaming the response as it's being generated.\nThis allows you to start printing or processing the beginning of the response before the full response is finished.\nThis can drastically reduce the perceived latency of queries.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo enable streaming, you need to use an LLM that supports streaming.\nRight now, streaming is supported by `OpenAI`, `HuggingFaceLLM`, and most LangChain LLMs (via `LangChainLLM`).\n\nConfigure query engine to use streaming:\n\nIf you are using the high-level API, set `streaming=True` when building a query engine.", "start_char_idx": 381605, "end_char_idx": 386160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f99b44ac-ef51-4064-b11f-376000a8d227": {"__data__": {"id_": "f99b44ac-ef51-4064-b11f-376000a8d227", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "node_type": "1", "metadata": {}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef046785-afb0-44f2-b971-a11f2947b864", "node_type": "1", "metadata": {}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "4dbd9c5f-178b-4ecd-aa3e-41f2824140f4", "node_type": "1", "metadata": {}, "hash": "05476a86e0c33954b5d3cb140dc4540548ff16afd96c5600b73c78cab406a6ae", "class_name": "RelatedNodeInfo"}, {"node_id": "9d66814f-bd9e-4e5b-a7be-fe9801064c3e", "node_type": "1", "metadata": {}, "hash": "ee020d95fe29d768d51f7d6bf4681f1b47b65f98380c2b53172a7dd14f2abc37", "class_name": "RelatedNodeInfo"}, {"node_id": "4f53b62a-6b12-4eb0-ade0-061997ded610", "node_type": "1", "metadata": {}, "hash": "b6f2b7f4a6404d0134ec387a87daf826681e632acc4cb6c670e7d3197cffc5b2", "class_name": "RelatedNodeInfo"}]}, "hash": "56da6f91d81b42c0bcdd08a6fbb6456a7f243f2ae32d5b910461d8ccb5a6c72e", "text": "File Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    streaming=True,\n    similarity_top_k=1\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you are using the low-level API to compose the query engine,\npass `streaming=True` when constructing the `Response Synthesizer`:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import get_response_synthesizer\nsynth = get_response_synthesizer(streaming=True, ...)\nquery_engine = RetrieverQueryEngine(response_synthesizer=synth, ...)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAfter properly configuring both the LLM and the query engine,\ncalling `query` now returns a `StreamingResponse` object.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?\", \n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe response is returned immediately when the LLM call *starts*, without having to wait for the full completion.\n\n> Note: In the case where the query engine makes multiple LLM calls, only the last LLM call will be streamed and the response is returned when the last LLM call starts.\n\nYou can obtain a `Generator` from the streaming response and iterate over the tokens as they arrive:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfor text in streaming_response.response_gen:\n    # do something with text as they arrive.", "start_char_idx": 386162, "end_char_idx": 389660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef046785-afb0-44f2-b971-a11f2947b864": {"__data__": {"id_": "ef046785-afb0-44f2-b971-a11f2947b864", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f99b44ac-ef51-4064-b11f-376000a8d227", "node_type": "1", "metadata": {}, "hash": "56da6f91d81b42c0bcdd08a6fbb6456a7f243f2ae32d5b910461d8ccb5a6c72e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e55610d1-f38c-4556-ab21-8baa89ea8746", "node_type": "1", "metadata": {}, "hash": "5a184d2afc52e38ef816783ed5d05f9044cceb3b0107e208121b3a7788b1a4e7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3c11af20-f2f5-4904-9253-0de11f04df45", "node_type": "1", "metadata": {}, "hash": "0565e80081af0e57493ce657a5c668404adc2bd3f053b13dd369a502a2015f62", "class_name": "RelatedNodeInfo"}, {"node_id": "91e5b4a4-c742-4dc8-9acb-32493d892ec9", "node_type": "1", "metadata": {}, "hash": "0d6c73e0baf744d2596f42ae3aff4850ae1d7d0073b1fa016cf96f176916c336", "class_name": "RelatedNodeInfo"}, {"node_id": "e8976cc4-4f5c-40ba-9df1-f095f713aed6", "node_type": "1", "metadata": {}, "hash": "ba8cf6892bad5aef013a5903da34527ee94a97f5f359c08f219bce10256bd18a", "class_name": "RelatedNodeInfo"}, {"node_id": "6b2690bf-b5cb-41b0-929b-8fd7b832cfee", "node_type": "1", "metadata": {}, "hash": "a9115d3dacb1cff2d46e3710f0b59568d17965dce7961a57060e78b4e9816e81", "class_name": "RelatedNodeInfo"}]}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "text": "File Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAlternatively, if you just want to print the text as they arrive:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstreaming_response.print_response_stream()\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee an end-to-end example\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\supporting_modules.md\nContent Type: code\nHeader Path: Supporting Modules\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\supporting_modules.md\nfile_name: supporting_modules.md\nfile_type: None\nfile_size: 99\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\nadvanced/query_transformations.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBuild a query engine from index:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAsk a question over your data\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = query_engine.query('Who is Paul Graham?')\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can directly build and configure a query engine from an index in 1 line of code:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    response_mode='tree_summarize',\n    verbose=True,\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.  \n\nSee **Response Modes** for a full list of response modes and what they do.", "start_char_idx": 389662, "end_char_idx": 394834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e55610d1-f38c-4556-ab21-8baa89ea8746": {"__data__": {"id_": "e55610d1-f38c-4556-ab21-8baa89ea8746", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef046785-afb0-44f2-b971-a11f2947b864", "node_type": "1", "metadata": {}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49f45f3d-b52b-4ba6-94bd-073ae87610c0", "node_type": "1", "metadata": {}, "hash": "741f829103d87723bad5d1f5d01b9ff474001ab22bbec0914af554f35a5c4547", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "872aad94-9da9-4557-97bd-abd802849708", "node_type": "1", "metadata": {}, "hash": "34861dd5b1749d45e3d1f7c8ccccc1f4e1a4b7ff5026ba661a5cc9798ee08ade", "class_name": "RelatedNodeInfo"}, {"node_id": "c5d7c57a-f321-4824-aff1-31e89e24375a", "node_type": "1", "metadata": {}, "hash": "0f1c13e9fad0dfcc0f473116347ed53b2dccecb1ef08b8270592317f4b3e3bdb", "class_name": "RelatedNodeInfo"}, {"node_id": "050c6272-35a7-41be-b9c7-a044c487524f", "node_type": "1", "metadata": {}, "hash": "f4c756cbd9d06f8a1c8656e46711ccc5b4c10af36236c5c9f38c2de8f94f69f2", "class_name": "RelatedNodeInfo"}]}, "hash": "5a184d2afc52e38ef816783ed5d05f9044cceb3b0107e208121b3a7788b1a4e7", "text": "See **Response Modes** for a full list of response modes and what they do.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\nhidden:\n---\nresponse_modes.md\nstreaming.md\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use the low-level composition API if you need more granular control.\nConcretely speaking, you would explicitly construct a `QueryEngine` object instead of calling `index.as_query_engine(...)`.\n> Note: You may need to look at API references or example notebooks.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.retrievers import VectorIndexRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/build index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/configure retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = VectorIndexRetriever(\n    index=index, \n    similarity_top_k=2,\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/configure response synthesizer\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse_synthesizer = get_response_synthesizer(\n    response_mode=\"tree_summarize\",\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/assemble query engine\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/query\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo enable streaming, you simply need to pass in a `streaming=True` flag\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    streaming=True,\n)\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?", "start_char_idx": 394760, "end_char_idx": 399929, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49f45f3d-b52b-4ba6-94bd-073ae87610c0": {"__data__": {"id_": "49f45f3d-b52b-4ba6-94bd-073ae87610c0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e55610d1-f38c-4556-ab21-8baa89ea8746", "node_type": "1", "metadata": {}, "hash": "5a184d2afc52e38ef816783ed5d05f9044cceb3b0107e208121b3a7788b1a4e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19e9220c-9515-4e08-9b2d-339a09be6cba", "node_type": "1", "metadata": {}, "hash": "dee63d8f226759fde5f74ade948b85f36884f06333e49c583c5b56d88d23cf98", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3c931fa1-b7bc-4665-8296-b69e8ec7022b", "node_type": "1", "metadata": {}, "hash": "741f829103d87723bad5d1f5d01b9ff474001ab22bbec0914af554f35a5c4547", "class_name": "RelatedNodeInfo"}]}, "hash": "741f829103d87723bad5d1f5d01b9ff474001ab22bbec0914af554f35a5c4547", "text": "\", \n)\nstreaming_response.print_response_stream()\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* Read the full streaming guide\n* See an end-to-end example\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDetailed inputs/outputs for each response synthesizer are found below.\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/API Example\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe following shows the setup for utilizing all kwargs.\n\n- `response_mode` specifies which response synthesizer to use\n- `service_context` defines the LLM and related settings for synthesis\n- `text_qa_template` and `refine_template` are the prompts used at various stages\n- `use_async` is used for only the `tree_summarize` response mode right now, to asynchronously build the summary tree\n- `streaming` configures whether to return a streaming response object or not\n\nIn the `synthesize`/`asyntheszie` functions, you can optionally provide additional source nodes, which will be added to the `response.source_nodes` list.", "start_char_idx": 399929, "end_char_idx": 401820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19e9220c-9515-4e08-9b2d-339a09be6cba": {"__data__": {"id_": "19e9220c-9515-4e08-9b2d-339a09be6cba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49f45f3d-b52b-4ba6-94bd-073ae87610c0", "node_type": "1", "metadata": {}, "hash": "741f829103d87723bad5d1f5d01b9ff474001ab22bbec0914af554f35a5c4547", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc007a85-6883-4a66-afa8-30af77b854cd", "node_type": "1", "metadata": {}, "hash": "b974c2210558ff9f02865dd3b29d3bb5175893a955e3c16017b0d20712de61ed", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "c8d66d8d-f99e-4cac-a2ae-208fa8323598", "node_type": "1", "metadata": {}, "hash": "e414025695bfaa10c2ef859de942dd85040b80d00395f49de1c34d197d7e8e01", "class_name": "RelatedNodeInfo"}, {"node_id": "1d0352ec-83b8-458d-9d7e-dd1dd5e57aff", "node_type": "1", "metadata": {}, "hash": "fdbb91daff0424a9d5c731a8144fca314b80a3e0ab042269f25f9b1672b9ca05", "class_name": "RelatedNodeInfo"}, {"node_id": "44dbce79-8521-48a7-beea-de14b5c6383c", "node_type": "1", "metadata": {}, "hash": "a363d96b66f0060257e1788bdd4240b36659923a79ca3f8e700ae29b6195cf2b", "class_name": "RelatedNodeInfo"}]}, "hash": "dee63d8f226759fde5f74ade948b85f36884f06333e49c583c5b56d88d23cf98", "text": "File Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/API Example\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import Node, NodeWithScore\nfrom llama_index import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(\n  response_mode=\"refine\",\n  service_context=service_context,\n  text_qa_template=text_qa_template,\n  refine_template=refine_template,\n  use_async=False,\n  streaming=False\n)\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/synchronous\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = response_synthesizer.synthesize(\n  \"query string\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..],\n  additional_source_nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..], \n)\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = await response_synthesizer.asynthesize(\n  \"query string\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..],\n  additional_source_nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..], \n)\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also directly return a string, using the lower-level `get_response` and `aget_response` functions\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse_str = response_synthesizer.get_response(\n  \"query string\", \n  text_chunks=[\"text1\", \"text2\", ...]\n)\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: code\nHeader Path: Module Guide/Example Notebooks\nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/response_synthesizers/refine.ipynb\n/examples/response_synthesizers/tree_summarize.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA `Response Synthesizer` is what generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a `Response` object.\n\nThe method for doing this can take many forms, from as simple as iterating over text chunks, to as complex as building a tree. The main idea here is to simplify the process of generating a response using an LLM across your data.\n\nWhen used in a query engine, the response synthesizer is used after nodes are retrieved from a retriever, and after any node-postprocessors are ran.\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nConfused about where response synthesizer fits in the pipeline?", "start_char_idx": 401822, "end_char_idx": 406425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc007a85-6883-4a66-afa8-30af77b854cd": {"__data__": {"id_": "bc007a85-6883-4a66-afa8-30af77b854cd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19e9220c-9515-4e08-9b2d-339a09be6cba", "node_type": "1", "metadata": {}, "hash": "dee63d8f226759fde5f74ade948b85f36884f06333e49c583c5b56d88d23cf98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "node_type": "1", "metadata": {}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1a35a11a-8fd9-45d7-ad98-06a245c10326", "node_type": "1", "metadata": {}, "hash": "a4ea8317fd20061437fd74f75d12fd2555ed37a0a17eaad1c8a417b732e85683", "class_name": "RelatedNodeInfo"}, {"node_id": "3a1f5112-7bc8-4e99-becc-4cf7fc32a113", "node_type": "1", "metadata": {}, "hash": "c7ac254c0c5548d042fe48378d78076aa2d91f94676f1d20a4298ab414e0215d", "class_name": "RelatedNodeInfo"}]}, "hash": "b974c2210558ff9f02865dd3b29d3bb5175893a955e3c16017b0d20712de61ed", "text": "Read the high-level concepts\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUse a response synthesizer on it's own:\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import Node\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode='compact')\n\nresponse = response_synthesizer.synthesize(\"query text\", nodes=[Node(text=\"text\"), ...])\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOr in a query engine after you've created an index:\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can find more details on all available response synthesizers, modes, and how to build your own below.\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBelow you can find detailed API information for each response synthesis module.", "start_char_idx": 406426, "end_char_idx": 409692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4": {"__data__": {"id_": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc007a85-6883-4a66-afa8-30af77b854cd", "node_type": "1", "metadata": {}, "hash": "b974c2210558ff9f02865dd3b29d3bb5175893a955e3c16017b0d20712de61ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcd9a011-a035-41e2-87a7-23f1405a9dfc", "node_type": "1", "metadata": {}, "hash": "67d6a49a3605f68a4dc05d6ae7e5e3f596b72391c637b340510e824dbf901f51", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "41e35ee7-a929-4f68-af37-634800af56ca", "node_type": "1", "metadata": {}, "hash": "a96f50d50412a686a498b199dd881097792befa3bc904e996972ae74b944e33f", "class_name": "RelatedNodeInfo"}, {"node_id": "cefcf65a-e0d8-4249-8784-d1b253b6d370", "node_type": "1", "metadata": {}, "hash": "babdd53092261df92ff07b828d983f08dfc44cb128306dad491aa52e0656c52d", "class_name": "RelatedNodeInfo"}, {"node_id": "46d530a0-a244-4710-9007-f0264c1aa5e2", "node_type": "1", "metadata": {}, "hash": "b2f83f426c617440838f1f3873b87d404fddd36a8f694327015e131490fcb642", "class_name": "RelatedNodeInfo"}, {"node_id": "32eef4f1-7aa6-4f39-b7b0-c6dcc26185f7", "node_type": "1", "metadata": {}, "hash": "f04fe1c00907969ac63902208a054b58a5eee434dd63ae98240f7bda60455f33", "class_name": "RelatedNodeInfo"}]}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "text": "File Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nConfiguring the response synthesizer for a query engine using `response_mode`:\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import Node, NodeWithScore\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode='compact')\n\nresponse = response_synthesizer.synthesize(\n  \"query text\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..]\n)\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOr, more commonly, in a query engine after you've created an index:\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring the Response Mode\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nResponse synthesizers are typically specified through a `response_mode` kwarg setting.\n\nSeveral response synthesizers are implemented already in LlamaIndex:\n\n- `refine`: \"create and refine\" an answer by sequentially going through each retrieved text chunk. \n    This makes a separate LLM call per Node. Good for more detailed answers.\n- `compact` (default): \"compact\" the prompt during each LLM call by stuffing as \n    many text chunks that can fit within the maximum prompt size. If there are \n    too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n    multiple compact prompts. The same as `refine`, but should result in less LLM calls.\n- `tree_summarize`: Given a set of text chunks and the query, recursively construct a tree \n    and return the root node as the response. Good for summarization purposes.\n- `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt. Good for quick\n    summarization purposes, but may lose detail due to truncation.\n- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n    without actually sending them. Then can be inspected by checking `response.source_nodes`.\n- `accumulate`: Given a set of text chunks and the query, apply the query to each text\n    chunk while accumulating the responses into an array. Returns a concatenated string of all\n    responses. Good for when you need to run the same query separately against each text\n    chunk.\n- `compact_accumulate`: The same as accumulate, but will \"compact\" each LLM prompt similar to\n    `compact`, and run the same query against each text chunk.\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Response Synthesizers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEach response synthesizer inherits from `llama_index.response_synthesizers.base.BaseSynthesizer`. The base API is extremely simple, which makes it easy to create your own response synthesizer.\n\nMaybe you want to customize which template is used at each step in `tree_summarize`, or maybe a new research paper came out detailing a new way to generate a response to a query, you can create your own response synthesizer and plug it into any query engine or use it on it's own.", "start_char_idx": 409694, "end_char_idx": 415528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcd9a011-a035-41e2-87a7-23f1405a9dfc": {"__data__": {"id_": "fcd9a011-a035-41e2-87a7-23f1405a9dfc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "node_type": "1", "metadata": {}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b", "node_type": "1", "metadata": {}, "hash": "de8679944de3bdb9de7f5a3dc0360ecc21480fcb375a28cf0b1e1671e3a1c17d", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "9291fcf4-5c26-46d4-8daf-f632e94482a6", "node_type": "1", "metadata": {}, "hash": "67d6a49a3605f68a4dc05d6ae7e5e3f596b72391c637b340510e824dbf901f51", "class_name": "RelatedNodeInfo"}]}, "hash": "67d6a49a3605f68a4dc05d6ae7e5e3f596b72391c637b340510e824dbf901f51", "text": "Below we show the `__init__()` function, as well as the two abstract methods that every response synthesizer must implement. The basic requirements are to process a query and text chunks, and return a string (or string generator) response.\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Response Synthesizers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nclass BaseSynthesizer(ABC):\n    \"\"\"Response builder class.\"\"\"\n\n    def __init__(\n        self,\n        service_context: Optional[ServiceContext] = None,\n        streaming: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._service_context = service_context or ServiceContext.from_defaults()\n        self._callback_manager = self._service_context.callback_manager\n        self._streaming = streaming\n\n    @abstractmethod\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"\n        ...\n\n    @abstractmethod\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"\n        ...\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe are adding more module guides soon!\nIn the meanwhile, please take a look at the API References.", "start_char_idx": 415530, "end_char_idx": 417452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b": {"__data__": {"id_": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcd9a011-a035-41e2-87a7-23f1405a9dfc", "node_type": "1", "metadata": {}, "hash": "67d6a49a3605f68a4dc05d6ae7e5e3f596b72391c637b340510e824dbf901f51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c7685ea-8047-4290-ac9a-498322fa81da", "node_type": "1", "metadata": {}, "hash": "950e12c152d44fd3ff8a79a58274466279aae9513e19d2c2d87eb400f47ddb89", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b8bff74e-ae63-4b2f-84a2-10cd31bcef79", "node_type": "1", "metadata": {}, "hash": "c2b96f437a911cb039627eab097318134ab140296b8af1be602263c27df4b19f", "class_name": "RelatedNodeInfo"}, {"node_id": "298e40fd-0964-4fca-930f-cf5b053eb630", "node_type": "1", "metadata": {}, "hash": "7906a00b38356f84c3193f0b599d37b6573662a6d937ef3c03d029fa3f4eeee3", "class_name": "RelatedNodeInfo"}, {"node_id": "09c96574-11c8-4d52-a6f4-0226d2004eb9", "node_type": "1", "metadata": {}, "hash": "1c83d30ea01a86b84c379fa65084817ab640d7edfb817c31fecd6fd89ce8062a", "class_name": "RelatedNodeInfo"}]}, "hash": "de8679944de3bdb9de7f5a3dc0360ecc21480fcb375a28cf0b1e1671e3a1c17d", "text": "In the meanwhile, please take a look at the API References.\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Vector Index Retrievers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* VectorIndexRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Vector Index Retrievers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nVectorIndexAutoRetriever </examples/vector_stores/chroma_auto_retriever.ipynb>\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/List Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* ListIndexRetriever \n* ListIndexEmbeddingRetriever \n* ListIndexLLMRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Tree Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* TreeSelectLeafRetriever\n* TreeSelectLeafEmbeddingRetriever\n* TreeAllLeafRetriever\n* TreeRootRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Keyword Table Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* KeywordTableGPTRetriever\n* KeywordTableSimpleRetriever\n* KeywordTableRAKERetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: code\nHeader Path: Module Guides/Knowledge Graph Index\nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\nCustom Retriever (KG Index and Vector Store Index) </examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.ipynb>\n```\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Knowledge Graph Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* KGTableRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Document Summary Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* DocumentSummaryIndexRetriever\n* DocumentSummaryIndexEmbeddingRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Composed Retrievers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* TransformRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Composed Retrievers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere we show the mapping from `retriever_mode` configuration to the selected retriever class.\n> Note that `retriever_mode` can mean different thing for different index classes.", "start_char_idx": 417393, "end_char_idx": 422211, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c7685ea-8047-4290-ac9a-498322fa81da": {"__data__": {"id_": "6c7685ea-8047-4290-ac9a-498322fa81da", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b", "node_type": "1", "metadata": {}, "hash": "de8679944de3bdb9de7f5a3dc0360ecc21480fcb375a28cf0b1e1671e3a1c17d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e", "node_type": "1", "metadata": {}, "hash": "718e2dcdd8d1aa111011d3ce529dc7010abf815229ba66d9395fcf924bf0f38a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "00b09109-7e4e-4405-81d9-eac4ceb4bbe1", "node_type": "1", "metadata": {}, "hash": "bfb9050957beedcf8a107173953b2cd6c1afe7f7667023dc8d3ae910032967dd", "class_name": "RelatedNodeInfo"}, {"node_id": "b1626bfc-e490-40e1-9a72-b01ed133699d", "node_type": "1", "metadata": {}, "hash": "4efeb17a5358f02c9012041326babfc41640dc46505551700ad9d2a9319266ee", "class_name": "RelatedNodeInfo"}, {"node_id": "8d38fd5d-8264-45fc-8417-3cce56944380", "node_type": "1", "metadata": {}, "hash": "421b15082007f3833143e12bc469f19ed9f7e94a8d056707d9b303015c38c643", "class_name": "RelatedNodeInfo"}]}, "hash": "950e12c152d44fd3ff8a79a58274466279aae9513e19d2c2d87eb400f47ddb89", "text": "> Note that `retriever_mode` can mean different thing for different index classes.\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Vector Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSpecifying `retriever_mode` has no effect (silently ignored).\n`vector_index.as_retriever(...)` always returns a VectorIndexRetriever.\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/List Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* `default`: ListIndexRetriever \n* `embedding`: ListIndexEmbeddingRetriever \n* `llm`: ListIndexLLMRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Tree Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* `select_leaf`: TreeSelectLeafRetriever\n* `select_leaf_embedding`: TreeSelectLeafEmbeddingRetriever\n* `all_leaf`: TreeAllLeafRetriever\n* `root`: TreeRootRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Keyword Table Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* `default`: KeywordTableGPTRetriever\n* `simple`: KeywordTableSimpleRetriever\n* `rake`: KeywordTableRAKERetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Knowledge Graph Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* `keyword`: KGTableRetriever\n* `embedding`: KGTableRetriever\n* `hybrid`: KGTableRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Document Summary Index\nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* `default`: DocumentSummaryIndexRetriever\n* `embedding`: DocumentSummaryIndexEmbeddingRetrievers\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: text\nHeader Path: Retriever/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRetrievers are responsible for fetching the most relevant context given a user query (or chat message).  \n\nIt can be built on top of Indices, but can also be defined independently.\nIt is used as a key building block in Query Engines (and Chat Engines) for retrieving relevant context.\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: text\nHeader Path: Retriever/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nConfused about where retriever fits in the pipeline? Read about high-level concepts\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: text\nHeader Path: Retriever/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet started with:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: text\nHeader Path: Retriever/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = index.as_retriever()\nnodes = retriever.retrieve(\"Who is Paul Graham?\")", "start_char_idx": 422129, "end_char_idx": 426886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a8f9c4b-5ccd-4765-9848-990d17ef768e": {"__data__": {"id_": "6a8f9c4b-5ccd-4765-9848-990d17ef768e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c7685ea-8047-4290-ac9a-498322fa81da", "node_type": "1", "metadata": {}, "hash": "950e12c152d44fd3ff8a79a58274466279aae9513e19d2c2d87eb400f47ddb89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "node_type": "1", "metadata": {}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ac33dca7-0115-4896-97d2-a1da715c057f", "node_type": "1", "metadata": {}, "hash": "a0f39a6c8260bd69ac2e2499a4242bb0ab1d2abf49a59d3a6bf11789c7a6b22f", "class_name": "RelatedNodeInfo"}, {"node_id": "85014ae4-1598-4cbc-adb0-282440392bb0", "node_type": "1", "metadata": {}, "hash": "aeaa2ed4d2e72c53c149d6c106703533379ee0167666fc448a5308f7e295e682", "class_name": "RelatedNodeInfo"}, {"node_id": "94979dcc-a9c3-41a5-a731-31d54ad85a5b", "node_type": "1", "metadata": {}, "hash": "4cc770f0f039c3cec2aeaa19fe18b624314cd789cdca471ce33d9dca9d41efa0", "class_name": "RelatedNodeInfo"}]}, "hash": "718e2dcdd8d1aa111011d3ce529dc7010abf815229ba66d9395fcf924bf0f38a", "text": "File Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: code\nHeader Path: Retriever/Usage Pattern\nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: code\nHeader Path: Retriever/Modules\nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet a retriever from index:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = index.as_retriever()\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRetrieve relevant context for a question:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnodes = retriever.retrieve('Who is Paul Graham?')\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note: To learn how to build an index, see Index\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can select the index-specific retriever class via `retriever_mode`. \nFor example, with a `ListIndex`:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = list_index.as_retriever(\n    retriever_mode='llm',\n)\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis creates a ListIndexLLMRetriever on top of the list index.\n\nSee **Retriever Modes** for a full list of (index-specific) retriever modes\nand the retriever classes they map to.", "start_char_idx": 426888, "end_char_idx": 431094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1": {"__data__": {"id_": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e", "node_type": "1", "metadata": {}, "hash": "718e2dcdd8d1aa111011d3ce529dc7010abf815229ba66d9395fcf924bf0f38a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9450061-cd10-4103-8133-a4093c11c82f", "node_type": "1", "metadata": {}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2cc59327-43fd-46e2-b2d0-ec9c077beac5", "node_type": "1", "metadata": {}, "hash": "38e240a498f979439f24d35f2224ae467159772f41ab3e449146cc00e78e46b4", "class_name": "RelatedNodeInfo"}, {"node_id": "b1cc8bdd-50ee-41c8-8b3a-ab587e4cb9d5", "node_type": "1", "metadata": {}, "hash": "d6454df9562c24cb4a9084e8644ab5cccdba2c71e684eb46ac48b6cb888cb5c3", "class_name": "RelatedNodeInfo"}, {"node_id": "2ef03dc7-3c9d-40db-9e66-e4473620b4b1", "node_type": "1", "metadata": {}, "hash": "ca1354f8e51336ccb860f8be13ea8217d1b6108b5931aa19caee2e546aa373af", "class_name": "RelatedNodeInfo"}, {"node_id": "a14e5911-54bd-4998-aac3-9591f2f97113", "node_type": "1", "metadata": {}, "hash": "5da99b9720b492903aa20ebff30c68ff22faaa0d44b2198700d3bc908b56f54e", "class_name": "RelatedNodeInfo"}]}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "text": "File Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\nhidden:\n---\nretriever_modes.md\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Configuring a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn the same way, you can pass kwargs to configure the selected retriever.\n> Note: take a look at the API reference for the selected retriever class' constructor parameters for a list of valid kwargs.\n\nFor example, if we selected the \"llm\" retriever mode, we might do the following:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Configuring a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = list_index.as_retriever(\n    retriever_mode='llm',\n    choice_batch_size=5,\n)\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use the low-level composition API if you need more granular control.  \n\nTo achieve the same outcome as above, you can directly import and construct the desired retriever class:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.list import ListIndexLLMRetriever\n\nretriever = ListIndexLLMRetriever(\n    index=list_index,\n    choice_batch_size=5,\n)\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/High-Level API/Advanced\nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\nDefine Custom Retriever </examples/query_engine/CustomRetrievers.ipynb>\n```\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports integrations with output parsing modules offered\nby other frameworks. These output parsing modules can be used in the following ways:\n- To provide formatting instructions for any prompt / query (through `output_parser.format`)\n- To provide \"parsing\" for LLM outputs (through `output_parser.parse`)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Guardrails\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGuardrails is an open-source Python package for specification/validation/correction of output schemas. See below for a code example.", "start_char_idx": 431096, "end_char_idx": 435416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9450061-cd10-4103-8133-a4093c11c82f": {"__data__": {"id_": "f9450061-cd10-4103-8133-a4093c11c82f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "node_type": "1", "metadata": {}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82d6e145-bda6-4955-8139-0ebd6de31919", "node_type": "1", "metadata": {}, "hash": "1f082b70c7ec1e1691b6db6e13e333bcdccd65d4f36b4af8a1128f1c4c5f9efe", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "038a69b4-043e-45c7-9ce7-aea38a9c30ae", "node_type": "1", "metadata": {}, "hash": "bf85aace88fc80e444b181f21ca8f49aa42fcff35ea72d60849a1e2db3ac2847", "class_name": "RelatedNodeInfo"}, {"node_id": "30d4603a-906a-457a-8787-b958f30381a3", "node_type": "1", "metadata": {}, "hash": "e83a7c0c248e85447e8aa15198840ce72d531d1604ae2a1d6a2ea0f9983bf5de", "class_name": "RelatedNodeInfo"}, {"node_id": "a99b73bc-0a2a-487c-b417-566d106bfbe3", "node_type": "1", "metadata": {}, "hash": "998a0393e6dcb8811fe8619711877c83c1eeaa67ecd585face46046d1019662f", "class_name": "RelatedNodeInfo"}, {"node_id": "14292a30-78f8-4036-bb75-12df73a72eed", "node_type": "1", "metadata": {}, "hash": "7453d184385fad3c9a03355ba1e752ec1f284207db45229ef0e333bcdc0c1066", "class_name": "RelatedNodeInfo"}]}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "text": "See below for a code example.\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Guardrails\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers import GuardrailsOutputParser\nfrom llama_index.llm_predictor import StructuredLLMPredictor\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/load documents, build index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex(documents, chunk_size=512)\nllm_predictor = StructuredLLMPredictor()\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define query / output spec\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nrail_spec = (\"\"\"\n<rail version=\"0.1\">\n\n<output>\n    <list name=\"points\" description=\"Bullet points regarding events in the author's life.\">\n        <object>\n            <string name=\"explanation\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation2\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation3\" format=\"one-line\" on-fail-one-line=\"noop\" />\n        </object>\n    </list>\n</output>\n\n<prompt>\n\nQuery string here.\n\n@xml_prefix_prompt\n\n{output_schema}\n\n@json_suffix_prompt_v2_wo_none\n</prompt>\n</rail>\n\"\"\")\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output parser\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\noutput_parser = GuardrailsOutputParser.from_rail_string(rail_spec, llm=llm_predictor.llm)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/format each prompt with output parser instructions\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\n\nqa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    service_context=ServiceContext.from_defaults(\n        llm_predictor=llm_predictor\n    ),\n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n)\nresponse = query_engine.query(\n    \"What are the three items the author did growing up?\", \n)\nprint(response)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOutput:\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n{'points': [{'explanation': 'Writing short stories', 'explanation2': 'Programming on an IBM 1401', 'explanation3': 'Using microcomputers'}]}\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Langchain\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLangchain also offers output parsing modules that you can use within LlamaIndex.", "start_char_idx": 435387, "end_char_idx": 440938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82d6e145-bda6-4955-8139-0ebd6de31919": {"__data__": {"id_": "82d6e145-bda6-4955-8139-0ebd6de31919", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9450061-cd10-4103-8133-a4093c11c82f", "node_type": "1", "metadata": {}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "node_type": "1", "metadata": {}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f2e09c03-26b5-4fad-8415-c20bd10525d2", "node_type": "1", "metadata": {}, "hash": "1a454bc8220d8464335a868b75fd94d5452bf18553415d20b4f354e41693364e", "class_name": "RelatedNodeInfo"}, {"node_id": "3eb032c3-57ef-4e7a-af31-7b7df20da4a3", "node_type": "1", "metadata": {}, "hash": "64509cc0262dc5a1df2f191b79698a96731c6593e1ce1b46f90a72040afdcc99", "class_name": "RelatedNodeInfo"}, {"node_id": "4ae5f656-cd55-48e5-abc6-78b2ff30365c", "node_type": "1", "metadata": {}, "hash": "65e778f862369d124bd7ba3ab08b0dbbc23eecceecf1486fa611829de250eb57", "class_name": "RelatedNodeInfo"}]}, "hash": "1f082b70c7ec1e1691b6db6e13e333bcdccd65d4f36b4af8a1128f1c4c5f9efe", "text": "File Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Langchain\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers import LangchainOutputParser\nfrom llama_index.llm_predictor import StructuredLLMPredictor\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/load documents, build index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nllm_predictor = StructuredLLMPredictor()\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output schema\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse_schemas = [\n    ResponseSchema(name=\"Education\", description=\"Describes the author's educational experience/background.\"),\n    ResponseSchema(name=\"Work\", description=\"Describes the author's work experience/background.\")\n]\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output parser\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nlc_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\noutput_parser = LangchainOutputParser(lc_output_parser)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/format each prompt with output parser instructions\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\nqa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    service_context=ServiceContext.from_defaults(\n        llm_predictor=llm_predictor\n    ),\n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n)\nresponse = query_engine.query(\n    \"What are a few things the author did growing up?\", \n)\nprint(str(response))\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOutput:\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n{'Education': 'Before college, the author wrote short stories and experimented with programming on an IBM 1401.', 'Work': 'The author worked on writing and programming outside of school.'}", "start_char_idx": 440940, "end_char_idx": 445778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16bd0293-7c88-4fb9-8a72-d944ffe333fb": {"__data__": {"id_": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82d6e145-bda6-4955-8139-0ebd6de31919", "node_type": "1", "metadata": {}, "hash": "1f082b70c7ec1e1691b6db6e13e333bcdccd65d4f36b4af8a1128f1c4c5f9efe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2", "node_type": "1", "metadata": {}, "hash": "c61b7484b150d732bcb42e7c9074f65e085dce210897002e1e3a04cd47c80ce3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "88eba848-fa30-49f3-8783-7ae042d90552", "node_type": "1", "metadata": {}, "hash": "e7aa315076f0e4b6a745878458d312703806d58c7bef4c832cd2c9e1a44056be", "class_name": "RelatedNodeInfo"}, {"node_id": "98f295d9-66cf-4771-88ea-20417403d118", "node_type": "1", "metadata": {}, "hash": "cc3298f6f7b7039b9a5b3fd9e41cfb4ae2ab7ed95f464759afb93f8ee976a5dd", "class_name": "RelatedNodeInfo"}, {"node_id": "60c02106-6a4a-482c-ab2a-8df3532b474b", "node_type": "1", "metadata": {}, "hash": "2b2b42c7e00665eedb74ec681242bc68c59df1bdd4c6de69fb2845805eb80a64", "class_name": "RelatedNodeInfo"}, {"node_id": "727954f9-af6d-4d68-a515-0e1b00bbc39f", "node_type": "1", "metadata": {}, "hash": "f021cf69754f7233064f912f4acdbf21f0e2e5233c47370548a05784437ed75d", "class_name": "RelatedNodeInfo"}]}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "text": "', 'Work': 'The author worked on writing and programming outside of school.'}\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: code\nHeader Path: Output Parsing/Guides\nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\n\n/examples/output_parsing/GuardrailsDemo.ipynb\n/examples/output_parsing/LangchainOutputParserDemo.ipynb\n/examples/output_parsing/guidance_pydantic_program.ipynb\n/examples/output_parsing/guidance_sub_question.ipynb\n/examples/output_parsing/openai_pydantic_program.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nContent Type: text\nHeader Path: Pydantic Program\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nfile_name: pydantic_program.md\nfile_type: None\nfile_size: 1271\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA pydantic program is a generic abstraction that takes in an input string and converts it to a structured Pydantic object type.\n\nBecause this abstraction is so generic, it encompasses a broad range of LLM workflows. The programs are composable and be for more generic or specific use cases.\n\nThere's a few general types of Pydantic Programs:\n- **LLM Text Completion Pydantic Programs**: These convert input text into a user-specified structured object through a text completion API + output parsing.\n- **LLM Function Calling Pydantic Program**: These convert input text into a user-specified structured object through an LLM function calling API.\n- **Prepackaged Pydantic Programs**: These convert input text into prespecified structured objects.\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nContent Type: text\nHeader Path: Pydantic Program/LLM Text Completion Pydantic Programs\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nfile_name: pydantic_program.md\nfile_type: None\nfile_size: 1271\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTODO: Coming soon!\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nContent Type: code\nHeader Path: Pydantic Program/LLM Function Calling Pydantic Programs\nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nfile_name: pydantic_program.md\nfile_type: None\nfile_size: 1271\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/output_parsing/openai_pydantic_program.ipynb\n/examples/output_parsing/guidance_pydantic_program.ipynb\n/examples/output_parsing/guidance_sub_question.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nContent Type: code\nHeader Path: Pydantic Program/Prepackaged Pydantic Programs\nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nfile_name: pydantic_program.md\nfile_type: None\nfile_size: 1271\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/output_parsing/df_program.ipynb\n/examples/output_parsing/evaporate_program.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nContent Type: text\nHeader Path: Structured Outputs\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2489\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe ability of LLMs to produce structured outputs are important for downstream applications that rely on reliably parsing output values. \nLlamaIndex itself also relies on structured output in the following ways.\n- **Document retrieval**: Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval. For instance, the tree index expects LLM calls to be in the format \"ANSWER: (number)\".\n- **Response synthesis**: Users may expect that the final response contains some degree of structure (e.g. a JSON output, a formatted SQL query, etc.)\n\nLlamaIndex provides a variety of modules enabling LLMs to produce outputs in a structured format. We provide modules at different levels of abstraction:\n- **Output Parsers**: These are modules that operate before and after an LLM text completion endpoint. They are not used with LLM function calling endpoints (since those contain structured outputs out of the box).\n- **Pydantic Programs**: These are generic modules that map an input prompt to a structured output, represented by a Pydantic object. They may use function calling APIs or text completion APIs + output parsers.\n- **Pre-defined Pydantic Program**: We have pre-defined Pydantic programs that map inputs to specific output types (like dataframes).\n\nSee the sections below for an overview of output parsers and Pydantic programs.\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nContent Type: text\nHeader Path: Structured Outputs/\ud83d\udd2c Anatomy of a Structured Output Function\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2489\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere we describe the different components of an LLM-powered structured output function. The pipeline depends on whether you're using a **generic LLM text completion API** or an **LLM function calling API**.\n\n!\n\nWith generic completion APIs, the inputs and outputs are handled by text prompts. The output parser plays a role before and after the LLM call in ensuring structured outputs. Before the LLM call, the output parser can\nappend format instructions to the prompt.", "start_char_idx": 445701, "end_char_idx": 451714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aed5c07e-3ab7-444d-8c32-f894592fe2b2": {"__data__": {"id_": "aed5c07e-3ab7-444d-8c32-f894592fe2b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "node_type": "1", "metadata": {}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9", "node_type": "1", "metadata": {}, "hash": "189ae8fea1540694ea3ba1fda1c13a9104e66df13d9d11990c18ae003ada3c9d", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "d87db227-b408-45bb-acb5-86b12488fd18", "node_type": "1", "metadata": {}, "hash": "3044ef47fd268ca12c96f7f62f9f70dcbc0567e9a6d4149de8bc9c0391c193ec", "class_name": "RelatedNodeInfo"}, {"node_id": "3daabe54-04ed-4728-bff1-58bd74f9cdb9", "node_type": "1", "metadata": {}, "hash": "17f7fdcb725494ecda3a6faa05222bbb7ec97687f17bf6d0e37901a9f565bbf0", "class_name": "RelatedNodeInfo"}, {"node_id": "8120e5b1-0e04-4361-acc5-40389a3d4a23", "node_type": "1", "metadata": {}, "hash": "a5107b2740f71571e7963e40b36c293d0db2b13b35cbfd0d5ef13d6467cb5bdb", "class_name": "RelatedNodeInfo"}]}, "hash": "c61b7484b150d732bcb42e7c9074f65e085dce210897002e1e3a04cd47c80ce3", "text": "Before the LLM call, the output parser can\nappend format instructions to the prompt. After the LLM call, the output parser can parse the output to the specified instructions.\n\nWith function calling APIs, the output is inherently in a structured format, and the input can take in the signature of the desired object. The structured output just needs to be cast in the right object format (e.g. Pydantic).\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nContent Type: code\nHeader Path: Structured Outputs/Output Parser Modules\nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2489\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\noutput_parser.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nContent Type: code\nHeader Path: Structured Outputs/Pydantic Program Modules\nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2489\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\npydantic_program.md\n```\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nContent Type: text\nHeader Path: Callbacks/Concept\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2492\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library. \nUsing the callback manager, as many callbacks as needed can be added.\n\nIn addition to logging data related to events, you can also track the duration and number of occurances\nof each event. \n\nFurthermore, a trace map of events is also recorded, and callbacks can use this data\nhowever they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events\nafter most operations.\n\n**Callback Event Types**  \nWhile each callback may not leverage each event type, the following events are available to be tracked:\n\n- `CHUNKING` -> Logs for the before and after of text splitting.\n- `NODE_PARSING` -> Logs for the documents and the nodes that they are parsed into.\n- `EMBEDDING` -> Logs for the number of texts embedded.\n- `LLM` -> Logs for the template and response of LLM calls.\n- `QUERY` -> Keeps track of the start and end of each query.\n- `RETRIEVE` -> Logs for the nodes retrieved for a query.\n- `SYNTHESIZE` -> Logs for the result for synthesize calls.\n- `TREE` -> Logs for the summary and level of summaries generated.\n- `SUB_QUESTIONS` -> Logs for the sub questions and answers generated.\n\nYou can implement your own callback to track and trace these events, or use an existing callback.\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nContent Type: text\nHeader Path: Callbacks/Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2492\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCurrently supported callbacks are as follows:\n\n- TokenCountingHandler -> Flexible token counting for prompt, completion, and embedding token usage. See the migration details here\n- LlamaDebugHanlder -> Basic tracking and tracing for events. Example usage can be found in the notebook below.\n- WandbCallbackHandler -> Tracking of events and traces using the Wandb Prompts frontend. More details are in the notebook below or at Wandb\n- AimCallback -> Tracking of LLM inputs and outputs. Example usage can be found in the notebook below.\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nContent Type: text\nHeader Path: Callbacks/Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2492\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\nhidden:\n---\n/examples/callbacks/TokenCountingHandler.ipynb\n/examples/callbacks/LlamaDebugHandler.ipynb\n/examples/callbacks/WandbCallbackHandler.ipynb\n/examples/callbacks/AimCallback.ipynb\ntoken_counting_migration.md\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: Token Counting - Migration Guide\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe existing token counting implementation has been __deprecated__. \n\nWe know token counting is important to many users, so this guide was created to walkthrough a (hopefully painless) transition. \n\nPreviously, token counting was kept track of on the `llm_predictor` and `embed_model` objects directly, and optionally printed to the console. This implementation used a static tokenizer for token counting (gpt-2), and the `last_token_usage` and `total_token_usage` attributes were not always kept track of properly.\n\nGoing forward, token counting as moved into a callback. Using the `TokenCountingHandler` callback, you now have more options for how tokens are counted, the lifetime of the token counts, and even creating separete token counters for different indexes.", "start_char_idx": 451630, "end_char_idx": 457102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9": {"__data__": {"id_": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2", "node_type": "1", "metadata": {}, "hash": "c61b7484b150d732bcb42e7c9074f65e085dce210897002e1e3a04cd47c80ce3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484", "node_type": "1", "metadata": {}, "hash": "80e78fffd7b5fa2cbf81c024a795f520cfdc746c6d7b13f609dcc1ed7ba0e3cb", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "70840c13-c0a8-4197-acfc-40bea23adbe0", "node_type": "1", "metadata": {}, "hash": "954c6c872ea77ad559408f33e34bb054714d6d7ac7233ef1460bef441ea17ddc", "class_name": "RelatedNodeInfo"}, {"node_id": "83722899-33b6-4e87-9f1a-884ed7627d55", "node_type": "1", "metadata": {}, "hash": "98de344efb058cb74b019867ee8ceb9432e31dc2596805e3fa33c27e447658a5", "class_name": "RelatedNodeInfo"}, {"node_id": "ddd83ffd-eabe-4b99-90af-124ba80a5438", "node_type": "1", "metadata": {}, "hash": "2a75f2713160344c4f73e1ccad485b3d6b9f6246342db2da46f018dd935d94d6", "class_name": "RelatedNodeInfo"}]}, "hash": "189ae8fea1540694ea3ba1fda1c13a9104e66df13d9d11990c18ae003ada3c9d", "text": "Here is a minimum example of using the new `TokenCountingHandler` with an OpenAI model:\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: Token Counting - Migration Guide\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport tiktoken\nfrom llama_index.callbacks import CallbackManager, TokenCountingHandler\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: NOTE: The tokenizer should be a function that takes in text and returns a list of tokens\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\"text-davinci-003\").encode\n    verbose=False  # set to true to see usage printed to the console\n)\n\ncallback_manager = CallbackManager([token_counter])\n\nservice_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n\ndocument = SimpleDirectoryReader(\"./data\").load_data()\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: if verbose is turned on, you will see embedding token usage printed\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: otherwise, you can access the count directly\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprint(token_counter.total_embedding_token_count)\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: reset the counts at your discretion!\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntoken_counter.reset_counts()\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: also track prompt, completion, and total LLM tokens, in addition to embeddings\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = index.as_query_engine().query(\"What did the author do growing up?\")\nprint('Embedding Tokens: ', token_counter.total_embedding_token_count, '\\n',\n      'LLM Prompt Tokens: ', token_counter.prompt_llm_token_count, '\\n',\n      'LLM Completion Tokens: ', token_counter.completion_llm_token_count, '\\n',\n      'Total LLM Token Count: ', token_counter.total_llm_token_count)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEach call to an LLM will cost some amount of money - for instance, OpenAI's gpt-3.5-turbo costs $0.002 / 1k tokens. The cost of building an index and querying depends on \n\n- the type of LLM used\n- the type of data structure used\n- parameters used during building \n- parameters used during querying\n\nThe cost of building and querying each index is a TODO in the reference documentation. In the meantime, we provide the following information:\n\n1. A high-level overview of the cost structure of the indices.\n2. A token predictor that you can use directly within LlamaIndex!", "start_char_idx": 457104, "end_char_idx": 461774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484": {"__data__": {"id_": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9", "node_type": "1", "metadata": {}, "hash": "189ae8fea1540694ea3ba1fda1c13a9104e66df13d9d11990c18ae003ada3c9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd01209c-9c3a-4eb0-8594-239574f103e7", "node_type": "1", "metadata": {}, "hash": "7d811f93b41cad65008c0176862140e8e18267816f04ecf320884101d51ea033", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b8c86f8b-5eb4-45ab-a1ec-52fd2593b30f", "node_type": "1", "metadata": {}, "hash": "c05581f92dfd81f82818e047c3807c9cf33cb3d809c6ae156017f24f987821e5", "class_name": "RelatedNodeInfo"}, {"node_id": "cb9484de-8627-4501-906e-291343164f4b", "node_type": "1", "metadata": {}, "hash": "3489a5933e175a2d830ad71908d55e63497c60412eff15669bd9ff629cd9289a", "class_name": "RelatedNodeInfo"}, {"node_id": "cc8303a3-e113-46f4-a075-42b6a782e8e3", "node_type": "1", "metadata": {}, "hash": "36b62737306258159a4aadf4a41a54dc948042c5f33b9dd3ebe535dcd85dba61", "class_name": "RelatedNodeInfo"}]}, "hash": "80e78fffd7b5fa2cbf81c024a795f520cfdc746c6d7b13f609dcc1ed7ba0e3cb", "text": "2. A token predictor that you can use directly within LlamaIndex!\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Indices with no LLM calls\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe following indices don't require LLM calls at all during building (0 cost):\n- `ListIndex`\n- `SimpleKeywordTableIndex` - uses a regex keyword extractor to extract keywords from each document\n- `RAKEKeywordTableIndex` - uses a RAKE keyword extractor to extract keywords from each document\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Indices with LLM calls\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe following indices do require LLM calls during build time:\n- `TreeIndex` - use LLM to hierarchically summarize the text to build the tree\n- `KeywordTableIndex` - use LLM to extract keywords from each document\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Query Time\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThere will always be >= 1 LLM call during query time, in order to synthesize the final answer. \nSome indices contain cost tradeoffs between index building and querying. `ListIndex`, for instance,\nis free to build, but running a query over a list index (without filtering or embedding lookups), will\ncall the LLM {math}`N` times.\n\nHere are some notes regarding each of the indices:\n- `ListIndex`: by default requires {math}`N` LLM calls, where N is the number of nodes.\n- `TreeIndex`: by default requires {math}`\\log (N)` LLM calls, where N is the number of leaf nodes. \n    - Setting `child_branch_factor=2` will be more expensive than the default `child_branch_factor=1` (polynomial vs logarithmic), because we traverse 2 children instead of just 1 for each parent node.\n- `KeywordTableIndex`: by default requires an LLM call to extract query keywords.\n    - Can do `index.as_retriever(retriever_mode=\"simple\")` or `index.as_retriever(retriever_mode=\"rake\")` to also use regex/RAKE keyword extractors on your query text.\n-  `VectorStoreIndex`: by default, requires one LLM call per query. If you increase the `similarity_top_k` or `chunk_size`, or change the `response_mode`, then this number will increase.\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex offers token **predictors** to predict token usage of LLM and embedding calls.\nThis allows you to estimate your costs during 1) index construction, and 2) index querying, before\nany respective LLM calls are made.\n\nTokens are counted using the `TokenCountingHandler` callback. See the example notebook for details on the setup.\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Using MockLLM\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo predict token usage of LLM calls, import and instantiate the MockLLM as shown below. The `max_tokens` parameter is used as a \"worst case\" prediction, where each LLM response will contain exactly that number of tokens. If `max_tokens` is not specified, then it will simply predict back the prompt.", "start_char_idx": 461709, "end_char_idx": 466061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd01209c-9c3a-4eb0-8594-239574f103e7": {"__data__": {"id_": "bd01209c-9c3a-4eb0-8594-239574f103e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484", "node_type": "1", "metadata": {}, "hash": "80e78fffd7b5fa2cbf81c024a795f520cfdc746c6d7b13f609dcc1ed7ba0e3cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39f0cb49-3072-4e99-824e-39b89862f502", "node_type": "1", "metadata": {}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "36616633-4d3b-481b-ac71-c52d704cf5da", "node_type": "1", "metadata": {}, "hash": "8da9dbab48d5b0c6b80240c50c128bf7f49db91cf181bea650653424458d3d84", "class_name": "RelatedNodeInfo"}, {"node_id": "aa7f8c98-34f1-4ec6-9841-cbce006ebae2", "node_type": "1", "metadata": {}, "hash": "a527c81ea27be46ad52da3d255cda4ad3689a1b296648093eb684dbbebe0ae8d", "class_name": "RelatedNodeInfo"}, {"node_id": "1efbc0eb-eb89-47db-8965-39835610c7bb", "node_type": "1", "metadata": {}, "hash": "bd14bd05180dcc08fa90591e1d213be7b63a90e6b384c6740b283f7089106d79", "class_name": "RelatedNodeInfo"}]}, "hash": "7d811f93b41cad65008c0176862140e8e18267816f04ecf320884101d51ea033", "text": "If `max_tokens` is not specified, then it will simply predict back the prompt.\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Using MockLLM\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, set_global_service_context\nfrom llama_index.llms import MockLLM\n\nllm = MockLLM(max_tokens=256)\n\nservice_context = ServiceContext.from_defaults(llm=llm)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/optionally set a global service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nset_global_service_context(service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/optionally set a global service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can then use this predictor during both index construction and querying.\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Using MockEmbedding\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou may also predict the token usage of embedding calls with `MockEmbedding`.\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Using MockEmbedding\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, set_global_service_context\nfrom llama_index import MockEmbedding\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/specify a MockLLMPredictor\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nembed_model = MockEmbedding(embed_dim=1536)\n\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/optionally set a global service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nset_global_service_context(service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRead about the full usage pattern below!\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\ncaption: Examples\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn order to measure LLM and Embedding token counts, you'll need to\n\n1.", "start_char_idx": 465983, "end_char_idx": 470848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39f0cb49-3072-4e99-824e-39b89862f502": {"__data__": {"id_": "39f0cb49-3072-4e99-824e-39b89862f502", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd01209c-9c3a-4eb0-8594-239574f103e7", "node_type": "1", "metadata": {}, "hash": "7d811f93b41cad65008c0176862140e8e18267816f04ecf320884101d51ea033", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "node_type": "1", "metadata": {}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2c420b97-581e-4f93-a7da-d3b136554c80", "node_type": "1", "metadata": {}, "hash": "f5fc8393dcd64e38a48252f96ead2f2d2aef6bacbeb13af5d86e1b78bde52963", "class_name": "RelatedNodeInfo"}, {"node_id": "f60cd002-f417-4f3f-a2bc-1a1666a150ec", "node_type": "1", "metadata": {}, "hash": "f64fe6946e6e33356ce362b42b0127d8c1fcd85d23e41f8a6e50cafc9c130b08", "class_name": "RelatedNodeInfo"}, {"node_id": "7b2f6c81-0a0f-4da3-8a2c-12c0ac7d710f", "node_type": "1", "metadata": {}, "hash": "6adca613739d7bbe325eb877c7165ae7761f46d27bcdaf0b6e55554a2c8f9180", "class_name": "RelatedNodeInfo"}, {"node_id": "3c629b58-2558-4626-87d6-aca4236b12da", "node_type": "1", "metadata": {}, "hash": "7ab613def70f57af6de2c2ff0fafb53e2fdaf011e1dde6bd38d2fc7942259ef3", "class_name": "RelatedNodeInfo"}]}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "text": "Setup `MockLLM` and `MockEmbedding` objects\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.llms import MockLLM\nfrom llama_index import MockEmbedding\n\nllm = MockLLM(max_tokens=256)\nembed_model = MockEmbedding(embed_dim=1536)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n2. Setup the `TokenCountingCallback` handler\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport tiktoken\nfrom llama_index.callbacks import CallbackManager, TokenCountingHandler\n\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n)\n\ncallback_manager = CallbackManager([token_counter])\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n3. Add them to the global `ServiceContext`\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, set_global_service_context\n\nset_global_service_context(\n    ServiceContext.from_defaults(\n        llm=llm, \n        embed_model=embed_model, \n        callback_manager=callback_manager\n    )\n)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n4. Construct an Index\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./docs/examples/data/paul_graham\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n5. Measure the counts!", "start_char_idx": 470849, "end_char_idx": 475086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb": {"__data__": {"id_": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39f0cb49-3072-4e99-824e-39b89862f502", "node_type": "1", "metadata": {}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec5952e9-f4da-452a-9210-1509bb76996d", "node_type": "1", "metadata": {}, "hash": "7c9418cadbe2a894f1312ac989c2464a536fc0613af17f8cfd2383eeac54332c", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2ea2aa38-021e-4b6b-89c7-527a836a0c27", "node_type": "1", "metadata": {}, "hash": "18295918124cf1396f321d852db952f16a9b5a3515aca0aeea70ef8958b257f4", "class_name": "RelatedNodeInfo"}, {"node_id": "5a023095-45e1-4385-b446-c75328fdfe72", "node_type": "1", "metadata": {}, "hash": "5d72397fc462605e7d8731096b836d16830770844602c4799267196811638f00", "class_name": "RelatedNodeInfo"}, {"node_id": "fc476e0c-e354-4a0e-8eca-6c98817c9859", "node_type": "1", "metadata": {}, "hash": "e1e9e1563a4a9134fe8095fe40af4eb19f2c6611a697301ca8c786873a3a6e8b", "class_name": "RelatedNodeInfo"}, {"node_id": "8d7fb08f-830e-48d5-bf7b-256fb6afa197", "node_type": "1", "metadata": {}, "hash": "98b337a1974c9d61d35f8fce9585dcb55350650da09de460e86daed6d8f63da0", "class_name": "RelatedNodeInfo"}]}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "text": "Measure the counts!\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprint(\n    \"Embedding Tokens: \",\n    token_counter.total_embedding_token_count,\n    \"\\n\",\n    \"LLM Prompt Tokens: \",\n    token_counter.prompt_llm_token_count,\n    \"\\n\",\n    \"LLM Completion Tokens: \",\n    token_counter.completion_llm_token_count,\n    \"\\n\",\n    \"Total LLM Token Count: \",\n    token_counter.total_llm_token_count,\n    \"\\n\",\n)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/reset counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntoken_counter.reset_counts()\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/reset counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n6. Run a query, mesaure again\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/reset counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"query\")\n\nprint(\n    \"Embedding Tokens: \",\n    token_counter.total_embedding_token_count,\n    \"\\n\",\n    \"LLM Prompt Tokens: \",\n    token_counter.prompt_llm_token_count,\n    \"\\n\",\n    \"LLM Completion Tokens: \",\n    token_counter.completion_llm_token_count,\n    \"\\n\",\n    \"Total LLM Token Count: \",\n    token_counter.total_llm_token_count,\n    \"\\n\",\n)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\modules.md\nContent Type: text\nHeader Path: Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 290\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNotebooks with usage of these components can be found below.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\modules.md\nContent Type: text\nHeader Path: Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 290\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\n\n../../../examples/evaluation/TestNYC-Evaluation.ipynb\n../../../examples/evaluation/TestNYC-Evaluation-Query.ipynb\n../../../examples/evaluation/QuestionGeneration.ipynb\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEvaluation in generative AI and retrieval is a difficult task. Due to the unpredictable nature of text, and a general lack of \"expected\" outcomes to compare against, there are many blockers to getting started with evaluation.\n\nHowever, LlamaIndex offers a few key modules for evaluating the quality of both Document retrieval and response synthesis.\nHere are some key questions for each component:\n\n- **Document retrieval**: Are the sources relevant to the query?\n- **Response synthesis**: Does the response match the retrieved context? Does it also match the query? \n\nThis guide describes how the evaluation components within LlamaIndex work. Note that our current evaluation modules\ndo *not* require ground-truth labels. Evaluation can be done with some combination of the query, context, response,\nand combine these with LLM calls.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Evaluation of the Response + Context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEach response from a `query_engine.query` calls returns both the synthesized response as well as source documents.\n\nWe can evaluate the response against the retrieved sources - without taking into account the query!\n\nThis allows you to measure hallucination - if the response does not match the retrieved sources, this means that the model may be \"hallucinating\" an answer since it is not rooting the answer in the context provided to it in the prompt.\n\nThere are two sub-modes of evaluation here. We can either get a binary response \"YES\"/\"NO\" on whether response matches *any* source context,\nand also get a response list across sources to see which sources match.\n\nThe `ResponseEvaluator` handles both modes for evaluating in this context.", "start_char_idx": 475067, "end_char_idx": 480629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec5952e9-f4da-452a-9210-1509bb76996d": {"__data__": {"id_": "ec5952e9-f4da-452a-9210-1509bb76996d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "node_type": "1", "metadata": {}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07e1b2c7-9b3f-4256-93a1-db834bab04d8", "node_type": "1", "metadata": {}, "hash": "b94644963a0d9e7a5ec557bfe45d6d47f59bd8badc004a01e279445195d2b350", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "62b532e5-e99c-4d07-a113-8924ff067a71", "node_type": "1", "metadata": {}, "hash": "7a86e404b5db1a8bb8bf375748623f77928bb5deece337f7841e111873803413", "class_name": "RelatedNodeInfo"}, {"node_id": "9e11533d-8fd3-461d-83fc-83f699cbd98c", "node_type": "1", "metadata": {}, "hash": "f52fc51af468c6671811dceac293ff82fa12351c69a72f99a203e3836c723a35", "class_name": "RelatedNodeInfo"}]}, "hash": "7c9418cadbe2a894f1312ac989c2464a536fc0613af17f8cfd2383eeac54332c", "text": "The `ResponseEvaluator` handles both modes for evaluating in this context.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Evaluation of the Query + Response + Source Context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis is similar to the above section, except now we also take into account the query. The goal is to determine if\nthe response + source context answers the query.\n\nAs with the above, there are two submodes of evaluation. \n- We can either get a binary response \"YES\"/\"NO\" on whether\nthe response matches the query, and whether any source node also matches the query.\n- We can also ignore the synthesized response, and check every source node to see\nif it matches the query.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Question Generation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn addition to evaluating queries, LlamaIndex can also use your data to generate questions to evaluate on. This means that you can automatically generate questions, and then run an evaluation pipeline to test if the LLM can actually answer questions accurately using your data.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor full usage details, see the usage pattern below.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNotebooks with usage of these components can be found below.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Binary Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis mode of evaluation will return \"YES\"/\"NO\" if the synthesized response matches any source context.", "start_char_idx": 480555, "end_char_idx": 484197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07e1b2c7-9b3f-4256-93a1-db834bab04d8": {"__data__": {"id_": "07e1b2c7-9b3f-4256-93a1-db834bab04d8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec5952e9-f4da-452a-9210-1509bb76996d", "node_type": "1", "metadata": {}, "hash": "7c9418cadbe2a894f1312ac989c2464a536fc0613af17f8cfd2383eeac54332c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "422218a9-2dc5-4297-8cb4-280ebd599a91", "node_type": "1", "metadata": {}, "hash": "47fc36050e751d1f22daaeb938476c167d4ec84f7d9872fb689621df57b80c3d", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1df1131e-791b-4c13-b2b9-fac7e3f0fbe7", "node_type": "1", "metadata": {}, "hash": "44460d72f5bf74b319952500b633d4896bd081e14c47841b528803c0268ac65f", "class_name": "RelatedNodeInfo"}, {"node_id": "06c9b143-e5af-4e8e-8354-26cdab4080ca", "node_type": "1", "metadata": {}, "hash": "99cce85546cc279317abb9f54cc43ea56a6c6f603926d90628acfbfd4e4fc6e1", "class_name": "RelatedNodeInfo"}]}, "hash": "b94644963a0d9e7a5ec557bfe45d6d47f59bd8badc004a01e279445195d2b350", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Binary Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import ResponseEvaluator\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/define evaluator\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nevaluator = ResponseEvaluator(service_context=service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\"What battles took place in New York City in the American Revolution?\")\neval_result = evaluator.evaluate(response)\nprint(str(eval_result))\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou'll get back either a `YES` or `NO` response.\n\n!\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Sources Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis mode of evaluation will return \"YES\"/\"NO\" for every source node.", "start_char_idx": 484199, "end_char_idx": 487702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "422218a9-2dc5-4297-8cb4-280ebd599a91": {"__data__": {"id_": "422218a9-2dc5-4297-8cb4-280ebd599a91", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07e1b2c7-9b3f-4256-93a1-db834bab04d8", "node_type": "1", "metadata": {}, "hash": "b94644963a0d9e7a5ec557bfe45d6d47f59bd8badc004a01e279445195d2b350", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a933fe9-3bf3-469b-b381-0fde2c77d1af", "node_type": "1", "metadata": {}, "hash": "a82967ac4a25c0427e823e185c9427d37d2b3985459baae4a693fa91b2763329", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "bbe39360-85bc-48ea-bb0b-5cabbdf5a707", "node_type": "1", "metadata": {}, "hash": "b360eb2d395ab7be20489da4a654901825bc229e2cfe94339d14c677930f4b1d", "class_name": "RelatedNodeInfo"}, {"node_id": "f1be8183-5f77-4bac-9853-97f8c646e8e0", "node_type": "1", "metadata": {}, "hash": "5b5c37b170c00a6cdb7f78839cf01e36bff96fa5c0d89f2d95bfd6ba6f311790", "class_name": "RelatedNodeInfo"}]}, "hash": "47fc36050e751d1f22daaeb938476c167d4ec84f7d9872fb689621df57b80c3d", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Sources Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\nfrom llama_index.evaluation import ResponseEvaluator\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/define evaluator\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nevaluator = ResponseEvaluator(service_context=service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\"What battles took place in New York City in the American Revolution?\")\neval_result = evaluator.evaluate_source_nodes(response)\nprint(str(eval_result))\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou'll get back a list of \"YES\"/\"NO\", corresponding to each source node in `response.source_nodes`.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Binary Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis mode of evaluation will return \"YES\"/\"NO\" if the synthesized response matches the query + any source context.", "start_char_idx": 487704, "end_char_idx": 491332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a933fe9-3bf3-469b-b381-0fde2c77d1af": {"__data__": {"id_": "7a933fe9-3bf3-469b-b381-0fde2c77d1af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "422218a9-2dc5-4297-8cb4-280ebd599a91", "node_type": "1", "metadata": {}, "hash": "47fc36050e751d1f22daaeb938476c167d4ec84f7d9872fb689621df57b80c3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c58fa53f-1bec-4b92-b3c0-28261fffc47e", "node_type": "1", "metadata": {}, "hash": "a72b0fb285709b9aa500c22f866cf120574de7e8a743ce66828e299d8ed999e6", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "c9f022c4-2428-4af6-8818-20c27f67d2c7", "node_type": "1", "metadata": {}, "hash": "f4f3547cd228572f88bca22fdb76dab36f12267a923684d20c75df1317d9e860", "class_name": "RelatedNodeInfo"}, {"node_id": "220062a2-05f1-4532-97b9-c9e378f4eda5", "node_type": "1", "metadata": {}, "hash": "1cf7a1db25ceb5a1e66c309e2a07d817c595531a34bcf000430b8431c042aec3", "class_name": "RelatedNodeInfo"}]}, "hash": "a82967ac4a25c0427e823e185c9427d37d2b3985459baae4a693fa91b2763329", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Binary Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import QueryResponseEvaluator\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/define evaluator\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nevaluator = QueryResponseEvaluator(service_context=service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\"What battles took place in New York City in the American Revolution?\")\neval_result = evaluator.evaluate(response)\nprint(str(eval_result))\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Sources Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis mode of evaluation will look at each source node, and see if each source node contains an answer to the query.", "start_char_idx": 491334, "end_char_idx": 494843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c58fa53f-1bec-4b92-b3c0-28261fffc47e": {"__data__": {"id_": "c58fa53f-1bec-4b92-b3c0-28261fffc47e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a933fe9-3bf3-469b-b381-0fde2c77d1af", "node_type": "1", "metadata": {}, "hash": "a82967ac4a25c0427e823e185c9427d37d2b3985459baae4a693fa91b2763329", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "node_type": "1", "metadata": {}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "6eed3470-65ae-4a30-94e4-cefb8b399503", "node_type": "1", "metadata": {}, "hash": "1783a6eb1cc1b6b987c4ef83f1ef53e62d143a3c29b5506f150a81f95869cf53", "class_name": "RelatedNodeInfo"}, {"node_id": "617168b3-3ae1-40f2-9e9a-0e2410dbd36a", "node_type": "1", "metadata": {}, "hash": "e326b3024b1dd9ea3d35d01fc2dcf2d0b1ae403947149dac811583900cce1d0f", "class_name": "RelatedNodeInfo"}]}, "hash": "a72b0fb285709b9aa500c22f866cf120574de7e8a743ce66828e299d8ed999e6", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Sources Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\nfrom llama_index.evaluation import QueryResponseEvaluator\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/define evaluator\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nevaluator = QueryResponseEvaluator(service_context=service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\"What battles took place in New York City in the American Revolution?\")\neval_result = evaluator.evaluate_source_nodes(response)\nprint(str(eval_result))\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Question Generation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can also generate questions to answer using your data. Using in combination with the above evaluators, you can create a fully automated evaluation pipeline over your data.", "start_char_idx": 494845, "end_char_idx": 498455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe": {"__data__": {"id_": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c58fa53f-1bec-4b92-b3c0-28261fffc47e", "node_type": "1", "metadata": {}, "hash": "a72b0fb285709b9aa500c22f866cf120574de7e8a743ce66828e299d8ed999e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25f0891c-e28f-439f-b61f-356860391fab", "node_type": "1", "metadata": {}, "hash": "7b32858201113b02d3a93d6a3fb16e6913c7a9e842583cdd811c7c0aeb26e9ba", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "4c1b9aa9-2e93-4c24-9677-b9fe35aecb55", "node_type": "1", "metadata": {}, "hash": "e5601d3637ebe625b425eebe7449bb42aa333fb878a6b0a7590bcdc9c0d2af02", "class_name": "RelatedNodeInfo"}, {"node_id": "ff7170af-6d0f-4f65-8b68-6e8b25416e3b", "node_type": "1", "metadata": {}, "hash": "818ddb8090dcfa4137cea2e7eb896774696c95c0dc203d5497c418832ebd9bf8", "class_name": "RelatedNodeInfo"}, {"node_id": "37e90aa5-f3f4-4fdb-904a-f7e2546c2cdc", "node_type": "1", "metadata": {}, "hash": "c13d1964eeb1e406a8d921b3decf181839a5ad5e7d5c6ba8eba105c99f0ef5b2", "class_name": "RelatedNodeInfo"}, {"node_id": "9c2ae263-8111-4b0a-aa1a-8499e26a67f5", "node_type": "1", "metadata": {}, "hash": "d0c03c0a05b47571ee21ddde5ec85c9f1b3bf05eb62e9dc6e2cf5d45e55feb4c", "class_name": "RelatedNodeInfo"}]}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Question Generation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\nfrom llama_index.evaluation import ResponseEvaluator\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build documents\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/define genertor, generate questions\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndata_generator = DatasetGenerator.from_documents(documents)\n\neval_questions = data_generator.generate_questions_from_nodes()\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/Concept\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe Playground module in LlamaIndex is a way to automatically test your data (i.e. documents) across a diverse combination of indices, models, embeddings, modes, etc. to decide which ones are best for your purposes. More options will continue to be added.\n\nFor each combination, you'll be able to compare the results for any query and compare the answers, latency, tokens used, and so on.\n\nYou may initialize a Playground with a list of pre-built indices, or initialize one from a list of Documents using the preset indices.\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA sample usage is given below.\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import download_loader\nfrom llama_index.indices.vector_store import VectorStoreIndex\nfrom llama_index.indices.tree.base import TreeIndex\nfrom llama_index.playground import Playground\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/load data\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWikipediaReader = download_loader(\"WikipediaReader\")\nloader = WikipediaReader()\ndocuments = loader.load_data(pages=['Berlin'])\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/define multiple index data structures (vector index, list index)\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindices = [VectorStoreIndex(documents), TreeIndex(documents)]\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/initialize playground\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nplayground = Playground(indices=indices)\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/playground compare\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nplayground.compare(\"What is the population of Berlin?\")", "start_char_idx": 498457, "end_char_idx": 504037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25f0891c-e28f-439f-b61f-356860391fab": {"__data__": {"id_": "25f0891c-e28f-439f-b61f-356860391fab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "node_type": "1", "metadata": {}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "node_type": "1", "metadata": {}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a5f910fc-19c2-4e85-b99c-cb02ca9f570a", "node_type": "1", "metadata": {}, "hash": "35d12bbf9c32bfaf6ede5b951cc1b9ab8ace9eb0f3ab27bf0c2106a17705583c", "class_name": "RelatedNodeInfo"}, {"node_id": "037cd9ea-c523-48fd-a581-855307260406", "node_type": "1", "metadata": {}, "hash": "88200e44b9c1f5ddc07862d48b7ba279ce7645bd9cfb5cf6fef24ac3860bfaca", "class_name": "RelatedNodeInfo"}, {"node_id": "2f22326b-f3fd-4295-b2b1-41c05122c2c3", "node_type": "1", "metadata": {}, "hash": "582ad4b5686adbf2ee3f2fcdcee5169a67bc82caf51a41a29f0da663de2a8b25", "class_name": "RelatedNodeInfo"}]}, "hash": "7b32858201113b02d3a93d6a3fb16e6913c7a9e842583cdd811c7c0aeb26e9ba", "text": "File Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: code\nHeader Path: Playground/Modules\nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n../../../examples/analysis/PlaygroundDemo.ipynb\n```\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Concept\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `ServiceContext` is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application.\nYou can use it to set the global configuration, as well as local configurations at specific parts of the pipeline.\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Configuring the service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `ServiceContext` is a simple python dataclass that you can directly construct by passing in the desired components.\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Configuring the service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@dataclass\nclass ServiceContext:\n    # The LLM used to generate natural language responses to queries.\n    llm_predictor: BaseLLMPredictor\n\n    # The PromptHelper object that helps with truncating and repacking text chunks to fit in the LLM's context window.\n    prompt_helper: PromptHelper\n\n    # The embedding model used to generate vector representations of text.\n    embed_model: BaseEmbedding\n\n    # The parser that converts documents into nodes.\n    node_parser: NodeParser\n\n    # The callback manager object that calls it's handlers on events. Provides basic logging and tracing capabilities.\n    callback_manager: CallbackManager\n\n    @classmethod\n    def from_defaults(cls, ...) -> \"ServiceContext\":\n      ...\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: code\nHeader Path: ServiceContext/Usage Pattern/Configuring the service context\nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nLearn how to configure specific modules:\n- LLM\n- Embedding Model\n- Node Parser\n\n```\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Configuring the service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe also expose some common kwargs (of the above components) via the `ServiceContext.from_defaults` method\nfor convenience (so you don't have to manually construct them).\n \n**Kwargs for node parser**:\n- `chunk_size`: The size of the text chunk for a node . Is used for the node parser when they aren't provided.\n- `chunk_overlap`: The amount of overlap between nodes (i.e. text chunks).\n\n**Kwargs for prompt helper**:\n- `context_window`: The size of the context window of the LLM. Typically we set this \n  automatically with the model metadata. But we also allow explicit override via this parameter\n  for additional control (or in case the default is not available for certain latest\n  models)\n- `num_output`: The number of maximum output from the LLM. Typically we set this\n  automatically given the model metadata. This parameter does not actually limit the model\n  output, it affects the amount of \"space\" we save for the output, when computing \n  available context window size for packing text from retrieved Nodes.", "start_char_idx": 504039, "end_char_idx": 508546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d69b2eb2-01f2-449a-99a7-588f7dc9d67d": {"__data__": {"id_": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25f0891c-e28f-439f-b61f-356860391fab", "node_type": "1", "metadata": {}, "hash": "7b32858201113b02d3a93d6a3fb16e6913c7a9e842583cdd811c7c0aeb26e9ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c16d2161-9918-4b03-96b7-ab1531cde427", "node_type": "1", "metadata": {}, "hash": "f6d16c8b8b4e92f06bcee35859f5af75a4f91abb7d7b9828cdc21912e9e47d1e", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "37f4e6bc-d13d-494c-a5c3-8729a1bf22f9", "node_type": "1", "metadata": {}, "hash": "69ba5059fe71bfe6d98f05cb867aee330860d1401a7781b2d3d8ea3e6dcc431c", "class_name": "RelatedNodeInfo"}, {"node_id": "813844cc-9bd9-4731-8bdc-a67f5cbb84d6", "node_type": "1", "metadata": {}, "hash": "be08d262634a7d3ca8d5bad5395a91493fa35fd1ab0ab24ba2aec821bb9f3bd4", "class_name": "RelatedNodeInfo"}, {"node_id": "7c825415-8a8a-497c-9631-0480cd167818", "node_type": "1", "metadata": {}, "hash": "e22e476aa2849e803bc373c7c7d668e0d1536abdd8810685da4f372b7c6bc57d", "class_name": "RelatedNodeInfo"}, {"node_id": "07fab882-9814-41a5-939b-40b9eae8261a", "node_type": "1", "metadata": {}, "hash": "42a75123789ea8d2445a009c21d908a7abceef9f9759442781d1bfa6d25826bc", "class_name": "RelatedNodeInfo"}]}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "text": "Here's a complete example that sets up all objects using their default settings:\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Configuring the service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\nfrom llama_index.llms import OpenAI\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom llama_index.node_parser import SimpleNodeParser\n\nllm = OpenAI(model='text-davinci-003', temperature=0, max_tokens=256)\nembed_model = OpenAIEmbedding()\nnode_parser = SimpleNodeParser(\n  text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n)\nprompt_helper = PromptHelper(\n  context_window=4096, \n  num_output=256, \n  chunk_overlap_ratio=0.1, \n  chunk_size_limit=None\n)\n\nservice_context = ServiceContext.from_defaults(\n  llm=llm,\n  embed_model=embed_model,\n  node_parser=node_parser,\n  prompt_helper=prompt_helper\n)\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Setting global configuration\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can set a service context as the global default that applies to the entire LlamaIndex pipeline:\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Setting global configuration\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import set_global_service_context\nset_global_service_context(service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Setting local configuration\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can pass in a service context to specific part of the pipeline to override the default configuration:\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Setting local configuration\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(service_context=service_context)\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nFile Name: Docs\\development\\privacy.md\nContent Type: text\nHeader Path: Privacy and Security\nLinks: \nfile_path: Docs\\development\\privacy.md\nfile_name: privacy.md\nfile_type: None\nfile_size: 948\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LLamaIndex sends your data to OpenAI for generating embeddings and natural language responses. However, it is important to note that this can be configured according to your preferences. LLamaIndex provides the flexibility to use your own embedding model or run a large language model locally if desired.\n\nFile Name: Docs\\development\\privacy.md\nContent Type: text\nHeader Path: Privacy and Security/Data Privacy\nLinks: \nfile_path: Docs\\development\\privacy.md\nfile_name: privacy.md\nfile_type: None\nfile_size: 948\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRegarding data privacy, when using LLamaIndex with OpenAI, the privacy details and handling of your data are subject to OpenAI's policies. And each custom service other than OpenAI have their own policies as well.\n\nFile Name: Docs\\development\\privacy.md\nContent Type: text\nHeader Path: Privacy and Security/Vector stores\nfile_path: Docs\\development\\privacy.md\nfile_name: privacy.md\nfile_type: None\nfile_size: 948\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLLamaIndex offers modules to connect with other vector stores within indexes to store embeddings. It is worth noting that each vector store has its own privacy policies and practices, and LLamaIndex does not assume responsibility for how they handle or use your data. Also by default LLamaIndex have a default option to store your embeddings locally.\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Context\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn \"agent\" is an automated reasoning and decision engine. It takes in a user input/query and can make internal decisions for executing\nthat query in order to return the correct result. The key agent components can include, but are not limited to:\n- Breaking down a complex question into smaller ones\n- Choosing an external Tool to use + coming up with parameters for calling the Tool\n- Planning out a set of tasks\n- Storing previously completed tasks in a memory module\n\nResearch developments in LLMs (e.g. ChatGPT Plugins), LLM research (ReAct, Toolformer) and LLM tooling (LangChain, Semantic Kernel) have popularized the concept of agents.", "start_char_idx": 508548, "end_char_idx": 514412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c16d2161-9918-4b03-96b7-ab1531cde427": {"__data__": {"id_": "c16d2161-9918-4b03-96b7-ab1531cde427", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "node_type": "1", "metadata": {}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25", "node_type": "1", "metadata": {}, "hash": "727857fa392684cfd622f209b870ddbbb15cb5cdbacace82d4896f7f863cdcf1", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "45ff6027-0e9a-468d-9b73-1858960cb1ca", "node_type": "1", "metadata": {}, "hash": "7b98b83d80ee8fdf4d193843511508b67b0c3fd603787822e6662f279058f9e4", "class_name": "RelatedNodeInfo"}, {"node_id": "b6e663ab-d603-46b9-bc95-5ed81ab2ff82", "node_type": "1", "metadata": {}, "hash": "1c79c2e5d6ea3499579723425ac48637dd363e4c4e40b9c308b18dd97a94f437", "class_name": "RelatedNodeInfo"}, {"node_id": "d63fad17-aaf3-4edb-bb99-0714cce6906e", "node_type": "1", "metadata": {}, "hash": "1bf3937e3401880e14190381752becf6d85c82b3c853fed4432e1f5a7874b06a", "class_name": "RelatedNodeInfo"}]}, "hash": "f6d16c8b8b4e92f06bcee35859f5af75a4f91abb7d7b9828cdc21912e9e47d1e", "text": "File Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides some amazing tools to manage and interact with your data within your LLM application. And it can be a core tool that you use while building an agent-based app.\n- On one hand, some components within LlamaIndex are \"agent-like\" - these make automated decisions to help a particular use case over your data.\n- On the other hand, LlamaIndex can be used as a core Tool within another agent framework.\n\nIn general, LlamaIndex components offer more explicit, constrained behavior for more specific use cases. Agent frameworks such as ReAct (implemented in LangChain) offer agents that are more unconstrained + \ncapable of general reasoning. \n\nThere are tradeoffs for using both - less-capable LLMs typically do better with more constraints. Take a look at our blog post on this for \na more information + a detailed analysis.\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/\"Agent-like\" Components within LlamaIndex\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides core modules capable of automated reasoning for different use cases over your data. Please check out our use cases doc for more details on high-level use cases that LlamaIndex can help fulfill.\n\nSome of these core modules are shown below along with example tutorials (not comprehensive, please click into the guides/how-tos for more details).\n\n**SubQuestionQueryEngine for Multi-Document Analysis**\n- Usage\n- Sub Question Query Engine (Intro)\n- 10Q Analysis (Uber)\n- 10K Analysis (Uber and Lyft)\n\n\n**Query Transformations**\n- How-To\n- Multi-Step Query Decomposition (Notebook)\n\n**Routing**\n- Usage\n- Router Query Engine Guide (Notebook)\n\n**LLM Reranking**\n- Second Stage Processing How-To\n- LLM Reranking Guide (Great Gatsby)\n\n**Chat Engines**\n- Chat Engines How-To\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/Using LlamaIndex as as Tool within an Agent Framework\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can be used as as Tool within an agent framework - including LangChain, ChatGPT. These integrations are described below.\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/Using LlamaIndex as as Tool within an Agent Framework/LangChain\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe have deep integrations with LangChain. \nLlamaIndex query engines can be easily packaged as Tools to be used within a LangChain agent, and LlamaIndex can also be used as a memory module / retriever. Check out our guides/tutorials below!\n\n**Resources**\n- LangChain integration guide\n- Building a Chatbot Tutorial (LangChain + LlamaIndex)\n- OnDemandLoaderTool Tutorial\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/Using LlamaIndex as as Tool within an Agent Framework/ChatGPT\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can be used as a ChatGPT retrieval plugin (we have a TODO to develop a more general plugin as well).\n\n**Resources**\n- LlamaIndex ChatGPT Retrieval Plugin\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/Using LlamaIndex as as Tool within an Agent Framework/Native OpenAIAgent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWith the new OpenAI API that supports function calling, it\u2019s never been easier to build your own agent!\n\nLearn how to write your own OpenAI agent in **under 50 lines of code**, or directly use our super simple\n`OpenAIAgent` implementation.", "start_char_idx": 514414, "end_char_idx": 519061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5213a7ed-3bfb-42b0-99f8-4f7882537a25": {"__data__": {"id_": "5213a7ed-3bfb-42b0-99f8-4f7882537a25", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c16d2161-9918-4b03-96b7-ab1531cde427", "node_type": "1", "metadata": {}, "hash": "f6d16c8b8b4e92f06bcee35859f5af75a4f91abb7d7b9828cdc21912e9e47d1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "node_type": "1", "metadata": {}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "920566e4-b853-445f-b94f-1401e4c45891", "node_type": "1", "metadata": {}, "hash": "b71104176f2495a51a15aacc0cf8612afa3d79f5e52c4b69fa3b09f216183c9a", "class_name": "RelatedNodeInfo"}, {"node_id": "d2353a21-f9bd-4f00-95ba-52430bf65476", "node_type": "1", "metadata": {}, "hash": "3291a8795d102006855734236c69889116b8e6a9cb4d62ae4eebebbd576a048a", "class_name": "RelatedNodeInfo"}, {"node_id": "4b1a051e-a7ff-4cea-9bc1-a872e4de5f09", "node_type": "1", "metadata": {}, "hash": "586bc07df23caaf05e44c7b71b924377dbe59b4ea607201a7f18077a22078a7a", "class_name": "RelatedNodeInfo"}]}, "hash": "727857fa392684cfd622f209b870ddbbb15cb5cdbacace82d4896f7f863cdcf1", "text": "File Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/Using LlamaIndex as as Tool within an Agent Framework/Native OpenAIAgent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\n/examples/agent/openai_agent.ipynb\n/examples/agent/openai_agent_with_query_engine.ipynb\n/examples/agent/openai_agent_retrieval.ipynb\n/examples/agent/openai_agent_query_cookbook.ipynb\n/examples/agent/openai_agent_query_plan.ipynb\n/examples/agent/openai_agent_context_retrieval.ipynb\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex is a python library, which means that integrating it with a full-stack web application will be a little different than what you might be used to.\n\nThis guide seeks to walk through the steps needed to create a basic API service written in python, and how this interacts with a TypeScript+React frontend.\n\nAll code examples here are available from the llama_index_starter_pack in the flask_react folder.\n\nThe main technologies used in this guide are as follows:\n\n- python3.11\n- llama_index\n- flask\n- typescript\n- react\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor this guide, our backend will use a Flask API server to communicate with our frontend code. If you prefer, you can also easily translate this to a FastAPI server, or any other python server library of your choice.\n\nSetting up a server using Flask is easy. You import the package, create the app object, and then create your endpoints. Let's create a basic skeleton for the server first:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef home():\n    return \"Hello World!\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5601)\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n_flask_demo.py_\n\nIf you run this file (`python flask_demo.py`), it will launch a server on port 5601. If you visit `http://localhost:5601/`, you will see the \"Hello World!\" text rendered in your browser. Nice!\n\nThe next step is deciding what functions we want to include in our server, and to start using LlamaIndex.\n\nTo keep things simple, the most basic operation we can provide is querying an existing index. Using the paul graham essay from LlamaIndex, create a documents folder and download+place the essay text file inside of it.", "start_char_idx": 519063, "end_char_idx": 522941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28cc8849-c8d0-4678-b1d6-5a94c066f2a0": {"__data__": {"id_": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25", "node_type": "1", "metadata": {}, "hash": "727857fa392684cfd622f209b870ddbbb15cb5cdbacace82d4896f7f863cdcf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df036a43-245d-4c3e-b036-d33f56dabd94", "node_type": "1", "metadata": {}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0d1dac4c-03c6-4dd6-ae47-d58444ea7dc2", "node_type": "1", "metadata": {}, "hash": "607532325b18a650ee3d9a0f5236e070ec14c9a1b52f6e2ebeffb6ae213dfd1d", "class_name": "RelatedNodeInfo"}, {"node_id": "7dbfa10c-386f-44aa-bf06-299b0b13eb72", "node_type": "1", "metadata": {}, "hash": "58faf1ed6f6b190d8c0b351059dbdafc95079adeb31921afd9002f61c6e42437", "class_name": "RelatedNodeInfo"}, {"node_id": "2a72a30f-f122-48e7-b2b5-25813db6d3ae", "node_type": "1", "metadata": {}, "hash": "ff56a794c1d94d170fba5baf39848312ac621c148ae544efe278d1f72abfbc60", "class_name": "RelatedNodeInfo"}, {"node_id": "cad717e1-fa4c-4cac-8db5-ab62967e12cb", "node_type": "1", "metadata": {}, "hash": "91d12b18b7150e8fdc52fc045728227dde02b2e6991a003abc282deeec1b586d", "class_name": "RelatedNodeInfo"}]}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/Basic Flask - Handling User Index Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, let's write some code to initialize our index:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/Basic Flask - Handling User Index Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport os\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nos.environ['OPENAI_API_KEY'] = \"your key here\"\n\nindex = None\n\ndef initialize_index():\n    global index\n    storage_context = StorageContext.from_defaults()\n    if os.path.exists(index_dir):\n        index = load_index_from_storage(storage_context)\n    else:\n        documents = SimpleDirectoryReader(\"./documents\").load_data()\n        index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n        storage_context.persist(index_dir)\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis function will initialize our index. If we call this just before starting the flask server in the `main` function, then our index will be ready for user queries!\n\nOur query endpoint will accept `GET` requests with the query text as a parameter. Here's what the full endpoint function will look like:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom flask import request\n\n@app.route(\"/query\", methods=[\"GET\"])\ndef query_index():\n  global index\n  query_text = request.args.get(\"text\", None)\n  if query_text is None:\n    return \"No text found, please include a ?text=blah parameter in the URL\", 400\n  query_engine = index.as_query_engine()\n  response = query_engine.query(query_text)\n  return str(response), 200\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, we've introduced a few new concepts to our server:\n\n- a new `/query` endpoint, defined by the function decorator\n- a new import from flask, `request`, which is used to get parameters from the request\n- if the `text` parameter is missing, then we return an error message and an appropriate HTML response code\n- otherwise, we query the index, and return the response as a string\n\nA full query example that you can test in your browser might look something like this: `http://localhost:5601/query?text=what did the author do growing up` (once you press enter, the browser will convert the spaces into \"%20\" characters).\n\nThings are looking pretty good! We now have a functional API. Using your own documents, you can easily provide an interface for any application to call the flask API and get answers to queries.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/Advanced Flask - Handling User Document Uploads\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThings are looking pretty cool, but how can we take this a step further? What if we want to allow users to build their own indexes by uploading their own documents? Have no fear, Flask can handle it all :muscle:.\n\nTo let users upload documents, we have to take some extra precautions. Instead of querying an existing index, the index will become **mutable**. If you have many users adding to the same index, we need to think about how to handle concurrency.", "start_char_idx": 522943, "end_char_idx": 528678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df036a43-245d-4c3e-b036-d33f56dabd94": {"__data__": {"id_": "df036a43-245d-4c3e-b036-d33f56dabd94", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "node_type": "1", "metadata": {}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce98334c-2023-441b-acea-e0729b9bee09", "node_type": "1", "metadata": {}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ef51f9c8-e76c-4b1d-9fb9-da90bdf12754", "node_type": "1", "metadata": {}, "hash": "00835070339ed821bba6c348cc1e5c0ba1ac93f10839614653c9351b35495197", "class_name": "RelatedNodeInfo"}, {"node_id": "ef4efee0-7de1-49f9-84df-f7d840d2995a", "node_type": "1", "metadata": {}, "hash": "2f2d1b5a31c21478e278f568f96c62c7dff4c39282604a4029cafa083ae4df9c", "class_name": "RelatedNodeInfo"}, {"node_id": "14ceac11-7853-42c0-a772-1d5f0cdafd76", "node_type": "1", "metadata": {}, "hash": "1feec6011aa6811f0b71888297617ace38092a336c567097a6e172b821115589", "class_name": "RelatedNodeInfo"}, {"node_id": "f077122e-bead-4974-9c39-e43fb995e396", "node_type": "1", "metadata": {}, "hash": "183ac8a82478974e856f9af92dfbfdb3ab5b759427c44ea87633c754359d56a4", "class_name": "RelatedNodeInfo"}]}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "text": "Our Flask server is threaded, which means multiple users can ping the server with requests which will be handled at the same time.\n\nOne option might be to create an index for each user or group, and store and fetch things from S3. But for this example, we will assume there is one locally stored index that users are interacting with.\n\nTo handle concurrent uploads and ensure sequential inserts into the index, we can use the `BaseManager` python package to provide sequential access to the index using a separate server and locks. This sounds scary, but it's not so bad! We will just move all our index operations (initializing, querying, inserting) into the `BaseManager` \"index_server\", which will be called from our Flask server.\n\nHere's a basic example of what our `index_server.py` will look like after we've moved our code:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/Advanced Flask - Handling User Document Uploads\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport os\nfrom multiprocessing import Lock\nfrom multiprocessing.managers import BaseManager\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, Document\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nos.environ['OPENAI_API_KEY'] = \"your key here\"\n\nindex = None\nlock = Lock()\n\ndef initialize_index():\n  global index\n\n  with lock:\n    # same as before ...\n  ...\n\ndef query_index(query_text):\n  global index\n  query_engine = index.as_query_engine()\n  response = query_engine.query(query_text)\n  return str(response)\n\nif __name__ == \"__main__\":\n    # init the global index\n    print(\"initializing index...\")\n    initialize_index()\n\n    # setup server\n    # NOTE: you might want to handle the password in a less hardcoded way\n    manager = BaseManager(('', 5602), b'password')\n    manager.register('query_index', query_index)\n    server = manager.get_server()\n\n    print(\"starting server...\")\n    server.serve_forever()\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n_index_server.py_\n\nSo, we've moved our functions, introduced the `Lock` object which ensures sequential access to the global index, registered our single function in the server, and started the server on port 5602 with the password `password`.\n\nThen, we can adjust our flask code as follows:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom multiprocessing.managers import BaseManager\nfrom flask import Flask, request\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmanager = BaseManager(('', 5602), b'password')\nmanager.register('query_index')\nmanager.connect()\n\n@app.route(\"/query\", methods=[\"GET\"])\ndef query_index():\n  global index\n  query_text = request.args.get(\"text\", None)\n  if query_text is None:\n    return \"No text found, please include a ?text=blah parameter in the URL\", 400\n  response = manager.query_index(query_text)._getvalue()\n  return str(response), 200\n\n@app.route(\"/\")\ndef home():\n    return \"Hello World!\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5601)\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n_flask_demo.py_\n\nThe two main changes are connecting to our existing `BaseManager` server and registering the functions, as well as calling the function through the manager in the `/query` endpoint.\n\nOne special thing to note is that `BaseManager` servers don't return objects quite as we expect. To resolve the return value into it's original object, we call the `_getvalue()` function.\n\nIf we allow users to upload their own documents, we should probably remove the Paul Graham essay from the documents folder, so let's do that first.", "start_char_idx": 528679, "end_char_idx": 534607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce98334c-2023-441b-acea-e0729b9bee09": {"__data__": {"id_": "ce98334c-2023-441b-acea-e0729b9bee09", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df036a43-245d-4c3e-b036-d33f56dabd94", "node_type": "1", "metadata": {}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "b1523a33-fb8a-45e3-bda8-62185f5f82bd", "node_type": "1", "metadata": {}, "hash": "f250baf79e559546473875e6784bbdccdf6b4f3dfa354d0793863a8e1ca3d5a6", "class_name": "RelatedNodeInfo"}, {"node_id": "f74083dc-d459-4b03-b3a9-5f954eaa7326", "node_type": "1", "metadata": {}, "hash": "c851892fbf793ffa7b102d5036febe0a5f464d55422e61de535d6f538fe3d596", "class_name": "RelatedNodeInfo"}, {"node_id": "16219dff-01fe-46d5-91f1-c548caf59882", "node_type": "1", "metadata": {}, "hash": "b0748124b88a87c4f6ed752184c44924543e8d079cc6b41ec9d6f5a15b02ee70", "class_name": "RelatedNodeInfo"}, {"node_id": "9db7b730-8131-417b-b359-4a3669318a5e", "node_type": "1", "metadata": {}, "hash": "ae7d0f82b3a7b4ddfcc7c7d546cb3cbbbe986613579e054daf92354fd870445c", "class_name": "RelatedNodeInfo"}]}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "text": "Then, let's add an endpoint to upload files! First, let's define our Flask endpoint function:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\nmanager.register('insert_into_index')\n...\n\n@app.route(\"/uploadFile\", methods=[\"POST\"])\ndef upload_file():\n    global manager\n    if 'file' not in request.files:\n        return \"Please send a POST request with a file\", 400\n\n    filepath = None\n    try:\n        uploaded_file = request.files[\"file\"]\n        filename = secure_filename(uploaded_file.filename)\n        filepath = os.path.join('documents', os.path.basename(filename))\n        uploaded_file.save(filepath)\n\n        if request.form.get(\"filename_as_doc_id\", None) is not None:\n            manager.insert_into_index(filepath, doc_id=filename)\n        else:\n            manager.insert_into_index(filepath)\n    except Exception as e:\n        # cleanup temp file\n        if filepath is not None and os.path.exists(filepath):\n            os.remove(filepath)\n        return \"Error: {}\".format(str(e)), 500\n\n    # cleanup temp file\n    if filepath is not None and os.path.exists(filepath):\n        os.remove(filepath)\n\n    return \"File inserted!\", 200\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNot too bad! You will notice that we write the file to disk. We could skip this if we only accept basic file formats like `txt` files, but written to disk we can take advantage of LlamaIndex's `SimpleDirectoryReader` to take care of a bunch of more complex file formats. Optionally, we also use a second `POST` argument to either use the filename as a doc_id or let LlamaIndex generate one for us. This will make more sense once we implement the frontend.\n\nWith these more complicated requests, I also suggest using a tool like Postman. Examples of using postman to test our endpoints are in the repository for this project.\n\nLastly, you'll notice we added a new function to the manager. Let's implement that inside `index_server.py`:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndef insert_into_index(doc_text, doc_id=None):\n    global index\n    document = SimpleDirectoryReader(input_files=[doc_text]).load_data()[0]\n    if doc_id is not None:\n        document.doc_id = doc_id\n\n    with lock:\n        index.insert(document)\n        index.storage_context.persist()\n\n...\nmanager.register('insert_into_index', insert_into_index)\n...\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEasy! If we launch both the `index_server.py` and then the `flask_demo.py` python files, we have a Flask API server that can handle multiple requests to insert documents into a vector index and respond to user queries!\n\nTo support some functionality in the frontend, I've adjusted what some responses look like from the Flask API, as well as added some functionality to keep track of which documents are stored in the index (LlamaIndex doesn't currently support this in a user-friendly way, but we can augment it ourselves!). Lastly, I had to add CORS support to the server using the `Flask-cors` python package.\n\nCheck out the complete `flask_demo.py` and `index_server.py` scripts in the repository for the final minor changes, the`requirements.txt` file, and a sample `Dockerfile` to help with deployment.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/React Frontend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGenerally, React and Typescript are one of the most popular libraries and languages for writing webapps today. This guide will assume you are familiar with how these tools work, because otherwise this guide will triple in length :smile:.\n\nIn the repository, the frontend code is organized inside of the `react_frontend` folder.\n\nThe most relevant part of the frontend will be the `src/apis` folder.", "start_char_idx": 534608, "end_char_idx": 540282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85": {"__data__": {"id_": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce98334c-2023-441b-acea-e0729b9bee09", "node_type": "1", "metadata": {}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f546fdee-56fd-4002-9441-746ae6318f44", "node_type": "1", "metadata": {}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e400ea9e-3d87-462c-b6e2-a5c578191c6c", "node_type": "1", "metadata": {}, "hash": "31a5b41b420f10728ce298555ddf54f31fee278bf06a91e1cd41c57e5414bb4d", "class_name": "RelatedNodeInfo"}, {"node_id": "c6ee80ea-65f5-42b7-9644-5eedf61423be", "node_type": "1", "metadata": {}, "hash": "e5f23e8a83a7089dab7e70e13f6672cef099c89a76632bdb542a0069cc5853e8", "class_name": "RelatedNodeInfo"}, {"node_id": "a161bf29-9df1-4011-b4ef-153fd555897f", "node_type": "1", "metadata": {}, "hash": "cd28a9a8f0ff53576017fc30fdcc631baa255e33a32763432a4a8e02e3704d54", "class_name": "RelatedNodeInfo"}, {"node_id": "e4cfde42-c780-45e2-889d-ad88f7171da1", "node_type": "1", "metadata": {}, "hash": "7b589cb43e2ce37e62666a93ca188998d2b1f048cb2fbfcca3b323da652188c1", "class_name": "RelatedNodeInfo"}, {"node_id": "525b7a92-a6db-4210-b68b-2d2d4bfc0bdc", "node_type": "1", "metadata": {}, "hash": "1a9b226dfe85feb5e7e2310f39b578af5c239891aa82e84425cd17be3103d092", "class_name": "RelatedNodeInfo"}]}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "text": "The most relevant part of the frontend will be the `src/apis` folder. This is where we make calls to the Flask server, supporting the following queries:\n\n- `/query` -- make a query to the existing index\n- `/uploadFile` -- upload a file to the flask server for insertion into the index\n- `/getDocuments` -- list the current document titles and a portion of their texts\n\nUsing these three queries, we can build a robust frontend that allows users to upload and keep track of their files, query the index, and view the query response and information about which text nodes were used to form the response.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/fetchDocuments.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis file contains the function to, you guessed it, fetch the list of current documents in the index. The code is as follows:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/fetchDocuments.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nexport type Document = {\n  id: string;\n  text: string;\n};\n\nconst fetchDocuments = async (): Promise<Document[]> => {\n  const response = await fetch(\"http://localhost:5601/getDocuments\", {\n    mode: \"cors\",\n  });\n\n  if (!response.ok) {\n    return [];\n  }\n\n  const documentList = (await response.json()) as Document[];\n  return documentList;\n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/fetchDocuments.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAs you can see, we make a query to the Flask server (here, it assumes running on localhost). Notice that we need to include the `mode: 'cors'` option, as we are making an external request.\n\nThen, we check if the response was ok, and if so, get the response json and return it. Here, the response json is a list of `Document` objects that are defined in the same file.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/queryIndex.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis file sends the user query to the flask server, and gets the response back, as well as details about which nodes in our index provided the response.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/queryIndex.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nexport type ResponseSources = {\n  text: string;\n  doc_id: string;\n  start: number;\n  end: number;\n  similarity: number;\n};\n\nexport type QueryResponse = {\n  text: string;\n  sources: ResponseSources[];\n};\n\nconst queryIndex = async (query: string): Promise<QueryResponse> => {\n  const queryURL = new URL(\"http://localhost:5601/query?text=1\");\n  queryURL.searchParams.append(\"text\", query);\n\n  const response = await fetch(queryURL, { mode: \"cors\" });\n  if (!response.ok) {\n    return { text: \"Error in query\", sources: [] };\n  }\n\n  const queryResponse = (await response.json()) as QueryResponse;\n\n  return queryResponse;\n};\n\nexport default queryIndex;\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/queryIndex.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis is similar to the `fetchDocuments.tsx` file, with the main difference being we include the query text as a parameter in the URL. Then, we check if the response is ok and return it with the appropriate typescript type.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/insertDocument.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nProbably the most complex API call is uploading a document. The function here accepts a file object and constructs a `POST` request using `FormData`.", "start_char_idx": 540213, "end_char_idx": 545733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f546fdee-56fd-4002-9441-746ae6318f44": {"__data__": {"id_": "f546fdee-56fd-4002-9441-746ae6318f44", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5", "node_type": "1", "metadata": {}, "hash": "566d0db4152fa794e55cb242d1a9fa3d486cbecf1c321e21d85f17730e3294bb", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "89e8b848-6f96-432b-a008-633b169734ce", "node_type": "1", "metadata": {}, "hash": "a8e5549d313144caea015f7aba737bb0fcbd89061c1b126a348bb979793c8e61", "class_name": "RelatedNodeInfo"}, {"node_id": "d5d05ee3-c578-46b5-a3da-44e3c54c24f9", "node_type": "1", "metadata": {}, "hash": "5de44f6e70d8aa14b21d00c1a9f66226d613cd6ff031e4816522c641a9dd04e6", "class_name": "RelatedNodeInfo"}, {"node_id": "69f8fca8-4d0c-4eca-927c-83001797ad85", "node_type": "1", "metadata": {}, "hash": "a31566b03bc2eccac19a12cab94eec73883d18513fcb0b777dee3a3786e50b66", "class_name": "RelatedNodeInfo"}, {"node_id": "066f7152-6f38-4f0f-a501-5600d66e757d", "node_type": "1", "metadata": {}, "hash": "1bdce6ac6fe5478262e02bd40e913c318a2c71cf3deb0ee028225ee31736659d", "class_name": "RelatedNodeInfo"}]}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "text": "The function here accepts a file object and constructs a `POST` request using `FormData`.\n\nThe actual response text is not used in the app but could be utilized to provide some user feedback on if the file failed to upload or not.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/insertDocument.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nconst insertDocument = async (file: File) => {\n  const formData = new FormData();\n  formData.append(\"file\", file);\n  formData.append(\"filename_as_doc_id\", \"true\");\n\n  const response = await fetch(\"http://localhost:5601/uploadFile\", {\n    mode: \"cors\",\n    method: \"POST\",\n    body: formData,\n  });\n\n  const responseText = response.text();\n  return responseText;\n};\n\nexport default insertDocument;\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/All the Other Frontend Good-ness\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAnd that pretty much wraps up the frontend portion! The rest of the react frontend code is some pretty basic react components, and my best attempt to make it look at least a little nice :smile:.\n\nI encourage to read the rest of the codebase and submit any PRs for improvements!\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/Conclusion\nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis guide has covered a ton of information. We went from a basic \"Hello World\" Flask server written in python, to a fully functioning LlamaIndex powered backend and how to connect that to a frontend application.\n\nAs you can see, we can easily augment and wrap the services provided by LlamaIndex (like the little external document tracker) to help provide a good user experience on the frontend.\n\nYou could take this and add many features (multi-index/user support, saving objects into S3, adding a Pinecone vector server, etc.). And when you build an app after reading this, be sure to share the final result in the Discord! Good Luck! :muscle:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis guide seeks to walk you through using LlamaIndex with a production-ready web app starter template\ncalled Delphic. All code examples here are available from\nthe Delphic repo\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/What We're Building\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere's a quick demo of the out-of-the-box functionality of Delphic:\n\nhttps://user-images.githubusercontent.com/5049984/233236432-aa4980b6-a510-42f3-887a-81485c9644e6.mp4\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Architectural Overview\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDelphic leverages the LlamaIndex python library to let users to create their own document collections they can then\nquery in a responsive frontend.\n\nWe chose a stack that provides a responsive, robust mix of technologies that can (1) orchestrate complex python\nprocessing tasks while providing (2) a modern, responsive frontend and (3) a secure backend to build additional\nfunctionality upon.\n\nThe core libraries are:\n\n1. Django\n2. Django Channels\n3. Django Ninja\n4. Redis\n5. Celery\n6. LlamaIndex\n7. Langchain\n8. React\n9. Docker & Docker Compose\n\nThanks to this modern stack built on the super stable Django web framework, the starter Delphic app boasts a streamlined\ndeveloper experience, built-in authentication and user management, asynchronous vector store processing, and\nweb-socket-based query connections for a responsive UI. In addition, our frontend is built with TypeScript and is based\non MUI React for a responsive and modern user interface.", "start_char_idx": 545644, "end_char_idx": 550993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d607c9f6-64f3-4c83-a796-b211f91fa8c5": {"__data__": {"id_": "d607c9f6-64f3-4c83-a796-b211f91fa8c5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f546fdee-56fd-4002-9441-746ae6318f44", "node_type": "1", "metadata": {}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822", "node_type": "1", "metadata": {}, "hash": "48af84f1c7b675ada7b29bd678427298414d187f71728887fa2e7cfb36d3df19", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8b0061c9-30b8-4557-8a86-ddda10331ccc", "node_type": "1", "metadata": {}, "hash": "9b608f07f5722b5d23c1706ac7a2d3b5b4256d7444d0c0c70acbe296cdfea301", "class_name": "RelatedNodeInfo"}, {"node_id": "739efa70-5c4d-4903-9b7d-7ae0e5b434bf", "node_type": "1", "metadata": {}, "hash": "5b25258bc2d443d17f40d011f135b4ce49e8fe5a3be21b7b8dc1b8ae3067d612", "class_name": "RelatedNodeInfo"}, {"node_id": "620307eb-ce1e-4985-98aa-594dc9bdc5c7", "node_type": "1", "metadata": {}, "hash": "1cbf1b394237fa64efdd496653203761da05a656bf9df125d9196d533b44dc55", "class_name": "RelatedNodeInfo"}]}, "hash": "566d0db4152fa794e55cb242d1a9fa3d486cbecf1c321e21d85f17730e3294bb", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/System Requirements\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCelery doesn't work on Windows. It may be deployable with Windows Subsystem for Linux, but configuring that is beyond\nthe scope of this tutorial. For this reason, we recommend you only follow this tutorial if you're running Linux or OSX.\nYou will need Docker and Docker Compose installed to deploy the application. Local development will require node version\nmanager (nvm).\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Project Directory Overview\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe Delphic application has a structured backend directory organization that follows common Django project conventions.\nFrom the repo root, in the `./delphic` subfolder, the main folders are:\n\n1. `contrib`: This directory contains custom modifications or additions to Django's built-in `contrib` apps.\n2. `indexes`: This directory contains the core functionality related to document indexing and LLM integration. It\n   includes:\n\n- `admin.py`: Django admin configuration for the app\n- `apps.py`: Application configuration\n- `models.py`: Contains the app's database models\n- `migrations`: Directory containing database schema migrations for the app\n- `signals.py`: Defines any signals for the app\n- `tests.py`: Unit tests for the app\n\n3. `tasks`: This directory contains tasks for asynchronous processing using Celery. The `index_tasks.py` file includes\n   the tasks for creating vector indexes.\n4. `users`: This directory is dedicated to user management, including:\n5. `utils`: This directory contains utility modules and functions that are used across the application, such as custom\n   storage backends, path helpers, and collection-related utilities.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Database Models\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe Delphic application has two core models: `Document` and `Collection`. These models represent the central entities\nthe application deals with when indexing and querying documents using LLMs. They're defined in\n`./delphic/indexes/models.py`.\n\n1. `Collection`:\n\n- `api_key`: A foreign key that links a collection to an API key. This helps associate jobs with the source API key.\n- `title`: A character field that provides a title for the collection.\n- `description`: A text field that provides a description of the collection.\n- `status`: A character field that stores the processing status of the collection, utilizing the `CollectionStatus`\n  enumeration.\n- `created`: A datetime field that records when the collection was created.\n- `modified`: A datetime field that records the last modification time of the collection.\n- `model`: A file field that stores the model associated with the collection.\n- `processing`: A boolean field that indicates if the collection is currently being processed.\n\n2. `Document`:\n\n- `collection`: A foreign key that links a document to a collection. This represents the relationship between documents\n  and collections.\n- `file`: A file field that stores the uploaded document file.\n- `description`: A text field that provides a description of the document.\n- `created`: A datetime field that records when the document was created.\n- `modified`: A datetime field that records the last modification time of the document.\n\nThese models provide a solid foundation for collections of documents and the indexes created from them with LlamaIndex.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDjango Ninja is a web framework for building APIs with Django and Python 3.7+ type hints. It provides a simple,\nintuitive, and expressive way of defining API endpoints, leveraging Python\u2019s type hints to automatically generate input\nvalidation, serialization, and documentation.\n\nIn the Delphic repo,\nthe `./config/api/endpoints.py`\nfile contains the API routes and logic for the API endpoints. Now, let\u2019s briefly address the purpose of each endpoint\nin the `endpoints.py` file:\n\n1. `/heartbeat`: A simple GET endpoint to check if the API is up and running. Returns `True` if the API is accessible.\n   This is helpful for Kubernetes setups that expect to be able to query your container to ensure it's up and running.\n\n2. `/collections/create`: A POST endpoint to create a new `Collection`. Accepts form parameters such\n   as `title`, `description`, and a list of `files`. Creates a new `Collection` and `Document` instances for each file,\n   and schedules a Celery task to create an index.", "start_char_idx": 550995, "end_char_idx": 556815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8821cf26-edd5-435a-9f1a-b0248cf8d822": {"__data__": {"id_": "8821cf26-edd5-435a-9f1a-b0248cf8d822", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5", "node_type": "1", "metadata": {}, "hash": "566d0db4152fa794e55cb242d1a9fa3d486cbecf1c321e21d85f17730e3294bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6184d959-1551-4667-8767-3dd54a1351c3", "node_type": "1", "metadata": {}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1ae0fcd0-1f6f-42f9-94ee-8382150cea36", "node_type": "1", "metadata": {}, "hash": "85c9ff20b7e3d35acac5ca4f6338407ba9b491a7c25550faeb258796d078ca72", "class_name": "RelatedNodeInfo"}, {"node_id": "52c768ef-e147-4809-84f2-b35c6c9b4a1b", "node_type": "1", "metadata": {}, "hash": "22c3440071b256314eeba7c95c27fd6c123f4cd7131b4c23b6e0963727fc67b4", "class_name": "RelatedNodeInfo"}, {"node_id": "99e103ce-f5fa-4bb6-afce-e0d061dfd115", "node_type": "1", "metadata": {}, "hash": "7290c4330d22865ad724f6a438ad024d8e6a3fc91f59c433c4db28d603987e97", "class_name": "RelatedNodeInfo"}]}, "hash": "48af84f1c7b675ada7b29bd678427298414d187f71728887fa2e7cfb36d3df19", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@collections_router.post(\"/create\")\nasync def create_collection(request,\n                            title: str = Form(...),\n                            description: str = Form(...),\n                            files: list[UploadedFile] = File(...), ):\n    key = None if getattr(request, \"auth\", None) is None else request.auth\n    if key is not None:\n        key = await key\n\n    collection_instance = Collection(\n        api_key=key,\n        title=title,\n        description=description,\n        status=CollectionStatusEnum.QUEUED,\n    )\n\n    await sync_to_async(collection_instance.save)()\n\n    for uploaded_file in files:\n        doc_data = uploaded_file.file.read()\n        doc_file = ContentFile(doc_data, uploaded_file.name)\n        document = Document(collection=collection_instance, file=doc_file)\n        await sync_to_async(document.save)()\n\n    create_index.si(collection_instance.id).apply_async()\n\n    return await sync_to_async(CollectionModelSchema)(\n        ...\n    )\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n3. `/collections/query` \u2014 a POST endpoint to query a document collection using the LLM. Accepts a JSON payload\n   containing `collection_id` and `query_str`, and returns a response generated by querying the collection. We don't\n   actually use this endpoint in our chat GUI (We use a websocket - see below), but you could build an app to integrate\n   to this REST endpoint to query a specific collection.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@collections_router.post(\"/query\",\n                         response=CollectionQueryOutput,\n                         summary=\"Ask a question of a document collection\", )\ndef query_collection_view(request: HttpRequest, query_input: CollectionQueryInput):\n    collection_id = query_input.collection_id\n    query_str = query_input.query_str\n    response = query_collection(collection_id, query_str)\n    return {\"response\": response}\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n4. `/collections/available`: A GET endpoint that returns a list of all collections created with the user's API key. The\n   output is serialized using the `CollectionModelSchema`.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@collections_router.get(\"/available\",\n                        response=list[CollectionModelSchema],\n                        summary=\"Get a list of all of the collections created with my api_key\", )\nasync def get_my_collections_view(request: HttpRequest):\n    key = None if getattr(request, \"auth\", None) is None else request.auth\n    if key is not None:\n        key = await key\n\n    collections = Collection.objects.filter(api_key=key)\n\n    return [\n        {\n            ...\n        }\n        async for collection in collections\n    ]\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n5. `/collections/{collection_id}/add_file`: A POST endpoint to add a file to an existing collection. Accepts\n   a `collection_id` path parameter, and form parameters such as `file` and `description`. Adds the file as a `Document`\n   instance associated with the specified collection.", "start_char_idx": 556817, "end_char_idx": 562232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6184d959-1551-4667-8767-3dd54a1351c3": {"__data__": {"id_": "6184d959-1551-4667-8767-3dd54a1351c3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822", "node_type": "1", "metadata": {}, "hash": "48af84f1c7b675ada7b29bd678427298414d187f71728887fa2e7cfb36d3df19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "cfc542d7-8752-42ab-a833-8d9277ad678c", "node_type": "1", "metadata": {}, "hash": "e6413f2553a06884361590a1ee5bf4cc70ff9b0d35d37886e9284d52958559fb", "class_name": "RelatedNodeInfo"}, {"node_id": "f66f044e-3eee-4483-8344-ece21a635523", "node_type": "1", "metadata": {}, "hash": "38e5f7b893a2df1d1454fdbe3a1d7652a1d68e6c962ee2d93fa517c997fbb48e", "class_name": "RelatedNodeInfo"}, {"node_id": "0e35aa78-97d4-4392-9225-2fac96eab48d", "node_type": "1", "metadata": {}, "hash": "88b699596ee3beb45dff4a98770bd836f988cfa1cb1e65a04035f501d7c5bb28", "class_name": "RelatedNodeInfo"}, {"node_id": "76b3aeb2-6333-4b90-b673-a51c9a7427f6", "node_type": "1", "metadata": {}, "hash": "d2620f0e490e8aba1f72e181f072dd8ff52a5c73b9ed7291d5028c465a91e4bd", "class_name": "RelatedNodeInfo"}]}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "text": "Adds the file as a `Document`\n   instance associated with the specified collection.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@collections_router.post(\"/{collection_id}/add_file\", summary=\"Add a file to a collection\")\nasync def add_file_to_collection(request,\n                                 collection_id: int,\n                                 file: UploadedFile = File(...),\n                                 description: str = Form(...), ):\n    collection = await sync_to_async(Collection.objects.get)(id=collection_id\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Intro to Websockets\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWebSockets are a communication protocol that enables bidirectional and full-duplex communication between a client and a\nserver over a single, long-lived connection. The WebSocket protocol is designed to work over the same ports as HTTP and\nHTTPS (ports 80 and 443, respectively) and uses a similar handshake process to establish a connection. Once the\nconnection is established, data can be sent in both directions as \u201cframes\u201d without the need to reestablish the\nconnection each time, unlike traditional HTTP requests.\n\nThere are several reasons to use WebSockets, particularly when working with code that takes a long time to load into\nmemory but is quick to run once loaded:\n\n1. **Performance**: WebSockets eliminate the overhead associated with opening and closing multiple connections for each\n   request, reducing latency.\n2. **Efficiency**: WebSockets allow for real-time communication without the need for polling, resulting in more\n   efficient use of resources and better responsiveness.\n3. **Scalability**: WebSockets can handle a large number of simultaneous connections, making it ideal for applications\n   that require high concurrency.\n\nIn the case of the Delphic application, using WebSockets makes sense as the LLMs can be expensive to load into memory.\nBy establishing a WebSocket connection, the LLM can remain loaded in memory, allowing subsequent requests to be\nprocessed quickly without the need to reload the model each time.\n\nThe ASGI configuration file `./config/asgi.py` defines how\nthe application should handle incoming connections, using the Django Channels `ProtocolTypeRouter` to route connections\nbased on their protocol type. In this case, we have two protocol types: \"http\" and \"websocket\".\n\nThe \u201chttp\u201d protocol type uses the standard Django ASGI application to handle HTTP requests, while the \u201cwebsocket\u201d\nprotocol type uses a custom `TokenAuthMiddleware` to authenticate WebSocket connections. The `URLRouter` within\nthe `TokenAuthMiddleware` defines a URL pattern for the `CollectionQueryConsumer`, which is responsible for handling\nWebSocket connections related to querying document collections.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Intro to Websockets\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\napplication = ProtocolTypeRouter(\n    {\n        \"http\": get_asgi_application(),\n        \"websocket\": TokenAuthMiddleware(\n            URLRouter(\n                [\n                    re_path(\n                        r\"ws/collections/(?P<collection_id>\\w+)/query/$\",\n                        CollectionQueryConsumer.as_asgi(),\n                    ),\n                ]\n            )\n        ),\n    }\n)\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Intro to Websockets\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis configuration allows clients to establish WebSocket connections with the Delphic application to efficiently query\ndocument collections using the LLMs, without the need to reload the models for each request.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `CollectionQueryConsumer` class\nin `config/api/websockets/queries.py` is\nresponsible for handling WebSocket connections related to querying document collections. It inherits from\nthe `AsyncWebsocketConsumer` class provided by Django Channels.\n\nThe `CollectionQueryConsumer` class has three main methods:\n\n1. `connect`: Called when a WebSocket is handshaking as part of the connection process.\n2. `disconnect`: Called when a WebSocket closes for any reason.\n3. `receive`: Called when the server receives a message from the WebSocket.", "start_char_idx": 562149, "end_char_idx": 568083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "522de47a-97a6-4d73-948f-ad40577a9d06": {"__data__": {"id_": "522de47a-97a6-4d73-948f-ad40577a9d06", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6184d959-1551-4667-8767-3dd54a1351c3", "node_type": "1", "metadata": {}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "716fffcb-e467-4b3f-b39b-40b128c950fa", "node_type": "1", "metadata": {}, "hash": "bc1f11fe85f292099878a8af572638629b18b9280808e41971daef1e47e7bd29", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "e12cb0b4-bbc6-4d53-b25d-b522cc0a075d", "node_type": "1", "metadata": {}, "hash": "e4bceb5b413b0d138e6282e24dcdb6f70b2e51a4d341fe47a01829f3e5f4954a", "class_name": "RelatedNodeInfo"}, {"node_id": "93f420f1-19c2-4478-9b9d-e12cb1d27eb1", "node_type": "1", "metadata": {}, "hash": "2a03923d06ae800c8f9fac91c2d9bb1422a9b469fd6fc24aa44a1bd1d72427ff", "class_name": "RelatedNodeInfo"}, {"node_id": "3e21a4a8-1405-4c27-8d5f-0de975f26508", "node_type": "1", "metadata": {}, "hash": "759923acb33b279a82ba76ac10e4a5cf4a8bc943a8808aa6745eddb3ae311e79", "class_name": "RelatedNodeInfo"}, {"node_id": "e00d7077-fbdc-410e-88a2-dbfd059192e8", "node_type": "1", "metadata": {}, "hash": "92fd6c66bb50454d98646ed7fa849b194914a14c4ac173910fc7ed3d84160dc9", "class_name": "RelatedNodeInfo"}, {"node_id": "09770b69-569d-422e-89e6-b71b288c5ae1", "node_type": "1", "metadata": {}, "hash": "9ea998e60bc700d38817a711ca13064a0b448dc8298c8617348a1ae30ebaf99a", "class_name": "RelatedNodeInfo"}]}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "text": "3. `receive`: Called when the server receives a message from the WebSocket.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket connect listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `connect` method is responsible for establishing the connection, extracting the collection ID from the connection\npath, loading the collection model, and accepting the connection.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket connect listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nasync def connect(self):\n    try:\n        self.collection_id = extract_connection_id(self.scope[\"path\"])\n        self.index = await load_collection_model(self.collection_id)\n        await self.accept()\n\nexcept ValueError as e:\nawait self.accept()\nawait self.close(code=4000)\nexcept Exception as e:\npass\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket disconnect listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `disconnect` method is empty in this case, as there are no additional actions to be taken when the WebSocket is\nclosed.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket receive listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `receive` method is responsible for processing incoming messages from the WebSocket. It takes the incoming message,\ndecodes it, and then queries the loaded collection model using the provided query. The response is then formatted as a\nmarkdown string and sent back to the client over the WebSocket connection.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket receive listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nasync def receive(self, text_data):\n    text_data_json = json.loads(text_data)\n\n    if self.index is not None:\n        query_str = text_data_json[\"query\"]\n        modified_query_str = f\"Please return a nicely formatted markdown string to this request:\\n\\n{query_str}\"\n        query_engine = self.index.as_query_engine()\n        response = query_engine.query(modified_query_str)\n\n        markdown_response = f\"## Response\\n\\n{response}\\n\\n\"\n        if response.source_nodes:\n            markdown_sources = f\"## Sources\\n\\n{response.get_formatted_sources()}\"\n        else:\n            markdown_sources = \"\"\n\n        formatted_response = f\"{markdown_response}{markdown_sources}\"\n\n        await self.send(json.dumps({\"response\": formatted_response}, indent=4))\n    else:\n        await self.send(json.dumps({\"error\": \"No index loaded for this connection.\"}, indent=4))\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket receive listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo load the collection model, the `load_collection_model` function is used, which can be found\nin `delphic/utils/collections.py`. This\nfunction retrieves the collection object with the given collection ID, checks if a JSON file for the collection model\nexists, and if not, creates one. Then, it sets up the `LLMPredictor` and `ServiceContext` before loading\nthe `VectorStoreIndex` using the cache file.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket receive listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nasync def load_collection_model(collection_id: str | int) -> VectorStoreIndex:\n    \"\"\"\n    Load the Collection model from cache or the database, and return the index.\n\n    Args:\n        collection_id (Union[str, int]): The ID of the Collection model instance.\n\n    Returns:\n        VectorStoreIndex: The loaded index.\n\n    This function performs the following steps:\n    1.", "start_char_idx": 568008, "end_char_idx": 573879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "716fffcb-e467-4b3f-b39b-40b128c950fa": {"__data__": {"id_": "716fffcb-e467-4b3f-b39b-40b128c950fa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dad26612-babc-4a44-84ef-3f98b427ac0c", "node_type": "1", "metadata": {}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "710f2056-4774-4562-9322-e9bd4e56e47f", "node_type": "1", "metadata": {}, "hash": "27e568ed32d6da847614a6fffe329845f6d707f33e8ea4aaa31094133907d0c4", "class_name": "RelatedNodeInfo"}, {"node_id": "7e46d80c-1192-4e95-9212-1c040333c957", "node_type": "1", "metadata": {}, "hash": "25049273d985ca30366bd2b3f03812fe6d910677343bd69235c8e6441834a4e9", "class_name": "RelatedNodeInfo"}, {"node_id": "9cf4a007-4214-431f-acab-37e91d82c897", "node_type": "1", "metadata": {}, "hash": "73b09ad8eb672f367f4688a9fd60f191c20c3619a31f7dd53f6b857df4dcc7a1", "class_name": "RelatedNodeInfo"}]}, "hash": "bc1f11fe85f292099878a8af572638629b18b9280808e41971daef1e47e7bd29", "text": "This function performs the following steps:\n    1. Retrieve the Collection object with the given collection_id.\n    2. Check if a JSON file with the name '/cache/model_{collection_id}.json' exists.\n    3. If the JSON file doesn't exist, load the JSON from the Collection.model FileField and save it to\n       '/cache/model_{collection_id}.json'.\n    4. Call VectorStoreIndex.load_from_disk with the cache_file_path.\n    \"\"\"\n    # Retrieve the Collection object\n    collection = await Collection.objects.aget(id=collection_id)\n    logger.info(f\"load_collection_model() - loaded collection {collection_id}\")\n\n    # Make sure there's a model\n    if collection.model.name:\n        logger.info(\"load_collection_model() - Setup local json index file\")\n\n        # Check if the JSON file exists\n        cache_dir = Path(settings.BASE_DIR) / \"cache\"\n        cache_file_path = cache_dir / f\"model_{collection_id}.json\"\n        if not cache_file_path.exists():\n            cache_dir.mkdir(parents=True, exist_ok=True)\n            with collection.model.open(\"rb\") as model_file:\n                with cache_file_path.open(\"w+\", encoding=\"utf-8\") as cache_file:\n                    cache_file.write(model_file.read().decode(\"utf-8\"))\n\n        # define LLM\n        logger.info(\n            f\"load_collection_model() - Setup service context with tokens {settings.MAX_TOKENS} and \"\n            f\"model {settings.MODEL_NAME}\"\n        )\n        llm_predictor = LLMPredictor(\n            llm=OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=512)\n        )\n        service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n        # Call VectorStoreIndex.load_from_disk\n        logger.info(\"load_collection_model() - Load llama index\")\n        index = VectorStoreIndex.load_from_disk(\n            cache_file_path, service_context=service_context\n        )\n        logger.info(\n            \"load_collection_model() - Llamaindex loaded and ready for query...\"\n        )\n\n    else:\n        logger.error(\n            f\"load_collection_model() - collection {collection_id} has no model!\"\n        )\n        raise ValueError(\"No model exists for this collection!\")\n\n    return index\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Overview\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe chose to use TypeScript, React and Material-UI (MUI) for the Delphic project\u2019s frontend for a couple reasons. First,\nas the most popular component library (MUI) for the most popular frontend framework (React), this choice makes this\nproject accessible to a huge community of developers. Second, React is, at this point, a stable and generally well-liked\nframework that delivers valuable abstractions in the form of its virtual DOM while still being relatively stable and, in\nour opinion, pretty easy to learn, again making it accessible.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Frontend Project Structure\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe frontend can be found in the `/frontend` directory of the\nrepo, with the React-related components being in `/frontend/src` . You\u2019ll notice there is a DockerFile in the `frontend`\ndirectory and several folders and files related to configuring our frontend web\nserver \u2014 nginx.\n\nThe `/frontend/src/App.tsx` file serves as the entry point of the application. It defines the main components, such as\nthe login form, the drawer layout, and the collection create modal. The main components are conditionally rendered based\non whether the user is logged in and has an authentication token.\n\nThe DrawerLayout2 component is defined in the`DrawerLayour2.tsx` file. This component manages the layout of the\napplication and provides the navigation and main content areas.\n\nSince the application is relatively simple, we can get away with not using a complex state management solution like\nRedux and just use React\u2019s useState hooks.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Grabbing Collections from the Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe collections available to the logged-in user are retrieved and displayed in the DrawerLayout2 component. The process\ncan be broken down into the following steps:\n\n1.", "start_char_idx": 573829, "end_char_idx": 579026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dad26612-babc-4a44-84ef-3f98b427ac0c": {"__data__": {"id_": "dad26612-babc-4a44-84ef-3f98b427ac0c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "716fffcb-e467-4b3f-b39b-40b128c950fa", "node_type": "1", "metadata": {}, "hash": "bc1f11fe85f292099878a8af572638629b18b9280808e41971daef1e47e7bd29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400", "node_type": "1", "metadata": {}, "hash": "dbb8412828fb2951465dbfd852768f350087a020e6e9e05b443ad1d53a4a8442", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "92b82f76-caaa-4ca6-811c-8c208c13734c", "node_type": "1", "metadata": {}, "hash": "639bfe734f2a88b0110be2ef9013ef30dcf1ffbbed92abd588103006ed56ee2f", "class_name": "RelatedNodeInfo"}, {"node_id": "2cd5c2bd-963a-4ba5-9615-7c0016d9970c", "node_type": "1", "metadata": {}, "hash": "3e70abe3b1cb13dca53e3718aaa420440e67dedb2424ffde92a73e759a93b921", "class_name": "RelatedNodeInfo"}, {"node_id": "915f2f95-9027-49f7-a8e8-9910c2c23fe6", "node_type": "1", "metadata": {}, "hash": "d2a6e992b350c20d4b35cc304584cf15a2019c72427a44dbb788ab2d830720a7", "class_name": "RelatedNodeInfo"}, {"node_id": "d4e4aca4-1228-412c-a869-47566bb7958b", "node_type": "1", "metadata": {}, "hash": "75c417795abfac11f22a6edcc2048221572b458ca0d39da45abd4452ed123214", "class_name": "RelatedNodeInfo"}]}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "text": "The process\ncan be broken down into the following steps:\n\n1. Initializing state variables:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Grabbing Collections from the Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nconst[collections, setCollections] = useState < CollectionModelSchema[] > ([]);\nconst[loading, setLoading] = useState(true);\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Grabbing Collections from the Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere, we initialize two state variables: `collections` to store the list of collections and `loading` to track whether\nthe collections are being fetched.\n\n2. Collections are fetched for the logged-in user with the `fetchCollections()` function:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Grabbing Collections from the Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nconst\nfetchCollections = async () = > {\ntry {\nconst accessToken = localStorage.getItem(\"accessToken\");\nif (accessToken) {\nconst response = await getMyCollections(accessToken);\nsetCollections(response.data);\n}\n} catch (error) {\nconsole.error(error);\n} finally {\nsetLoading(false);\n}\n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Grabbing Collections from the Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `fetchCollections` function retrieves the collections for the logged-in user by calling the `getMyCollections` API\nfunction with the user's access token. It then updates the `collections` state with the retrieved data and sets\nthe `loading` state to `false` to indicate that fetching is complete.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Displaying Collections\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe latest collectios are displayed in the drawer like this:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Displaying Collections\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n< List >\n{collections.map((collection) = > (\n    < div key={collection.id} >\n    < ListItem disablePadding >\n    < ListItemButton\n    disabled={\n    collection.status != = CollectionStatus.COMPLETE | |\n    !collection.has_model\n    }\n    onClick={() = > handleCollectionClick(collection)}\nselected = {\n    selectedCollection & &\n    selectedCollection.id == = collection.id\n}\n>\n< ListItemText\nprimary = {collection.title} / >\n          {collection.status == = CollectionStatus.RUNNING ? (\n    < CircularProgress\n    size={24}\n    style={{position: \"absolute\", right: 16}}\n    / >\n): null}\n< / ListItemButton >\n    < / ListItem >\n        < / div >\n))}\n< / List >\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Displaying Collections\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou\u2019ll notice that the `disabled` property of a collection\u2019s `ListItemButton` is set based on whether the collection's\nstatus is not `CollectionStatus.COMPLETE` or the collection does not have a model (`!collection.has_model`). If either\nof these conditions is true, the button is disabled, preventing users from selecting an incomplete or model-less\ncollection. Where the CollectionStatus is RUNNING, we also show a loading wheel over the button.\n\nIn a separate `useEffect` hook, we check if any collection in the `collections` state has a status\nof `CollectionStatus.RUNNING` or `CollectionStatus.QUEUED`.", "start_char_idx": 578966, "end_char_idx": 584602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14ddcb99-9b21-4ce8-8cc8-38a1abfea400": {"__data__": {"id_": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dad26612-babc-4a44-84ef-3f98b427ac0c", "node_type": "1", "metadata": {}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14c27008-5937-4330-9775-61c1868104c5", "node_type": "1", "metadata": {}, "hash": "43ed4273bd08cd6ce2d235818fa84cd3b440ff95d3eb937e7239f491abcf7a06", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "9e69cfef-1cb9-4673-aadb-5718751b3c8a", "node_type": "1", "metadata": {}, "hash": "3fcd5abd19cb5b7d3a6574c10bbccd39d08a445aa69178a8e1cc2ae042a2dab5", "class_name": "RelatedNodeInfo"}, {"node_id": "70fc7448-eb16-4b30-bf32-459082292708", "node_type": "1", "metadata": {}, "hash": "0414475b61aacf66f7b41cad8a2b1acb68d631c807b45ce18aa162187f5a264b", "class_name": "RelatedNodeInfo"}, {"node_id": "81e9c672-8e58-419e-a558-49379373a75d", "node_type": "1", "metadata": {}, "hash": "a6e97de4e9fca0100f3c9f3e7df404a61f99b3e107266f0bc4bcccfc41cf1aae", "class_name": "RelatedNodeInfo"}]}, "hash": "dbb8412828fb2951465dbfd852768f350087a020e6e9e05b443ad1d53a4a8442", "text": "If so, we set up an interval to repeatedly call\nthe `fetchCollections` function every 15 seconds (15,000 milliseconds) to update the collection statuses. This way, the\napplication periodically checks for completed collections, and the UI is updated accordingly when the processing is\ndone.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Displaying Collections\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nuseEffect(() = > {\n    let\ninterval: NodeJS.Timeout;\nif (\n    collections.some(\n        (collection) = >\ncollection.status == = CollectionStatus.RUNNING | |\ncollection.status == = CollectionStatus.QUEUED\n)\n) {\n    interval = setInterval(() = > {\n    fetchCollections();\n}, 15000);\n}\nreturn () = > clearInterval(interval);\n}, [collections]);\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat View Component\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `ChatView` component in `frontend/src/chat/ChatView.tsx` is responsible for handling and displaying a chat interface\nfor a user to interact with a collection. The component establishes a WebSocket connection to communicate in real-time\nwith the server, sending and receiving messages.\n\nKey features of the `ChatView` component include:\n\n1. Establishing and managing the WebSocket connection with the server.\n2. Displaying messages from the user and the server in a chat-like format.\n3. Handling user input to send messages to the server.\n4. Updating the messages state and UI based on received messages from the server.\n5. Displaying connection status and errors, such as loading messages, connecting to the server, or encountering errors\n   while loading a collection.\n\nTogether, all of this allows users to interact with their selected collection with a very smooth, low-latency\nexperience.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe WebSocket connection in the `ChatView` component is used to establish real-time communication between the client and\nthe server. The WebSocket connection is set up and managed in the `ChatView` component as follows:\n\nFirst, we want to initialize the the WebSocket reference:\n\nconst websocket = useRef<WebSocket | null>(null);\n\nA `websocket` reference is created using `useRef`, which holds the WebSocket object that will be used for\ncommunication. `useRef` is a hook in React that allows you to create a mutable reference object that persists across\nrenders. It is particularly useful when you need to hold a reference to a mutable object, such as a WebSocket\nconnection, without causing unnecessary re-renders.\n\nIn the `ChatView` component, the WebSocket connection needs to be established and maintained throughout the lifetime of\nthe component, and it should not trigger a re-render when the connection state changes. By using `useRef`, you ensure\nthat the WebSocket connection is kept as a reference, and the component only re-renders when there are actual state\nchanges, such as updating messages or displaying errors.\n\nThe `setupWebsocket` function is responsible for establishing the WebSocket connection and setting up event handlers to\nhandle different WebSocket events.\n\nOverall, the setupWebsocket function looks like this:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nconst setupWebsocket = () => {  \n  setConnecting(true);  \n  // Here, a new WebSocket object is created using the specified URL, which includes the   \n  // selected collection's ID and the user's authentication token.  \n    \n  websocket.current = new WebSocket(  \n    `ws://localhost:8000/ws/collections/${selectedCollection.id}/query/?token=${authToken}`  \n  );  \n  \n  websocket.current.onopen = (event) => {  \n    //...  \n  };  \n  \n  websocket.current.onmessage = (event) => {  \n    //...  \n  };  \n  \n  websocket.current.onclose = (event) => {  \n    //...  \n  };  \n  \n  websocket.current.onerror = (event) => {  \n    //...  \n  };  \n  \n  return () => {  \n    websocket.current?.close();  \n  };  \n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNotice in a bunch of places we trigger updates to the GUI based on the information from the web socket client.\n\nWhen the component first opens and we try to establish a connection, the `onopen` listener is triggered.", "start_char_idx": 584603, "end_char_idx": 590664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14c27008-5937-4330-9775-61c1868104c5": {"__data__": {"id_": "14c27008-5937-4330-9775-61c1868104c5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400", "node_type": "1", "metadata": {}, "hash": "dbb8412828fb2951465dbfd852768f350087a020e6e9e05b443ad1d53a4a8442", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94", "node_type": "1", "metadata": {}, "hash": "596a282cd8020eed7a0e3e979a696cdcc65fe1221906cd27acd8a7902b1d2f34", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "5fa243cd-4c13-470e-85e8-6cc591c573e0", "node_type": "1", "metadata": {}, "hash": "f5d73266a3264fa5befc9df822af751b8aa111e30e5002cc4d06cc35663f5306", "class_name": "RelatedNodeInfo"}, {"node_id": "81f733f2-318d-48cd-b64c-6dd5478fbd63", "node_type": "1", "metadata": {}, "hash": "4f1a70ed93695c911ac863d09b929d88865d8a884df57ce1b02e813c67530bd9", "class_name": "RelatedNodeInfo"}, {"node_id": "bc39d683-788b-4286-8c03-a2237a41817c", "node_type": "1", "metadata": {}, "hash": "761fe765126de7e161e0f951e8e21a8a016a49a38b74fff2ade5518ad3d3683d", "class_name": "RelatedNodeInfo"}]}, "hash": "43ed4273bd08cd6ce2d235818fa84cd3b440ff95d3eb937e7239f491abcf7a06", "text": "In the\ncallback, the component updates the states to reflect that the connection is established, any previous errors are\ncleared, and no messages are awaiting responses:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwebsocket.current.onopen = (event) => {  \n  setError(false);  \n  setConnecting(false);  \n  setAwaitingMessage(false);  \n  \n  console.log(\"WebSocket connected:\", event);  \n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n`onmessage`is triggered when a new message is received from the server through the WebSocket connection. In the\ncallback, the received data is parsed and the `messages` state is updated with the new message from the server:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwebsocket.current.onmessage = (event) => {  \n  const data = JSON.parse(event.data);  \n  console.log(\"WebSocket message received:\", data);  \n  setAwaitingMessage(false);  \n  \n  if (data.response) {  \n    // Update the messages state with the new message from the server  \n    setMessages((prevMessages) => [  \n      ...prevMessages,  \n      {  \n        sender_id: \"server\",  \n        message: data.response,  \n        timestamp: new Date().toLocaleTimeString(),  \n      },  \n    ]);  \n  }  \n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n`onclose`is triggered when the WebSocket connection is closed. In the callback, the component checks for a specific\nclose code (`4000`) to display a warning toast and update the component states accordingly. It also logs the close\nevent:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwebsocket.current.onclose = (event) => {  \n  if (event.code === 4000) {  \n    toast.warning(  \n      \"Selected collection's model is unavailable. Was it created properly?\"  \n    );  \n    setError(true);  \n    setConnecting(false);  \n    setAwaitingMessage(false);  \n  }  \n  console.log(\"WebSocket closed:\", event);  \n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFinally, `onerror` is triggered when an error occurs with the WebSocket connection.", "start_char_idx": 590665, "end_char_idx": 595103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94": {"__data__": {"id_": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14c27008-5937-4330-9775-61c1868104c5", "node_type": "1", "metadata": {}, "hash": "43ed4273bd08cd6ce2d235818fa84cd3b440ff95d3eb937e7239f491abcf7a06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c", "node_type": "1", "metadata": {}, "hash": "03de927b2bbd47b32970de07588b746f6cf27b4800b8a086bf7ea480effcda95", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "984ee8d9-3183-4a7e-9648-0c93d1a97c3a", "node_type": "1", "metadata": {}, "hash": "c0696ecb6e7a7e99ffa17aecc83a3b9261e882be14eeb9900ead618b581fad05", "class_name": "RelatedNodeInfo"}, {"node_id": "806b115b-52e6-46b9-a17c-8d3f98522927", "node_type": "1", "metadata": {}, "hash": "8e25a79220d9f5175f6a4bd491daa781eeb65ecf702713f8b71f575ab95b61d2", "class_name": "RelatedNodeInfo"}, {"node_id": "589174b0-b27e-46e7-acfb-3c0f5105cfbc", "node_type": "1", "metadata": {}, "hash": "befd931d431fa7fa99755595701baf4092967b74a8af07a93aae8b602ef2aa0a", "class_name": "RelatedNodeInfo"}]}, "hash": "596a282cd8020eed7a0e3e979a696cdcc65fe1221906cd27acd8a7902b1d2f34", "text": "In the callback, the component\nupdates the states to reflect the error and logs the error event:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwebsocket.current.onerror = (event) => {\n      setError(true);\n      setConnecting(false);\n      setAwaitingMessage(false);\n\n      console.error(\"WebSocket error:\", event);\n    };\n  ```\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Rendering our Chat Messages\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn the `ChatView` component, the layout is determined using CSS styling and Material-UI components. The main layout\nconsists of a container with a `flex` display and a column-oriented `flexDirection`. This ensures that the content\nwithin the container is arranged vertically.\n\nThere are three primary sections within the layout:\n\n1. The chat messages area: This section takes up most of the available space and displays a list of messages exchanged\n   between the user and the server. It has an overflow-y set to \u2018auto\u2019, which allows scrolling when the content\n   overflows the available space. The messages are rendered using the `ChatMessage` component for each message and\n   a `ChatMessageLoading` component to show the loading state while waiting for a server response.\n2. The divider: A Material-UI `Divider` component is used to separate the chat messages area from the input area,\n   creating a clear visual distinction between the two sections.\n3. The input area: This section is located at the bottom and allows the user to type and send messages. It contains\n   a `TextField` component from Material-UI, which is set to accept multiline input with a maximum of 2 rows. The input\n   area also includes a `Button` component to send the message. The user can either click the \"Send\" button or press \"\n   Enter\" on their keyboard to send the message.\n\nThe user inputs accepted in the `ChatView` component are text messages that the user types in the `TextField`. The\ncomponent processes these text inputs and sends them to the server through the WebSocket connection.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Prerequisites\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo deploy the app, you're going to need Docker and Docker Compose installed. If you're on Ubuntu or another, common\nLinux distribution, DigitalOcean has\na great Docker tutorial and\nanother great tutorial\nfor Docker Compose\nyou can follow. If those don't work for you, try\nthe official docker documentation.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe project is based on django-cookiecutter, and it\u2019s pretty easy to get it deployed on a VM and configured to serve\nHTTPs traffic for a specific domain. The configuration is somewhat involved, however \u2014 not because of this project, but\nit\u2019s just a fairly involved topic to configure your certificates, DNS, etc.\n\nFor the purposes of this guide, let\u2019s just get running locally. Perhaps we\u2019ll release a guide on production deployment.\nIn the meantime, check out\nthe Django Cookiecutter project docs\nfor starters.\n\nThis guide assumes your goal is to get the application up and running for use. If you want to develop, most likely you\nwon\u2019t want to launch the compose stack with the \u2014 profiles fullstack flag and will instead want to launch the react\nfrontend using the node development server.", "start_char_idx": 595104, "end_char_idx": 599877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f4c1d8c-f359-481a-b841-e364b1ace56c": {"__data__": {"id_": "7f4c1d8c-f359-481a-b841-e364b1ace56c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94", "node_type": "1", "metadata": {}, "hash": "596a282cd8020eed7a0e3e979a696cdcc65fe1221906cd27acd8a7902b1d2f34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "936e4894-59f4-4fae-812f-9cedd308bd4a", "node_type": "1", "metadata": {}, "hash": "67a772a64c948b00adc97c3bc5203ff012636751548eecb5a11ad1bb177a5675", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "eb1d53a1-ab5e-4e33-913a-7cf24c76b83f", "node_type": "1", "metadata": {}, "hash": "10857ada186dc32f49f4b563908b062023ba849d2f0c12bb9374f7ae6a1aeb76", "class_name": "RelatedNodeInfo"}, {"node_id": "0990ce95-ea2e-463b-a0eb-aeb5aaa435ae", "node_type": "1", "metadata": {}, "hash": "33fc09321bf9423eb9270691720880d3bc0d92d6daea32c0085439989d78d266", "class_name": "RelatedNodeInfo"}, {"node_id": "1efa5e14-1c8a-4255-940c-32d59f1c2d60", "node_type": "1", "metadata": {}, "hash": "979482d022fd53b1631eda14492e83ebb25e6c756640d7cd7a0ad73fd9bcc253", "class_name": "RelatedNodeInfo"}]}, "hash": "03de927b2bbd47b32970de07588b746f6cf27b4800b8a086bf7ea480effcda95", "text": "To deploy, first clone the repo:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngit clone https://github.com/yourusername/delphic.git\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nChange into the project directory:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncd delphic\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCopy the sample environment files:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmkdir -p ./.envs/.local/  \ncp -a ./docs/sample_envs/local/.frontend ./frontend  \ncp -a ./docs/sample_envs/local/.django ./.envs/.local  \ncp -a ./docs/sample_envs/local/.postgres ./.envs/.local\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEdit the `.django` and `.postgres` configuration files to include your OpenAI API key and set a unique password for your\ndatabase user. You can also set the response token limit in the .django file or switch which OpenAI model you want to\nuse. GPT4 is supported, assuming you\u2019re authorized to access it.\n\nBuild the docker compose stack with the `--profiles fullstack` flag:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nsudo docker-compose --profiles fullstack -f local.yml build\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe fullstack flag instructs compose to build a docker container from the frontend folder and this will be launched\nalong with all of the needed, backend containers. It takes a long time to build a production React container, however,\nso we don\u2019t recommend you develop this way. Follow\nthe instructions in the project readme.md for development environment\nsetup instructions.", "start_char_idx": 599879, "end_char_idx": 604657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "936e4894-59f4-4fae-812f-9cedd308bd4a": {"__data__": {"id_": "936e4894-59f4-4fae-812f-9cedd308bd4a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c", "node_type": "1", "metadata": {}, "hash": "03de927b2bbd47b32970de07588b746f6cf27b4800b8a086bf7ea480effcda95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57d98462-a7ab-4c26-8cfa-c07b48f6f17d", "node_type": "1", "metadata": {}, "hash": "23e160dfe08668d48eae9b90ee4f6b4e2a157446b9f8d5fb2298082058dec76d", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2a56c91e-5fd2-4fbb-908d-1ec63cdb2351", "node_type": "1", "metadata": {}, "hash": "52271a0a7b0c5eaaf4d7b71b5f9c33acdaf6de00922ce4fb3ad0a971cf1b20c0", "class_name": "RelatedNodeInfo"}, {"node_id": "3ef5868c-09ff-4247-919d-ec84260604a5", "node_type": "1", "metadata": {}, "hash": "e50037f5b7e3ed4fa4d02e6c478a2737f3ffe4009e74693b43b1f5902870afd6", "class_name": "RelatedNodeInfo"}, {"node_id": "748bfeef-d12b-42d3-874e-8737ab6b65ab", "node_type": "1", "metadata": {}, "hash": "938048aaa13e2086dfc23f59935b0b0c3816ca9cd73c3aca9ed0294f7d452da0", "class_name": "RelatedNodeInfo"}]}, "hash": "67a772a64c948b00adc97c3bc5203ff012636751548eecb5a11ad1bb177a5675", "text": "Follow\nthe instructions in the project readme.md for development environment\nsetup instructions.\n\nFinally, bring up the application:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nsudo docker-compose -f local.yml up\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, visit `localhost:3000` in your browser to see the frontend, and use the Delphic application locally.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Setup Users\nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn order to actually use the application (at the moment, we intend to make it possible to share certain models with\nunauthenticated users), you need a login. You can use either a superuser or non-superuser. In either case, someone needs\nto first create a superuser using the console:\n\n**Why set up a Django superuser?** A Django superuser has all the permissions in the application and can manage all\naspects of the system, including creating, modifying, and deleting users, collections, and other data. Setting up a\nsuperuser allows you to fully control and manage the application.\n\n**How to create a Django superuser:**\n\n1 Run the following command to create a superuser:\n\nsudo docker-compose -f local.yml run django python manage.py createsuperuser\n\n2 You will be prompted to provide a username, email address, and password for the superuser. Enter the required\ninformation.\n\n**How to create additional users using Django admin:**\n\n1. Start your Delphic application locally following the deployment instructions.\n2. Visit the Django admin interface by navigating to `http://localhost:8000/admin` in your browser.\n3. Log in with the superuser credentials you created earlier.\n4. Click on \u201cUsers\u201d under the \u201cAuthentication and Authorization\u201d section.\n5. Click on the \u201cAdd user +\u201d button in the top right corner.\n6. Enter the required information for the new user, such as username and password. Click \u201cSave\u201d to create the user.\n7. To grant the new user additional permissions or make them a superuser, click on their username in the user list,\n   scroll down to the \u201cPermissions\u201d section, and configure their permissions accordingly. Save your changes.\n\nFile Name: Docs\\end_to_end_tutorials\\apps.md\nContent Type: text\nHeader Path: Full-Stack Web Application\nfile_path: Docs\\end_to_end_tutorials\\apps.md\nfile_name: apps.md\nfile_type: None\nfile_size: 742\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit.\n\nWe provide tutorials and resources to help you get started in this area.\n\nRelevant Resources:\n- Fullstack Application Guide\n- Fullstack Application with Delphic\n- A Guide to Extracting Terms and Definitions\n- LlamaIndex Starter Pack\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex is an interface between your data and LLM's; it offers the toolkit for you to setup a query interface around your data for any downstream task, whether it's question-answering, summarization, or more.\n\nIn this tutorial, we show you how to build a context augmented chatbot. We use Langchain for the underlying Agent/Chatbot abstractions, and we use LlamaIndex for the data retrieval/lookup/querying! The result is a chatbot agent that has access to a rich set of \"data interface\" Tools that LlamaIndex provides to answer queries over your data.\n\n**Note**: This is a continuation of some initial work building a query interface over SEC 10-K filings - check it out here.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Context\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn this tutorial, we build an \"10-K Chatbot\" by downloading the raw UBER 10-K HTML filings from Dropbox. The user can choose to ask questions regarding the 10-K filings.", "start_char_idx": 604561, "end_char_idx": 610143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57d98462-a7ab-4c26-8cfa-c07b48f6f17d": {"__data__": {"id_": "57d98462-a7ab-4c26-8cfa-c07b48f6f17d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "936e4894-59f4-4fae-812f-9cedd308bd4a", "node_type": "1", "metadata": {}, "hash": "67a772a64c948b00adc97c3bc5203ff012636751548eecb5a11ad1bb177a5675", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff0056b9-5f40-4b65-b337-973ab0014686", "node_type": "1", "metadata": {}, "hash": "acade153dff820885cbce8848a7524b9ddbe32f3022cf27fbc0b889763845ebc", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "8066023d-e814-45de-bdc2-b331be7e8df4", "node_type": "1", "metadata": {}, "hash": "3fed6bcfd8ed393fadc8380d82f135cfd32d6b7d3437de21136d4030ffcc3537", "class_name": "RelatedNodeInfo"}, {"node_id": "1a716a35-a27d-46ef-bd71-7c33323126a4", "node_type": "1", "metadata": {}, "hash": "5edfc4a059dbe561c3cec3af0edf6b867e7366149b84e3eb406b84d60c1ca9b4", "class_name": "RelatedNodeInfo"}]}, "hash": "23e160dfe08668d48eae9b90ee4f6b4e2a157446b9f8d5fb2298082058dec76d", "text": "The user can choose to ask questions regarding the 10-K filings.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Ingest Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLet's first download the raw 10-k files, from 2019-2022.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/download files\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!mkdir data\n!wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip\n!unzip data/UBER.zip -d data\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/download files\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe use the Unstructured library to parse the HTML files into formatted text.\nWe have a direct integration with Unstructured through LlamaHub - this allows us to convert any text into a Document format that LlamaIndex can ingest.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/download files\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import download_loader, VectorStoreIndex, ServiceContext, StorageContext, load_index_from_storage\nfrom pathlib import Path\n\nyears = [2022, 2021, 2020, 2019]\nUnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n\nloader = UnstructuredReader()\ndoc_set = {}\nall_docs = []\nfor year in years:\n    year_docs = loader.load_data(file=Path(f'./data/UBER/UBER_{year}.html'), split_documents=False)\n    # insert year metadata into each year\n    for d in year_docs:\n        d.metadata = {\"year\": year}\n    doc_set[year] = year_docs\n    all_docs.extend(year_docs)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up Vector Indices for each year\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe first setup a vector index for each year. Each vector index allows us \nto ask questions about the 10-K filing of a given year.\n\nWe build each index and save it to disk.", "start_char_idx": 610079, "end_char_idx": 613203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff0056b9-5f40-4b65-b337-973ab0014686": {"__data__": {"id_": "ff0056b9-5f40-4b65-b337-973ab0014686", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57d98462-a7ab-4c26-8cfa-c07b48f6f17d", "node_type": "1", "metadata": {}, "hash": "23e160dfe08668d48eae9b90ee4f6b4e2a157446b9f8d5fb2298082058dec76d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c51945eb-302c-4d86-90c2-dd965b7f1868", "node_type": "1", "metadata": {}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "437e4f47-ffdd-40dd-a21f-5d074f720231", "node_type": "1", "metadata": {}, "hash": "c83ae1fc5b6e3e4310bc9cf4ce8b72de5b6c286e71a5aa635fdd60e7dc252ef5", "class_name": "RelatedNodeInfo"}, {"node_id": "8619cde6-3639-40f4-b458-81a52505ab74", "node_type": "1", "metadata": {}, "hash": "8b97e9091dbf7f201d7ebe07a1be3b507ebccbe157c759a5c9d8393ceee932d7", "class_name": "RelatedNodeInfo"}]}, "hash": "acade153dff820885cbce8848a7524b9ddbe32f3022cf27fbc0b889763845ebc", "text": "We build each index and save it to disk.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/initialize simple vector indices + global vector index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nservice_context = ServiceContext.from_defaults(chunk_size=512)\nindex_set = {}\nfor year in years:\n    storage_context = StorageContext.from_defaults()\n    cur_index = VectorStoreIndex.from_documents(\n        doc_set[year], \n        service_context=service_context,\n        storage_context=storage_context,\n    )\n    index_set[year] = cur_index\n    storage_context.persist(persist_dir=f'./storage/{year}')\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/initialize simple vector indices + global vector index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo load an index from disk, do the following\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Load indices from disk\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_set = {}\nfor year in years:\n    storage_context = StorageContext.from_defaults(persist_dir=f'./storage/{year}')\n    cur_index = load_index_from_storage(storage_context=storage_context)\n    index_set[year] = cur_index\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Composing a Graph to Synthesize Answers Across 10-K Filings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSince we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings. \n\nTo address this, we compose a \"graph\" which consists of a list index defined over the 4 vector indices. Querying this graph would first retrieve information from each vector index, and combine information together via the list index.", "start_char_idx": 613163, "end_char_idx": 615928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c51945eb-302c-4d86-90c2-dd965b7f1868": {"__data__": {"id_": "c51945eb-302c-4d86-90c2-dd965b7f1868", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff0056b9-5f40-4b65-b337-973ab0014686", "node_type": "1", "metadata": {}, "hash": "acade153dff820885cbce8848a7524b9ddbe32f3022cf27fbc0b889763845ebc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c9c6365-4eb9-4dce-bdbc-ece01038550d", "node_type": "1", "metadata": {}, "hash": "b30bec469e11d89dc619bc0ee351932910bd8346b0fee593b11f6d0701193dd5", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3e7db9ab-0518-496c-bde8-9b1e0f2955d5", "node_type": "1", "metadata": {}, "hash": "27b20d12ad0764a920f13817ce057194500e5325883a110c7218e49f7716b53a", "class_name": "RelatedNodeInfo"}, {"node_id": "72077ad7-27c8-4e4d-a54c-097473a95004", "node_type": "1", "metadata": {}, "hash": "0cbcc028a378662bc86a357c042405b1d16feeb240bb91739bf220d84b36548c", "class_name": "RelatedNodeInfo"}, {"node_id": "d23269e7-9c04-4e98-a766-dee3f17a884a", "node_type": "1", "metadata": {}, "hash": "0b33792f6c885e0c443d8ed7057621cbc9ecb5ad0084b43368477b3f89204a7f", "class_name": "RelatedNodeInfo"}, {"node_id": "58719532-4381-4a45-9db6-9944aa10f590", "node_type": "1", "metadata": {}, "hash": "adef584f01e6af59e2d7e76c2109a1f1a881a50ececec3e3b29da13a99ab432a", "class_name": "RelatedNodeInfo"}]}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "text": "File Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Composing a Graph to Synthesize Answers Across 10-K Filings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ListIndex, LLMPredictor, ServiceContext, load_graph_from_storage\nfrom langchain import OpenAI\nfrom llama_index.indices.composability import ComposableGraph\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/describe each index to help traversal of composed graph\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_summaries = [f\"UBER 10-k Filing for {year} fiscal year\" for year in years]\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define an LLMPredictor set number of output tokens\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, max_tokens=512))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\nstorage_context = StorageContext.from_defaults()\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/allows us to synthesize information across each index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph = ComposableGraph.from_indices(\n    ListIndex,\n    [index_set[y] for y in years], \n    index_summaries=index_summaries,\n    service_context=service_context,\n    storage_context = storage_context,\n)\nroot_id = graph.root_id\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/[optional] save to disk\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context.persist(persist_dir=f'./storage/root')\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/[optional] load from disk, so you don't need to build graph from scratch\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph = load_graph_from_storage(\n    root_id=root_id, \n    service_context=service_context,\n    storage_context=storage_context,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up the Tools + Langchain Chatbot Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe use Langchain to setup the outer chatbot agent, which has access to a set of Tools.\nLlamaIndex provides some wrappers around indices and graphs so that they can be easily used within a Tool interface.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/do imports\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\nfrom langchain.agents import initialize_agent\n\nfrom llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/do imports\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe want to define a separate Tool for each index (corresponding to a given year), as well \nas the graph. We can define all tools under a central `LlamaToolkit` interface.\n\nBelow, we define a `IndexToolConfig` for our graph.", "start_char_idx": 615930, "end_char_idx": 621062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c9c6365-4eb9-4dce-bdbc-ece01038550d": {"__data__": {"id_": "6c9c6365-4eb9-4dce-bdbc-ece01038550d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c51945eb-302c-4d86-90c2-dd965b7f1868", "node_type": "1", "metadata": {}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56119b9c-ea21-4227-b3e6-1493babf84d6", "node_type": "1", "metadata": {}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "edd8b01c-c34b-4ca6-b23a-aeb0a80d8e26", "node_type": "1", "metadata": {}, "hash": "58667287208b1a9e0508abf65ba4b93b36fe5d3f5719576e0bcec8660a34dc3d", "class_name": "RelatedNodeInfo"}, {"node_id": "66dafd3b-417d-44f2-9cb8-e17eff5347be", "node_type": "1", "metadata": {}, "hash": "f99156030e53ba4d94d4e838af4b177ab3d28d624b24af4d2f31bec7de363984", "class_name": "RelatedNodeInfo"}]}, "hash": "b30bec469e11d89dc619bc0ee351932910bd8346b0fee593b11f6d0701193dd5", "text": "Below, we define a `IndexToolConfig` for our graph. Note that we also import a `DecomposeQueryTransform` module for use within each vector index within the graph - this allows us to \"decompose\" the overall query into a query that can be answered from each subindex. (see example below).\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define a decompose transform\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor, verbose=True\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define custom retrievers\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\n\ncustom_query_engines = {}\nfor index in index_set.values():\n    query_engine = index.as_query_engine()\n    query_engine = TransformQueryEngine(\n        query_engine,\n        query_transform=decompose_transform,\n        transform_extra_info={'index_summary': index.index_struct.summary},\n    )\n    custom_query_engines[index.index_id] = query_engine\ncustom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n    response_mode='tree_summarize',\n    verbose=True,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/construct query engine\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph_query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/tool config\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph_config = IndexToolConfig(\n    query_engine=graph_query_engine,\n    name=f\"Graph Index\",\n    description=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber.", "start_char_idx": 621011, "end_char_idx": 623841, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56119b9c-ea21-4227-b3e6-1493babf84d6": {"__data__": {"id_": "56119b9c-ea21-4227-b3e6-1493babf84d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c9c6365-4eb9-4dce-bdbc-ece01038550d", "node_type": "1", "metadata": {}, "hash": "b30bec469e11d89dc619bc0ee351932910bd8346b0fee593b11f6d0701193dd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "node_type": "1", "metadata": {}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "4b41fc5f-6cd9-4ffc-aa78-c6d8e54bbe39", "node_type": "1", "metadata": {}, "hash": "a6c60eee815c7989c80ab8b432e9e1dd7158603deb7f416624e7ae8d05272c30", "class_name": "RelatedNodeInfo"}, {"node_id": "e47dfe0e-495d-440f-9241-cc0c14b80566", "node_type": "1", "metadata": {}, "hash": "51a7d6cf9fa9dea0837c1962826599c21aa1ccfefa887b936858134edb87a003", "class_name": "RelatedNodeInfo"}, {"node_id": "7759f0d7-7e25-4712-a447-3adbdb4dafbb", "node_type": "1", "metadata": {}, "hash": "d745e21ac19485d2ba920f486a5a5565994d8965e71f436a9ca63d284b09a7bb", "class_name": "RelatedNodeInfo"}, {"node_id": "1ffa6103-38a6-42ec-afd2-efd386dd31dc", "node_type": "1", "metadata": {}, "hash": "f106a22ad4c509c919246c575588045de51b35395d598c73d384619867dd0f19", "class_name": "RelatedNodeInfo"}]}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "text": "\",\n    tool_kwargs={\"return_direct\": True}\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/tool config\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBesides the `IndexToolConfig` object for the graph, we also define an `IndexToolConfig` corresponding to each index:\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define toolkit\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_configs = []\nfor y in range(2019, 2023):\n    query_engine = index_set[y].as_query_engine(\n        similarity_top_k=3,\n    )\n    tool_config = IndexToolConfig(\n        query_engine=query_engine, \n        name=f\"Vector Index {y}\",\n        description=f\"useful for when you want to answer queries about the {y} SEC 10-K for Uber\",\n        tool_kwargs={\"return_direct\": True}\n    )\n    index_configs.append(tool_config)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define toolkit\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFinally, we combine these configs with our `LlamaToolkit`:\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define toolkit\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntoolkit = LlamaToolkit(\n    index_configs=index_configs + [graph_config],\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define toolkit\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFinally, we call `create_llama_chat_agent` to create our Langchain chatbot agent, which\nhas access to the 5 Tools we defined above:\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define toolkit\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm=OpenAI(temperature=0)\nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n    verbose=True\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can now test the agent with various queries.\n\nIf we test it with a simple \"hello\" query, the agent does not use any Tools.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent_chain.run(input=\"hi, i am bob\")\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: code\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```\n> Entering new AgentExecutor chain...\n\nThought: Do I need to use a tool? No\nAI: Hi Bob, nice to meet you! How can I help you today?\n\n> Finished chain.\n'Hi Bob, nice to meet you! How can I help you today?'", "start_char_idx": 623841, "end_char_idx": 628605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3453be55-5f04-4dd1-8870-191f9c5fd82e": {"__data__": {"id_": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56119b9c-ea21-4227-b3e6-1493babf84d6", "node_type": "1", "metadata": {}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4044733-7eda-41ed-ab44-516f207a5536", "node_type": "1", "metadata": {}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "394f6166-dfb9-459b-94e6-3efd3836717b", "node_type": "1", "metadata": {}, "hash": "5998e617e287e74a5c98d2a8beea209c4a1ae01cab0175188675b42bb9a910d1", "class_name": "RelatedNodeInfo"}, {"node_id": "2dc63329-0589-4bdc-98f1-81d61a1c2796", "node_type": "1", "metadata": {}, "hash": "d396abc0d2a7954867b490612cf643782d2d6ad10e9c34e463f4da59641514f2", "class_name": "RelatedNodeInfo"}, {"node_id": "8a03e044-a964-482d-b48e-4eef196677a9", "node_type": "1", "metadata": {}, "hash": "5600f256804a7386823f9602e7e982c154a9c6e93bf0fb1f756df26724b39f03", "class_name": "RelatedNodeInfo"}, {"node_id": "faf180dc-3423-4bfe-a347-42687a9736a1", "node_type": "1", "metadata": {}, "hash": "7bd5ddb2adc0265c8adf9526e336f5098463ddbd67a46e80c3ed51cace8cb9ec", "class_name": "RelatedNodeInfo"}]}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "text": "'Hi Bob, nice to meet you! How can I help you today?'\n```\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf we test it with a query regarding the 10-k of a given year, the agent will use\nthe relevant vector index Tool.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent_chain.run(input=\"What were some of the biggest risk factors in 2020 for Uber?\")\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: code\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```\n> Entering new AgentExecutor chain...\n\nThought: Do I need to use a tool? Yes\nAction: Vector Index 2020\nAction Input: Risk Factors\n...\n\nObservation: \n\nRisk Factors\n\nThe COVID-19 pandemic and the impact of actions to mitigate the pandemic has adversely affected and continues to adversely affect our business, financial condition, and results of operations.\n\n...\n'\\n\\nRisk Factors\\n\\nThe COVID-19 pandemic and the impact of actions to mitigate the pandemic has adversely affected and continues to adversely affect our business,\n\n```\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFinally, if we test it with a query to compare/contrast risk factors across years,\nthe agent will use the graph index Tool.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncross_query_str = (\n    \"Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\"\n)\nagent_chain.run(input=cross_query_str)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: code\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```\n> Entering new AgentExecutor chain...\n\nThought: Do I need to use a tool? Yes\nAction: Graph Index\nAction Input: Compare/contrast the risk factors described in the Uber 10-K across years.> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2022 fiscal year?\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2022 fiscal year?\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 964 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n> Got response: \nThe risk factors described in the Uber 10-K for the 2022 fiscal year include: the potential for changes in the classification of Drivers, the potential for increased competition, the potential for...\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2021 fiscal year?\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2021 fiscal year?\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 590 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n> Got response: \n1. The COVID-19 pandemic and the impact of actions to mitigate the pandemic have adversely affected and may continue to adversely affect parts of our business.\n\n2. Our business would be adversely ...\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2020 fiscal year?\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2020 fiscal year?", "start_char_idx": 628552, "end_char_idx": 633979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4044733-7eda-41ed-ab44-516f207a5536": {"__data__": {"id_": "e4044733-7eda-41ed-ab44-516f207a5536", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "node_type": "1", "metadata": {}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8", "node_type": "1", "metadata": {}, "hash": "4f258d579fc345c1ae314aa72d27d5a525a23370b1ab6a23f1fe7273d096d1b2", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7755ee58-4020-43b4-93de-6884e6fb133b", "node_type": "1", "metadata": {}, "hash": "f4a3c8297b27aebdbfb082b63d0e114aec3d6379d33532a12adf9889bdfba7ef", "class_name": "RelatedNodeInfo"}, {"node_id": "a4bdb967-6c11-42dd-89b1-fd0eb2b8fb1b", "node_type": "1", "metadata": {}, "hash": "9a7c2a389cbc6e995f4f7cb621e09194f4528bcafc478473fb2af7400c6c560a", "class_name": "RelatedNodeInfo"}, {"node_id": "30ac8e2f-551e-4d9d-bd5e-8766930322fb", "node_type": "1", "metadata": {}, "hash": "9a07e902a59b7002fe4bb60a018ecea2629688e0cd434d8ae9f4074e32258b98", "class_name": "RelatedNodeInfo"}, {"node_id": "01d84a18-fafb-4941-967f-3112652eb5a6", "node_type": "1", "metadata": {}, "hash": "318cac52d3ad2c95d507a25c382c26474204b7670893bcc48418b031550ae6fe", "class_name": "RelatedNodeInfo"}]}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "text": "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 516 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n> Got response: \nThe risk factors described in the Uber 10-K for the 2020 fiscal year include: the timing of widespread adoption of vaccines against the virus, additional actions that may be taken by governmental ...\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2019 fiscal year?\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2019 fiscal year?\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1020 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\nINFO:llama_index.indices.common.tree.base:> Building index from nodes: 0 chunks\n> Got response: \nRisk factors described in the Uber 10-K for the 2019 fiscal year include: competition from other transportation providers; the impact of government regulations; the impact of litigation; the impac...\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 7039 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 72 tokens\n\nObservation: \nIn 2020, the risk factors included the timing of widespread adoption of vaccines against the virus, additional actions that may be taken by governmental authorities, the further impact on the business of Drivers\n\n...\n\n```\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up the Chatbot Loop\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to converse with our SEC-augmented chatbot!\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up the Chatbot Loop\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwhile True:\n    text_input = input(\"User: \")\n    response = agent_chain.run(input=text_input)\n    print(f'Agent: {response}')\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up the Chatbot Loop\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere's an example of the loop in action:\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up the Chatbot Loop\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUser:  What were some of the legal proceedings against Uber in 2022?\nAgent: \n\nIn 2022, legal proceedings against Uber include a motion to compel arbitration, an appeal of a ruling that Proposition 22 is unconstitutional, a complaint alleging that drivers are employees and entitled to protections under the wage and labor laws, a summary judgment motion, allegations of misclassification of drivers and related employment violations in New York, fraud related to certain deductions, class actions in Australia alleging that Uber entities conspired to injure the group members during the period 2014 to 2017 by either directly breaching transport legislation or commissioning offenses against transport legislation by UberX Drivers in Australia, and claims of lost income and decreased value of certain taxi. Additionally, Uber is facing a challenge in California Superior Court alleging that Proposition 22 is unconstitutional, and a preliminary injunction order prohibiting Uber from classifying Drivers as independent contractors and from violating various wage and hour laws.\n\nUser:\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Notebook\nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTake a look at our corresponding notebook.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots.md\nContent Type: text\nHeader Path: Chatbots\nfile_path: Docs\\end_to_end_tutorials\\chatbots.md\nfile_name: chatbots.md\nfile_type: None\nfile_size: 333\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nChatbots are an incredibly popular use case for LLM's. LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents.", "start_char_idx": 633980, "end_char_idx": 639414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6dd9730e-36d5-4009-88e1-5e237e7767c8": {"__data__": {"id_": "6dd9730e-36d5-4009-88e1-5e237e7767c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4044733-7eda-41ed-ab44-516f207a5536", "node_type": "1", "metadata": {}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441", "node_type": "1", "metadata": {}, "hash": "8ba80fd34c4d6077da47a0dd44f97bb9e35b7441e2b96dda4986c048db106e01", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f09fe913-223f-431a-a64c-75f594606793", "node_type": "1", "metadata": {}, "hash": "e962278af8fe51e7970f6da1b748c971aa7d7b419d652bf7fe25c173d7780b16", "class_name": "RelatedNodeInfo"}, {"node_id": "9912ac21-cfcb-4e72-8552-12e6c54aa10a", "node_type": "1", "metadata": {}, "hash": "abc91f0c220d89ea534e595591d637b829c08944209c395449596a64fd8c910d", "class_name": "RelatedNodeInfo"}, {"node_id": "1153b072-6b34-4917-b560-7868b64009ff", "node_type": "1", "metadata": {}, "hash": "9cfd71943c0bed86681cd5d18c258a378b6993839810a7117958fa5b0b343a0f", "class_name": "RelatedNodeInfo"}]}, "hash": "4f258d579fc345c1ae314aa72d27d5a525a23370b1ab6a23f1fe7273d096d1b2", "text": "LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents.\n\nRelevant Resources:\n- Building a Chatbot\n- Using with a LangChain Agent\n\nFile Name: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nContent Type: text\nHeader Path: Discover LlamaIndex Video Series\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nfile_name: discover_llamaindex.md\nfile_type: None\nfile_size: 1275\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis page contains links to videos + associated notebooks for our ongoing video tutorial series \"Discover LlamaIndex\".\n\nFile Name: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nContent Type: text\nHeader Path: Discover LlamaIndex Video Series/SubQuestionQueryEngine + 10K Analysis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nfile_name: discover_llamaindex.md\nfile_type: None\nfile_size: 1275\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis video covers the `SubQuestionQueryEngine` and how it can be applied to financial documents to help decompose complex queries into multiple sub-questions.\n\nYoutube\n\nNotebook\n\nFile Name: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nContent Type: text\nHeader Path: Discover LlamaIndex Video Series/Discord Document Management\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nfile_name: discover_llamaindex.md\nfile_type: None\nfile_size: 1275\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis video covers managing documents from a source that is consantly updating (i.e Discord) and how you can avoid document duplication and save embedding tokens.\n\nYoutube\n\nNotebook + Supplimentary Material\n\nReference Docs\n\nFile Name: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nContent Type: text\nHeader Path: Discover LlamaIndex Video Series/Joint Text to SQL and Semantic Search\nfile_path: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nfile_name: discover_llamaindex.md\nfile_type: None\nfile_size: 1275\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis video covers the tools built into LlamaIndex for combining SQL and semantic search into a single unified query interface.\n\nYoutube\n\nNotebook\n\nFile Name: Docs\\end_to_end_tutorials\\privacy.md\nContent Type: text\nHeader Path: Private Setup\nfile_path: Docs\\end_to_end_tutorials\\privacy.md\nfile_name: privacy.md\nfile_type: None\nfile_size: 169\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRelevant Resources:\n- Using LlamaIndex with Local Models\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlama Index has many use cases (semantic search, summarization, etc.) that are well documented. However, this doesn't mean we can't apply Llama Index to very specific use cases!\n\nIn this tutorial, we will go through the design process of using Llama Index to extract terms and definitions from text, while allowing users to query those terms later. Using Streamlit, we can provide an easy way to build frontend for running and testing all of this, and quickly iterate with our design.\n\nThis tutorial assumes you have Python3.9+ and the following packages installed:\n\n- llama-index\n- streamlit\n\nAt the base level, our objective is to take text from a document, extract terms and definitions, and then provide a way for users to query that knowledge base of terms and definitions. The tutorial will go over features from both Llama Index and Streamlit, and hopefully provide some interesting solutions for common problems that come up.\n\nThe final version of this tutorial can be found here and a live hosted demo is available on Huggingface Spaces.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Uploading Text\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nStep one is giving users a way to upload documents. Let\u2019s write some code using Streamlit to provide the interface for this! Use the following code and launch the app with `streamlit run app.py`.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Uploading Text\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport streamlit as st\n\nst.title(\"\ud83e\udd99 Llama Index Term Extractor \ud83e\udd99\")\n\ndocument_text = st.text_area(\"Or enter raw text\")\nif st.button(\"Extract Terms and Definitions\") and document_text:\n    with st.spinner(\"Extracting...\"):\n        extracted_terms = document text  # this is a placeholder!", "start_char_idx": 639334, "end_char_idx": 644812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441": {"__data__": {"id_": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8", "node_type": "1", "metadata": {}, "hash": "4f258d579fc345c1ae314aa72d27d5a525a23370b1ab6a23f1fe7273d096d1b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "node_type": "1", "metadata": {}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "6096b9e3-fcbc-4aad-bbc1-ba2ba633f284", "node_type": "1", "metadata": {}, "hash": "60e558ee03e6f032bf7befad319be4157403bf56b6f6f74079fd67ba584deb0b", "class_name": "RelatedNodeInfo"}, {"node_id": "6bf3ee39-9515-4002-a9f6-049f1d3b26ec", "node_type": "1", "metadata": {}, "hash": "75e19492e328af0e653d6217a1960ac0a3a21cd94e7a8d93a5556e941d3b5abf", "class_name": "RelatedNodeInfo"}, {"node_id": "ca4d5a00-f5ab-47a8-97c9-00f8f7660b47", "node_type": "1", "metadata": {}, "hash": "46c9dbb47802daf7343e30af7872af0c00e8c5b1d504afb0a6ebf84bad034688", "class_name": "RelatedNodeInfo"}]}, "hash": "8ba80fd34c4d6077da47a0dd44f97bb9e35b7441e2b96dda4986c048db106e01", "text": "st.write(extracted_terms)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Uploading Text\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSuper simple right! But you'll notice that the app doesn't do anything useful yet. To use llama_index, we also need to setup our OpenAI LLM. There are a bunch of possible settings for the LLM, so we can let the user figure out what's best. We should also let the user set the prompt that will extract the terms (which will also help us debug what works best).\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/LLM Settings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis next step introduces some tabs to our app, to separate it into different panes that provide different features. Let's create a tab for LLM settings and for uploading text:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/LLM Settings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport os\nimport streamlit as st\n\nDEFAULT_TERM_STR = (\n    \"Make a list of terms and definitions that are defined in the context, \"\n    \"with one pair on each line. \"\n    \"If a term is missing it's definition, use your best judgment. \"\n    \"Write each line as as follows:\\nTerm: <term> Definition: <definition>\"\n)\n\nst.title(\"\ud83e\udd99 Llama Index Term Extractor \ud83e\udd99\")\n\nsetup_tab, upload_tab = st.tabs([\"Setup\", \"Upload/Extract Terms\"])\n\nwith setup_tab:\n    st.subheader(\"LLM Setup\")\n    api_key = st.text_input(\"Enter your OpenAI API key here\", type=\"password\")\n    llm_name = st.selectbox('Which LLM?', [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-4\"])\n    model_temperature = st.slider(\"LLM Temperature\", min_value=0.0, max_value=1.0, step=0.1)\n    term_extract_str = st.text_area(\"The query to extract terms and definitions with.\", value=DEFAULT_TERM_STR)\n\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    document_text = st.text_area(\"Or enter raw text\")\n    if st.button(\"Extract Terms and Definitions\") and document_text:\n        with st.spinner(\"Extracting...\"):\n            extracted_terms = document text  # this is a placeholder!\n        st.write(extracted_terms)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/LLM Settings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow our app has two tabs, which really helps with the organization. You'll also noticed I added a default prompt to extract terms -- you can change this later once you try extracting some terms, it's just the prompt I arrived at after experimenting a bit.\n\nSpeaking of extracting terms, it's time to add some functions to do just that!\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Extracting and Storing Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow that we are able to define LLM settings and upload text, we can try using Llama Index to extract the terms from text for us!\n\nWe can add the following functions to both initialize our LLM, as well as use it to extract terms from the input text.", "start_char_idx": 644817, "end_char_idx": 649341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa": {"__data__": {"id_": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441", "node_type": "1", "metadata": {}, "hash": "8ba80fd34c4d6077da47a0dd44f97bb9e35b7441e2b96dda4986c048db106e01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47", "node_type": "1", "metadata": {}, "hash": "49defd404fba505f60b24efbb9033e99a730a0c06bf60ce5ff71441855e30fc1", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "55916261-7870-443d-8cbd-4b51467a77d6", "node_type": "1", "metadata": {}, "hash": "832af99c6255ffca1f7494dfe1a03194b4c301546a9dcedc3a0ced00d79db5fe", "class_name": "RelatedNodeInfo"}, {"node_id": "76ead195-2cbb-415d-8494-0bcb751662f0", "node_type": "1", "metadata": {}, "hash": "0b8340ad54712d5501d2b8c96bf935cd56aa21baddf7fd1dd33b5efacf7e9fa6", "class_name": "RelatedNodeInfo"}, {"node_id": "6ac06fb4-4158-4fa2-b621-af3966e83d18", "node_type": "1", "metadata": {}, "hash": "ffbafe46a741ae995be77c63257db2865862a47653043c84e4a38b37e9f4e92b", "class_name": "RelatedNodeInfo"}, {"node_id": "c9f6c430-aa2b-43f7-a063-8888fe5435b3", "node_type": "1", "metadata": {}, "hash": "40a91e2f74e267146128ca32c39300d6dfc3e30f4c177854347d1b99b1338f45", "class_name": "RelatedNodeInfo"}]}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Extracting and Storing Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Document, ListIndex, LLMPredictor, ServiceContext, load_index_from_storage\n\ndef get_llm(llm_name, model_temperature, api_key, max_tokens=256):\n    os.environ['OPENAI_API_KEY'] = api_key\n    if llm_name == \"text-davinci-003\":\n        return OpenAI(temperature=model_temperature, model_name=llm_name, max_tokens=max_tokens)\n    else:\n        return ChatOpenAI(temperature=model_temperature, model_name=llm_name, max_tokens=max_tokens)\n\ndef extract_terms(documents, term_extract_str, llm_name, model_temperature, api_key):\n    llm = get_llm(llm_name, model_temperature, api_key, max_tokens=1024)\n\n    service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm),\n                                                   chunk_size=1024)\n\n    temp_index = ListIndex.from_documents(documents, service_context=service_context)\n    query_engine = temp_index.as_query_engine(response_mode=\"tree_summarize\")\n    terms_definitions = str(query_engine.query(term_extract_str))\n    terms_definitions = [x for x in terms_definitions.split(\"\\n\") if x and 'Term:' in x and 'Definition:' in x]\n    # parse the text into a dict\n    terms_to_definition = {x.split(\"Definition:\")[0].split(\"Term:\")[-1].strip(): x.split(\"Definition:\")[-1].strip() for x in terms_definitions}\n    return terms_to_definition\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Extracting and Storing Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, using the new functions, we can finally extract our terms!\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Extracting and Storing Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    document_text = st.text_area(\"Or enter raw text\")\n    if st.button(\"Extract Terms and Definitions\") and document_text:\n        with st.spinner(\"Extracting...\"):\n            extracted_terms = extract_terms([Document(text=document_text)],\n                                            term_extract_str, llm_name,\n                                            model_temperature, api_key)\n        st.write(extracted_terms)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Extracting and Storing Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThere's a lot going on now, let's take a moment to go over what is happening.\n\n`get_llm()` is instantiating the LLM based on the user configuration from the setup tab. Based on the model name, we need to use the appropriate class (`OpenAI` vs. `ChatOpenAI`).\n\n`extract_terms()` is where all the good stuff happens. First, we call `get_llm()` with `max_tokens=1024`, since we don't want to limit the model too much when it is extracting our terms and definitions (the default is 256 if not set). Then, we define our `ServiceContext` object, aligning `num_output` with our `max_tokens` value, as well as setting the chunk size to be no larger than the output. When documents are indexed by Llama Index, they are broken into chunks (also called nodes) if they are large, and `chunk_size` sets the size for these chunks.\n\nNext, we create a temporary list index and pass in our service context. A list index will read every single piece of text in our index, which is perfect for extracting terms. Finally, we use our pre-defined query text to extract terms, using `response_mode=\"tree_summarize`. This response mode will generate a tree of summaries from the bottom up, where each parent summarizes its children. Finally, the top of the tree is returned, which will contain all our extracted terms and definitions.\n\nLastly, we do some minor post processing. We assume the model followed instructions and put a term/definition pair on each line. If a line is missing the `Term:` or `Definition:` labels, we skip it. Then, we convert this to a dictionary for easy storage!\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Saving Extracted Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow that we can extract terms, we need to put them somewhere so that we can query for them later. A `VectorStoreIndex` should be a perfect choice for now! But in addition, our app should also keep track of which terms are inserted into the index so that we can inspect them later. Using `st.session_state`, we can store the current list of terms in a session dict, unique to each user!\n\nFirst things first though, let's add a feature to initialize a global vector index and another function to insert the extracted terms.", "start_char_idx": 649343, "end_char_idx": 655558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5b23877-b4ba-41c2-9081-12725a1b5b47": {"__data__": {"id_": "c5b23877-b4ba-41c2-9081-12725a1b5b47", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "node_type": "1", "metadata": {}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201", "node_type": "1", "metadata": {}, "hash": "fd598698587a580a0391bf9cc9c05ce5d28d41129bf3d7ad454c42b7a8a9cf11", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "26917647-8356-409e-b1d2-f587e7ad7b88", "node_type": "1", "metadata": {}, "hash": "80c017d2c18b86e0d25937a58ca35edad55185351eb0f7a21dd4a37912e05258", "class_name": "RelatedNodeInfo"}, {"node_id": "2d135c7d-8eaa-478b-bdd1-ad2578ab3094", "node_type": "1", "metadata": {}, "hash": "f68f1ccfc523b8bac200e4e155065f005e11a23b1c6424910bb6c67198547cc7", "class_name": "RelatedNodeInfo"}, {"node_id": "6e5af7bb-9eff-44b7-a85a-279d750c3dc2", "node_type": "1", "metadata": {}, "hash": "d5395ade6f5a94df0aeadd66a22b1cebc4396c4cacd3a85ec4bc08bf2841eed6", "class_name": "RelatedNodeInfo"}]}, "hash": "49defd404fba505f60b24efbb9033e99a730a0c06bf60ce5ff71441855e30fc1", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Saving Extracted Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\nif 'all_terms' not in st.session_state:\n    st.session_state['all_terms'] = DEFAULT_TERMS\n...\n\ndef insert_terms(terms_to_definition):\n    for term, definition in terms_to_definition.items():\n        doc = Document(text=f\"Term: {term}\\nDefinition: {definition}\")\n        st.session_state['llama_index'].insert(doc)\n\n@st.cache_resource\ndef initialize_index(llm_name, model_temperature, api_key):\n    \"\"\"Create the VectorStoreIndex object.\"\"\"\n    llm = get_llm(llm_name, model_temperature, api_key)\n\n    service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n\n    index = VectorStoreIndex([], service_context=service_context)\n\n    return index\n\n...\n\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    if st.button(\"Initialize Index and Reset Terms\"):\n        st.session_state['llama_index'] = initialize_index(llm_name, model_temperature, api_key)\n        st.session_state['all_terms'] = {}\n\n    if \"llama_index\" in st.session_state:\n        st.markdown(\"Either upload an image/screenshot of a document, or enter the text manually.\")\n        document_text = st.text_area(\"Or enter raw text\")\n        if st.button(\"Extract Terms and Definitions\") and (uploaded_file or document_text):\n            st.session_state['terms'] = {}\n            terms_docs = {}\n            with st.spinner(\"Extracting...\"):\n                terms_docs.update(extract_terms([Document(text=document_text)], term_extract_str, llm_name, model_temperature, api_key))\n            st.session_state['terms'].update(terms_docs)\n\n        if \"terms\" in st.session_state and st.session_state[\"terms\"]::\n            st.markdown(\"Extracted terms\")\n            st.json(st.session_state['terms'])\n\n            if st.button(\"Insert terms?\"):\n                with st.spinner(\"Inserting terms\"):\n                    insert_terms(st.session_state['terms'])\n                st.session_state['all_terms'].update(st.session_state['terms'])\n                st.session_state['terms'] = {}\n                st.experimental_rerun()\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Saving Extracted Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow you are really starting to leverage the power of streamlit! Let's start with the code under the upload tab. We added a button to initialize the vector index, and we store it in the global streamlit state dictionary, as well as resetting the currently extracted terms. Then, after extracting terms from the input text, we store it the extracted terms in the global state again and give the user a chance to review them before inserting. If the insert button is pressed, then we call our insert terms function, update our global tracking of inserted terms, and remove the most recently extracted terms from the session state.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Querying for Extracted Terms/Definitions\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWith the terms and definitions extracted and saved, how can we use them? And how will the user even remember what's previously been saved?? We can simply add some more tabs to the app to handle these features.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Querying for Extracted Terms/Definitions\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\nsetup_tab, terms_tab, upload_tab, query_tab = st.tabs(\n    [\"Setup\", \"All Terms\", \"Upload/Extract Terms\", \"Query Terms\"]\n)\n...\nwith terms_tab:\n    with terms_tab:\n    st.subheader(\"Current Extracted Terms and Definitions\")\n    st.json(st.session_state[\"all_terms\"])\n...\nwith query_tab:\n    st.subheader(\"Query for Terms/Definitions!\")\n    st.markdown(\n        (\n            \"The LLM will attempt to answer your query, and augment it's answers using the terms/definitions you've inserted. \"\n            \"If a term is not in the index, it will answer using it's internal knowledge.\"\n        )\n    )\n    if st.button(\"Initialize Index and Reset Terms\", key=\"init_index_2\"):\n        st.session_state[\"llama_index\"] = initialize_index(\n            llm_name, model_temperature, api_key\n        )\n        st.session_state[\"all_terms\"] = {}\n\n    if \"llama_index\" in st.session_state:\n        query_text = st.text_input(\"Ask about a term or definition:\")\n        if query_text:\n            query_text = query_text + \"\\nIf you can't find the answer, answer the query with the best of your knowledge.\"", "start_char_idx": 655560, "end_char_idx": 661337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e87e40a-d75c-4bca-bf66-2cb20b1e1201": {"__data__": {"id_": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47", "node_type": "1", "metadata": {}, "hash": "49defd404fba505f60b24efbb9033e99a730a0c06bf60ce5ff71441855e30fc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "node_type": "1", "metadata": {}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a1b98ca6-819a-4536-81f2-b981441c17c1", "node_type": "1", "metadata": {}, "hash": "777c60a53b55932834eb4efdfb02f1371a49893f9de0fd3c8239bcd430c3ba95", "class_name": "RelatedNodeInfo"}, {"node_id": "24995949-e50d-41b4-80ed-8a7b44de0c9d", "node_type": "1", "metadata": {}, "hash": "e5253e4e6c6bcae38b8d49d777700a90eb75e26b7db0561a73f6b11861690b12", "class_name": "RelatedNodeInfo"}, {"node_id": "c8341126-de12-4e80-9eb5-b118eca3e5a4", "node_type": "1", "metadata": {}, "hash": "c3521d5953a0a69dae1745e32ab578d92da60e78e0eb773330e7bc9abf63dd38", "class_name": "RelatedNodeInfo"}]}, "hash": "fd598698587a580a0391bf9cc9c05ce5d28d41129bf3d7ad454c42b7a8a9cf11", "text": "with st.spinner(\"Generating answer...\"):\n                response = st.session_state[\"llama_index\"].query(\n                    query_text, similarity_top_k=5, response_mode=\"compact\"\n                )\n            st.markdown(str(response))\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Querying for Extracted Terms/Definitions\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWhile this is mostly basic, some important things to note:\n\n- Our initialize button has the same text as our other button. Streamlit will complain about this, so we provide a unique key instead.\n- Some additional text has been added to the query! This is to try and compensate for times when the index does not have the answer.\n- In our index query, we've specified two options:\n  - `similarity_top_k=5` means the index will fetch the top 5 closest matching terms/definitions to the query.\n  - `response_mode=\"compact\"` means as much text as possible from the 5 matching terms/definitions will be used in each LLM call. Without this, the index would make at least 5 calls to the LLM, which can slow things down for the user.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWell, actually I hope you've been testing as we went. But now, let's try one complete test.\n\n1. Refresh the app\n2. Enter your LLM settings\n3. Head over to the query tab\n4. Ask the following: `What is a bunnyhug?`\n5. The app should give some nonsense response. If you didn't know, a bunnyhug is another word for a hoodie, used by people from the Canadian Prairies!\n6. Let's add this definition to the app. Open the upload tab and enter the following text: `A bunnyhug is a common term used to describe a hoodie. This term is used by people from the Canadian Prairies.`\n7. Click the extract button. After a few moments, the app should display the correctly extracted term/definition. Click the insert term button to save it!\n8. If we open the terms tab, the term and definition we just extracted should be displayed\n9. Go back to the query tab and try asking what a bunnyhug is. Now, the answer should be correct!\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWith our base app working, it might feel like a lot of work to build up a useful index. What if we gave the user some kind of starting point to show off the app's query capabilities? We can do just that! First, let's make a small change to our app so that we save the index to disk after every upload:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndef insert_terms(terms_to_definition):\n    for term, definition in terms_to_definition.items():\n        doc = Document(text=f\"Term: {term}\\nDefinition: {definition}\")\n        st.session_state['llama_index'].insert(doc)\n    # TEMPORARY - save to disk\n    st.session_state['llama_index'].storage_context.persist()\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, we need some document to extract from! The repository for this project used the wikipedia page on New York City, and you can find the text here.\n\nIf you paste the text into the upload tab and run it (it may take some time), we can insert the extracted terms. Make sure to also copy the text for the extracted terms into a notepad or similar before inserting into the index! We will need them in a second.\n\nAfter inserting, remove the line of code we used to save the index to disk.", "start_char_idx": 661350, "end_char_idx": 666650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6efea28-e895-40cf-9ca9-21a5b32921bb": {"__data__": {"id_": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201", "node_type": "1", "metadata": {}, "hash": "fd598698587a580a0391bf9cc9c05ce5d28d41129bf3d7ad454c42b7a8a9cf11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7", "node_type": "1", "metadata": {}, "hash": "18ebaf1f87fd810a5d9741c3d51fd342383fc58d0e6e1108bffba8205383627e", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a0b19d2d-3f10-4e64-9d84-79b1e7539cf3", "node_type": "1", "metadata": {}, "hash": "7895101163952bb86ec5b2f5602e5c1c63a6c1955174767d7d54f98a836a307e", "class_name": "RelatedNodeInfo"}, {"node_id": "d530c033-2e49-4ed0-8be0-10fb13ac6abf", "node_type": "1", "metadata": {}, "hash": "82a5c348eac6fa43988a07e26cbabc7a6d6af015a37cfd859baf4fe9a0c18854", "class_name": "RelatedNodeInfo"}, {"node_id": "1fd96e80-2333-4431-9fd6-c05da53d4fd6", "node_type": "1", "metadata": {}, "hash": "299fdc802caabb2a8ca2829e4aa13df1c2939deb7fdc4cbc6574b952bbfe45e7", "class_name": "RelatedNodeInfo"}, {"node_id": "c0764f8c-3df3-4eb5-b37e-a0ac7708814c", "node_type": "1", "metadata": {}, "hash": "33b26bacf1a6879e0b9713b4cf56f9c88b67d0410246fdaa0c934f5c8594b6cc", "class_name": "RelatedNodeInfo"}]}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "text": "After inserting, remove the line of code we used to save the index to disk. With a starting index now saved, we can modify our `initialize_index` function to look like this:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@st.cache_resource\ndef initialize_index(llm_name, model_temperature, api_key):\n    \"\"\"Load the Index object.\"\"\"\n    llm = get_llm(llm_name, model_temperature, api_key)\n\n    service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n\n    index = load_index_from_storage(service_context=service_context)\n\n    return index\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDid you remember to save that giant list of extracted terms in a notepad? Now when our app initializes, we want to pass in the default terms that are in the index to our global terms state:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\nif \"all_terms\" not in st.session_state:\n    st.session_state[\"all_terms\"] = DEFAULT_TERMS\n...\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRepeat the above anywhere where we were previously resetting the `all_terms` values.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 2 - (Refining) Better Prompts\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you play around with the app a bit now, you might notice that it stopped following our prompt! Remember, we added to our `query_str` variable that if the term/definition could not be found, answer to the best of its knowledge. But now if you try asking about random terms (like bunnyhug!), it may or may not follow those instructions.\n\nThis is due to the concept of \"refining\" answers in Llama Index. Since we are querying across the top 5 matching results, sometimes all the results do not fit in a single prompt! OpenAI models typically have a max input size of 4097 tokens. So, Llama Index accounts for this by breaking up the matching results into chunks that will fit into the prompt. After Llama Index gets an initial answer from the first API call, it sends the next chunk to the API, along with the previous answer, and asks the model to refine that answer.\n\nSo, the refine process seems to be messing with our results! Rather than appending extra instructions to the `query_str`, remove that, and Llama Index will let us provide our own custom prompts! Let's create those now, using the default prompts and chat specific prompts as a guide. Using a new file `constants.py`, let's create some new query templates:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 2 - (Refining) Better Prompts\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\nfrom langchain.prompts.chat import (\n    AIMessagePromptTemplate,\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Text QA templates\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDEFAULT_TEXT_QA_PROMPT_TMPL = (\n    \"Context information is below.", "start_char_idx": 666575, "end_char_idx": 672351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30ff2ffa-0405-4b62-b988-ea7cd4226bd7": {"__data__": {"id_": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "node_type": "1", "metadata": {}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8307386e-c419-4780-bf97-9408b4967a9e", "node_type": "1", "metadata": {}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "862af2ce-1189-47b6-98a3-ddb8ae2e94d8", "node_type": "1", "metadata": {}, "hash": "18d47bb2cebcf2de83e7eeddc501521a04173a4286dadac27d348e93246c0111", "class_name": "RelatedNodeInfo"}, {"node_id": "fe5fcf31-cc29-4408-8f3f-6cdf1fabac33", "node_type": "1", "metadata": {}, "hash": "72d060dc8fe3fcd2c2bfc9877b86b07d5805f4d7a3c4386e5f5e618cfa908ece", "class_name": "RelatedNodeInfo"}, {"node_id": "294dca04-e918-4d16-81ce-784ed3a08e3f", "node_type": "1", "metadata": {}, "hash": "6c34e4e2e7874f9199159ab27c7ce5554c884a420b716fea7b983e32668217d8", "class_name": "RelatedNodeInfo"}]}, "hash": "18ebaf1f87fd810a5d9741c3d51fd342383fc58d0e6e1108bffba8205383627e", "text": "\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the context information answer the following question \"\n    \"(if you don't know the answer, use the best of your knowledge): {query_str}\\n\"\n)\nTEXT_QA_TEMPLATE = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Refine templates\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDEFAULT_REFINE_PROMPT_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context and using the best of your knowledge, improve the existing answer. \"\n    \"If you can't improve the existing answer, just repeat it again.\"\n)\nDEFAULT_REFINE_PROMPT = RefinePrompt(DEFAULT_REFINE_PROMPT_TMPL)\n\nCHAT_REFINE_PROMPT_TMPL_MSGS = [\n    HumanMessagePromptTemplate.from_template(\"{query_str}\"),\n    AIMessagePromptTemplate.from_template(\"{existing_answer}\"),\n    HumanMessagePromptTemplate.from_template(\n        \"We have the opportunity to refine the above answer \"\n        \"(only if needed) with some more context below.\\n\"\n        \"------------\\n\"\n        \"{context_msg}\\n\"\n        \"------------\\n\"\n        \"Given the new context and using the best of your knowledge, improve the existing answer. \"\n    \"If you can't improve the existing answer, just repeat it again.\"\n    ),\n]\n\nCHAT_REFINE_PROMPT_LC = ChatPromptTemplate.from_messages(CHAT_REFINE_PROMPT_TMPL_MSGS)\nCHAT_REFINE_PROMPT = RefinePrompt.from_langchain_prompt(CHAT_REFINE_PROMPT_LC)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/refine prompt selector\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDEFAULT_REFINE_PROMPT_SEL_LC = ConditionalPromptSelector(\n    default_prompt=DEFAULT_REFINE_PROMPT.get_langchain_prompt(),\n    conditionals=[(is_chat_model, CHAT_REFINE_PROMPT.get_langchain_prompt())],\n)\nREFINE_TEMPLATE = RefinePrompt(\n    langchain_prompt_selector=DEFAULT_REFINE_PROMPT_SEL_LC\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/refine prompt selector\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThat seems like a lot of code, but it's not too bad! If you looked at the default prompts, you might have noticed that there are default prompts, and prompts specific to chat models. Continuing that trend, we do the same for our custom prompts. Then, using a prompt selector, we can combine both prompts into a single object. If the LLM being used is a chat model (ChatGPT, GPT-4), then the chat prompts are used. Otherwise, use the normal prompt templates.\n\nAnother thing to note is that we only defined one QA template. In a chat model, this will be converted to a single \"human\" message.\n\nSo, now we can import these prompts into our app and use them during the query.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/refine prompt selector\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom constants import REFINE_TEMPLATE, TEXT_QA_TEMPLATE\n...\n    if \"llama_index\" in st.session_state:\n        query_text = st.text_input(\"Ask about a term or definition:\")\n        if query_text:\n            query_text = query_text  # Notice we removed the old instructions\n            with st.spinner(\"Generating answer...\"):\n                response = st.session_state[\"llama_index\"].query(\n                    query_text, similarity_top_k=5, response_mode=\"compact\",\n                    text_qa_template=TEXT_QA_TEMPLATE, refine_template=REFINE_TEMPLATE\n                )\n            st.markdown(str(response))\n...\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/refine prompt selector\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you experiment a bit more with queries, hopefully you notice that the responses follow our instructions a little better now!", "start_char_idx": 672352, "end_char_idx": 677969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8307386e-c419-4780-bf97-9408b4967a9e": {"__data__": {"id_": "8307386e-c419-4780-bf97-9408b4967a9e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7", "node_type": "1", "metadata": {}, "hash": "18ebaf1f87fd810a5d9741c3d51fd342383fc58d0e6e1108bffba8205383627e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b", "node_type": "1", "metadata": {}, "hash": "d6345df814f8c9f8800b9d5ef5385b81330418d3c20ae0a07ddcd5cd8252df77", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "9b7e4e7c-911c-4c48-a02f-cb33dac04468", "node_type": "1", "metadata": {}, "hash": "5899c104c946157d35d1ffb145da3f0c32f21a1af536feb179e5f2aff05b915b", "class_name": "RelatedNodeInfo"}, {"node_id": "ddcf50e2-c728-4522-bbb6-b18e29fb2a26", "node_type": "1", "metadata": {}, "hash": "41651a9a2abe4c2083069fca5c5c9c14df67e86b298f6c27b11d3c183445c092", "class_name": "RelatedNodeInfo"}, {"node_id": "50fd7008-631d-4845-b3da-5825d697de42", "node_type": "1", "metadata": {}, "hash": "f83d66e98296c6a2beeecc1d49f56dd429697892ec3df1321d120520bf926546", "class_name": "RelatedNodeInfo"}, {"node_id": "68ae0c0f-f71c-44eb-bbf1-a2795f3db639", "node_type": "1", "metadata": {}, "hash": "76b95f285c68c9df4f0a51ae33782fbeedcf320a689ee58beaf285d7d3fc92a3", "class_name": "RelatedNodeInfo"}]}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 3 - Image Support\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlama index also supports images! Using Llama Index, we can upload images of documents (papers, letters, etc.), and Llama Index handles extracting the text. We can leverage this to also allow users to upload images of their documents and extract terms and definitions from them.\n\nIf you get an import error about PIL, install it using `pip install Pillow` first.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 3 - Image Support\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom PIL import Image\nfrom llama_index.readers.file.base import DEFAULT_FILE_EXTRACTOR, ImageParser\n\n@st.cache_resource\ndef get_file_extractor():\n    image_parser = ImageParser(keep_image=True, parse_text=True)\n    file_extractor = DEFAULT_FILE_EXTRACTOR\n    file_extractor.update(\n        {\n            \".jpg\": image_parser,\n            \".png\": image_parser,\n            \".jpeg\": image_parser,\n        }\n    )\n\n    return file_extractor\n\nfile_extractor = get_file_extractor()\n...\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    if st.button(\"Initialize Index and Reset Terms\", key=\"init_index_1\"):\n        st.session_state[\"llama_index\"] = initialize_index(\n            llm_name, model_temperature, api_key\n        )\n        st.session_state[\"all_terms\"] = DEFAULT_TERMS\n\n    if \"llama_index\" in st.session_state:\n        st.markdown(\n            \"Either upload an image/screenshot of a document, or enter the text manually.\"\n        )\n        uploaded_file = st.file_uploader(\n            \"Upload an image/screenshot of a document:\", type=[\"png\", \"jpg\", \"jpeg\"]\n        )\n        document_text = st.text_area(\"Or enter raw text\")\n        if st.button(\"Extract Terms and Definitions\") and (\n            uploaded_file or document_text\n        ):\n            st.session_state[\"terms\"] = {}\n            terms_docs = {}\n            with st.spinner(\"Extracting (images may be slow)...\"):\n                if document_text:\n                    terms_docs.update(\n                        extract_terms(\n                            [Document(text=document_text)],\n                            term_extract_str,\n                            llm_name,\n                            model_temperature,\n                            api_key,\n                        )\n                    )\n                if uploaded_file:\n                    Image.open(uploaded_file).convert(\"RGB\").save(\"temp.png\")\n                    img_reader = SimpleDirectoryReader(\n                        input_files=[\"temp.png\"], file_extractor=file_extractor\n                    )\n                    img_docs = img_reader.load_data()\n                    os.remove(\"temp.png\")\n                    terms_docs.update(\n                        extract_terms(\n                            img_docs,\n                            term_extract_str,\n                            llm_name,\n                            model_temperature,\n                            api_key,\n                        )\n                    )\n            st.session_state[\"terms\"].update(terms_docs)\n\n        if \"terms\" in st.session_state and st.session_state[\"terms\"]:\n            st.markdown(\"Extracted terms\")\n            st.json(st.session_state[\"terms\"])\n\n            if st.button(\"Insert terms?\"):\n                with st.spinner(\"Inserting terms\"):\n                    insert_terms(st.session_state[\"terms\"])\n                st.session_state[\"all_terms\"].update(st.session_state[\"terms\"])\n                st.session_state[\"terms\"] = {}\n                st.experimental_rerun()\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 3 - Image Support\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere, we added the option to upload a file using Streamlit. Then the image is opened and saved to disk (this seems hacky but it keeps things simple). Then we pass the image path to the reader, extract the documents/text, and remove our temp image file.\n\nNow that we have the documents, we can call `extract_terms()` the same as before.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Conclusion/TLDR\nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn this tutorial, we covered a ton of information, while solving some common issues and problems along the way:\n\n- Using different indexes for different use cases (List vs. Vector index)\n- Storing global state values with Streamlit's `session_state` concept\n- Customizing internal prompts with Llama Index\n- Reading text from images with Llama Index\n\nThe final version of this tutorial can be found here and a live hosted demo is available on Huggingface Spaces.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex offers a variety of different use cases.\n\nFor simple queries, we may want to use a single index data structure, such as a `VectorStoreIndex` for semantic search, or `ListIndex` for summarization.\n\nFor more complex queries, we may want to use a composable graph.\n\nBut how do we integrate indexes and graphs into our LLM application? Different indexes and graphs may be better suited for different types of queries that you may want to run.", "start_char_idx": 677971, "end_char_idx": 684855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b": {"__data__": {"id_": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8307386e-c419-4780-bf97-9408b4967a9e", "node_type": "1", "metadata": {}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4", "node_type": "1", "metadata": {}, "hash": "af64eb5d1eef876fb19d2823481667aa718b452902b44283312ed84f276ce71e", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "eee0cf49-41ea-4605-93d2-6a985f5c870f", "node_type": "1", "metadata": {}, "hash": "854e8b3b3597e09ec7ad4ec961ea962840fa9da3511c2a423c85199f97b0cc63", "class_name": "RelatedNodeInfo"}, {"node_id": "863991c9-7dd3-4688-bfa4-374dcf558157", "node_type": "1", "metadata": {}, "hash": "a9c285e31d052ba406cd9c578b09a45dc15a14ae4af60dbe30333ceb3a73bced", "class_name": "RelatedNodeInfo"}, {"node_id": "9ea697f8-6d10-4ff1-946a-cd3cb406e694", "node_type": "1", "metadata": {}, "hash": "4c1f4d135a51e7d53c527150c0f27eb6ec269ab8083039d161a679e500dbd4ea", "class_name": "RelatedNodeInfo"}]}, "hash": "d6345df814f8c9f8800b9d5ef5385b81330418d3c20ae0a07ddcd5cd8252df77", "text": "Different indexes and graphs may be better suited for different types of queries that you may want to run.\n\nIn this guide, we show how you can unify the diverse use cases of different index/graph structures under a **single** query framework.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn this example, we will analyze Wikipedia articles of different cities: Boston, Seattle, San Francisco, and more.\n\nThe below code snippet downloads the relevant data into files.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom pathlib import Path\nimport requests\n\nwiki_titles = [\"Toronto\", \"Seattle\", \"Chicago\", \"Boston\", \"Houston\"]\n\nfor title in wiki_titles:\n    response = requests.get(\n        'https://en.wikipedia.org/w/api.php',\n        params={\n            'action': 'query',\n            'format': 'json',\n            'titles': title,\n            'prop': 'extracts',\n            # 'exintro': True,\n            'explaintext': True,\n        }\n    ).json()\n    page = next(iter(response['query']['pages'].values()))\n    wiki_text = page['extract']\n\n    data_path = Path('data')\n    if not data_path.exists():\n        Path.mkdir(data_path)\n\n    with open(data_path / f\"{title}.txt\", 'w') as fp:\n        fp.write(wiki_text)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe next snippet loads all files into Document objects.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Load all wiki documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncity_docs = {}\nfor wiki_title in wiki_titles:\n    city_docs[wiki_title] = SimpleDirectoryReader(input_files=[f\"data/{wiki_title}.txt\"]).load_data()\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining the Set of Indexes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe will now define a set of indexes and graphs over our data. You can think of each index/graph as a lightweight structure\nthat solves a distinct use case.\n\nWe will first define a vector index over the documents of each city.", "start_char_idx": 684749, "end_char_idx": 688384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4": {"__data__": {"id_": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b", "node_type": "1", "metadata": {}, "hash": "d6345df814f8c9f8800b9d5ef5385b81330418d3c20ae0a07ddcd5cd8252df77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6", "node_type": "1", "metadata": {}, "hash": "3e1c099691bb4e3bc479dee16f81cdf157d1b898a1e94ce0dd906a77ebfa0f20", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "99f07d67-47b4-478c-b87b-c7bbf90f7509", "node_type": "1", "metadata": {}, "hash": "8204abe877593024ae848a4e071958ef2a1b5adee65c6a65162fd8768b6676d1", "class_name": "RelatedNodeInfo"}, {"node_id": "0ecb87cb-eb82-4e30-b344-16d287c03ed7", "node_type": "1", "metadata": {}, "hash": "6269de3f86e589349139aab3e41e5180e54d60c99289756fd964b4ffc6c8a5da", "class_name": "RelatedNodeInfo"}, {"node_id": "65c62462-8add-4bb8-9690-655bdcbddaf6", "node_type": "1", "metadata": {}, "hash": "5bc654278c1be413157897c3e1bdbbdfd9f46acab41abb72a611e0c1c54c18f7", "class_name": "RelatedNodeInfo"}]}, "hash": "af64eb5d1eef876fb19d2823481667aa718b452902b44283312ed84f276ce71e", "text": "We will first define a vector index over the documents of each city.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining the Set of Indexes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, ServiceContext, StorageContext\nfrom langchain.llms.openai import OpenAIChat\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/set service context\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor_gpt4 = LLMPredictor(llm=OpenAIChat(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(\n    llm_predictor=llm_predictor_gpt4, chunk_size=1024\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Build city document index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_indices = {}\nfor wiki_title in wiki_titles:\n    storage_context = StorageContext.from_defaults()\n    # build vector index\n    vector_indices[wiki_title] = VectorStoreIndex.from_documents(\n        city_docs[wiki_title],\n        service_context=service_context,\n        storage_context=storage_context,\n    )\n    # set id for vector index\n    vector_indices[wiki_title].index_struct.index_id = wiki_title\n    # persist to disk\n    storage_context.persist(persist_dir=f'./storage/{wiki_title}')\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Build city document index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuerying a vector index lets us easily perform semantic search over a given city's documents.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Build city document index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = vector_indices[\"Toronto\"].as_query_engine().query(\"What are the sports teams in Toronto?\")\nprint(str(response))\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Build city document index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nExample response:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Build city document index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe sports teams in Toronto are the Toronto Maple Leafs (NHL), Toronto Blue Jays (MLB), Toronto Raptors (NBA), Toronto Argonauts (CFL), Toronto FC (MLS), Toronto Rock (NLL), Toronto Wolfpack (RFL), and Toronto Rush (NARL).\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining a Graph for Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe will now define a composed graph in order to run **compare/contrast** queries (see use cases doc).\nThis graph contains a keyword table composed on top of existing vector indexes.\n\nTo do this, we first want to set the \"summary text\" for each vector index.", "start_char_idx": 688316, "end_char_idx": 693346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7e94e2e-e638-416c-b26b-6f00d0deb2d6": {"__data__": {"id_": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4", "node_type": "1", "metadata": {}, "hash": "af64eb5d1eef876fb19d2823481667aa718b452902b44283312ed84f276ce71e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "467795da-1869-46ce-8473-5f97601af966", "node_type": "1", "metadata": {}, "hash": "1e99cc7284d3cf6afc40e4c2246e19690ff3bb1dd8bc82a074522bb998cc4403", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1d64353b-2c93-472b-af1a-029696461c4a", "node_type": "1", "metadata": {}, "hash": "6a6fc46145a60e671570c12929f2de4b34da56acadfb8cfebf900a2f20709838", "class_name": "RelatedNodeInfo"}, {"node_id": "6ecd87b1-2ce9-4ad4-8864-361467317d67", "node_type": "1", "metadata": {}, "hash": "bda2e5ce6076b6d2f2c23eca605a3d9e692a9c2c4202c69dc4cd9cac729eff40", "class_name": "RelatedNodeInfo"}, {"node_id": "0f0df44d-e555-49a8-9337-992e8f4cdd82", "node_type": "1", "metadata": {}, "hash": "b60926c12e02a0eea02ba30e506491327fe4379c19a4f073760dfc9a9fc18165", "class_name": "RelatedNodeInfo"}]}, "hash": "3e1c099691bb4e3bc479dee16f81cdf157d1b898a1e94ce0dd906a77ebfa0f20", "text": "To do this, we first want to set the \"summary text\" for each vector index.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining a Graph for Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_summaries = {}\nfor wiki_title in wiki_titles:\n    # set summary text for city\n    index_summaries[wiki_title] = (\n        f\"This content contains Wikipedia articles about {wiki_title}. \"\n        f\"Use this index if you need to lookup specific facts about {wiki_title}.\\n\"\n        \"Do not use this index if you want to analyze multiple cities.\"\n    )\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining a Graph for Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNext, we compose a keyword table on top of these vector indexes, with these indexes and summaries, in order to build the graph.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining a Graph for Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.composability import ComposableGraph\n\ngraph = ComposableGraph.from_indices(\n    SimpleKeywordTableIndex,\n    [index for _, index in vector_indices.items()],\n    [summary for _, summary in index_summaries.items()],\n    max_keywords_per_chunk=50\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/get root index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nroot_index = graph.get_index(graph.index_struct.root_id, SimpleKeywordTableIndex)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/set id of root index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nroot_index.set_index_id(\"compare_contrast\")\nroot_summary = (\n    \"This index contains Wikipedia articles about multiple cities. \"\n    \"Use this index if you want to compare multiple cities. \"\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/set id of root index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuerying this graph (with a query transform module), allows us to easily compare/contrast between different cities.\nAn example is shown below.", "start_char_idx": 693272, "end_char_idx": 697127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "467795da-1869-46ce-8473-5f97601af966": {"__data__": {"id_": "467795da-1869-46ce-8473-5f97601af966", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6", "node_type": "1", "metadata": {}, "hash": "3e1c099691bb4e3bc479dee16f81cdf157d1b898a1e94ce0dd906a77ebfa0f20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "node_type": "1", "metadata": {}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "06959c18-5d5a-436c-94c2-c825c9fd19e4", "node_type": "1", "metadata": {}, "hash": "7867182d15b72fc605ee32a6e1e6e94822df50f37c5f7116bbe01209ed66270c", "class_name": "RelatedNodeInfo"}, {"node_id": "073fab27-83ad-447d-a4dd-1a986579cccd", "node_type": "1", "metadata": {}, "hash": "77877e8fe682ce5e129bfc70dab3393cbd0847dd62ccb24e23b285524acf5f3c", "class_name": "RelatedNodeInfo"}, {"node_id": "943e09d5-0632-48fc-a3e8-d9f514189867", "node_type": "1", "metadata": {}, "hash": "f86fe4f2e85207dab3de82b6a4f7d7c246625a7e51390a766ecc35dd55ddb94d", "class_name": "RelatedNodeInfo"}]}, "hash": "1e99cc7284d3cf6afc40e4c2246e19690ff3bb1dd8bc82a074522bb998cc4403", "text": "An example is shown below.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/define decompose_transform\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor_chatgpt, verbose=True\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/define custom query engines\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\ncustom_query_engines = {}\nfor index in vector_indices.values():\n    query_engine = index.as_query_engine(service_context=service_context)\n    query_engine = TransformQueryEngine(\n        query_engine,\n        query_transform=decompose_transform,\n        transform_extra_info={'index_summary': index.index_struct.summary},\n    )\n    custom_query_engines[index.index_id] = query_engine\ncustom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n    retriever_mode='simple',\n    response_mode='tree_summarize',\n    service_context=service_context,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/define query engine\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/query the graph\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_str = (\n    \"Compare and contrast the arts and culture of Houston and Boston. \"\n)\nresponse_chatgpt = query_engine.query(query_str)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining the Unified Query Interface\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow that we've defined the set of indexes/graphs, we want to build an **outer abstraction** layer that provides a unified query interface\nto our data structures. This means that during query-time, we can query this outer abstraction layer and trust that the right index/graph\nwill be used for the job.\n\nThere are a few ways to do this, both within our framework as well as outside of it!\n\n- Build a **router query engine** on top of your existing indexes/graphs\n- Define each index/graph as a Tool within an agent framework (e.g. LangChain).\n\nFor the purposes of this tutorial, we follow the former approach. If you want to take a look at how the latter approach works,\ntake a look at our example tutorial here.\n\nLet's take a look at an example of building a router query engine to automatically \"route\" any query to the set of indexes/graphs that you have define under the hood.\n\nFirst, we define the query engines for the set of indexes/graph that we want to route our query to. We also give each a description (about what data it holds and what it's useful for) to help the router choose between them depending on the specific query.", "start_char_idx": 697101, "end_char_idx": 701432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4": {"__data__": {"id_": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "467795da-1869-46ce-8473-5f97601af966", "node_type": "1", "metadata": {}, "hash": "1e99cc7284d3cf6afc40e4c2246e19690ff3bb1dd8bc82a074522bb998cc4403", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57d82275-bdb8-4707-83cf-2f2442d7befa", "node_type": "1", "metadata": {}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "3a1645f5-42af-445d-a6f9-f9151e840905", "node_type": "1", "metadata": {}, "hash": "c34c27e91c578a30fb86dc91bb1c469a2f967e01900c45656c67c3a1950aa579", "class_name": "RelatedNodeInfo"}, {"node_id": "3532dee7-3488-402a-9323-c4ac4ab111f6", "node_type": "1", "metadata": {}, "hash": "16b749788442bc834ef66f0e68b596072fa407e2f49ea345dd6804e2505fdc30", "class_name": "RelatedNodeInfo"}, {"node_id": "71589527-1312-4862-bf76-ddf5f4067218", "node_type": "1", "metadata": {}, "hash": "af1932bc0b9e2d9027e6c49cff65ed117afc9996fdba96f51973904720085f43", "class_name": "RelatedNodeInfo"}, {"node_id": "94ce6c98-d7c0-42d5-b410-5f252c6e4add", "node_type": "1", "metadata": {}, "hash": "cb1c83e287f275759455cac5aeff5fd6db619d85ab82a09176f90c4bc56920dd", "class_name": "RelatedNodeInfo"}]}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining the Unified Query Interface\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.tools.query_engine import QueryEngineTool\n\nquery_engine_tools = []\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/add vector index tools\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfor wiki_title in wiki_titles:\n    index = vector_indices[wiki_title]\n    summary = index_summaries[wiki_title]\n\n    query_engine = index.as_query_engine(service_context=service_context)\n    vector_tool = QueryEngineTool.from_defaults(query_engine, description=summary)\n    query_engine_tools.append(vector_tool)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/add graph tool\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph_description = (\n    \"This tool contains Wikipedia articles about multiple cities. \"\n    \"Use this tool if you want to compare multiple cities. \"\n)\ngraph_tool = QueryEngineTool.from_defaults(graph_query_engine, description=graph_description)\nquery_engine_tools.append(graph_tool)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/add graph tool\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, we can define the routing logic and overall router query engine.\nHere, we use the `LLMSingleSelector`, which uses LLM to choose a underlying query engine to route the query to.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/add graph tool\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.query_engine.router_query_engine import RouterQueryEngine\nfrom llama_index.selectors.llm_selectors import LLMSingleSelector\n\n\nrouter_query_engine = RouterQueryEngine(\n    selector=LLMSingleSelector.from_defaults(service_context=service_context),\n    query_engine_tools=query_engine_tools\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Querying our Unified Interface\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe advantage of a unified query interface is that it can now handle different types of queries.\n\nIt can now handle queries about specific cities (by routing to the specific city vector index), and also compare/contrast different cities.\n\nLet's take a look at a few examples!\n\n**Asking a Compare/Contrast Question**\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/ask a compare/contrast question\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = router_query_engine.query(\n    \"Compare and contrast the arts and culture of Houston and Boston.\",\n)\nprint(str(response)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/ask a compare/contrast question\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Asking Questions about specific Cities**\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/ask a compare/contrast question\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = router_query_engine.query(\"What are the sports teams in Toronto?\")", "start_char_idx": 701434, "end_char_idx": 707022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57d82275-bdb8-4707-83cf-2f2442d7befa": {"__data__": {"id_": "57d82275-bdb8-4707-83cf-2f2442d7befa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "node_type": "1", "metadata": {}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6", "node_type": "1", "metadata": {}, "hash": "2ad03f3872044972ec87f7f571a78e0a5013b71cefa01ebc67058f53533959b8", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "775cfdcb-ec0a-400b-bdff-380c5cd33594", "node_type": "1", "metadata": {}, "hash": "17ec6cbb3203e567273501704e2afbca55f257102bfa90404a31a76008a3f744", "class_name": "RelatedNodeInfo"}, {"node_id": "1bd244f0-784b-441e-a312-5854ad193643", "node_type": "1", "metadata": {}, "hash": "e3232e6cc4ede0f08002234f02ce0077c1125a425fa5aff98c8ebbae81e46206", "class_name": "RelatedNodeInfo"}, {"node_id": "233df86f-00fe-437b-ae64-54905559b456", "node_type": "1", "metadata": {}, "hash": "93b03e42f3cb747c999b20df84d37ad4b750d74a023114644604b426801111ce", "class_name": "RelatedNodeInfo"}, {"node_id": "68f1cda2-f23d-4779-babd-9a366faa6269", "node_type": "1", "metadata": {}, "hash": "9f5221e99287f77cb5368bfa27f0665e099658d32b259fdf8435a00262af779f", "class_name": "RelatedNodeInfo"}]}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "text": "print(str(response))\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/ask a compare/contrast question\nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis \"outer\" abstraction is able to handle different queries by routing to the right underlying abstractions.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAt a high-level, LlamaIndex gives you the ability to query your data for any downstream LLM use case,\nwhether it's question-answering, summarization, or a component in a chatbot.\n\nThis section describes the different ways you can query your data with LlamaIndex, roughly in order\nof simplest (top-k semantic search), to more advanced capabilities.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Semantic Search\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe most basic example usage of LlamaIndex is through semantic search. We provide\na simple in-memory vector store for you to get started, but you can also choose\nto use any one of our vector store integrations:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Semantic Search\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Semantic Search\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Tutorials**\n- Starter Tutorial\n- Basic Usage Pattern\n\n**Guides**\n- Example (Notebook)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Summarization\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer.\nFor instance, a summarization query could look like one of the following: \n- \"What is a summary of this collection of text?\"\n- \"Give me a summary of person X's experience with the company.\"\n\nIn general, a list index would be suited for this use case. A list index by default goes through all the data.\n\nEmpirically, setting `response_mode=\"tree_summarize\"` also leads to better summarization results.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Summarization\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = ListIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(\n    response_mode=\"tree_summarize\"\n)\nresponse = query_engine.query(\"<summarization_query>\")\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Queries over Structured Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports queries over structured data, whether that's a Pandas DataFrame or a SQL Database.\n\nHere are some relevant resources:\n\n**Tutorials**\n\n- Guide on Text-to-SQL\n\n**Guides**\n- SQL Guide (Core) (Notebook)\n- Pandas Demo (Notebook)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Synthesis over Heterogeneous Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports synthesizing across heterogeneous data sources. This can be done by composing a graph over your existing data.\nSpecifically, compose a list index over your subindices. A list index inherently combines information for each node; therefore\nit can synthesize information across your heterogeneous data sources.", "start_char_idx": 707023, "end_char_idx": 712612, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fa1858f-bb91-4e38-8aca-8cead6f750f6": {"__data__": {"id_": "9fa1858f-bb91-4e38-8aca-8cead6f750f6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57d82275-bdb8-4707-83cf-2f2442d7befa", "node_type": "1", "metadata": {}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efa0c9b7-e632-419d-8e96-465fde932bcd", "node_type": "1", "metadata": {}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "61024bcd-7861-4906-a3fa-1a56a6bf9307", "node_type": "1", "metadata": {}, "hash": "c7472c842671eb160804939e6e8cf1f2b2323669df511fdab4d8e72d6ae0a348", "class_name": "RelatedNodeInfo"}, {"node_id": "14be06b7-297d-4ada-af94-28249b913c15", "node_type": "1", "metadata": {}, "hash": "907c8c841135a0fbf69392769beb2c7c2725df761bfedbb8aec3e2b852c6dbf8", "class_name": "RelatedNodeInfo"}, {"node_id": "54cc4e5b-f7e1-4202-b4a7-3beb688ea5c5", "node_type": "1", "metadata": {}, "hash": "9df7aaed7b6801b6421974870470e9041bf7d1907793f4bbe33c3d9c6a804232", "class_name": "RelatedNodeInfo"}]}, "hash": "2ad03f3872044972ec87f7f571a78e0a5013b71cefa01ebc67058f53533959b8", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Synthesis over Heterogeneous Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, ListIndex\nfrom llama_index.indices.composability import ComposableGraph\n\nindex1 = VectorStoreIndex.from_documents(notion_docs)\nindex2 = VectorStoreIndex.from_documents(slack_docs)\n\ngraph = ComposableGraph.from_indices(ListIndex, [index1, index2], index_summaries=[\"summary1\", \"summary2\"])\nquery_engine = graph.as_query_engine()\nresponse = query_engine.query(\"<query_str>\")\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Synthesis over Heterogeneous Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Guides**\n- Composability\n- City Analysis (Notebook)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Routing over Heterogeneous Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex also supports routing over heterogeneous data sources with `RouterQueryEngine` - for instance, if you want to \"route\" a query to an \nunderlying Document or a sub-index.\n\n\nTo do this, first build the sub-indices over different data sources.\nThen construct the corresponding query engines, and give each query engine a description to obtain a `QueryEngineTool`.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Routing over Heterogeneous Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import TreeIndex, VectorStoreIndex\nfrom llama_index.tools import QueryEngineTool\n\n...\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/define sub-indices\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex1 = VectorStoreIndex.from_documents(notion_docs)\nindex2 = VectorStoreIndex.from_documents(slack_docs)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/define query engines and tools\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntool1 = QueryEngineTool.from_defaults(\n    query_engine=index1.as_query_engine(), \n    description=\"Use this query engine to do...\",\n)\ntool2 = QueryEngineTool.from_defaults(\n    query_engine=index2.as_query_engine(), \n    description=\"Use this query engine for something else...\",\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/define query engines and tools\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen, we define a `RouterQueryEngine` over them.\nBy default, this uses a `LLMSingleSelector` as the router, which uses the LLM to choose the best sub-index to router the query to, given the descriptions.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/define query engines and tools\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.query_engine import RouterQueryEngine\n\nquery_engine = RouterQueryEngine.from_defaults(\n    query_engine_tools=[tool1, tool2]\n)\n\nresponse = query_engine.query(\n    \"In Notion, give me a summary of the product roadmap.\"", "start_char_idx": 712614, "end_char_idx": 717316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efa0c9b7-e632-419d-8e96-465fde932bcd": {"__data__": {"id_": "efa0c9b7-e632-419d-8e96-465fde932bcd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6", "node_type": "1", "metadata": {}, "hash": "2ad03f3872044972ec87f7f571a78e0a5013b71cefa01ebc67058f53533959b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baf924cc-b96b-46fd-84c1-79d69be105f3", "node_type": "1", "metadata": {}, "hash": "c7ead2bc19323b9b676ceda4679afdcf0217af2f97346b1989813e0e8701e363", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "33e31028-ed65-4bd1-a21d-aceb00b3b6cc", "node_type": "1", "metadata": {}, "hash": "b83290c0aa73ec1bbd30160d4459253efbcdb5122dd2fa4aedb77d3ffc59272c", "class_name": "RelatedNodeInfo"}, {"node_id": "7bf7eee9-2e8f-47e6-8212-df37ee35c911", "node_type": "1", "metadata": {}, "hash": "59e9f53e7aef6df28d7654db88212737b161318a776e6f39e0b53065602138ac", "class_name": "RelatedNodeInfo"}, {"node_id": "7fd0ebf0-ba68-481e-aa52-3df13b97d7bc", "node_type": "1", "metadata": {}, "hash": "a6ee9c9a7e23e7a2cead560f055185fc5d424672f68abe683b47e81bd52110c3", "class_name": "RelatedNodeInfo"}, {"node_id": "d7f0528f-a8e7-4da6-92ff-25942d99c739", "node_type": "1", "metadata": {}, "hash": "f67c7d60eb3bb0ed85f03ec6e7dc3539b8b62195e60b6240eeffddcca13f8e2e", "class_name": "RelatedNodeInfo"}]}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "text": ")\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/define query engines and tools\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Guides**\n- Router Query Engine Guide (Notebook)\n- City Analysis Unified Query Interface (Notebook)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can explicitly perform compare/contrast queries with a **query transformation** module within a ComposableGraph.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor_chatgpt, verbose=True\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis module will help break down a complex query into a simpler one over your existing index structure.\n\n**Guides**\n- Query Transformations\n- City Analysis Compare/Contrast Example (Notebook)\n\nYou can also rely on the LLM to *infer* whether to perform compare/contrast queries (see Multi-Document Queries below).\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Document Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBesides the explicit synthesis/routing flows described above, LlamaIndex can support more general multi-document queries as well. \nIt can do this through our `SubQuestionQueryEngine` class. Given a query, this query engine will generate a \"query plan\" containing\nsub-queries against sub-documents before synthesizing the final answer.\n\nTo do this, first define an index for each document/data source, and wrap it with a `QueryEngineTool` (similar to above):\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Document Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.tools import QueryEngineTool, ToolMetadata\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sept_engine, \n        metadata=ToolMetadata(name='sept_22', description='Provides information about Uber quarterly financials ending September 2022')\n    ),\n    QueryEngineTool(\n        query_engine=june_engine, \n        metadata=ToolMetadata(name='june_22', description='Provides information about Uber quarterly financials ending June 2022')\n    ),\n    QueryEngineTool(\n        query_engine=march_engine, \n        metadata=ToolMetadata(name='march_22', description='Provides information about Uber quarterly financials ending March 2022')\n    ),\n]\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Document Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen, we define a `SubQuestionQueryEngine` over these tools:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Document Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.query_engine import SubQuestionQueryEngine\n\nquery_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Document Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis query engine can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer.\nThis makes it especially well-suited for compare/contrast queries across documents as well as queries pertaining to a specific document.", "start_char_idx": 717317, "end_char_idx": 722885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baf924cc-b96b-46fd-84c1-79d69be105f3": {"__data__": {"id_": "baf924cc-b96b-46fd-84c1-79d69be105f3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efa0c9b7-e632-419d-8e96-465fde932bcd", "node_type": "1", "metadata": {}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "node_type": "1", "metadata": {}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "1489f6c5-e7b4-4b63-a335-da063a794271", "node_type": "1", "metadata": {}, "hash": "384a79c4b666018e367ef34f382dfeb9204fdd0e0fc83db521610f787f4f23f3", "class_name": "RelatedNodeInfo"}, {"node_id": "d68d6704-95bb-4b0c-8753-6602d33b20e3", "node_type": "1", "metadata": {}, "hash": "0cfdafd780daedc38008c6d4836bce05f196a25414e2d75f4a1c39d057102e2f", "class_name": "RelatedNodeInfo"}]}, "hash": "c7ead2bc19323b9b676ceda4679afdcf0217af2f97346b1989813e0e8701e363", "text": "**Guides**\n- Sub Question Query Engine (Intro)\n- 10Q Analysis (Uber)\n- 10K Analysis (Uber and Lyft)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Step Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can also support iterative multi-step queries. Given a complex query, break it down into an initial subquestions,\nand sequentially generate subquestions based on returned answers until the final answer is returned.\n\nFor instance, given a question \"Who was in the first batch of the accelerator program the author started?\",\nthe module will first decompose the query into a simpler initial question \"What was the accelerator program the author started?\",\nquery the index, and then ask followup questions.\n\n**Guides**\n- Query Transformations\n- Multi-Step Query Decomposition (Notebook)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Temporal Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can support queries that require an understanding of time. It can do this in two ways:\n- Decide whether the query requires utilizing temporal relationships between nodes (prev/next relationships) in order to retrieve additional context to answer the question.\n- Sort by recency and filter outdated context.\n\n**Guides**\n- Second-Stage Postprocessing Guide\n- Prev/Next Postprocessing\n- Recency Postprocessing\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Additional Resources\nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n- A Guide to Creating a Unified Query Framework over your ndexes\n- A Guide to Extracting Terms and Definitions\n- SEC 10k Analysis\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA lot of modern data systems depend on structured data, such as a Postgres DB or a Snowflake data warehouse.\nLlamaIndex provides a lot of advanced features, powered by LLM's, to both create structured data from\nunstructured data, as well as analyze this structured data through augmented text-to-SQL capabilities.\n\nThis guide helps walk through each of these capabilities. Specifically, we cover the following topics:\n- **Setup**: Defining up our example SQL Table.\n- **Building our Table Index**: How to go from sql database to a Table Schema Index\n- **Using natural language SQL queries**: How to query our SQL database using natural language.\n\nWe will walk through a toy example table which contains city/population/country information.\nA notebook for this tutorial is available here.", "start_char_idx": 722887, "end_char_idx": 726332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a3fee34-1975-4ed2-aa31-79f5dce3ba13": {"__data__": {"id_": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "baf924cc-b96b-46fd-84c1-79d69be105f3", "node_type": "1", "metadata": {}, "hash": "c7ead2bc19323b9b676ceda4679afdcf0217af2f97346b1989813e0e8701e363", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "node_type": "1", "metadata": {}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "4173ece5-0263-4f96-9601-b024f4520896", "node_type": "1", "metadata": {}, "hash": "235ba6ea2370be1a64e93359650bebd5ef52a128f2060f6a57d1a0e687eb93be", "class_name": "RelatedNodeInfo"}, {"node_id": "9944fcba-9019-4ba9-a3c5-574daf2cb5e3", "node_type": "1", "metadata": {}, "hash": "eb5f201de7fd6aafd1eca8be538edf7744b759ef216cf00636e6bab1eb9a095e", "class_name": "RelatedNodeInfo"}, {"node_id": "0e30dee4-2d8c-46d8-aec5-d1b76a642603", "node_type": "1", "metadata": {}, "hash": "9560c782a7dbd6026c087cfefa6f431004eb73775766389b8514a109e3cd8c6d", "class_name": "RelatedNodeInfo"}, {"node_id": "1aabb644-ea1d-4306-93c5-5bf6eb4611a9", "node_type": "1", "metadata": {}, "hash": "5c5b52c7d79744414c45b79d351ebe13ecaccc6840d3a202a25dd19a7e41d70a", "class_name": "RelatedNodeInfo"}]}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "text": "A notebook for this tutorial is available here.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFirst, we use SQLAlchemy to setup a simple sqlite db:\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n\nengine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData()\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe then create a toy `city_stats` table:\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/create city SQL table\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntable_name = \"city_stats\"\ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column(\"city_name\", String(16), primary_key=True),\n    Column(\"population\", Integer),\n    Column(\"country\", String(16), nullable=False),\n)\nmetadata_obj.create_all(engine)\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/create city SQL table\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow it's time to insert some datapoints!\n\nIf you want to look into filling into this table by inferring structured datapoints\nfrom unstructured data, take a look at the below section. Otherwise, you can choose\nto directly populate this table:\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/create city SQL table\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom sqlalchemy import insert\nrows = [\n    {\"city_name\": \"Toronto\", \"population\": 2731571, \"country\": \"Canada\"},\n    {\"city_name\": \"Tokyo\", \"population\": 13929286, \"country\": \"Japan\"},\n    {\"city_name\": \"Berlin\", \"population\": 600000, \"country\": \"Germany\"},\n]\nfor row in rows:\n    stmt = insert(city_stats_table).values(**row)\n    with engine.connect() as connection:\n        cursor = connection.execute(stmt)\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/create city SQL table\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFinally, we can wrap the SQLAlchemy engine with our SQLDatabase wrapper;\nthis allows the db to be used within LlamaIndex:\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/create city SQL table\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SQLDatabase\n\nsql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Natural language SQL\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOnce we have constructed our SQL database, we can use the NLSQLTableQueryEngine to\nconstruct natural language queries that are synthesized into SQL queries.\n\nNote that we need to specify the tables we want to use with this query engine.\nIf we don't the query engine will pull all the schema context, which could\noverflow the context window of the LLM.", "start_char_idx": 726285, "end_char_idx": 731456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2": {"__data__": {"id_": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "node_type": "1", "metadata": {}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "node_type": "1", "metadata": {}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a5aaada9-567d-4840-b2ef-5d4d18e90c54", "node_type": "1", "metadata": {}, "hash": "fe9fe0b67f2086a238b8d79345a1447abde1b9ce90155b04acea95b3ce13f222", "class_name": "RelatedNodeInfo"}, {"node_id": "372fe491-ee84-4139-bbd3-aeba672ceae2", "node_type": "1", "metadata": {}, "hash": "a385cf17507a9571e6f12094d6a8cfa3555e81dc8bb04b151715e5c3316e9f17", "class_name": "RelatedNodeInfo"}, {"node_id": "bf4e0f2a-c7a0-4209-9ddb-37dcbc926b81", "node_type": "1", "metadata": {}, "hash": "d993b4fa31b1a153803da83979e2f252d94a17fe881c5ed5247e72041e42602f", "class_name": "RelatedNodeInfo"}, {"node_id": "4cd1fe65-7376-4018-957c-b12a0add9cb2", "node_type": "1", "metadata": {}, "hash": "b32e60d2c31bf705cfea083a8efe4c24941f00c528c388304ad7fb638b15e1d3", "class_name": "RelatedNodeInfo"}]}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "text": "File Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Natural language SQL\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"city_stats\"],\n)\nquery_str = (\n    \"Which city has the highest population?\"\n)\nresponse = query_engine.query(query_str)\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Natural language SQL\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis query engine should used in any case where you can specify the tables you want\nto query over beforehand, or the total size of all the table schema plus the rest of\nthe prompt fits your context window.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Building our Table Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf we don't know ahead of time which table we would like to use, and the total size of\nthe table schema overflows your context window size, we should store the table schema \nin an index so that during query time we can retrieve the right schema.\n\nThe way we can do this is using the SQLTableNodeMapping object, which takes in a \nSQLDatabase and produces a Node object for each SQLTableSchema object passed \ninto the ObjectIndex constructor.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Building our Table Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntable_node_mapping = SQLTableNodeMapping(sql_database)\ntable_schema_objs = [(SQLTableSchema(table_name=\"city_stats\")), ...] # one SQLTableSchema for each table\n\nobj_index = ObjectIndex.from_objects(\n    table_schema_objs,\n    table_node_mapping,\n    VectorStoreIndex,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Building our Table Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere you can see we define our table_node_mapping, and a single SQLTableSchema with the\n\"city_stats\" table name. We pass these into the ObjectIndex constructor, along with the\nVectorStoreIndex class definition we want to use. This will give us a VectorStoreIndex where\neach Node contains table schema and other context information. You can also add any additional\ncontext information you'd like.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/manually set extra context text\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncity_stats_text = (\n    \"This table gives information regarding the population and country of a given city.\\n\"\n    \"The user will query with codewords, where 'foo' corresponds to population and 'bar'\"\n    \"corresponds to city.\"\n)\n\ntable_node_mapping = SQLTableNodeMapping(sql_database)\ntable_schema_objs = [(SQLTableSchema(table_name=\"city_stats\", context_str=city_stats_text))]\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Using natural language SQL queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOnce we have defined our table schema index obj_index, we can construct a SQLTableRetrieverQueryEngine\nby passing in our SQLDatabase, and a retriever constructed from our object index.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Using natural language SQL queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = SQLTableRetrieverQueryEngine(\n    sql_database, obj_index.as_retriever(similarity_top_k=1)\n)\nresponse = query_engine.query(\"Which city has the highest population?\")", "start_char_idx": 731458, "end_char_idx": 736796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe11e513-a41b-435b-a4c3-5ff192d6feb0": {"__data__": {"id_": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "node_type": "1", "metadata": {}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "node_type": "1", "metadata": {}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "6e492ad7-038f-4098-ad95-c572129140aa", "node_type": "1", "metadata": {}, "hash": "0529a6d7344af2a258d18dfe5a87c620c86f5ed2fb689de7f29d73487c0cb103", "class_name": "RelatedNodeInfo"}, {"node_id": "a401e806-2317-44b7-b540-f06cc7515fdd", "node_type": "1", "metadata": {}, "hash": "3ffa1f82e91ee6367f53cbf832e945939661489fca5eb689780e3c59dfa0afdb", "class_name": "RelatedNodeInfo"}, {"node_id": "856827b8-2ae2-4f33-bb4a-ff39be844c3a", "node_type": "1", "metadata": {}, "hash": "c9aa5f4a20747fa2989c5bbec04b9f0324aeedaaa4c810c1984bb1a31d21ef09", "class_name": "RelatedNodeInfo"}, {"node_id": "6bee4ff0-8395-4897-8cb5-7510719e6322", "node_type": "1", "metadata": {}, "hash": "19f2226138ad840a4aeb4891654572abca4f9c236d19d7d77abb8e25f9730c56", "class_name": "RelatedNodeInfo"}]}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "text": "print(response)\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Using natural language SQL queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow when we query the retriever query engine, it will retrieve the relevant table schema\nand synthesize a SQL query and a response from the results of that query.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Concluding Thoughts\nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis is it for now! We're constantly looking for ways to improve our structured data support.\nIf you have any questions let us know in our Discord.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data.md\nContent Type: text\nHeader Path: Structured Data\nfile_path: Docs\\end_to_end_tutorials\\structured_data.md\nfile_name: structured_data.md\nfile_type: None\nfile_size: 224\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRelevant Resources:\n- A Guide to LlamaIndex + Structured Data\n- Airbyte SQL Index Guide\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe general usage pattern of LlamaIndex is as follows:\n\n1. Load in documents (either manually, or through a data loader)\n2. Parse the Documents into Nodes\n3. Construct Index (from Nodes or Documents)\n4. [Optional, Advanced] Building indices on top of other indices\n5. Query the index\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/1. Load in Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe first step is to load in data. This data is represented in the form of `Document` objects.\nWe provide a variety of data loaders which will load in Documents\nthrough the `load_data` function, e.g.:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/1. Load in Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('./data').load_data()\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/1. Load in Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to construct documents manually. LlamaIndex exposes the `Document` struct.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/1. Load in Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Document\n\ntext_list = [text1, text2, ...]\ndocuments = [Document(text=t) for t in text_list]\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/1. Load in Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA Document represents a lightweight container around the data source. You can now choose to proceed with one of the\nfollowing steps:\n\n1. Feed the Document object directly into the index (see section 3).\n2. First convert the Document into Node objects (see section 2).\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/2. Parse the Documents into Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe next step is to parse these Document objects into Node objects. Nodes represent \"chunks\" of source Documents,\nwhether that is a text chunk, an image, or more. They also contain metadata and relationship information\nwith other nodes and index structures.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes.", "start_char_idx": 736797, "end_char_idx": 742187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25e4fcbf-2304-4368-a573-0dd04a2fdc9e": {"__data__": {"id_": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "node_type": "1", "metadata": {}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "node_type": "1", "metadata": {}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2c266b44-9983-4b8f-9f3b-5cbaaceaa6b5", "node_type": "1", "metadata": {}, "hash": "c0337d223b957d2b7062b1e383e66d1b22550c6dddbd042e243647184a2e3223", "class_name": "RelatedNodeInfo"}, {"node_id": "cec635fa-2615-442b-b2c6-c429087ea181", "node_type": "1", "metadata": {}, "hash": "ccc28a8cc89f0c8182ac8e55cd0c488b4a08bf528d09d037933e4d3f86ed97ba", "class_name": "RelatedNodeInfo"}, {"node_id": "50b69303-cb23-449a-8035-3f3052ba3c58", "node_type": "1", "metadata": {}, "hash": "8e393148857de11224c932ddea8ce6cb75b947a6ccdfdf76c957d90ae8319a68", "class_name": "RelatedNodeInfo"}, {"node_id": "b7d815f1-6f92-4100-b2e6-076e82f1bcf3", "node_type": "1", "metadata": {}, "hash": "7bb4888b383063542cc48b5995c575a5f030635636ae4fdb34fb7c149902c808", "class_name": "RelatedNodeInfo"}]}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "text": "You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes.\n\nFor instance, you can do\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/2. Parse the Documents into Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser import SimpleNodeParser\n\nparser = SimpleNodeParser()\n\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/2. Parse the Documents into Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to construct Node objects manually and skip the first section. For instance,\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/2. Parse the Documents into Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n\nnode1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\nnode2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/set relationships\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnode1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id)\nnode2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id)\nnodes = [node1, node2]\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/set relationships\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `RelatedNodeInfo` class can also store additional `metadata` if needed:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/set relationships\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnode2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=node1.node_id, metadata={\"key\": \"val\"})\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can now build an index over these Document objects. The simplest high-level abstraction is to load-in the Document objects during index initialization (this is relevant if you came directly from step 1 and skipped step 2).\n\n`from_documents` also takes an optional argument `show_progress`. Set it to `True` to display a progress bar during index construction.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to build an index over a set of Node objects directly (this is a continuation of step 2).\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex(nodes)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDepending on which index you use, LlamaIndex may make LLM calls in order to build the index.", "start_char_idx": 742095, "end_char_idx": 747520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccef981f-9e59-48d8-bb73-711d3aabdfe1": {"__data__": {"id_": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "node_type": "1", "metadata": {}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c691987-9616-4fb7-89a8-83dfc8587080", "node_type": "1", "metadata": {}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "ec709318-af84-4a34-9912-b83cf1ffda69", "node_type": "1", "metadata": {}, "hash": "091dfb4dddc38035492d4c52c829a3268effc68aa1c1638e5296eb20c2efef1c", "class_name": "RelatedNodeInfo"}, {"node_id": "cdcb393b-f093-494d-8dd8-6d709f7f5457", "node_type": "1", "metadata": {}, "hash": "e462ba1deba7ab6f88e642edd49d86e9ecf20401b89e5cf52184e0548f8f709c", "class_name": "RelatedNodeInfo"}, {"node_id": "79c66cc3-606d-4dfd-b2c1-9b28af6e3dd7", "node_type": "1", "metadata": {}, "hash": "d3d52b6f51abb242e35a00157d0c9c143fbb32142437956585af38e09495e834", "class_name": "RelatedNodeInfo"}, {"node_id": "8d68c899-e84c-43ac-b263-e57a81b96d46", "node_type": "1", "metadata": {}, "hash": "bcaa8a6a28a8b09860862fe39b0f3ff66de0c9ff0d68ffb9527cfa75c004aff3", "class_name": "RelatedNodeInfo"}]}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "text": "File Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Reusing Nodes across Index Structures\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you have multiple Node objects defined, and wish to share these Node\nobjects across multiple index structures, you can do that.\nSimply instantiate a StorageContext object,\nadd the Node objects to the underlying DocumentStore,\nand pass the StorageContext around.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Reusing Nodes across Index Structures\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import StorageContext\n\nstorage_context = StorageContext.from_defaults()\nstorage_context.docstore.add_documents(nodes)\n\nindex1 = VectorStoreIndex(nodes, storage_context=storage_context)\nindex2 = ListIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Reusing Nodes across Index Structures\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**NOTE**: If the `storage_context` argument isn't specified, then it is implicitly\ncreated for each index during index construction. You can access the docstore\nassociated with a given index through `index.storage_context`.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Inserting Documents or Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also take advantage of the `insert` capability of indices to insert Document objects\none at a time instead of during index construction.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Inserting Documents or Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex([])\nfor doc in documents:\n    index.insert(doc)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Inserting Documents or Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you want to insert nodes on directly you can use `insert_nodes` function\ninstead.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Inserting Documents or Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/nodes: Sequence[Node]\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex([])\nindex.insert_nodes(nodes)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/nodes: Sequence[Node]\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee the Document Management How-To for more details on managing documents and an example notebook.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Customizing Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWhen creating documents, you can also attach useful metadata. Any metadata added to a document will be copied to the nodes that get created from their respective source document.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 747522, "end_char_idx": 752805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c691987-9616-4fb7-89a8-83dfc8587080": {"__data__": {"id_": "5c691987-9616-4fb7-89a8-83dfc8587080", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "node_type": "1", "metadata": {}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "171bb256-7674-4c94-bfe7-2609e57291d1", "node_type": "1", "metadata": {}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "5b1cb693-9230-4c52-a786-35b978e4da8f", "node_type": "1", "metadata": {}, "hash": "c35cb847b1c0d684b2b2a66b0778b9f005125ac5c585bd48d6ab9f206c709dbb", "class_name": "RelatedNodeInfo"}, {"node_id": "fd8bb99a-8f6f-4474-9d17-f8c4db6e89db", "node_type": "1", "metadata": {}, "hash": "588442eae32ebadac2a4488d7ebbb3f04743e16730731767f3fc5d1beb83ec78", "class_name": "RelatedNodeInfo"}, {"node_id": "9b159eb3-6f7b-47bd-9b1d-180e8743ec8f", "node_type": "1", "metadata": {}, "hash": "02f9e0bd9af588d1c22ebb50edb8d49fd18f175e1212b081a6e124062257d46d", "class_name": "RelatedNodeInfo"}, {"node_id": "f1550131-2086-43ce-832e-f350dfc70b2b", "node_type": "1", "metadata": {}, "hash": "22583802b3f983896b9d77df439574c8047366ba5ed1c802ba39a36657716f9c", "class_name": "RelatedNodeInfo"}]}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "text": "Index Construction/Customizing Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument = Document(\n    text='text',\n    metadata={\n        'filename': '<doc_file_name>',\n        'category': '<category>'\n    }\n)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Customizing Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMore information and approaches to this are discussed in the section Customizing Documents.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Customizing LLM's\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, we use OpenAI's `text-davinci-003` model. You may choose to use another LLM when constructing\nan index.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Customizing LLM's\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import LLMPredictor, VectorStoreIndex, ServiceContext\nfrom langchain import OpenAI\n\n...\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/define LLM\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/configure service context\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/build index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/build index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee the Custom LLM's How-To for more details.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Global ServiceContext\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you wanted the service context from the last section to always be the default, you can configure one like so:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Global ServiceContext\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import set_global_service_context\nset_global_service_context(service_context)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Global ServiceContext\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis service context will always be used as the default if not specified as a keyword argument in LlamaIndex functions.\n\nFor more details on the service context, including how to create a global service context, see the page Customizing the ServiceContext.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 752806, "end_char_idx": 757973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "171bb256-7674-4c94-bfe7-2609e57291d1": {"__data__": {"id_": "171bb256-7674-4c94-bfe7-2609e57291d1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c691987-9616-4fb7-89a8-83dfc8587080", "node_type": "1", "metadata": {}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "node_type": "1", "metadata": {}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "7bbfdb31-657f-48e6-a55b-060470c3748b", "node_type": "1", "metadata": {}, "hash": "ff2f0cd64fe0da5f278410aa16122d304358c6392374e6a02dd3717239725aba", "class_name": "RelatedNodeInfo"}, {"node_id": "89632ac4-256c-4965-bb79-9757bf15c38e", "node_type": "1", "metadata": {}, "hash": "41a075895de8349027278922d56f3158d092dc9b68db84caa98787f791c200f7", "class_name": "RelatedNodeInfo"}, {"node_id": "a592c58d-7ca7-480e-80bd-65288a4c5d5c", "node_type": "1", "metadata": {}, "hash": "a552534d4ebe66504035d6badc622b3269ff2f37ce918de800ce18166d29fff8", "class_name": "RelatedNodeInfo"}, {"node_id": "2e38e707-5f98-47f9-8659-1ea63e7634b6", "node_type": "1", "metadata": {}, "hash": "73fe69a6a3e8d87c67b5515e47ae53f88da258b34c60e8e693e350022e833831", "class_name": "RelatedNodeInfo"}]}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "text": "Index Construction/Customizing Prompts\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDepending on the index used, we used default prompt templates for constructing the index (and also insertion/querying).\nSee Custom Prompts How-To for more details on how to customize your prompt.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Customizing embeddings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor embedding-based indices, you can choose to pass in a custom embedding model. See\nCustom Embeddings How-To for more details.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Cost Analysis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCreating an index, inserting to an index, and querying an index may use tokens. We can track\ntoken usage through the outputs of these operations. When running operations,\nthe token usage will be printed.\n\nYou can also fetch the token usage through `index.llm_predictor.last_token_usage`.\nSee Cost Analysis How-To for more details.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/[Optional] Save the index for future use\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, data is stored in-memory.\nTo persist to disk:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/[Optional] Save the index for future use\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/[Optional] Save the index for future use\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou may omit persist_dir to persist to `./storage` by default.\n\nTo reload from disk:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/[Optional] Save the index for future use\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import StorageContext, load_index_from_storage\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/rebuild storage context\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/load index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = load_index_from_storage(storage_context)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/load index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**NOTE**: If you had initialized the index with a custom\n`ServiceContext` object, you will also need to pass in the same\nServiceContext during `load_index_from_storage`.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/load index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 757974, "end_char_idx": 763294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7342149e-ef54-4ae0-ab55-aa6c2bec14a0": {"__data__": {"id_": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "171bb256-7674-4c94-bfe7-2609e57291d1", "node_type": "1", "metadata": {}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "node_type": "1", "metadata": {}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "9553bf60-44e2-4b30-b0f3-01015787894b", "node_type": "1", "metadata": {}, "hash": "6658e6b4055229c954a6fcba43e8177945560d136bbefc5deb63575aec2c4361", "class_name": "RelatedNodeInfo"}, {"node_id": "3e771a02-63d1-41fc-a448-0007671e9cae", "node_type": "1", "metadata": {}, "hash": "27d0e405c129d38cd5559803e0763619deafec62fd5d5e40858bc74d98492bd9", "class_name": "RelatedNodeInfo"}, {"node_id": "881abcf2-7975-4eaf-b94d-d555990138ba", "node_type": "1", "metadata": {}, "hash": "e5a7d00bab4beeb4104e0e6a72f16fe4ac9475107a1f8e23988d53ee273ecdf6", "class_name": "RelatedNodeInfo"}, {"node_id": "da0e4cec-c02d-4b47-8b9e-e8cacd7eda8d", "node_type": "1", "metadata": {}, "hash": "a409eec7424f11201bf113bab1cc34cb078bf496c108e38e89e482dc5ed63a63", "class_name": "RelatedNodeInfo"}]}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "text": "Index Construction/when first building the index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)\n\n...\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/when loading the index from disk\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = load_index_from_storage(\n    service_context=service_context,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/4. [Optional, Advanced] Building indices on top of other indices\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can build indices on top of other indices!\nComposability gives you greater power in indexing your heterogeneous sources of data. For a discussion on relevant use cases,\nsee our Query Use Cases. For technical details and examples, see our Composability How-To.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/5. Query the index.\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAfter building the index, you can now query it with a `QueryEngine`. Note that a \"query\" is simply an input to an LLM -\nthis means that you can use the index for question-answering, but you can also do more than that!\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/High-level API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo start, you can query an index with the default `QueryEngine` (i.e., using default configs), as follows:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/High-level API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nresponse = query_engine.query(\"Write an email to the user given their background information.\")\nprint(response)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Low-level API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe also support a low-level composition API that gives you more granular control over the query logic.\nBelow we highlight a few of the possible customizations.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Low-level API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.retrievers import VectorIndexRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/build index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/configure retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=2,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 763295, "end_char_idx": 768486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b38af219-da17-45df-a80a-fe8ae2a44c6f": {"__data__": {"id_": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "node_type": "1", "metadata": {}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "node_type": "1", "metadata": {}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "a8ff3da0-6f08-4913-af29-ae21347f7cc7", "node_type": "1", "metadata": {}, "hash": "2f4345fe18a98986f8cd1d95fb052be2341c7ea9f079b0c304a5d58c308392f4", "class_name": "RelatedNodeInfo"}, {"node_id": "4aaae6fe-08c6-44db-9fbc-aff16d3a2710", "node_type": "1", "metadata": {}, "hash": "fb7097a2b2dd8acc96713c4c7380305d037c8e5467d00619d83ce0d20a3698bb", "class_name": "RelatedNodeInfo"}, {"node_id": "ad0981f2-0977-4a76-bc37-1e4b826af027", "node_type": "1", "metadata": {}, "hash": "94bfb40ca8ffa7a7e8e5af8711a5336cb7f4999d6c915fdb318bf6a25b56ba3b", "class_name": "RelatedNodeInfo"}, {"node_id": "1b6805b3-e806-46f4-8161-8a56124dd83b", "node_type": "1", "metadata": {}, "hash": "5751d89852790717f6c78313ac28a54947704baae180a203eafbcd836288e58d", "class_name": "RelatedNodeInfo"}]}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "text": "Index Construction/configure response synthesizer\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse_synthesizer = get_response_synthesizer()\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/assemble query engine\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff=0.7)\n    ]\n\n)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/query\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/query\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou may also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.\n\nFor a full list of implemented components and the supported configurations, please see the detailed reference docs.\n\nIn the following, we discuss some commonly used configurations in detail.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn index can have a variety of index-specific retrieval modes.\nFor instance, a list index supports the default `ListIndexRetriever` that retrieves all nodes, and\n`ListIndexEmbeddingRetriever` that retrieves the top-k nodes by embedding similarity.\n\nFor convienience, you can also use the following shorthand:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n# ListIndexRetriever\n    retriever = index.as_retriever(retriever_mode='default')\n    # ListIndexEmbeddingRetriever\n    retriever = index.as_retriever(retriever_mode='embedding')\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAfter choosing your desired retriever, you can construct your query engine:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine(retriever)\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe full list of retrievers for each index (and their shorthand) is documented in the Query Reference.\n\n(setting-response-mode)=\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring response synthesis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAfter a retriever fetches relevant nodes, a `BaseSynthesizer` synthesizes the final response by combining the information.\n\nYou can configure it via\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 768487, "end_char_idx": 773712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d2789b5-4e82-42b9-9a53-33bc52a366b5": {"__data__": {"id_": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "node_type": "1", "metadata": {}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "node_type": "1", "metadata": {}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "f1309865-2516-42cb-b374-9811c3cb803f", "node_type": "1", "metadata": {}, "hash": "7bf86fb0bc4dea30130bbfc91086bdce25b94ef46da03b1ae131a75b5d2e7662", "class_name": "RelatedNodeInfo"}, {"node_id": "d26219ad-4f57-48b4-a730-1fefa3530c91", "node_type": "1", "metadata": {}, "hash": "610769980b9237744200fdc6f1e7ad81eae41b7e0674bb58833f288abd3773ce", "class_name": "RelatedNodeInfo"}, {"node_id": "d80aaf95-855c-4fd3-94f1-e90606cd8f27", "node_type": "1", "metadata": {}, "hash": "15a8e78581c83a5aa137eb32da9ed644846e534c639eff86d4c180680d1c762e", "class_name": "RelatedNodeInfo"}, {"node_id": "3da92898-d25d-4a1f-8c47-d2c71b3db512", "node_type": "1", "metadata": {}, "hash": "8f45d8e14c8ff2fac17a64af98b3fea357b04122582af7386f882152c8e37a4a", "class_name": "RelatedNodeInfo"}]}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "text": "Index Construction/Configuring response synthesis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode=<response_mode>)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring response synthesis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRight now, we support the following options:\n\n- `default`: \"create and refine\" an answer by sequentially going through each retrieved `Node`;\n  This makes a separate LLM call per Node. Good for more detailed answers.\n- `compact`: \"compact\" the prompt during each LLM call by stuffing as\n  many `Node` text chunks that can fit within the maximum prompt size. If there are\n  too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n  multiple prompts.\n- `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree\n  and return the root node as the response. Good for summarization purposes.\n- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,\n  without actually sending them. Then can be inspected by checking `response.source_nodes`.\n  The response object is covered in more detail in Section 5.\n- `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text\n  chunk while accumulating the responses into an array. Returns a concatenated string of all\n  responses. Good for when you need to run the same query separately against each text\n  chunk.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring response synthesis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = ListIndex.from_documents(documents)\nretriever = index.as_retriever()\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/default\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='default')\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/compact\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='compact')\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/tree summarize\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='tree_summarize')\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/no text\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='no_text')\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring node postprocessors (i.e. filtering and augmentation)\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe also support advanced `Node` filtering and augmentation that can further improve the relevancy of the retrieved `Node` objects.\nThis can help reduce the time/number of LLM calls/cost or improve response quality.\n\nFor example:\n\n- `KeywordNodePostprocessor`: filters nodes by `required_keywords` and `exclude_keywords`.\n- `SimilarityPostprocessor`: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers)\n- `PrevNextNodePostprocessor`: augments retrieved `Node` objects with additional relevant context based on `Node` relationships.\n\nThe full list of node postprocessors is documented in the Node Postprocessor Reference.\n\nTo configure the desired node postprocessors:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring node postprocessors (i.e.", "start_char_idx": 773713, "end_char_idx": 779380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "767c4308-525a-46c7-b3dd-1c7a3a792ad9": {"__data__": {"id_": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "node_type": "1", "metadata": {}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53b92866-0f8e-48d1-bb79-a2d943008412", "node_type": "1", "metadata": {}, "hash": "76a39cd8d7738e591c73d577b1d61a76c9ad29cf7077dec16ea41a05f6d2ad35", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "5c6787bb-f3de-4152-b236-e5adb248eef5", "node_type": "1", "metadata": {}, "hash": "ac63e5ba7bfa1642c09ba30f1d2802d4db035db136f8f3b486a067042827de8f", "class_name": "RelatedNodeInfo"}, {"node_id": "702a4dc3-2218-45fb-a918-ae81910bb597", "node_type": "1", "metadata": {}, "hash": "b689429ca991e0f06ce1d34d3fa4fe7a8de3a8c04552652c23e6bed25cd5f457", "class_name": "RelatedNodeInfo"}, {"node_id": "fbfa05e3-53b0-4be3-bf05-9c83f8471bb0", "node_type": "1", "metadata": {}, "hash": "1f6a53dec9c1bcc60d09370fce7336cfe78810ded0b7ee3f63a0fd14e46fbf31", "class_name": "RelatedNodeInfo"}, {"node_id": "5ba6c70f-6801-497a-bc71-79b34146ac45", "node_type": "1", "metadata": {}, "hash": "c1c1a71a8a7bb69bad7604f30213511db0803d5661fd94e961b1182e20eecba4", "class_name": "RelatedNodeInfo"}]}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "text": "Index Construction/Configuring node postprocessors (i.e. filtering and augmentation)\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnode_postprocessors = [\n    KeywordNodePostprocessor(\n        required_keywords=[\"Combinator\"],\n        exclude_keywords=[\"Italy\"]\n    )\n]\nquery_engine = RetrieverQueryEngine.from_args(\n    retriever, node_postprocessors=node_postprocessors\n)\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/5. Parsing the response\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe object returned is a `Response` object.\nThe object contains both the response text as well as the \"sources\" of the response:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/5. Parsing the response\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = query_engine.query(\"<query_str>\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/response.response\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstr(response)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/get sources\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse.source_nodes\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/formatted sources\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse.get_formatted_sources()\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/formatted sources\nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn example is shown below.\n!\n\nFile Name: Docs\\end_to_end_tutorials\\use_cases.md\nContent Type: code\nHeader Path: Use Cases\nfile_path: Docs\\end_to_end_tutorials\\use_cases.md\nfile_name: use_cases.md\nfile_type: None\nfile_size: 286\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/end_to_end_tutorials/question_and_answer.md\n/end_to_end_tutorials/chatbots.md\n/end_to_end_tutorials/agents.md\n/end_to_end_tutorials/structured_data.md\n/end_to_end_tutorials/apps.md\n/end_to_end_tutorials/privacy.md\n```\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: code\nHeader Path: High-Level Concepts\nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nIf you haven't, install and complete starter tutorial before you read this. It will make a lot more sense!\n```\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex helps you build LLM-powered applications (e.g. Q&A, chatbot, and agents) over custom data.\n\nIn this high-level concepts guide, you will learn:\n* the retrieval augmented generation (RAG) paradigm for combining LLM with custom data,\n* key concepts and modules in LlamaIndex for composing your own RAG pipeline.\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRetrieval augmented generation (RAG) is a paradigm for augmenting LLM with custom data.\nIt generally consists of two stages: \n1) **indexing stage**: preparing a knowledge base, and\n2) **querying stage**: retrieving relevant context from the knowledge to assist the LLM in responding to a question\n\n!", "start_char_idx": 779324, "end_char_idx": 784542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53b92866-0f8e-48d1-bb79-a2d943008412": {"__data__": {"id_": "53b92866-0f8e-48d1-bb79-a2d943008412", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "node_type": "1", "metadata": {}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9206bf00-08cf-4199-9276-96ef119a8e61", "node_type": "1", "metadata": {}, "hash": "741492fa2ea72962132f7b8aea6ab7e5dd917c0ca02d513b93a78277ee28fd82", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "2fea4afe-a138-4675-92c5-f6415ca97313", "node_type": "1", "metadata": {}, "hash": "ab5c68d6620d09e659234d1d36e3f4d83bb1e06ffdc3d74e845800f6b36d7484", "class_name": "RelatedNodeInfo"}, {"node_id": "9a0d5a48-9818-4626-bb90-e6d0fbc3ac7d", "node_type": "1", "metadata": {}, "hash": "e1f3566396c24264a86417c6f1554804b6274be2c092f92b05799daed8f8e570", "class_name": "RelatedNodeInfo"}, {"node_id": "edff671d-9458-4bdb-ac51-0a7d9d7f7d42", "node_type": "1", "metadata": {}, "hash": "b74591e8d5f15a42e91b0c980d57b1f1f4c4b54798b0e78de95a024a56e1350a", "class_name": "RelatedNodeInfo"}]}, "hash": "76a39cd8d7738e591c73d577b1d61a76c9ad29cf7077dec16ea41a05f6d2ad35", "text": "LlamaIndex provides the essential toolkit for making both steps super easy.\nLet's explore each stage in detail.\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)/Indexing Stage\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex help you prepare the knowledge base with a suite of data connectors and indexes.\n! \n\n**Data Connectors**:\nA data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).\n\n**Documents / Nodes**: A `Document` is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A `Node` is the atomic unit of data in LlamaIndex and represents a \"chunk\" of a source `Document`. It's a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations.\n\n**Data Indexes**: \nOnce you've ingested your data, LlamaIndex help you index data into a format that's easy to retrieve.\nUnder the hood, LlamaIndex parse the raw documents into intermediate representations, calculate vector embeddings, and infer metadata, etc.\nThe most commonly used index is the VectorStoreIndex\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)/Querying Stage\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn the querying stage, the RAG pipeline retrieves the most relevant context given a user query,\nand pass that to the LLM (along with the query) to synthesize a response.\nThis gives the LLM up-to-date knowledge that is not in its original training data,\n(also reducing hallucination).\nThe key challenge in the querying stage is retrieval, orchestration, and reasoning over (potentially many) knowledge bases.\n\nLlamaIndex provides composable modules that help you build and integrate RAG pipelines for Q&A (query engine), chatbot (chat engine), or as part of an agent.\nThese building blocks can be customized to reflect ranking preferences, as well as composed to reason over multiple knowledge bases in a structured way.\n\n!\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)/Querying Stage/Building Blocks\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Retrievers**: \nA retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query.\nThe specific retrieval logic differs for difference indices, the most popular being dense retrieval against a vector index.\n\n**Node Postprocessors**:\nA node postprocessor takes in a set of nodes, then apply transformation, filtering, or re-ranking logic to them. \n\n**Response Synthesizers**:\nA response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)/Querying Stage/Pipelines\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Query Engines**:\nA query engine is an end-to-end pipeline that allow you to ask question over your data.\nIt takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n\n\n**Chat Engines**: \nA chat engine is an end-to-end pipeline for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer).\n\n**Agents**: \nAn agent is an automated decision maker (powered by an LLM) that interacts with the world via a set of tools.\nAgent may be used in the same fashion as query engines or chat engines. \nThe main distinction is that an agent dynamically decides the best sequence of actions, instead of following a predetermined logic.\nThis gives it additional flexibility to tackle more complex tasks.\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)/Querying Stage/Pipelines\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* tell me how to customize things.\n* curious about a specific module? Check out the module guides \ud83d\udc48\n* have a use case in mind?", "start_char_idx": 784545, "end_char_idx": 789594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9206bf00-08cf-4199-9276-96ef119a8e61": {"__data__": {"id_": "9206bf00-08cf-4199-9276-96ef119a8e61", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53b92866-0f8e-48d1-bb79-a2d943008412", "node_type": "1", "metadata": {}, "hash": "76a39cd8d7738e591c73d577b1d61a76c9ad29cf7077dec16ea41a05f6d2ad35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b3d4168-8e37-453a-9138-87c7e58ab499", "node_type": "1", "metadata": {}, "hash": "8d3fc5859de66e460483ed769451d0743888fed2c106a1a0f724674bf51830c6", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "c76cac1f-267c-4a55-8d5f-c50ff3f2cc54", "node_type": "1", "metadata": {}, "hash": "726295230836e04a450f0606d41f631e6d6fa38344c6a4786f32442d6ca1e5ee", "class_name": "RelatedNodeInfo"}, {"node_id": "517e8542-2a5f-4174-ac04-97dd59ac391c", "node_type": "1", "metadata": {}, "hash": "02a5d5d7d301e024b522cba7394044cb4f0a69ff2734ae96b86a2b99aa8e6f44", "class_name": "RelatedNodeInfo"}, {"node_id": "02097577-3e0b-4ef8-bac8-942c1468a056", "node_type": "1", "metadata": {}, "hash": "27208c3fdf2ea4bf8a1b67ea95e79db1b4669fb7b88a583fdb652d2fdaf05c06", "class_name": "RelatedNodeInfo"}]}, "hash": "741492fa2ea72962132f7b8aea6ab7e5dd917c0ca02d513b93a78277ee28fd82", "text": "Check out the module guides \ud83d\udc48\n* have a use case in mind? Check out the end-to-end tutorials\n\nFile Name: Docs\\getting_started\\installation.md\nContent Type: text\nHeader Path: Installation and Setup/Installation from Pip\nLinks: \nfile_path: Docs\\getting_started\\installation.md\nfile_name: installation.md\nfile_type: None\nfile_size: 987\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can simply do:\n\nFile Name: Docs\\getting_started\\installation.md\nContent Type: text\nHeader Path: Installation and Setup/Installation from Pip\nLinks: \nfile_path: Docs\\getting_started\\installation.md\nfile_name: installation.md\nfile_type: None\nfile_size: 987\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\npip install llama-index\n\nFile Name: Docs\\getting_started\\installation.md\nContent Type: text\nHeader Path: Installation and Setup/Installation from Source\nLinks: \nfile_path: Docs\\getting_started\\installation.md\nfile_name: installation.md\nfile_type: None\nfile_size: 987\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGit clone this repository: `git clone https://github.com/jerryjliu/llama_index.git`. Then do:\n\n- `pip install -e .` if you want to do an editable install (you can modify source files) of just the package itself.\n- `pip install -r requirements.txt` if you want to install optional dependencies + dependencies used for development (e.g. unit testing).\n\nFile Name: Docs\\getting_started\\installation.md\nContent Type: text\nHeader Path: Installation and Setup/Environment Setup\nLinks: \nfile_path: Docs\\getting_started\\installation.md\nfile_name: installation.md\nfile_type: None\nfile_size: 987\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, we use the OpenAI GPT-3 `text-davinci-003` model. In order to use this, you must have an OPENAI_API_KEY setup.\nYou can register an API key by logging into OpenAI's page and creating a new API token.\n\nFile Name: Docs\\getting_started\\installation.md\nContent Type: text\nHeader Path: Installation and Setup/Environment Setup\nLinks: \nfile_path: Docs\\getting_started\\installation.md\nfile_name: installation.md\nfile_type: None\nfile_size: 987\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also customize the underlying LLM. You may\nneed additional environment keys + tokens setup depending on the LLM provider.\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: code\nHeader Path: Starter Tutorial\nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nMake sure you've followed the installation steps first.\n```\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere is a starter example for using LlamaIndex.\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex examples can be found in the `examples` folder of the LlamaIndex repository.\nWe first want to download this `examples` folder.", "start_char_idx": 789538, "end_char_idx": 793192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b3d4168-8e37-453a-9138-87c7e58ab499": {"__data__": {"id_": "8b3d4168-8e37-453a-9138-87c7e58ab499", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9206bf00-08cf-4199-9276-96ef119a8e61", "node_type": "1", "metadata": {}, "hash": "741492fa2ea72962132f7b8aea6ab7e5dd917c0ca02d513b93a78277ee28fd82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "265d7b64-ddd6-4507-a17a-f8649b7bca3f", "node_type": "1", "metadata": {}, "hash": "8573ecbb93516da074d72677f0b4a5e3bde8ccb655a72442dc61709d06819bcf", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "0ac65bc7-bb7b-4414-b9f9-04d8bbab3889", "node_type": "1", "metadata": {}, "hash": "73136f2f360ee94c992a38f3ea369d4ab9185a3e362f4c733202fc3bea90c43b", "class_name": "RelatedNodeInfo"}, {"node_id": "95efa0d0-2f98-4265-a077-0af2b5891a0c", "node_type": "1", "metadata": {}, "hash": "65bfc6ba6644a57d838b1e54cc4d5d781e71b239bb0a15a81a61c3bb2e91928d", "class_name": "RelatedNodeInfo"}, {"node_id": "dd53061c-f28d-44b2-a91a-b9fcb039faf8", "node_type": "1", "metadata": {}, "hash": "032ad4adecf264ac58c15a66f66b9dd0a58da127ca05fb0ab8d75ebb7ce6710a", "class_name": "RelatedNodeInfo"}]}, "hash": "8d3fc5859de66e460483ed769451d0743888fed2c106a1a0f724674bf51830c6", "text": "We first want to download this `examples` folder. An easy way to do this is to just clone the repo:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n$ git clone https://github.com/jerryjliu/llama_index.git\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNext, navigate to your newly-cloned repository, and verify the contents:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n$ cd llama_index\n$ ls\nLICENSE                data_requirements.txt  tests/\nMANIFEST.in            examples/              pyproject.toml\nMakefile               experimental/          requirements.txt\nREADME.md              llama_index/             setup.py\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe now want to navigate to the following folder:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n$ cd examples/paul_graham_essay\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis contains LlamaIndex examples around Paul Graham's essay, \"What I Worked On\". A comprehensive set of examples are already provided in `TestEssay.ipynb`. For the purposes of this tutorial, we can focus on a simple example of getting LlamaIndex up and running.\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Build and Query Index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCreate a new `.py` file with the following:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Build and Query Index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Build and Query Index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis builds an index over the documents in the `data` folder (which in this case just consists of the essay text). We then run the following\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Build and Query Index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")", "start_char_idx": 793143, "end_char_idx": 797686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "265d7b64-ddd6-4507-a17a-f8649b7bca3f": {"__data__": {"id_": "265d7b64-ddd6-4507-a17a-f8649b7bca3f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7", "node_type": "4", "metadata": {}, "hash": "5e1d492c27de80f90b6191043510b08bdb3dc0289c65e4b30bee387d80a09d2b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b3d4168-8e37-453a-9138-87c7e58ab499", "node_type": "1", "metadata": {}, "hash": "8d3fc5859de66e460483ed769451d0743888fed2c106a1a0f724674bf51830c6", "class_name": "RelatedNodeInfo"}, "5": [{"node_id": "de5151fa-e027-4b87-afd3-6ac0abd85d55", "node_type": "1", "metadata": {}, "hash": "3b40291a3e8c5e857368f9a8632a6463308c7feb5762f6f555944ebef7b5b52f", "class_name": "RelatedNodeInfo"}, {"node_id": "b75f2d32-d00c-4f20-82fd-ddc51d695bd8", "node_type": "1", "metadata": {}, "hash": "f637d27b000b9fba516037ae5b932384fb045eab183360bdf58904d80b606e88", "class_name": "RelatedNodeInfo"}, {"node_id": "4cb61500-a2fb-4865-a15c-61be52c02e8f", "node_type": "1", "metadata": {}, "hash": "0a52c034583f1c548128e541a3916898db1c3c95a2789e45e73439217fedc19b", "class_name": "RelatedNodeInfo"}]}, "hash": "8573ecbb93516da074d72677f0b4a5e3bde8ccb655a72442dc61709d06819bcf", "text": "print(response)\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Build and Query Index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou should get back a response similar to the following: `The author wrote short stories and tried to program on an IBM 1401.`\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Viewing Queries and Events Using Logging\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn a Jupyter notebook, you can view info and/or debugging logging using the following snippet:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Viewing Queries and Events Using Logging\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Viewing Queries and Events Using Logging\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can set the level to `DEBUG` for verbose output, or use `level=logging.INFO` for less.\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Saving and Loading\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, data is stored in-memory.\nTo persist to disk (under `./storage`):\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Saving and Loading\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.storage_context.persist()\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Saving and Loading\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo reload from disk:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Saving and Loading\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import StorageContext, load_index_from_storage\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/rebuild storage context\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/load index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = load_index_from_storage(storage_context)\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: code\nHeader Path: Starter Tutorial/load index\nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{admonition} Next Steps\n* learn more about the high-level concepts.\n* tell me how to customize things.\n* curious about a specific module? check out the guides \ud83d\udc48\n* have a use case in mind? check out the end-to-end tutorials\n```", "start_char_idx": 797687, "end_char_idx": 802401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "caa3d881-2b6a-4a21-897c-9040f14abbd0": {"__data__": {"id_": "caa3d881-2b6a-4a21-897c-9040f14abbd0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9281baa-c7fb-427b-b9a6-2871fde16930", "node_type": "1", "metadata": {}, "hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a6b808b-c800-4276-bb45-e411144750c8", "node_type": "1", "metadata": {}, "hash": "385546f8839bcd57982e313be5ef2559532a3a74a5be12eedc313d3125cd74de", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9281baa-c7fb-427b-b9a6-2871fde16930", "node_type": "1", "metadata": {}, "hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "class_name": "RelatedNodeInfo"}}, "hash": "74ac7475481c990ffe046c550864909cb409d022b732827a4c9dcf2efedc567b", "text": "File Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere is a sample of some of the incredible applications and tools built on top of LlamaIndex!\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Meru - Dense Data Retrieval API\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHosted API service. Includes a \"Dense Data Retrieval\" API built on top of LlamaIndex where users can upload their documents and query them.\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Algovera\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBuild AI workflows using building blocks. Many workflows built on top of LlamaIndex.\n\n[Website].\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/ChatGPT LlamaIndex\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nInterface that allows users to upload long docs and chat with the bot.", "start_char_idx": 0, "end_char_idx": 1614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a6b808b-c800-4276-bb45-e411144750c8": {"__data__": {"id_": "9a6b808b-c800-4276-bb45-e411144750c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9281baa-c7fb-427b-b9a6-2871fde16930", "node_type": "1", "metadata": {}, "hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "caa3d881-2b6a-4a21-897c-9040f14abbd0", "node_type": "1", "metadata": {}, "hash": "74ac7475481c990ffe046c550864909cb409d022b732827a4c9dcf2efedc567b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0b79f38-f54d-49a8-b5a3-5827fb3d638b", "node_type": "1", "metadata": {}, "hash": "862f7ca88c65c3a4572e372531f6d618ee9de4f208912d303ddbde4838815099", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9281baa-c7fb-427b-b9a6-2871fde16930", "node_type": "1", "metadata": {}, "hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "class_name": "RelatedNodeInfo"}}, "hash": "385546f8839bcd57982e313be5ef2559532a3a74a5be12eedc313d3125cd74de", "text": "[Tweet thread]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/AgentHQ\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA web tool to build agents, interacting with LlamaIndex data structures.[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/PapersGPT\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFeed any of the following content into GPT to give it deep customized knowledge:\n- Scientific Papers\n- Substack Articles\n- Podcasts\n- Github Repos\nand more.\n\n[Tweet thread]\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/VideoQues + DocsQues\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**VideoQues**: A tool that answers your queries on YouTube videos. \n[LinkedIn post here].\n\n**DocsQues**: A tool that answers your questions on longer documents (including .pdfs!)\n[LinkedIn post here].", "start_char_idx": 1615, "end_char_idx": 2992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0b79f38-f54d-49a8-b5a3-5827fb3d638b": {"__data__": {"id_": "c0b79f38-f54d-49a8-b5a3-5827fb3d638b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9281baa-c7fb-427b-b9a6-2871fde16930", "node_type": "1", "metadata": {}, "hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a6b808b-c800-4276-bb45-e411144750c8", "node_type": "1", "metadata": {}, "hash": "385546f8839bcd57982e313be5ef2559532a3a74a5be12eedc313d3125cd74de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "129e80a0-343f-4e02-ad24-7da18d5945c5", "node_type": "1", "metadata": {}, "hash": "ecb1412810d8511072d1ea14642472dd639349713592c4c2b6af2eaa5a4db3ce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9281baa-c7fb-427b-b9a6-2871fde16930", "node_type": "1", "metadata": {}, "hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "class_name": "RelatedNodeInfo"}}, "hash": "862f7ca88c65c3a4572e372531f6d618ee9de4f208912d303ddbde4838815099", "text": "[LinkedIn post here].\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/PaperBrain\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA platform to access/understand research papers.\n\n[Tweet thread].\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/CACTUS\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nContextual search on top of LinkedIn search results. \n[LinkedIn post here].\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Personal Note Chatbot\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA chatbot that can answer questions over a directory of Obsidian notes. \n[Tweet thread].\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/RHOBH AMA\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAsk questions about the Real Housewives of Beverly Hills.", "start_char_idx": 2971, "end_char_idx": 4475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "129e80a0-343f-4e02-ad24-7da18d5945c5": {"__data__": {"id_": "129e80a0-343f-4e02-ad24-7da18d5945c5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9281baa-c7fb-427b-b9a6-2871fde16930", "node_type": "1", "metadata": {}, "hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0b79f38-f54d-49a8-b5a3-5827fb3d638b", "node_type": "1", "metadata": {}, "hash": "862f7ca88c65c3a4572e372531f6d618ee9de4f208912d303ddbde4838815099", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d9281baa-c7fb-427b-b9a6-2871fde16930", "node_type": "1", "metadata": {}, "hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "class_name": "RelatedNodeInfo"}}, "hash": "ecb1412810d8511072d1ea14642472dd639349713592c4c2b6af2eaa5a4db3ce", "text": "[Tweet thread]\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Mynd\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA journaling app that uses AI to uncover insights and patterns over time.", "start_char_idx": 4476, "end_char_idx": 4865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60b5bfaf-706e-42d7-a723-9df702496b39": {"__data__": {"id_": "60b5bfaf-706e-42d7-a723-9df702496b39", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b", "node_type": "1", "metadata": {}, "hash": "4520438bbb63add46db8092c10706b2a7f98bbecf23496be1d3866be12bf1b92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85cd9d1c-f2ed-4826-98e8-8f352c020431", "node_type": "1", "metadata": {}, "hash": "c0821ea1a5be7ff15030d1d1ad49a477e91ce5add07577d7bbfae6a1760d4ced", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b", "node_type": "1", "metadata": {}, "hash": "4520438bbb63add46db8092c10706b2a7f98bbecf23496be1d3866be12bf1b92", "class_name": "RelatedNodeInfo"}}, "hash": "9ec5c9e0e5fe3be8845573ca4c79c3390b03cc2020700b23a225c4bc26d32b92", "text": "[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/CoFounder\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe First AI Co-Founder for Your Start-up \ud83d\ude4c\n\nCoFounder is a platform to revolutionize the start-up ecosystem by providing founders with unparalleled tools, resources, and support. We are changing how founders build their companies from 0-1\u2014productizing the accelerator/incubator programs using AI.", "start_char_idx": 0, "end_char_idx": 603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85cd9d1c-f2ed-4826-98e8-8f352c020431": {"__data__": {"id_": "85cd9d1c-f2ed-4826-98e8-8f352c020431", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b", "node_type": "1", "metadata": {}, "hash": "4520438bbb63add46db8092c10706b2a7f98bbecf23496be1d3866be12bf1b92", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60b5bfaf-706e-42d7-a723-9df702496b39", "node_type": "1", "metadata": {}, "hash": "9ec5c9e0e5fe3be8845573ca4c79c3390b03cc2020700b23a225c4bc26d32b92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fa49686-6f5f-4a1a-89b0-93ffe41006a4", "node_type": "1", "metadata": {}, "hash": "6676bbcc56ba85b210a3a8fe824e508ad4e2262447f8538c665a6b93f2a6b2bf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b", "node_type": "1", "metadata": {}, "hash": "4520438bbb63add46db8092c10706b2a7f98bbecf23496be1d3866be12bf1b92", "class_name": "RelatedNodeInfo"}}, "hash": "c0821ea1a5be7ff15030d1d1ad49a477e91ce5add07577d7bbfae6a1760d4ced", "text": "Current features:\n\n* AI Investor Matching and Introduction and Tracking\n* AI Pitch Deck creation\n* Real-time Pitch Deck practice/feedback\n* Automatic Competitive Analysis / Watchlist\n* More coming soon...\n\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Al-X by OpenExO\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYour Digital Transformation Co-Pilot\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/AnySummary\nLinks: \nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSummarize any document, audio or video with AI\n[Website]\n\nFile Name: Docs\\community\\app_showcase.md\nContent Type: text\nHeader Path: App Showcase/Blackmaria\nfile_path: Docs\\community\\app_showcase.md\nfile_name: app_showcase.md\nfile_type: None\nfile_size: 3927\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nPython package for webscraping in Natural language.\n[Tweet thread]\n[Github]\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**NOTE**: This is a work-in-progress, stay tuned for more exciting updates on this front!", "start_char_idx": 605, "end_char_idx": 2314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fa49686-6f5f-4a1a-89b0-93ffe41006a4": {"__data__": {"id_": "5fa49686-6f5f-4a1a-89b0-93ffe41006a4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b", "node_type": "1", "metadata": {}, "hash": "4520438bbb63add46db8092c10706b2a7f98bbecf23496be1d3866be12bf1b92", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85cd9d1c-f2ed-4826-98e8-8f352c020431", "node_type": "1", "metadata": {}, "hash": "c0821ea1a5be7ff15030d1d1ad49a477e91ce5add07577d7bbfae6a1760d4ced", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b", "node_type": "1", "metadata": {}, "hash": "4520438bbb63add46db8092c10706b2a7f98bbecf23496be1d3866be12bf1b92", "class_name": "RelatedNodeInfo"}}, "hash": "6676bbcc56ba85b210a3a8fe824e508ad4e2262447f8538c665a6b93f2a6b2bf", "text": "File Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe OpenAI ChatGPT Retrieval Plugin\noffers a centralized API specification for any document storage system to interact \nwith ChatGPT. Since this can be deployed on any service, this means that more and more\ndocument retrieval services will implement this spec; this allows them to not only\ninteract with ChatGPT, but also interact with any LLM toolkit that may use \na retrieval service.\n\nLlamaIndex provides a variety of integrations with the ChatGPT Retrieval Plugin.\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/Loading Data from LlamaHub into the ChatGPT Retrieval Plugin\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe ChatGPT Retrieval Plugin defines an `/upsert` endpoint for users to load\ndocuments. This offers a natural integration point with LlamaHub, which offers\nover 65 data loaders from various API's and document formats.", "start_char_idx": 2316, "end_char_idx": 3810, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ed32ab1-3669-4106-ac1d-2f66c3bdb29d": {"__data__": {"id_": "0ed32ab1-3669-4106-ac1d-2f66c3bdb29d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236", "node_type": "1", "metadata": {}, "hash": "ef9c76e6c0c10e120aa1805a956c3532e019b02b7a02de48252084024805b4ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b30ebacc-9abf-42ad-9f40-9e5e88833711", "node_type": "1", "metadata": {}, "hash": "35f9c836cad41909017022efaf9e93d1cb5d76157b6b708618ee0188e8e6b536", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236", "node_type": "1", "metadata": {}, "hash": "ef9c76e6c0c10e120aa1805a956c3532e019b02b7a02de48252084024805b4ba", "class_name": "RelatedNodeInfo"}}, "hash": "44b50cb9c25d6163f3ff7d64eb124f97295ae175c296072257efed15974ca1f3", "text": "Here is a sample code snippet of showing how to load a document from LlamaHub\ninto the JSON format that `/upsert` expects:\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/Loading Data from LlamaHub into the ChatGPT Retrieval Plugin\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import download_loader, Document\nfrom typing import Dict, List\nimport json\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/download loader, load documents\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\nloader = SimpleWebPageReader(html_to_text=True)\nurl = \"http://www.paulgraham.com/worked.html\"\ndocuments = loader.load_data(urls=[url])\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/Convert LlamaIndex Documents to JSON format\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndef dump_docs_to_json(documents: List[Document], out_path: str) -> Dict:\n    \"\"\"Convert LlamaIndex Documents to JSON format and save it.\"\"\"", "start_char_idx": 0, "end_char_idx": 1809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b30ebacc-9abf-42ad-9f40-9e5e88833711": {"__data__": {"id_": "b30ebacc-9abf-42ad-9f40-9e5e88833711", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236", "node_type": "1", "metadata": {}, "hash": "ef9c76e6c0c10e120aa1805a956c3532e019b02b7a02de48252084024805b4ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ed32ab1-3669-4106-ac1d-2f66c3bdb29d", "node_type": "1", "metadata": {}, "hash": "44b50cb9c25d6163f3ff7d64eb124f97295ae175c296072257efed15974ca1f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6910eea8-072f-42ba-8c11-43f5781d409e", "node_type": "1", "metadata": {}, "hash": "6ae082112a9a3bce10ab77503d8dcdb7ef3eb8fd7b6f0940a368d54fccc94f55", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236", "node_type": "1", "metadata": {}, "hash": "ef9c76e6c0c10e120aa1805a956c3532e019b02b7a02de48252084024805b4ba", "class_name": "RelatedNodeInfo"}}, "hash": "35f9c836cad41909017022efaf9e93d1cb5d76157b6b708618ee0188e8e6b536", "text": "result_json = []\n    for doc in documents:\n        cur_dict = {\n            \"text\": doc.get_text(),\n            \"id\": doc.get_doc_id(),\n            # NOTE: feel free to customize the other fields as you wish\n            # fields taken from https://github.com/openai/chatgpt-retrieval-plugin/tree/main/scripts/process_json#usage\n            # \"source\": ...,\n            # \"source_id\": ...,\n            # \"url\": url,\n            # \"created_at\": ...,\n            # \"author\": \"Paul Graham\",\n        }\n        result_json.append(cur_dict)\n    \n    json.dump(result_json, open(out_path, 'w'))\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/Convert LlamaIndex Documents to JSON format\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor more details, check out the full example notebook.\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/ChatGPT Retrieval Plugin Data Loader\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe ChatGPT Retrieval Plugin data loader can be accessed on LlamaHub.\n\nIt allows you to easily load data from any docstore that implements the plugin API, into a LlamaIndex data structure.", "start_char_idx": 1814, "end_char_idx": 3473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6910eea8-072f-42ba-8c11-43f5781d409e": {"__data__": {"id_": "6910eea8-072f-42ba-8c11-43f5781d409e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236", "node_type": "1", "metadata": {}, "hash": "ef9c76e6c0c10e120aa1805a956c3532e019b02b7a02de48252084024805b4ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b30ebacc-9abf-42ad-9f40-9e5e88833711", "node_type": "1", "metadata": {}, "hash": "35f9c836cad41909017022efaf9e93d1cb5d76157b6b708618ee0188e8e6b536", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236", "node_type": "1", "metadata": {}, "hash": "ef9c76e6c0c10e120aa1805a956c3532e019b02b7a02de48252084024805b4ba", "class_name": "RelatedNodeInfo"}}, "hash": "6ae082112a9a3bce10ab77503d8dcdb7ef3eb8fd7b6f0940a368d54fccc94f55", "text": "Example code:\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/ChatGPT Retrieval Plugin Data Loader\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.readers import ChatGPTRetrievalPluginReader\nimport os\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/load documents\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nbearer_token = os.getenv(\"BEARER_TOKEN\")\nreader = ChatGPTRetrievalPluginReader(\n    endpoint_url=\"http://localhost:8000\",\n    bearer_token=bearer_token\n)\ndocuments = reader.load_data(\"What did the author do growing up?\")", "start_char_idx": 3475, "end_char_idx": 4580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b784149f-4d79-4495-baf7-04894159cac1": {"__data__": {"id_": "b784149f-4d79-4495-baf7-04894159cac1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2495892e-f217-42d3-b47c-f13cc073d7ac", "node_type": "1", "metadata": {}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3527ee3-d586-45cf-ac35-6ed9dae27802", "node_type": "1", "metadata": {}, "hash": "5e99c994f1c48176472a14344b48a5c517ea0234284b62c9a8d298414e2cff33", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2495892e-f217-42d3-b47c-f13cc073d7ac", "node_type": "1", "metadata": {}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "class_name": "RelatedNodeInfo"}}, "hash": "03391ff7326c6a47d68f6b0c6235851852408757c5d83ec827d09ca2ef60e4b7", "text": "File Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/build and query index\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ListIndex\nindex = ListIndex(documents)\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/set Logging to DEBUG for more detailed outputs\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine(\n    response_mode=\"compact\"\n)\nresponse = query_engine.query(\n    \"Summarize the retrieved content and describe what the author did growing up\",\n)\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/set Logging to DEBUG for more detailed outputs\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor more details, check out the full example notebook.", "start_char_idx": 0, "end_char_idx": 1545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3527ee3-d586-45cf-ac35-6ed9dae27802": {"__data__": {"id_": "b3527ee3-d586-45cf-ac35-6ed9dae27802", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2495892e-f217-42d3-b47c-f13cc073d7ac", "node_type": "1", "metadata": {}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b784149f-4d79-4495-baf7-04894159cac1", "node_type": "1", "metadata": {}, "hash": "03391ff7326c6a47d68f6b0c6235851852408757c5d83ec827d09ca2ef60e4b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9ad55ed-47aa-4c1b-b760-c898291c8c33", "node_type": "1", "metadata": {}, "hash": "783ddefd6c786fb0f1f3cfa759ab5414439ee105ae04b2917ab9021c2aaec554", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2495892e-f217-42d3-b47c-f13cc073d7ac", "node_type": "1", "metadata": {}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "class_name": "RelatedNodeInfo"}}, "hash": "5e99c994f1c48176472a14344b48a5c517ea0234284b62c9a8d298414e2cff33", "text": "File Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/ChatGPT Retrieval Plugin Index\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe ChatGPT Retrieval Plugin Index allows you to easily build a vector index over any documents, with storage backed by a document store implementing the \nChatGPT endpoint.\n\nNote: this index is a vector index, allowing top-k retrieval.\n\nExample code:\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/ChatGPT Retrieval Plugin Index\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.vector_store import ChatGPTRetrievalPluginIndex\nfrom llama_index import SimpleDirectoryReader\nimport os\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/load documents\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('./paul_graham_essay/data').load_data()\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9ad55ed-47aa-4c1b-b760-c898291c8c33": {"__data__": {"id_": "e9ad55ed-47aa-4c1b-b760-c898291c8c33", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2495892e-f217-42d3-b47c-f13cc073d7ac", "node_type": "1", "metadata": {}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3527ee3-d586-45cf-ac35-6ed9dae27802", "node_type": "1", "metadata": {}, "hash": "5e99c994f1c48176472a14344b48a5c517ea0234284b62c9a8d298414e2cff33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "037a136f-42d9-4099-8139-a859c61c9690", "node_type": "1", "metadata": {}, "hash": "555ee5bc897781941f99b545d7d3ed76c4148d6261a238cdcc2a746c7c772454", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2495892e-f217-42d3-b47c-f13cc073d7ac", "node_type": "1", "metadata": {}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "class_name": "RelatedNodeInfo"}}, "hash": "783ddefd6c786fb0f1f3cfa759ab5414439ee105ae04b2917ab9021c2aaec554", "text": "load_data()\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/build index\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nbearer_token = os.getenv(\"BEARER_TOKEN\")\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/initialize without metadata filter\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = ChatGPTRetrievalPluginIndex(\n    documents, \n    endpoint_url=\"http://localhost:8000\",\n    bearer_token=bearer_token,\n)\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/query index\nLinks: \nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine(\n    similarity_top_k=3,\n    response_mode=\"compact\",\n)\nresponse = query_engine.query(\"What did the author do growing up?\")", "start_char_idx": 3188, "end_char_idx": 4717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "037a136f-42d9-4099-8139-a859c61c9690": {"__data__": {"id_": "037a136f-42d9-4099-8139-a859c61c9690", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2495892e-f217-42d3-b47c-f13cc073d7ac", "node_type": "1", "metadata": {}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9ad55ed-47aa-4c1b-b760-c898291c8c33", "node_type": "1", "metadata": {}, "hash": "783ddefd6c786fb0f1f3cfa759ab5414439ee105ae04b2917ab9021c2aaec554", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2495892e-f217-42d3-b47c-f13cc073d7ac", "node_type": "1", "metadata": {}, "hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "class_name": "RelatedNodeInfo"}}, "hash": "555ee5bc897781941f99b545d7d3ed76c4148d6261a238cdcc2a746c7c772454", "text": ")\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\community\\integrations\\chatgpt_plugins.md\nContent Type: text\nHeader Path: ChatGPT Plugin Integrations/ChatGPT Retrieval Plugin Integrations/query index\nfile_path: Docs\\community\\integrations\\chatgpt_plugins.md\nfile_name: chatgpt_plugins.md\nfile_type: None\nfile_size: 4729\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor more details, check out the full example notebook.", "start_char_idx": 4648, "end_char_idx": 5150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c58a5832-c6e1-4660-bb8b-0b305cf97f2c": {"__data__": {"id_": "c58a5832-c6e1-4660-bb8b-0b305cf97f2c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8877944a-5895-49cd-8397-2777043dd11b", "node_type": "1", "metadata": {}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84c56aae-2e36-40d8-949c-d689fdcd64f0", "node_type": "1", "metadata": {}, "hash": "9da881b544e6c4afa4a9a46baf05da1cbf41557ffb592006034474e8ce81718b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8877944a-5895-49cd-8397-2777043dd11b", "node_type": "1", "metadata": {}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "class_name": "RelatedNodeInfo"}}, "hash": "84d5bf069d69d7757dd6e3296db165f4e5d02d11325fc891ba01ff366e7dc709", "text": "File Name: Docs\\community\\integrations\\graph_stores.md\nContent Type: text\nHeader Path: Using Graph Stores/`NebulaGraphStore`\nLinks: \nfile_path: Docs\\community\\integrations\\graph_stores.md\nfile_name: graph_stores.md\nfile_type: None\nfile_size: 541\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support a `NebulaGraphStore` integration, for persisting graphs directly in Nebula! Furthermore, you can generate cypher queries and return natural language responses for your Nebula graphs using the `KnowledgeGraphQueryEngine`.\n\nSee the associated guides below:\n\nFile Name: Docs\\community\\integrations\\graph_stores.md\nContent Type: text\nHeader Path: Using Graph Stores/`NebulaGraphStore`\nLinks: \nfile_path: Docs\\community\\integrations\\graph_stores.md\nfile_name: graph_stores.md\nfile_type: None\nfile_size: 541\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nNebula Graph Store </examples/index_structs/knowledge_graph/NebulaGraphKGIndexDemo.ipynb>\nKnowledge Graph Query Engine </examples/query_engine/knowledge_graph_query_engine.ipynb>\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGraphsignal provides observability for AI agents and LLM-powered applications. It helps developers ensure AI applications run as expected and users have the best experience.\n\nGraphsignal **automatically** traces and monitors LlamaIndex. Traces and metrics provide execution details for query, retrieval, and index operations.", "start_char_idx": 0, "end_char_idx": 1782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84c56aae-2e36-40d8-949c-d689fdcd64f0": {"__data__": {"id_": "84c56aae-2e36-40d8-949c-d689fdcd64f0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8877944a-5895-49cd-8397-2777043dd11b", "node_type": "1", "metadata": {}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c58a5832-c6e1-4660-bb8b-0b305cf97f2c", "node_type": "1", "metadata": {}, "hash": "84d5bf069d69d7757dd6e3296db165f4e5d02d11325fc891ba01ff366e7dc709", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6c79f20-6e57-4dbc-922a-e3074073104f", "node_type": "1", "metadata": {}, "hash": "b0171cc53f83dbca35c5013ec2c3799f6c482ad822bc310936588d975c517cbd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8877944a-5895-49cd-8397-2777043dd11b", "node_type": "1", "metadata": {}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "class_name": "RelatedNodeInfo"}}, "hash": "9da881b544e6c4afa4a9a46baf05da1cbf41557ffb592006034474e8ce81718b", "text": "Traces and metrics provide execution details for query, retrieval, and index operations. These insights include **prompts**, **completions**, **embedding statistics**, **retrieved nodes**, **parameters**, **latency**, and **exceptions**.\n\nWhen OpenAI APIs are used, Graphsignal provides additional insights such as **token counts** and **costs** per deployment, model or any context.", "start_char_idx": 1694, "end_char_idx": 2077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6c79f20-6e57-4dbc-922a-e3074073104f": {"__data__": {"id_": "e6c79f20-6e57-4dbc-922a-e3074073104f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8877944a-5895-49cd-8397-2777043dd11b", "node_type": "1", "metadata": {}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84c56aae-2e36-40d8-949c-d689fdcd64f0", "node_type": "1", "metadata": {}, "hash": "9da881b544e6c4afa4a9a46baf05da1cbf41557ffb592006034474e8ce81718b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cb5bde9-973e-4fd5-a8cd-b012b5e49ed4", "node_type": "1", "metadata": {}, "hash": "6a00c494ef4dec7389d5d6188c9e86e88187bae9aafd47bbcd073d7a3b9ac493", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8877944a-5895-49cd-8397-2777043dd11b", "node_type": "1", "metadata": {}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "class_name": "RelatedNodeInfo"}}, "hash": "b0171cc53f83dbca35c5013ec2c3799f6c482ad822bc310936588d975c517cbd", "text": "File Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Installation and Setup\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAdding Graphsignal tracer is simple, just install and configure it:\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Installation and Setup\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\npip install graphsignal\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: code\nHeader Path: Tracing with Graphsignal/Provide an API key directly or via GRAPHSIGNAL_API_KEY environment variable\nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nimport graphsignal\n\ngraphsignal.configure(api_key='my-api-key', deployment='my-llama-index-app-prod')\n```\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Provide an API key directly or via GRAPHSIGNAL_API_KEY environment variable\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can get an API key here.\n\nSee the Quick Start guide, Integration guide, and an example app for more information.", "start_char_idx": 2079, "end_char_idx": 3876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cb5bde9-973e-4fd5-a8cd-b012b5e49ed4": {"__data__": {"id_": "2cb5bde9-973e-4fd5-a8cd-b012b5e49ed4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8877944a-5895-49cd-8397-2777043dd11b", "node_type": "1", "metadata": {}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6c79f20-6e57-4dbc-922a-e3074073104f", "node_type": "1", "metadata": {}, "hash": "b0171cc53f83dbca35c5013ec2c3799f6c482ad822bc310936588d975c517cbd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8877944a-5895-49cd-8397-2777043dd11b", "node_type": "1", "metadata": {}, "hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "class_name": "RelatedNodeInfo"}}, "hash": "6a00c494ef4dec7389d5d6188c9e86e88187bae9aafd47bbcd073d7a3b9ac493", "text": "See the Quick Start guide, Integration guide, and an example app for more information.\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Tracing Other Functions\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo additionally trace any function or code, you can use a decorator or a context manager:\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Tracing Other Functions\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwith graphsignal.start_trace('load-external-data'):\n    reader.load_data()\n\nFile Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Tracing Other Functions\nLinks: \nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee Python API Reference for complete instructions.", "start_char_idx": 3790, "end_char_idx": 5128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b38d046d-ae64-4c9b-8144-06d6834a1819": {"__data__": {"id_": "b38d046d-ae64-4c9b-8144-06d6834a1819", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634", "node_type": "1", "metadata": {}, "hash": "c219a0cf1ce7ea6fb6019c24f8f6ab08129b4f50db9a4c46f7828629906eeff3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1b126a3-f651-4ec8-8868-9bed2e78f7b1", "node_type": "1", "metadata": {}, "hash": "16dc86e3511ab3361a1aa7f2b1e94c6ca292ceaf5c59ae716ccfb92fbd14427b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634", "node_type": "1", "metadata": {}, "hash": "c219a0cf1ce7ea6fb6019c24f8f6ab08129b4f50db9a4c46f7828629906eeff3", "class_name": "RelatedNodeInfo"}}, "hash": "c7eb3dc0b8ff4c1bea48efce4f34e9dd3169fe54566e8e46aaed115cc383ed9e", "text": "File Name: Docs\\community\\integrations\\graphsignal.md\nContent Type: text\nHeader Path: Tracing with Graphsignal/Useful Links\nfile_path: Docs\\community\\integrations\\graphsignal.md\nfile_name: graphsignal.md\nfile_type: None\nfile_size: 2249\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* Tracing and Monitoring LlamaIndex Applications\n* Monitor OpenAI API Latency, Tokens, Rate Limits, and More\n* OpenAI API Cost Tracking: Analyzing Expenses by Model, Deployment, and Context\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGuidance is a guidance language for controlling large language models developed by Microsoft.\n\nGuidance programs allow you to interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOne particularly exciting aspect of guidance is the ability to output structured objects (think JSON following a specific schema, or a pydantic object). Instead of just \"suggesting\" the desired output structure to the LLM, guidance can actually \"force\" the LLM output to follow the desired schema. This allows the LLM to focus on the content rather than the syntax, and completely eliminate the possibility of output parsing issues.\n\nThis is particularly powerful for weaker LLMs which be smaller in parameter count, and not trained on sufficient source code data to be able to reliably produce well-formed, hierarchical structured output.", "start_char_idx": 0, "end_char_idx": 2035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1b126a3-f651-4ec8-8868-9bed2e78f7b1": {"__data__": {"id_": "a1b126a3-f651-4ec8-8868-9bed2e78f7b1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634", "node_type": "1", "metadata": {}, "hash": "c219a0cf1ce7ea6fb6019c24f8f6ab08129b4f50db9a4c46f7828629906eeff3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b38d046d-ae64-4c9b-8144-06d6834a1819", "node_type": "1", "metadata": {}, "hash": "c7eb3dc0b8ff4c1bea48efce4f34e9dd3169fe54566e8e46aaed115cc383ed9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63f07a12-74d3-4d22-ae66-9d3d5ab4399e", "node_type": "1", "metadata": {}, "hash": "55ec3913793bef189844a6d5a771c99af8cf8b8e83bec4eb0ab57e9e6dbdd9a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634", "node_type": "1", "metadata": {}, "hash": "c219a0cf1ce7ea6fb6019c24f8f6ab08129b4f50db9a4c46f7828629906eeff3", "class_name": "RelatedNodeInfo"}}, "hash": "16dc86e3511ab3361a1aa7f2b1e94c6ca292ceaf5c59ae716ccfb92fbd14427b", "text": "File Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn LlamaIndex, we provide an initial integration with guidance, to make it super easy for generating structured output (more specifically pydantic objects).\n\nFor example, if we want to generate an album of songs, with the following schema:\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nclass Song(BaseModel):\n    title: str\n    length_seconds: int\n    \nclass Album(BaseModel):\n    name: str\n    artist: str\n    songs: List[Song]\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIt's as simple as creating a `GuidancePydanticProgram`, specifying our desired pydantic class `Album`, \nand supplying a suitable prompt template.\n\n> Note: guidance uses handlebars-style templates, which uses double braces for variable substitution, and single braces for literal braces. This is the opposite convention of Python format strings.", "start_char_idx": 2037, "end_char_idx": 3876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63f07a12-74d3-4d22-ae66-9d3d5ab4399e": {"__data__": {"id_": "63f07a12-74d3-4d22-ae66-9d3d5ab4399e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634", "node_type": "1", "metadata": {}, "hash": "c219a0cf1ce7ea6fb6019c24f8f6ab08129b4f50db9a4c46f7828629906eeff3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1b126a3-f651-4ec8-8868-9bed2e78f7b1", "node_type": "1", "metadata": {}, "hash": "16dc86e3511ab3361a1aa7f2b1e94c6ca292ceaf5c59ae716ccfb92fbd14427b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634", "node_type": "1", "metadata": {}, "hash": "c219a0cf1ce7ea6fb6019c24f8f6ab08129b4f50db9a4c46f7828629906eeff3", "class_name": "RelatedNodeInfo"}}, "hash": "55ec3913793bef189844a6d5a771c99af8cf8b8e83bec4eb0ab57e9e6dbdd9a5", "text": "This is the opposite convention of Python format strings. \n\n> Note: We provide an utility function `from llama_index.prompts.guidance_utils import convert_to_handlebars` that can convert from the Python format string style template to guidance handlebars-style template.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprogram = GuidancePydanticProgram(\n    output_cls=Album, \n    prompt_template_str=\"Generate an example album, with an artist and a list of songs. Using the movie {{movie_name}} as inspiration\",\n    guidance_llm=OpenAI('text-davinci-003'),\n    verbose=True,\n)\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow we can run the program by calling it with additional user input. \nHere let's go for something spooky and create an album inspired by the Shining.", "start_char_idx": 3819, "end_char_idx": 5240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8284800-a627-47ba-a595-2bf621e5bfcc": {"__data__": {"id_": "f8284800-a627-47ba-a595-2bf621e5bfcc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "node_type": "1", "metadata": {}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d676ddeb-3565-4c6a-985b-c1161668436c", "node_type": "1", "metadata": {}, "hash": "209904f1692154051c100b4049308b5c191b4ac3f8391cf6d218a0a016c61b72", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "node_type": "1", "metadata": {}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "class_name": "RelatedNodeInfo"}}, "hash": "dabff93b9974353d35367350c249525c653c74d731a2a10e886725addc7f780a", "text": "Here let's go for something spooky and create an album inspired by the Shining.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\noutput = program(movie_name='The Shining')\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe have our pydantic object:\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAlbum(name='The Shining', artist='Jack Torrance', songs=[Song(title='All Work and No Play', length_seconds=180), Song(title='The Overlook Hotel', length_seconds=240), Song(title='The Shining', length_seconds=210)])\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Creating a guidance program to generate pydantic objects\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.", "start_char_idx": 0, "end_char_idx": 1727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d676ddeb-3565-4c6a-985b-c1161668436c": {"__data__": {"id_": "d676ddeb-3565-4c6a-985b-c1161668436c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "node_type": "1", "metadata": {}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8284800-a627-47ba-a595-2bf621e5bfcc", "node_type": "1", "metadata": {}, "hash": "dabff93b9974353d35367350c249525c653c74d731a2a10e886725addc7f780a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4077c693-e2f5-4093-ac5a-85f8435a7afc", "node_type": "1", "metadata": {}, "hash": "69f4124539afdbe1eca9ede5a912589325fa6c4005a9412433e1f9896856b485", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "node_type": "1", "metadata": {}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "class_name": "RelatedNodeInfo"}}, "hash": "209904f1692154051c100b4049308b5c191b4ac3f8391cf6d218a0a016c61b72", "text": "md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can play with this notebook for more details.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Using guidance to improve the robustness of our sub-question query engine.\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides a toolkit of advanced query engines for tackling different use-cases.\nSeveral relies on structured output in intermediate steps.\nWe can use guidance to improve the robustness of these query engines, by making sure the\nintermediate response has the expected structure (so that they can be parsed correctly to a structured object).\n\nAs an example, we implement a `GuidanceQuestionGenerator` that can be plugged into a `SubQuestionQueryEngine` to make it more robust than using the default setting.\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/Using guidance to improve the robustness of our sub-question query engine.\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.question_gen.guidance_generator import GuidanceQuestionGenerator\nfrom guidance.llms import OpenAI as GuidanceOpenAI\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/define guidance based question generator\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.", "start_char_idx": 1704, "end_char_idx": 3559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4077c693-e2f5-4093-ac5a-85f8435a7afc": {"__data__": {"id_": "4077c693-e2f5-4093-ac5a-85f8435a7afc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "node_type": "1", "metadata": {}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d676ddeb-3565-4c6a-985b-c1161668436c", "node_type": "1", "metadata": {}, "hash": "209904f1692154051c100b4049308b5c191b4ac3f8391cf6d218a0a016c61b72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff83f3c5-dab5-4b42-9742-1341cd52b835", "node_type": "1", "metadata": {}, "hash": "478aa8a2805f16cc6c6d04051b73f315293aae6b956e3f03a0e4883bb87f8236", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "node_type": "1", "metadata": {}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "class_name": "RelatedNodeInfo"}}, "hash": "69f4124539afdbe1eca9ede5a912589325fa6c4005a9412433e1f9896856b485", "text": "md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquestion_gen = GuidanceQuestionGenerator.from_defaults(guidance_llm=GuidanceOpenAI('text-davinci-003'), verbose=False)\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/define query engine tools\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine_tools = .\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/construct sub-question query engine\nLinks: \nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ns_engine = SubQuestionQueryEngine.from_defaults(\n    question_gen=question_gen  # use guidance based question_gen defined above\n    query_engine_tools=query_engine_tools, \n)\n\nFile Name: Docs\\community\\integrations\\guidance.md\nContent Type: text\nHeader Path: Guidance/Structured Output/construct sub-question query engine\nfile_path: Docs\\community\\integrations\\guidance.md\nfile_name: guidance.md\nfile_type: None\nfile_size: 4223\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee this notebook for more details.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff83f3c5-dab5-4b42-9742-1341cd52b835": {"__data__": {"id_": "ff83f3c5-dab5-4b42-9742-1341cd52b835", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "node_type": "1", "metadata": {}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4077c693-e2f5-4093-ac5a-85f8435a7afc", "node_type": "1", "metadata": {}, "hash": "69f4124539afdbe1eca9ede5a912589325fa6c4005a9412433e1f9896856b485", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "node_type": "1", "metadata": {}, "hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "class_name": "RelatedNodeInfo"}}, "hash": "478aa8a2805f16cc6c6d04051b73f315293aae6b956e3f03a0e4883bb87f8236", "text": "File Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens\nLinks: \nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis page covers how to use TruLens to evaluate and track LLM apps built on Llama-Index.", "start_char_idx": 5070, "end_char_idx": 5478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8210398-936d-4da0-afaa-cafbda9d4703": {"__data__": {"id_": "d8210398-936d-4da0-afaa-cafbda9d4703", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "node_type": "1", "metadata": {}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8aad2f35-e698-41fa-87ed-7173b63d324d", "node_type": "1", "metadata": {}, "hash": "9d82e3b9b8509be9bd173a82634199b269e2b67ddf26abd596dfa190bb76d086", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "node_type": "1", "metadata": {}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "class_name": "RelatedNodeInfo"}}, "hash": "ea0702b0772383062e345b5a8ac82c92e944a0c12fb15148ac644c2ecc055a56", "text": "File Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?\nLinks: \nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTruLens is an opensource package that provides instrumentation and evaluation tools for large language model (LLM) based applications. This includes feedback function evaluations of relevance, sentiment and more, plus in-depth tracing including cost and latency.\n\n!TruLens Architecture\n\nAs you iterate on new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up. You'll also be able to view evaluations at a record level, and explore the app metadata for each record.\n\nFile Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?/Installation and Setup\nLinks: \nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAdding TruLens is simple, just install it from pypi!", "start_char_idx": 0, "end_char_idx": 1291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8aad2f35-e698-41fa-87ed-7173b63d324d": {"__data__": {"id_": "8aad2f35-e698-41fa-87ed-7173b63d324d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "node_type": "1", "metadata": {}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8210398-936d-4da0-afaa-cafbda9d4703", "node_type": "1", "metadata": {}, "hash": "ea0702b0772383062e345b5a8ac82c92e944a0c12fb15148ac644c2ecc055a56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29f19384-7403-42eb-b681-19b889439afb", "node_type": "1", "metadata": {}, "hash": "ea3727cffc782d6ab1f454e1740abddc701d3b2520184b9bf18b5f6597c30a45", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "node_type": "1", "metadata": {}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "class_name": "RelatedNodeInfo"}}, "hash": "9d82e3b9b8509be9bd173a82634199b269e2b67ddf26abd596dfa190bb76d086", "text": "File Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?/Installation and Setup\nLinks: \nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\npip install trulens-eval\n\nFile Name: Docs\\community\\integrations\\trulens.md\nContent Type: code\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?/Installation and Setup\nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom trulens_eval import TruLlama\n\n```\n\nFile Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?/Try it out!\nLinks: (link_text: Open In Colab, link_url: fhttps://colab.research.google.com/github/truera/trulens/blob/google-colab/trulens_eval/examples/colab/quickstarts/llama_index_quickstart_colab.ipynb)\nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllama_index_quickstart.ipynb\n!", "start_char_idx": 1293, "end_char_idx": 2647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29f19384-7403-42eb-b681-19b889439afb": {"__data__": {"id_": "29f19384-7403-42eb-b681-19b889439afb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "node_type": "1", "metadata": {}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8aad2f35-e698-41fa-87ed-7173b63d324d", "node_type": "1", "metadata": {}, "hash": "9d82e3b9b8509be9bd173a82634199b269e2b67ddf26abd596dfa190bb76d086", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5302d647-66eb-4181-83fb-4b6b2a0f3ffd", "node_type": "1", "metadata": {}, "hash": "60ae2fcfacb8a40b02946a7d4e114ab0e8caae9b73246fd17f912a7c52dc1327", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "node_type": "1", "metadata": {}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "class_name": "RelatedNodeInfo"}}, "hash": "ea3727cffc782d6ab1f454e1740abddc701d3b2520184b9bf18b5f6597c30a45", "text": "[Open In Colab](https://colab.research.google.com/github/truera/trulens/blob/google-colab/trulens_eval/examples/colab/quickstarts/llama_index_quickstart_colab.ipynb)\n\nFile Name: Docs\\community\\integrations\\trulens.md\nContent Type: text\nHeader Path: Evaluating and Tracking with TruLens/What is TruLens?/Read more\nfile_path: Docs\\community\\integrations\\trulens.md\nfile_name: trulens.md\nfile_type: None\nfile_size: 1688\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* Build and Evaluate LLM Apps with LlamaIndex and TruLens\n\n* trulens.org\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides both Tool abstractions for a Langchain agent as well as a memory module.\n\nThe API reference of the Tool abstractions + memory modules are here.\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use any data loader as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex allows you to use any data loader within the LlamaIndex core repo or in LlamaHub as an \"on-demand\" data query Tool within a LangChain agent.", "start_char_idx": 2647, "end_char_idx": 4275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5302d647-66eb-4181-83fb-4b6b2a0f3ffd": {"__data__": {"id_": "5302d647-66eb-4181-83fb-4b6b2a0f3ffd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "node_type": "1", "metadata": {}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29f19384-7403-42eb-b681-19b889439afb", "node_type": "1", "metadata": {}, "hash": "ea3727cffc782d6ab1f454e1740abddc701d3b2520184b9bf18b5f6597c30a45", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2", "node_type": "1", "metadata": {}, "hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "class_name": "RelatedNodeInfo"}}, "hash": "60ae2fcfacb8a40b02946a7d4e114ab0e8caae9b73246fd17f912a7c52dc1327", "text": "The Tool will 1) load data using the data loader, 2) index the data, and 3) query the data and return the response in an ad-hoc manner.\n\n**Resources**\n- OnDemandLoaderTool Tutorial\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides Tool abstractions so that you can use a LlamaIndex query engine along with a Langchain agent.", "start_char_idx": 4277, "end_char_idx": 4957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f911b85-4221-47f6-a216-26ec753f3446": {"__data__": {"id_": "2f911b85-4221-47f6-a216-26ec753f3446", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10810e34-526c-413d-98e3-255cac0f8ee1", "node_type": "1", "metadata": {}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "436fd2be-6597-4d79-896d-58c726d0f4f0", "node_type": "1", "metadata": {}, "hash": "d480d39d5ef1c6bf263d11bfda71f9478f5bcc6de211998f99b14413cf6d14e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10810e34-526c-413d-98e3-255cac0f8ee1", "node_type": "1", "metadata": {}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "class_name": "RelatedNodeInfo"}}, "hash": "1db0515f685e963440fac925e697ffefedce17c950798c649fa11fb98c411f92", "text": "For instance, you can choose to create a \"Tool\" from an `QueryEngine` directly as follows:\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool\n\ntool_config = IndexToolConfig(\n    query_engine=query_engine, \n    name=f\"Vector Index\",\n    description=f\"useful for when you want to answer queries about X\",\n    tool_kwargs={\"return_direct\": True}\n)\n\ntool = LlamaIndexTool.from_tool_config(tool_config)\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to provide a `LlamaToolkit`:\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntoolkit = LlamaToolkit(\n    index_configs=index_configs,", "start_char_idx": 0, "end_char_idx": 1691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "436fd2be-6597-4d79-896d-58c726d0f4f0": {"__data__": {"id_": "436fd2be-6597-4d79-896d-58c726d0f4f0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10810e34-526c-413d-98e3-255cac0f8ee1", "node_type": "1", "metadata": {}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f911b85-4221-47f6-a216-26ec753f3446", "node_type": "1", "metadata": {}, "hash": "1db0515f685e963440fac925e697ffefedce17c950798c649fa11fb98c411f92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ed0193f-b2bd-4f01-9a63-53e5d3fba802", "node_type": "1", "metadata": {}, "hash": "67cedb04a562b962d95412a5c76ba82773ec4ce271c9935b7a0b81e6cdd130f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10810e34-526c-413d-98e3-255cac0f8ee1", "node_type": "1", "metadata": {}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "class_name": "RelatedNodeInfo"}}, "hash": "d480d39d5ef1c6bf263d11bfda71f9478f5bcc6de211998f99b14413cf6d14e8", "text": ")\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSuch a toolkit can be used to create a downstream Langchain-based chat agent through\nour `create_llama_agent` and `create_llama_chat_agent` commands:\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.langchain_helpers.agents import create_llama_chat_agent\n\nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n    verbose=True\n)\n\nagent_chain.run(input=\"Query about X\")\n\nFile Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Use a query engine as a Langchain Tool\nLinks: \nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can take a look at the full tutorial notebook here.", "start_char_idx": 1692, "end_char_idx": 3270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ed0193f-b2bd-4f01-9a63-53e5d3fba802": {"__data__": {"id_": "4ed0193f-b2bd-4f01-9a63-53e5d3fba802", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10810e34-526c-413d-98e3-255cac0f8ee1", "node_type": "1", "metadata": {}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "436fd2be-6597-4d79-896d-58c726d0f4f0", "node_type": "1", "metadata": {}, "hash": "d480d39d5ef1c6bf263d11bfda71f9478f5bcc6de211998f99b14413cf6d14e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18ed6626-69f0-4378-be4f-1280d22459c6", "node_type": "1", "metadata": {}, "hash": "0494715f4b948fddb927c9b18fba19b6c186761e2c2b996ebfe6175c2bfb431b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10810e34-526c-413d-98e3-255cac0f8ee1", "node_type": "1", "metadata": {}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "class_name": "RelatedNodeInfo"}}, "hash": "67cedb04a562b962d95412a5c76ba82773ec4ce271c9935b7a0b81e6cdd130f8", "text": "File Name: Docs\\community\\integrations\\using_with_langchain.md\nContent Type: text\nHeader Path: Using with Langchain \ud83e\udd9c\ud83d\udd17/Llama Demo Notebook: Tool + Memory module\nfile_path: Docs\\community\\integrations\\using_with_langchain.md\nfile_name: using_with_langchain.md\nfile_type: None\nfile_size: 2499\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe provide another demo notebook showing how you can build a chat agent with the following components.\n- Using LlamaIndex as a generic callable tool with a Langchain agent\n- Using LlamaIndex as a memory module; this allows you to insert arbitrary amounts of conversation history with a Langchain chatbot!\n\nPlease see the notebook here.\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex offers multiple integration points with vector stores / vector databases:\n\n1. LlamaIndex can use a vector store itself as an index. Like any other index, this index can store documents and be used to answer queries.\n2. LlamaIndex can load data from vector stores, similar to any other data connector. This data can then be used within LlamaIndex data structures.\n\n(vector-store-index)=\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Using a Vector Store as an Index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex also supports different vector stores\nas the storage backend for `VectorStoreIndex`.", "start_char_idx": 3272, "end_char_idx": 5159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18ed6626-69f0-4378-be4f-1280d22459c6": {"__data__": {"id_": "18ed6626-69f0-4378-be4f-1280d22459c6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10810e34-526c-413d-98e3-255cac0f8ee1", "node_type": "1", "metadata": {}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ed0193f-b2bd-4f01-9a63-53e5d3fba802", "node_type": "1", "metadata": {}, "hash": "67cedb04a562b962d95412a5c76ba82773ec4ce271c9935b7a0b81e6cdd130f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10810e34-526c-413d-98e3-255cac0f8ee1", "node_type": "1", "metadata": {}, "hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "class_name": "RelatedNodeInfo"}}, "hash": "0494715f4b948fddb927c9b18fba19b6c186761e2c2b996ebfe6175c2bfb431b", "text": "- Chroma (`ChromaVectorStore`) Installation\n- DeepLake (`DeepLakeVectorStore`) Installation\n- Qdrant (`QdrantVectorStore`) Installation Python Client\n- Weaviate (`WeaviateVectorStore`). Installation. Python Client.\n- Pinecone (`PineconeVectorStore`). Installation/Quickstart.\n- Faiss (`FaissVectorStore`). Installation.", "start_char_idx": 5161, "end_char_idx": 5480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45e3ce5a-8dbe-4e8a-a391-cc2c29bd9f96": {"__data__": {"id_": "45e3ce5a-8dbe-4e8a-a391-cc2c29bd9f96", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "node_type": "1", "metadata": {}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b84b4357-5200-409c-bf2d-0be1fab0441f", "node_type": "1", "metadata": {}, "hash": "bb521dcd0e7d7a6bcbe8e0480877537c2fed32d07d2fc63cf25f2bda6f2f6a03", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "node_type": "1", "metadata": {}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "class_name": "RelatedNodeInfo"}}, "hash": "d1e15cbcc7cdb74330f528818cec49772b771c22c2fdf70028d0096cb17716e8", "text": "Installation/Quickstart.\n- Faiss (`FaissVectorStore`). Installation.\n- Milvus (`MilvusVectorStore`). Installation\n- Zilliz (`MilvusVectorStore`). Quickstart\n- MyScale (`MyScaleVectorStore`). Quickstart. Installation/Python Client.\n- Supabase (`SupabaseVectorStore`). Quickstart.\n- DocArray (`DocArrayHnswVectorStore`, `DocArrayInMemoryVectorStore`). Installation/Python Client.\n- MongoDB Atlas (`MongoDBAtlasVectorSearch`). [Installation/Quickstart] (https://www.mongodb.com/atlas/database).\n- Redis (`RedisVectorStore`). Installation.\n\nA detailed API reference is found here.\n\nSimilar to any other index within LlamaIndex (tree, keyword table, list), `VectorStoreIndex` can be constructed upon any collection\nof documents. We use the vector store within the index to store embeddings for the input text chunks.\n\nOnce constructed, the index can be used for querying.\n\n**Default Vector Store Index Construction/Querying**\n\nBy default, `VectorStoreIndex` uses a in-memory `SimpleVectorStore`\nthat's initialized as part of the default storage context.", "start_char_idx": 0, "end_char_idx": 1048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b84b4357-5200-409c-bf2d-0be1fab0441f": {"__data__": {"id_": "b84b4357-5200-409c-bf2d-0be1fab0441f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "node_type": "1", "metadata": {}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45e3ce5a-8dbe-4e8a-a391-cc2c29bd9f96", "node_type": "1", "metadata": {}, "hash": "d1e15cbcc7cdb74330f528818cec49772b771c22c2fdf70028d0096cb17716e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e84b17a5-8c32-44dc-8fe0-fdb1ad963e08", "node_type": "1", "metadata": {}, "hash": "8654b763b7d9d11aad3c1cf4eef968c822082fe350af0fd6c147e9b6c8e67c7f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "node_type": "1", "metadata": {}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "class_name": "RelatedNodeInfo"}}, "hash": "bb521dcd0e7d7a6bcbe8e0480877537c2fed32d07d2fc63cf25f2bda6f2f6a03", "text": "File Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Using a Vector Store as an Index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Load documents and build index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.", "start_char_idx": 1050, "end_char_idx": 2601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e84b17a5-8c32-44dc-8fe0-fdb1ad963e08": {"__data__": {"id_": "e84b17a5-8c32-44dc-8fe0-fdb1ad963e08", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "node_type": "1", "metadata": {}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b84b4357-5200-409c-bf2d-0be1fab0441f", "node_type": "1", "metadata": {}, "hash": "bb521dcd0e7d7a6bcbe8e0480877537c2fed32d07d2fc63cf25f2bda6f2f6a03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37dc6775-88ee-4f77-9ee4-e4932ea9bdec", "node_type": "1", "metadata": {}, "hash": "a4ef6e7fd00288e9fdc3848af49cdd18638687d46d761eb8ec487e15ad3f8672", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "node_type": "1", "metadata": {}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "class_name": "RelatedNodeInfo"}}, "hash": "8654b763b7d9d11aad3c1cf4eef968c822082fe350af0fd6c147e9b6c8e67c7f", "text": "md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Custom Vector Store Index Construction/Querying**\n\nWe can query over a custom vector store as follows:\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\nfrom llama_index.vector_stores import DeepLakeVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store and customize storage context\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(\n    vector_store = DeepLakeVectorStore(dataset_path=\"<dataset_path>\")\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Load documents and build index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('./paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\nFile Name: Docs\\community\\integrations\\vector_stores.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37dc6775-88ee-4f77-9ee4-e4932ea9bdec": {"__data__": {"id_": "37dc6775-88ee-4f77-9ee4-e4932ea9bdec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "node_type": "1", "metadata": {}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e84b17a5-8c32-44dc-8fe0-fdb1ad963e08", "node_type": "1", "metadata": {}, "hash": "8654b763b7d9d11aad3c1cf4eef968c822082fe350af0fd6c147e9b6c8e67c7f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "node_type": "1", "metadata": {}, "hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "class_name": "RelatedNodeInfo"}}, "hash": "a4ef6e7fd00288e9fdc3848af49cdd18638687d46d761eb8ec487e15ad3f8672", "text": "storage_context=storage_context)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBelow we show more examples of how to construct various vector stores we support.", "start_char_idx": 4280, "end_char_idx": 5171, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8d60a4c-25cf-43e9-a388-952342591041": {"__data__": {"id_": "b8d60a4c-25cf-43e9-a388-952342591041", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "node_type": "1", "metadata": {}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "277d968a-88eb-491c-a9e5-85bf55f39f11", "node_type": "1", "metadata": {}, "hash": "993ccdf411673d8eebe7b0ff5446842942c2e08c9ad02c9a7275710bcd727156", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "node_type": "1", "metadata": {}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "class_name": "RelatedNodeInfo"}}, "hash": "2feb79328dacdf8cdee63520441762c4f0e821971d6f4082aa9c91cbabfa0bc1", "text": "**Redis**\n\nFirst, start Redis-Stack (or get url from Redis provider)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocker run --name redis-vecdb -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen connect and use Redis as a vector database with LlamaIndex\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.vector_stores import RedisVectorStore\nvector_store = RedisVectorStore(\n    index_name=\"llm-project\",\n    redis_url=\"redis://localhost:6379\",\n    overwrite=True\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.", "start_char_idx": 0, "end_char_idx": 1611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "277d968a-88eb-491c-a9e5-85bf55f39f11": {"__data__": {"id_": "277d968a-88eb-491c-a9e5-85bf55f39f11", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "node_type": "1", "metadata": {}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8d60a4c-25cf-43e9-a388-952342591041", "node_type": "1", "metadata": {}, "hash": "2feb79328dacdf8cdee63520441762c4f0e821971d6f4082aa9c91cbabfa0bc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0cf3648-44c2-4393-8319-4f5a36faed00", "node_type": "1", "metadata": {}, "hash": "66086896f7512f4eaa09458e6b2643bcdadff9f6c2687c2fec37a62ddb44addc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "node_type": "1", "metadata": {}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "class_name": "RelatedNodeInfo"}}, "hash": "993ccdf411673d8eebe7b0ff5446842942c2e08c9ad02c9a7275710bcd727156", "text": "md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis can be used with the `VectorStoreIndex` to provide a query interface for retrieval, querying, deleting, persisting the index, and more.\n\n**DeepLake**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Query index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport os\nimport getpath\nfrom llama_index.vector_stores import DeepLakeVectorStore\n\nos.environ[\"OPENAI_API_KEY\"] = getpath.getpath(\"OPENAI_API_KEY: \")\nos.environ[\"ACTIVELOOP_TOKEN\"] = getpath.getpath(\"ACTIVELOOP_TOKEN: \")\ndataset_path = \"hub://adilkhan/paul_graham_essay\"\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.", "start_char_idx": 1583, "end_char_idx": 3143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0cf3648-44c2-4393-8319-4f5a36faed00": {"__data__": {"id_": "f0cf3648-44c2-4393-8319-4f5a36faed00", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "node_type": "1", "metadata": {}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "277d968a-88eb-491c-a9e5-85bf55f39f11", "node_type": "1", "metadata": {}, "hash": "993ccdf411673d8eebe7b0ff5446842942c2e08c9ad02c9a7275710bcd727156", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9886d89a-600e-4752-b244-e82a88999cf6", "node_type": "1", "metadata": {}, "hash": "308eaec48ee03d3d79475876f0fd39956265ee9ec5d99a94ec3ee7ee76e12727", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "node_type": "1", "metadata": {}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "class_name": "RelatedNodeInfo"}}, "hash": "66086896f7512f4eaa09458e6b2643bcdadff9f6c2687c2fec37a62ddb44addc", "text": "md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Faiss**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport faiss\nfrom llama_index.vector_stores import FaissVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/create faiss index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nd = 1536\nfaiss_index = faiss.IndexFlatL2(d)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = FaissVectorStore(faiss_index)\n\n.\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/persist() takes in optional arg persist_path. If none give, will use default paths.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9886d89a-600e-4752-b244-e82a88999cf6": {"__data__": {"id_": "9886d89a-600e-4752-b244-e82a88999cf6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "node_type": "1", "metadata": {}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0cf3648-44c2-4393-8319-4f5a36faed00", "node_type": "1", "metadata": {}, "hash": "66086896f7512f4eaa09458e6b2643bcdadff9f6c2687c2fec37a62ddb44addc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "node_type": "1", "metadata": {}, "hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "class_name": "RelatedNodeInfo"}}, "hash": "308eaec48ee03d3d79475876f0fd39956265ee9ec5d99a94ec3ee7ee76e12727", "text": "If none give, will use default paths.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context.persist()\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/persist() takes in optional arg persist_path. If none give, will use default paths.", "start_char_idx": 4629, "end_char_idx": 5099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d056ad7a-d759-48e5-9bb4-fe59e7230f79": {"__data__": {"id_": "d056ad7a-d759-48e5-9bb4-fe59e7230f79", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "392fdffd-9b49-404d-888c-bba04b0af715", "node_type": "1", "metadata": {}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "032b4796-6f12-4c57-95de-5e6287b9b309", "node_type": "1", "metadata": {}, "hash": "7227483d02e6c60057be6156405b43444290a5d765ed62e471a3a5bba0434ffc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "392fdffd-9b49-404d-888c-bba04b0af715", "node_type": "1", "metadata": {}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "class_name": "RelatedNodeInfo"}}, "hash": "e459f789e94432cb2c21d094b88585fc1a98b730ff6a8c8b129045c10f1d8ce5", "text": "If none give, will use default paths.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Weaviate**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/persist() takes in optional arg persist_path. If none give, will use default paths.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport weaviate\nfrom llama_index.vector_stores import WeaviateVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/creating a Weaviate client\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresource_owner_config = weaviate.AuthClientPassword(\n    username=\"<username>\",\n    password=\"<password>\",\n)\nclient = weaviate.Client(\n    \"https://<cluster-id>.semi.network/\", auth_client_secret=resource_owner_config\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.", "start_char_idx": 0, "end_char_idx": 1537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "032b4796-6f12-4c57-95de-5e6287b9b309": {"__data__": {"id_": "032b4796-6f12-4c57-95de-5e6287b9b309", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "392fdffd-9b49-404d-888c-bba04b0af715", "node_type": "1", "metadata": {}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d056ad7a-d759-48e5-9bb4-fe59e7230f79", "node_type": "1", "metadata": {}, "hash": "e459f789e94432cb2c21d094b88585fc1a98b730ff6a8c8b129045c10f1d8ce5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "726bab5c-341d-4653-a1b7-568794b28cdb", "node_type": "1", "metadata": {}, "hash": "ca57768a24dc6e495f4ab4cec3ea76f0eff05255c9cea742a950784bdee771de", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "392fdffd-9b49-404d-888c-bba04b0af715", "node_type": "1", "metadata": {}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "class_name": "RelatedNodeInfo"}}, "hash": "7227483d02e6c60057be6156405b43444290a5d765ed62e471a3a5bba0434ffc", "text": "md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = WeaviateVectorStore(weaviate_client=client)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Pinecone**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport pinecone\nfrom llama_index.vector_stores import PineconeVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Creating a Pinecone index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\napi_key = \"api_key\"\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\npinecone.create_index(\n    \"quickstart\",\n    dimension=1536,\n    metric=\"euclidean\",\n    pod_type=\"p1\"\n)\nindex = pinecone.Index(\"quickstart\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.", "start_char_idx": 1509, "end_char_idx": 3124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "726bab5c-341d-4653-a1b7-568794b28cdb": {"__data__": {"id_": "726bab5c-341d-4653-a1b7-568794b28cdb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "392fdffd-9b49-404d-888c-bba04b0af715", "node_type": "1", "metadata": {}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "032b4796-6f12-4c57-95de-5e6287b9b309", "node_type": "1", "metadata": {}, "hash": "7227483d02e6c60057be6156405b43444290a5d765ed62e471a3a5bba0434ffc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b703bcbb-235e-4a9a-adae-ae27ffc8b7b2", "node_type": "1", "metadata": {}, "hash": "377da729cc7e5233dc3c06140aed8e949384c1f3893202413c3615cad981555b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "392fdffd-9b49-404d-888c-bba04b0af715", "node_type": "1", "metadata": {}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "class_name": "RelatedNodeInfo"}}, "hash": "ca57768a24dc6e495f4ab4cec3ea76f0eff05255c9cea742a950784bdee771de", "text": "Index(\"quickstart\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/reuse pinecone indexes)\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmetadata_filters = {\"title\": \"paul_graham_essay\"}\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = PineconeVectorStore(\n    pinecone_index=index,\n    metadata_filters=metadata_filters\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Qdrant**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport qdrant_client\nfrom llama_index.", "start_char_idx": 3050, "end_char_idx": 4656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b703bcbb-235e-4a9a-adae-ae27ffc8b7b2": {"__data__": {"id_": "b703bcbb-235e-4a9a-adae-ae27ffc8b7b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "392fdffd-9b49-404d-888c-bba04b0af715", "node_type": "1", "metadata": {}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "726bab5c-341d-4653-a1b7-568794b28cdb", "node_type": "1", "metadata": {}, "hash": "ca57768a24dc6e495f4ab4cec3ea76f0eff05255c9cea742a950784bdee771de", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "392fdffd-9b49-404d-888c-bba04b0af715", "node_type": "1", "metadata": {}, "hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "class_name": "RelatedNodeInfo"}}, "hash": "377da729cc7e5233dc3c06140aed8e949384c1f3893202413c3615cad981555b", "text": "vector_stores import QdrantVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Creating a Qdrant vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.", "start_char_idx": 4656, "end_char_idx": 4924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96f54233-b16b-40fe-aae0-385b9a91fc34": {"__data__": {"id_": "96f54233-b16b-40fe-aae0-385b9a91fc34", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64e62721-99d2-4653-89a4-327bce7cb75b", "node_type": "1", "metadata": {}, "hash": "01313ab8cdfdfc9475fb755f41c81aee633f4976305f0aff0bdf06376688116c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03ab4346-2c32-4920-a14a-79b02cfe75dd", "node_type": "1", "metadata": {}, "hash": "7a588769e2e28e14af6c27d05e05268fc1546b42e95dc24d62eb8fae6ee40581", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64e62721-99d2-4653-89a4-327bce7cb75b", "node_type": "1", "metadata": {}, "hash": "01313ab8cdfdfc9475fb755f41c81aee633f4976305f0aff0bdf06376688116c", "class_name": "RelatedNodeInfo"}}, "hash": "ab2b63d08d97ca1d6781cd5ef4b71cd33b5920136e6f3eeb7ae34950942cc1cc", "text": "md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nclient = qdrant_client.QdrantClient(\n    host=\"<qdrant-host>\",\n    api_key=\"<qdrant-api-key>\",\n    https=True\n)\ncollection_name = \"paul_graham\"\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = QdrantVectorStore(\n    client=client,\n    collection_name=collection_name,\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Chroma**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport chromadb\nfrom llama_index.vector_stores import ChromaVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/By default, Chroma will operate purely in-memory.", "start_char_idx": 0, "end_char_idx": 1668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03ab4346-2c32-4920-a14a-79b02cfe75dd": {"__data__": {"id_": "03ab4346-2c32-4920-a14a-79b02cfe75dd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64e62721-99d2-4653-89a4-327bce7cb75b", "node_type": "1", "metadata": {}, "hash": "01313ab8cdfdfc9475fb755f41c81aee633f4976305f0aff0bdf06376688116c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96f54233-b16b-40fe-aae0-385b9a91fc34", "node_type": "1", "metadata": {}, "hash": "ab2b63d08d97ca1d6781cd5ef4b71cd33b5920136e6f3eeb7ae34950942cc1cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdb7123d-fc53-41b2-8a65-ef26675c6dfc", "node_type": "1", "metadata": {}, "hash": "23ed4bee7a44176a65d2eab7aab5ff6df16efe12d5a92e651e8b84f1624af6c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64e62721-99d2-4653-89a4-327bce7cb75b", "node_type": "1", "metadata": {}, "hash": "01313ab8cdfdfc9475fb755f41c81aee633f4976305f0aff0bdf06376688116c", "class_name": "RelatedNodeInfo"}}, "hash": "7a588769e2e28e14af6c27d05e05268fc1546b42e95dc24d62eb8fae6ee40581", "text": "Links: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchroma_client = chromadb.Client()\nchroma_collection = chroma_client.create_collection(\"quickstart\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = ChromaVectorStore(\n    chroma_collection=chroma_collection,\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Milvus**\n\n- Milvus Index offers the ability to store both Documents and their embeddings. Documents are limited to the predefined Document attributes and does not include metadata.", "start_char_idx": 1669, "end_char_idx": 2934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fdb7123d-fc53-41b2-8a65-ef26675c6dfc": {"__data__": {"id_": "fdb7123d-fc53-41b2-8a65-ef26675c6dfc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64e62721-99d2-4653-89a4-327bce7cb75b", "node_type": "1", "metadata": {}, "hash": "01313ab8cdfdfc9475fb755f41c81aee633f4976305f0aff0bdf06376688116c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03ab4346-2c32-4920-a14a-79b02cfe75dd", "node_type": "1", "metadata": {}, "hash": "7a588769e2e28e14af6c27d05e05268fc1546b42e95dc24d62eb8fae6ee40581", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64e62721-99d2-4653-89a4-327bce7cb75b", "node_type": "1", "metadata": {}, "hash": "01313ab8cdfdfc9475fb755f41c81aee633f4976305f0aff0bdf06376688116c", "class_name": "RelatedNodeInfo"}}, "hash": "23ed4bee7a44176a65d2eab7aab5ff6df16efe12d5a92e651e8b84f1624af6c3", "text": "Documents are limited to the predefined Document attributes and does not include metadata.\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport pymilvus\nfrom llama_index.vector_stores import MilvusVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = MilvusVectorStore(\n    host='localhost',\n    port=19530,\n    overwrite='True'\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Note**: `MilvusVectorStore` depends on the `pymilvus` library.\nUse `pip install pymilvus` if not already installed.\nIf you get stuck at building wheel for `grpcio`, check if you are using python 3.11\n(there's a known issue: https://github.com/milvus-io/pymilvus/issues/1308)\nand try downgrading.\n\n**Zilliz**\n\n- Zilliz Cloud (hosted version of Milvus) uses the Milvus Index with some extra arguments.", "start_char_idx": 2844, "end_char_idx": 4541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe988892-00ed-40d6-bd55-5575565ad14c": {"__data__": {"id_": "fe988892-00ed-40d6-bd55-5575565ad14c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "node_type": "1", "metadata": {}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb240758-46ff-40f9-99ef-1f67ac5345fc", "node_type": "1", "metadata": {}, "hash": "1c200eeb8fdccb8e1bdde273e532d69ec5dbd0ab3c48217656fa5e22e2fe05e2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "node_type": "1", "metadata": {}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "class_name": "RelatedNodeInfo"}}, "hash": "bbb06337933b0800e1f3708b8aacdb048bf08a0ac1d4cfb1d45f4da020160740", "text": "File Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport pymilvus\nfrom llama_index.vector_stores import MilvusVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = MilvusVectorStore(\n    host='foo.vectordb.zillizcloud.com',\n    port=403,\n    user=\"db_admin\",\n    password=\"foo\",\n    use_secure=True,\n    overwrite='True'\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Note**: `MilvusVectorStore` depends on the `pymilvus` library.\nUse `pip install pymilvus` if not already installed.\nIf you get stuck at building wheel for `grpcio`, check if you are using python 3.11\n(there's a known issue: https://github.com/milvus-io/pymilvus/issues/1308)\nand try downgrading.\n\n**MyScale**\n\nFile Name: Docs\\community\\integrations\\vector_stores.", "start_char_idx": 0, "end_char_idx": 1648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb240758-46ff-40f9-99ef-1f67ac5345fc": {"__data__": {"id_": "cb240758-46ff-40f9-99ef-1f67ac5345fc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "node_type": "1", "metadata": {}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe988892-00ed-40d6-bd55-5575565ad14c", "node_type": "1", "metadata": {}, "hash": "bbb06337933b0800e1f3708b8aacdb048bf08a0ac1d4cfb1d45f4da020160740", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a96841ca-0274-4186-bdd7-2dd8aecf02a9", "node_type": "1", "metadata": {}, "hash": "33abbfd1c0bbede017f316316b87aefa30dea2198479bc1945783c243ce44033", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "node_type": "1", "metadata": {}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "class_name": "RelatedNodeInfo"}}, "hash": "1c200eeb8fdccb8e1bdde273e532d69ec5dbd0ab3c48217656fa5e22e2fe05e2", "text": "**MyScale**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport clickhouse_connect\nfrom llama_index.vector_stores import MyScaleVectorStore\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Creating a MyScale client\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nclient = clickhouse_connect.get_client(\n    host='YOUR_CLUSTER_HOST',\n    port=8443,\n    username='YOUR_USERNAME',\n    password='YOUR_CLUSTER_PASSWORD'\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = MyScaleVectorStore(\n    myscale_client=client\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.", "start_char_idx": 1582, "end_char_idx": 3156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a96841ca-0274-4186-bdd7-2dd8aecf02a9": {"__data__": {"id_": "a96841ca-0274-4186-bdd7-2dd8aecf02a9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "node_type": "1", "metadata": {}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb240758-46ff-40f9-99ef-1f67ac5345fc", "node_type": "1", "metadata": {}, "hash": "1c200eeb8fdccb8e1bdde273e532d69ec5dbd0ab3c48217656fa5e22e2fe05e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0f157a3-5025-4f7a-9fd0-f00ef7b15858", "node_type": "1", "metadata": {}, "hash": "d544b353d7a6da68e88caae5e9e37397508cd71695b34b0ce380081e97672de9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "node_type": "1", "metadata": {}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "class_name": "RelatedNodeInfo"}}, "hash": "33abbfd1c0bbede017f316316b87aefa30dea2198479bc1945783c243ce44033", "text": "md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**DocArray**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.vector_stores import (\n    DocArrayHnswVectorStore, \n    DocArrayInMemoryVectorStore,\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = DocArrayHnswVectorStore(work_dir='hnsw_index')\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/alternatively, construct the in-memory vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = DocArrayInMemoryVectorStore()\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/alternatively, construct the in-memory vector store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.", "start_char_idx": 3128, "end_char_idx": 4795, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0f157a3-5025-4f7a-9fd0-f00ef7b15858": {"__data__": {"id_": "d0f157a3-5025-4f7a-9fd0-f00ef7b15858", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "node_type": "1", "metadata": {}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a96841ca-0274-4186-bdd7-2dd8aecf02a9", "node_type": "1", "metadata": {}, "hash": "33abbfd1c0bbede017f316316b87aefa30dea2198479bc1945783c243ce44033", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e", "node_type": "1", "metadata": {}, "hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "class_name": "RelatedNodeInfo"}}, "hash": "d544b353d7a6da68e88caae5e9e37397508cd71695b34b0ce380081e97672de9", "text": "md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**MongoDBAtlas**\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Provide URI to constructor,", "start_char_idx": 4795, "end_char_idx": 5101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "775d675f-dbdf-42ee-9d5e-c725e9b7fdc5": {"__data__": {"id_": "775d675f-dbdf-42ee-9d5e-c725e9b7fdc5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "node_type": "1", "metadata": {}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "336c0ae4-4ee7-43a9-b2d5-94924f8947ed", "node_type": "1", "metadata": {}, "hash": "21b88c3cb34c57a0a4b39919578bff95dbf4e366148aa5263342c1bf05566864", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "node_type": "1", "metadata": {}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "class_name": "RelatedNodeInfo"}}, "hash": "cb0d6d45f66737149a5ca1b44d7888c1dccf1a147417926039429a7710899974", "text": "md\nContent Type: text\nHeader Path: Using Vector Stores/Provide URI to constructor, or use environment variable\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport pymongo\nfrom llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\nfrom llama_index.indices.vector_store.base import VectorStoreIndex\nfrom llama_index.storage.storage_context import StorageContext\nfrom llama_index.readers.file.base import SimpleDirectoryReader\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/mongo_uri = os.environ[\"MONGO_URI\"]\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmongo_uri = \"mongodb+srv://<username>:<password>@<host>?retryWrites=true&w=majority\"\nmongodb_client = pymongo.MongoClient(mongo_uri)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct store\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstore = MongoDBAtlasVectorSearch(mongodb_client)\nstorage_context = StorageContext.from_defaults(vector_store=store)\nuber_docs = SimpleDirectoryReader(input_files=[\"./data/10k/uber_2021.pdf\"]).load_data()\n\nFile Name: Docs\\community\\integrations\\vector_stores.", "start_char_idx": 0, "end_char_idx": 1694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "336c0ae4-4ee7-43a9-b2d5-94924f8947ed": {"__data__": {"id_": "336c0ae4-4ee7-43a9-b2d5-94924f8947ed", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "node_type": "1", "metadata": {}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "775d675f-dbdf-42ee-9d5e-c725e9b7fdc5", "node_type": "1", "metadata": {}, "hash": "cb0d6d45f66737149a5ca1b44d7888c1dccf1a147417926039429a7710899974", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fac2a33-b3b9-43a6-85e5-5557ebc23269", "node_type": "1", "metadata": {}, "hash": "c2df17712f95abf8988588650f524e344eb8c11077a6964ad19f1c3ad038e77f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "node_type": "1", "metadata": {}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "class_name": "RelatedNodeInfo"}}, "hash": "21b88c3cb34c57a0a4b39919578bff95dbf4e366148aa5263342c1bf05566864", "text": "pdf\"]).load_data()\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(uber_docs, storage_context=storage_context)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/construct index\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nExample notebooks can be found here.\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Loading Data from Vector Stores using Data Connector\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports loading data from the following sources. See Data Connectors for more details and API documentation.\n\nChroma stores both documents and vectors.", "start_char_idx": 1621, "end_char_idx": 2978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fac2a33-b3b9-43a6-85e5-5557ebc23269": {"__data__": {"id_": "5fac2a33-b3b9-43a6-85e5-5557ebc23269", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "node_type": "1", "metadata": {}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "336c0ae4-4ee7-43a9-b2d5-94924f8947ed", "node_type": "1", "metadata": {}, "hash": "21b88c3cb34c57a0a4b39919578bff95dbf4e366148aa5263342c1bf05566864", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86135e2e-a847-4de5-b87e-868ac0f17e97", "node_type": "1", "metadata": {}, "hash": "cc60ce768021769d3476b0c39d499435d23ca6e6bdcddc0405dca4b94951e774", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "node_type": "1", "metadata": {}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "class_name": "RelatedNodeInfo"}}, "hash": "c2df17712f95abf8988588650f524e344eb8c11077a6964ad19f1c3ad038e77f", "text": "See Data Connectors for more details and API documentation.\n\nChroma stores both documents and vectors. This is an example of how to use Chroma:\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/Loading Data from Vector Stores using Data Connector\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.readers.chroma import ChromaReader\nfrom llama_index.indices import ListIndex\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/This requires a collection name and a persist directory.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nreader = ChromaReader(\n    collection_name=\"chroma_collection\",\n    persist_directory=\"examples/data_connectors/chroma_collection\"\n)\n\nquery_vector=[n1, n2, n3, ...]\n\ndocuments = reader.load_data(collection_name=\"demo\", query_vector=query_vector, limit=5)\nindex = ListIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"<query_text>\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/This requires a collection name and a persist directory.\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQdrant also stores both documents and vectors.", "start_char_idx": 2876, "end_char_idx": 4721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86135e2e-a847-4de5-b87e-868ac0f17e97": {"__data__": {"id_": "86135e2e-a847-4de5-b87e-868ac0f17e97", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "node_type": "1", "metadata": {}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fac2a33-b3b9-43a6-85e5-5557ebc23269", "node_type": "1", "metadata": {}, "hash": "c2df17712f95abf8988588650f524e344eb8c11077a6964ad19f1c3ad038e77f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "node_type": "1", "metadata": {}, "hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "class_name": "RelatedNodeInfo"}}, "hash": "cc60ce768021769d3476b0c39d499435d23ca6e6bdcddc0405dca4b94951e774", "text": "This is an example of how to use Qdrant:\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/This requires a collection name and a persist directory.", "start_char_idx": 4722, "end_char_idx": 4928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f631b18-8411-4438-9307-c9038c6fc816": {"__data__": {"id_": "1f631b18-8411-4438-9307-c9038c6fc816", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73c5cb41-444a-497b-9712-8883e99a63c9", "node_type": "1", "metadata": {}, "hash": "76ee8df3cb51834a2cb3f3a660c7e9a907fb1b8a92a406410183bc869c10f0c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a662638-6ad4-494d-8676-f6274686a19f", "node_type": "1", "metadata": {}, "hash": "48a489cab05ed0363eb88169192662b95af93622b8e708d16da60f6ad9371d32", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "73c5cb41-444a-497b-9712-8883e99a63c9", "node_type": "1", "metadata": {}, "hash": "76ee8df3cb51834a2cb3f3a660c7e9a907fb1b8a92a406410183bc869c10f0c7", "class_name": "RelatedNodeInfo"}}, "hash": "f85704546583957a5f9de63218c93038650fcb0e457b7dff43147ca345277183", "text": "Links: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.readers.qdrant import QdrantReader\n\nreader = QdrantReader(host=\"localhost\")\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/query_vector = [0.3, 0.3, 0.3, 0.3, .]\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_vector = [n1, n2, n3, .]\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/for more details\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = reader.load_data(collection_name=\"demo\", query_vector=query_vector, limit=5)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/for more details\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNOTE: Since Weaviate can store a hybrid of document and vector objects,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a662638-6ad4-494d-8676-f6274686a19f": {"__data__": {"id_": "5a662638-6ad4-494d-8676-f6274686a19f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73c5cb41-444a-497b-9712-8883e99a63c9", "node_type": "1", "metadata": {}, "hash": "76ee8df3cb51834a2cb3f3a660c7e9a907fb1b8a92a406410183bc869c10f0c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f631b18-8411-4438-9307-c9038c6fc816", "node_type": "1", "metadata": {}, "hash": "f85704546583957a5f9de63218c93038650fcb0e457b7dff43147ca345277183", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "851aefec-48d1-4bdc-b4ed-ff3f727f0f0a", "node_type": "1", "metadata": {}, "hash": "15948ee16243e8fc86e52c250ac2e877fef28351ccfcccde4f1a9f468b48b96b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "73c5cb41-444a-497b-9712-8883e99a63c9", "node_type": "1", "metadata": {}, "hash": "76ee8df3cb51834a2cb3f3a660c7e9a907fb1b8a92a406410183bc869c10f0c7", "class_name": "RelatedNodeInfo"}}, "hash": "48a489cab05ed0363eb88169192662b95af93622b8e708d16da60f6ad9371d32", "text": "the user may either choose to explicitly specify `class_name` and `properties` in order to query documents, or they may choose to specify a raw GraphQL query. See below for usage.\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/1) load data using class_name and properties\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = reader.load_data(\n    class_name=\"<class_name>\",\n    properties=[\"property1\", \"property2\", \"...\"],\n    separate_documents=True\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/2) example GraphQL query\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery = \"\"\"\n{\n    Get {\n        <class_name> {\n            <property1>\n            <property2>\n        }\n    }\n}\n\"\"\"\n\ndocuments = reader.load_data(graphql_query=query, separate_documents=True)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/2) example GraphQL query\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNOTE: Both Pinecone and Faiss data loaders assume that the respective data sources only store vectors; text content is stored elsewhere. Therefore, both data loaders require that the user specifies an `id_to_text_map` in the load_data call.", "start_char_idx": 1545, "end_char_idx": 3363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "851aefec-48d1-4bdc-b4ed-ff3f727f0f0a": {"__data__": {"id_": "851aefec-48d1-4bdc-b4ed-ff3f727f0f0a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73c5cb41-444a-497b-9712-8883e99a63c9", "node_type": "1", "metadata": {}, "hash": "76ee8df3cb51834a2cb3f3a660c7e9a907fb1b8a92a406410183bc869c10f0c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a662638-6ad4-494d-8676-f6274686a19f", "node_type": "1", "metadata": {}, "hash": "48a489cab05ed0363eb88169192662b95af93622b8e708d16da60f6ad9371d32", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "73c5cb41-444a-497b-9712-8883e99a63c9", "node_type": "1", "metadata": {}, "hash": "76ee8df3cb51834a2cb3f3a660c7e9a907fb1b8a92a406410183bc869c10f0c7", "class_name": "RelatedNodeInfo"}}, "hash": "15948ee16243e8fc86e52c250ac2e877fef28351ccfcccde4f1a9f468b48b96b", "text": "For instance, this is an example usage of the Pinecone data loader `PineconeReader`:\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/2) example GraphQL query\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.readers.pinecone import PineconeReader\n\nreader = PineconeReader(api_key=api_key, environment=\"us-west1-gcp\")\n\nid_to_text_map = {\n    \"id1\": \"text blob 1\",\n    \"id2\": \"text blob 2\",\n}\n\nquery_vector=[n1, n2, n3, ..]\n\ndocuments = reader.load_data(\n    index_name=\"quickstart\", id_to_text_map=id_to_text_map, top_k=3, vector=query_vector, separate_documents=True\n)\n\nFile Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/2) example GraphQL query\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nExample notebooks can be found here.", "start_char_idx": 3365, "end_char_idx": 4560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f92ac4c-c144-4662-bd84-33d3ffb867bc": {"__data__": {"id_": "7f92ac4c-c144-4662-bd84-33d3ffb867bc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bf84f43-dde5-4866-89d0-bcab22576f63", "node_type": "1", "metadata": {}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a06a4f97-9432-4d71-9126-26efe752e422", "node_type": "1", "metadata": {}, "hash": "31b86f35b4100cd3b32e710fa26c9755f1d3be2401002d2f9b131d40ad218f9f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6bf84f43-dde5-4866-89d0-bcab22576f63", "node_type": "1", "metadata": {}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "class_name": "RelatedNodeInfo"}}, "hash": "1fea7a8d80596b5f2834d1b8b3827badce28d0bbb919f35aa82af867ebd24ffd", "text": "File Name: Docs\\community\\integrations\\vector_stores.md\nContent Type: text\nHeader Path: Using Vector Stores/2) example GraphQL query\nLinks: \nfile_path: Docs\\community\\integrations\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 15046\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\ncaption: Examples\nmaxdepth: 1\n---\n././examples/vector_stores/SimpleIndexDemo.ipynb\n././examples/vector_stores/SimpleIndexDemoMMR.ipynb\n././examples/vector_stores/RedisIndexDemo.ipynb\n././examples/vector_stores/QdrantIndexDemo.ipynb\n././examples/vector_stores/FaissIndexDemo.ipynb\n././examples/vector_stores/DeepLakeIndexDemo.ipynb\n././examples/vector_stores/MyScaleIndexDemo.ipynb\n././examples/vector_stores/MetalIndexDemo.ipynb\n././examples/vector_stores/WeaviateIndexDemo.ipynb\n././examples/vector_stores/OpensearchDemo.ipynb\n././examples/vector_stores/PineconeIndexDemo.ipynb\n././examples/vector_stores/ChromaIndexDemo.ipynb\n././examples/vector_stores/LanceDBIndexDemo.ipynb\n././examples/vector_stores/MilvusIndexDemo.ipynb\n././examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb\n././examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb\n././examples/vector_stores/AsyncIndexCreationDemo.ipynb\n././examples/vector_stores/SupabaseVectorIndexDemo.ipynb\n././examples/vector_stores/DocArrayHnswIndexDemo.ipynb\n././examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb\n././examples/vector_stores/MongoDBAtlasVectorSearch.ipynb\n././examples/vector_stores/postgres.ipynb\n\nFile Name: Docs\\community\\integrations.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a06a4f97-9432-4d71-9126-26efe752e422": {"__data__": {"id_": "a06a4f97-9432-4d71-9126-26efe752e422", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bf84f43-dde5-4866-89d0-bcab22576f63", "node_type": "1", "metadata": {}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f92ac4c-c144-4662-bd84-33d3ffb867bc", "node_type": "1", "metadata": {}, "hash": "1fea7a8d80596b5f2834d1b8b3827badce28d0bbb919f35aa82af867ebd24ffd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad4eaa11-b7d7-4892-bc18-6db5c9b48769", "node_type": "1", "metadata": {}, "hash": "5696f37eca9a1654661fa85075c265d797bca1445783c1a061719f666a0daf97", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6bf84f43-dde5-4866-89d0-bcab22576f63", "node_type": "1", "metadata": {}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "class_name": "RelatedNodeInfo"}}, "hash": "31b86f35b4100cd3b32e710fa26c9755f1d3be2401002d2f9b131d40ad218f9f", "text": "ipynb\n\nFile Name: Docs\\community\\integrations.md\nContent Type: text\nHeader Path: Integrations\nLinks: \nfile_path: Docs\\community\\integrations.md\nfile_name: integrations.md\nfile_type: None\nfile_size: 383\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex has a number of community integrations, from vector stores, to prompt trackers, tracers, and more!\n\nFile Name: Docs\\community\\integrations.md\nContent Type: text\nHeader Path: Integrations\nLinks: \nfile_path: Docs\\community\\integrations.md\nfile_name: integrations.md\nfile_type: None\nfile_size: 383\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nintegrations/graphsignal.md\nintegrations/guidance.md\nintegrations/trulens.md\nintegrations/chatgpt_plugins.md\nintegrations/using_with_langchain.md\nintegrations/graph_stores.md\nintegrations/vector_stores.md\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 646\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThese guide provide an overview of how to use our agent classes.\n\nFor more detailed guides on how to use specific tools, check out our tools module guides.", "start_char_idx": 1559, "end_char_idx": 2944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad4eaa11-b7d7-4892-bc18-6db5c9b48769": {"__data__": {"id_": "ad4eaa11-b7d7-4892-bc18-6db5c9b48769", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bf84f43-dde5-4866-89d0-bcab22576f63", "node_type": "1", "metadata": {}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a06a4f97-9432-4d71-9126-26efe752e422", "node_type": "1", "metadata": {}, "hash": "31b86f35b4100cd3b32e710fa26c9755f1d3be2401002d2f9b131d40ad218f9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04394911-77be-4d35-b5e7-431af7087cc3", "node_type": "1", "metadata": {}, "hash": "ce465f270ed2c87a78408aec316aab6f0773661aaf387dbada713661e45b0182", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6bf84f43-dde5-4866-89d0-bcab22576f63", "node_type": "1", "metadata": {}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "class_name": "RelatedNodeInfo"}}, "hash": "5696f37eca9a1654661fa85075c265d797bca1445783c1a061719f666a0daf97", "text": "For more detailed guides on how to use specific tools, check out our tools module guides.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\modules.md\nContent Type: code\nHeader Path: Module Guides/OpenAI Agent\nfile_path: Docs\\core_modules\\agent_modules\\agents\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 646\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/agent/openai_agent.ipynb\n/examples/agent/openai_agent_with_query_engine.ipynb\n/examples/agent/openai_agent_retrieval.ipynb\n/examples/agent/openai_agent_query_cookbook.ipynb\n/examples/agent/openai_agent_query_plan.ipynb\n/examples/agent/openai_agent_context_retrieval.ipynb\n```\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\modules.md\nContent Type: code\nHeader Path: Module Guides/ReAct Agent\nfile_path: Docs\\core_modules\\agent_modules\\agents\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 646\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/agent/react_agent_with_query_engine.ipynb\n```\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nData Agents are LLM-powered knowledge workers in LlamaIndex that can intelligently perform various tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d function. They are capable of the following:\n\n- Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured.", "start_char_idx": 2855, "end_char_idx": 4631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04394911-77be-4d35-b5e7-431af7087cc3": {"__data__": {"id_": "04394911-77be-4d35-b5e7-431af7087cc3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bf84f43-dde5-4866-89d0-bcab22576f63", "node_type": "1", "metadata": {}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad4eaa11-b7d7-4892-bc18-6db5c9b48769", "node_type": "1", "metadata": {}, "hash": "5696f37eca9a1654661fa85075c265d797bca1445783c1a061719f666a0daf97", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6bf84f43-dde5-4866-89d0-bcab22576f63", "node_type": "1", "metadata": {}, "hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "class_name": "RelatedNodeInfo"}}, "hash": "ce465f270ed2c87a78408aec316aab6f0773661aaf387dbada713661e45b0182", "text": "- Calling any external service API in a structured fashion, and processing the response + storing it for later.\n\nIn that sense, agents are a step beyond our query engines in that they can not only \"read\" from a static source of data, but can dynamically ingest and modify data from a variety of different tools.\n\nBuilding a data agent requires the following core components:\n\n- A reasoning loop\n- Tool abstractions\n\nA data agent is initialized with set of APIs, or Tools, to interact with; these APIs can be called by the agent to return information or modify state. Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool.", "start_char_idx": 4632, "end_char_idx": 5343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "469b13f6-1879-4e2d-86f4-58608a2fe361": {"__data__": {"id_": "469b13f6-1879-4e2d-86f4-58608a2fe361", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0", "node_type": "1", "metadata": {}, "hash": "6319320dc240344335701df12777a3160bcb93f3f0e66382a213fd9d52a94a16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16fce20f-be0d-4dc9-bc9c-44e1842c6e06", "node_type": "1", "metadata": {}, "hash": "9bf20cb927d30f415046eeadb4fc3dac18b2cd24298f3ae856d675cfbc2d6159", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0", "node_type": "1", "metadata": {}, "hash": "6319320dc240344335701df12777a3160bcb93f3f0e66382a213fd9d52a94a16", "class_name": "RelatedNodeInfo"}}, "hash": "77d7838cd18dbbc272c15ececc81aa73819ab4d239dc179450e6a6fd960e88f9", "text": "File Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Reasoning Loop\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe reasoning loop depends on the type of agent. We have support for the following agents: \n- OpenAI Function agent (built on top of the OpenAI Function API)\n- a ReAct agent (which works across any chat/text completion endpoint).\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Tool Abstractions\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can learn more about our Tool abstractions in our Tools section.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Blog Post\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor full details, please check out our detailed blog post.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.", "start_char_idx": 0, "end_char_idx": 1558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16fce20f-be0d-4dc9-bc9c-44e1842c6e06": {"__data__": {"id_": "16fce20f-be0d-4dc9-bc9c-44e1842c6e06", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0", "node_type": "1", "metadata": {}, "hash": "6319320dc240344335701df12777a3160bcb93f3f0e66382a213fd9d52a94a16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "469b13f6-1879-4e2d-86f4-58608a2fe361", "node_type": "1", "metadata": {}, "hash": "77d7838cd18dbbc272c15ececc81aa73819ab4d239dc179450e6a6fd960e88f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a46f800-df81-4eda-b5a0-a2244ca97e03", "node_type": "1", "metadata": {}, "hash": "6d671394c3aca276eb52e60329e4a92df1aa087a19c0e5445d401e3fc2f33a3f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0", "node_type": "1", "metadata": {}, "hash": "6319320dc240344335701df12777a3160bcb93f3f0e66382a213fd9d52a94a16", "class_name": "RelatedNodeInfo"}}, "hash": "9bf20cb927d30f415046eeadb4fc3dac18b2cd24298f3ae856d675cfbc2d6159", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nData agents can be used in the following manner (the example uses the OpenAI Function API)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.agent import OpenAIAgent\nfrom llama_index.llms import OpenAI\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/import and define tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/initialize llm\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/initialize openai agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a46f800-df81-4eda-b5a0-a2244ca97e03": {"__data__": {"id_": "5a46f800-df81-4eda-b5a0-a2244ca97e03", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0", "node_type": "1", "metadata": {}, "hash": "6319320dc240344335701df12777a3160bcb93f3f0e66382a213fd9d52a94a16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16fce20f-be0d-4dc9-bc9c-44e1842c6e06", "node_type": "1", "metadata": {}, "hash": "9bf20cb927d30f415046eeadb4fc3dac18b2cd24298f3ae856d675cfbc2d6159", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0", "node_type": "1", "metadata": {}, "hash": "6319320dc240344335701df12777a3160bcb93f3f0e66382a213fd9d52a94a16", "class_name": "RelatedNodeInfo"}}, "hash": "6d671394c3aca276eb52e60329e4a92df1aa087a19c0e5445d401e3fc2f33a3f", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/initialize openai agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee our usage pattern guide for more details.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/initialize openai agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Modules\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLearn more about our different agent types in our module guides below.\n\nAlso take a look at our tools section!", "start_char_idx": 3098, "end_char_idx": 4502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbce3433-c5aa-4437-918a-55b061bcef40": {"__data__": {"id_": "bbce3433-c5aa-4437-918a-55b061bcef40", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b7c415a-d0cb-4a25-8639-1d943d367102", "node_type": "1", "metadata": {}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f64c7dde-ee0f-44fe-9359-6c2291604a98", "node_type": "1", "metadata": {}, "hash": "ca0b8dab7c699c4f023cdd8850df82c1f4efd6ed913f527342ee05614dfe07d1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9b7c415a-d0cb-4a25-8639-1d943d367102", "node_type": "1", "metadata": {}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "class_name": "RelatedNodeInfo"}}, "hash": "98ee78e9f0350368ac64a604e0f64831becfb77be72d48a1d19d7ce56afd1170", "text": "Also take a look at our tools section!\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\root.md\nContent Type: text\nHeader Path: Data Agents/Concept/Modules\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2409\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn agent is initialized from a set of Tools. Here's an example of instantiating a ReAct\nagent from a set of Tools.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.tools import FunctionTool\nfrom llama_index.llms import OpenAI\nfrom llama_index.agent import ReActAgent\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/define sample Tool\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndef multiply(a: int,", "start_char_idx": 0, "end_char_idx": 1707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f64c7dde-ee0f-44fe-9359-6c2291604a98": {"__data__": {"id_": "f64c7dde-ee0f-44fe-9359-6c2291604a98", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b7c415a-d0cb-4a25-8639-1d943d367102", "node_type": "1", "metadata": {}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbce3433-c5aa-4437-918a-55b061bcef40", "node_type": "1", "metadata": {}, "hash": "98ee78e9f0350368ac64a604e0f64831becfb77be72d48a1d19d7ce56afd1170", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72cc8a69-5259-4f17-9dab-ef0f12c531b3", "node_type": "1", "metadata": {}, "hash": "aad49d7eb54d918d43ccb503c7af3edad1972476b0a1ae513a7ec482e3578a85", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9b7c415a-d0cb-4a25-8639-1d943d367102", "node_type": "1", "metadata": {}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "class_name": "RelatedNodeInfo"}}, "hash": "ca0b8dab7c699c4f023cdd8850df82c1f4efd6ed913f527342ee05614dfe07d1", "text": "b: int) -> int:\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return a * b\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/initialize llm\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/initialize ReAct agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent = ReActAgent.from_tools([multiply_tool], llm=llm, verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/initialize ReAct agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn agent supports both `chat` and `query` endpoints, inheriting from our `ChatEngine` and `QueryEngine` respectively.", "start_char_idx": 1708, "end_char_idx": 3164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72cc8a69-5259-4f17-9dab-ef0f12c531b3": {"__data__": {"id_": "72cc8a69-5259-4f17-9dab-ef0f12c531b3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b7c415a-d0cb-4a25-8639-1d943d367102", "node_type": "1", "metadata": {}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f64c7dde-ee0f-44fe-9359-6c2291604a98", "node_type": "1", "metadata": {}, "hash": "ca0b8dab7c699c4f023cdd8850df82c1f4efd6ed913f527342ee05614dfe07d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3050f0c7-639e-433c-a51e-360048a44846", "node_type": "1", "metadata": {}, "hash": "d3247939dcdf59c20c3b15ee78268f356c9c5bd14660b0cd9a3b0d13e5872755", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9b7c415a-d0cb-4a25-8639-1d943d367102", "node_type": "1", "metadata": {}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "class_name": "RelatedNodeInfo"}}, "hash": "aad49d7eb54d918d43ccb503c7af3edad1972476b0a1ae513a7ec482e3578a85", "text": "inheriting from our `ChatEngine` and `QueryEngine` respectively.\n\nExample usage:\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/initialize ReAct agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent.chat(\"What is 2123 * 215123\")\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Query Engine Tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIt is easy to wrap query engines as tools for an agent as well.", "start_char_idx": 3100, "end_char_idx": 3998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3050f0c7-639e-433c-a51e-360048a44846": {"__data__": {"id_": "3050f0c7-639e-433c-a51e-360048a44846", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b7c415a-d0cb-4a25-8639-1d943d367102", "node_type": "1", "metadata": {}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72cc8a69-5259-4f17-9dab-ef0f12c531b3", "node_type": "1", "metadata": {}, "hash": "aad49d7eb54d918d43ccb503c7af3edad1972476b0a1ae513a7ec482e3578a85", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9b7c415a-d0cb-4a25-8639-1d943d367102", "node_type": "1", "metadata": {}, "hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "class_name": "RelatedNodeInfo"}}, "hash": "d3247939dcdf59c20c3b15ee78268f356c9c5bd14660b0cd9a3b0d13e5872755", "text": "Simply do the following:\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Query Engine Tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.agent import ReActAgent\nfrom llama_index.tools import QueryEngineTool\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/NOTE: lyft_index and uber_index are both SimpleVectorIndex instances\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nlyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\nuber_engine = uber_index.as_query_engine(similarity_top_k=3)\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=lyft_engine,\n        metadata=ToolMetadata(\n            name=\"lyft_10k\",\n            description=\"Provides information about Lyft financials for year 2021. \"\n            \"Use a detailed plain text question as input to the tool.", "start_char_idx": 3999, "end_char_idx": 5290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f701998-d2dc-40f8-95ef-eed75c2be17a": {"__data__": {"id_": "1f701998-d2dc-40f8-95ef-eed75c2be17a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3399da4-ee87-4514-84fb-805c1728e658", "node_type": "1", "metadata": {}, "hash": "b184212a41d9892e8649c3391792c691507b74d39e6d6e949a723976c63ae5cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56fba29e-2ab2-4832-87cd-c149d6a32fe6", "node_type": "1", "metadata": {}, "hash": "0075b8d4fa6e3a5fbbbbd1d2e9ce1f0981f7fde32d7738483f40b4fd148572b8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b3399da4-ee87-4514-84fb-805c1728e658", "node_type": "1", "metadata": {}, "hash": "b184212a41d9892e8649c3391792c691507b74d39e6d6e949a723976c63ae5cf", "class_name": "RelatedNodeInfo"}}, "hash": "e24e2ab95a9e2284ae4fd2a9f33514fce87bf701d475ff83b71a1a0829cc79da", "text": "\"Use a detailed plain text question as input to the tool.\",\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=uber_engine,\n        metadata=ToolMetadata(\n            name=\"uber_10k\",\n            description=\"Provides information about Uber financials for year 2021. \"\n            \"Use a detailed plain text question as input to the tool.\",\n        ),\n    ),\n]\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/initialize ReAct agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Use other agents as Tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA nifty feature of our agents is that since they inherit from `BaseQueryEngine`, you can easily define other agents as tools\nthrough our `QueryEngineTool`.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Use other agents as Tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.tools import QueryEngineTool\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sql_agent,\n        metadata=ToolMetadata(\n            name=\"sql_agent\",\n            description=\"Agent that can execute SQL queries.\"", "start_char_idx": 0, "end_char_idx": 1933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56fba29e-2ab2-4832-87cd-c149d6a32fe6": {"__data__": {"id_": "56fba29e-2ab2-4832-87cd-c149d6a32fe6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3399da4-ee87-4514-84fb-805c1728e658", "node_type": "1", "metadata": {}, "hash": "b184212a41d9892e8649c3391792c691507b74d39e6d6e949a723976c63ae5cf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f701998-d2dc-40f8-95ef-eed75c2be17a", "node_type": "1", "metadata": {}, "hash": "e24e2ab95a9e2284ae4fd2a9f33514fce87bf701d475ff83b71a1a0829cc79da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b3399da4-ee87-4514-84fb-805c1728e658", "node_type": "1", "metadata": {}, "hash": "b184212a41d9892e8649c3391792c691507b74d39e6d6e949a723976c63ae5cf", "class_name": "RelatedNodeInfo"}}, "hash": "0075b8d4fa6e3a5fbbbbd1d2e9ce1f0981f7fde32d7738483f40b4fd148572b8", "text": "),\n    ),\n    QueryEngineTool(\n        query_engine=gmail_agent,\n        metadata=ToolMetadata(\n            name=\"gmail_agent\",\n            description=\"Tool that can send emails on Gmail.\"\n        ),\n    ),\n]\n\nouter_agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also use agents in more advanced settings. For instance, being able to retrieve tools from an index during query-time, and\nbeing able to perform query planning over an existing set of Tools.\n\nThese are largely implemented with our `OpenAIAgent` classes (which depend on the OpenAI Function API). Support\nfor our more general `ReActAgent` is something we're actively investigating.\n\nNOTE: these are largely still in beta. The abstractions may change and become more general over time.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/Function Retrieval Agents\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf the set of Tools is very large, you can create an `ObjectIndex` to index the tools, and then pass in an `ObjectRetriever` to the agent during query-time, to first dynamically retrieve the relevant tools before having the agent pick from the candidate tools.\n\nWe first build an `ObjectIndex` over an existing set of Tools.", "start_char_idx": 1942, "end_char_idx": 3844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db5a729d-92c2-412b-a5a1-a622ee0f629d": {"__data__": {"id_": "db5a729d-92c2-412b-a5a1-a622ee0f629d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f326b656-9b2d-4837-abd3-67e6877c7439", "node_type": "1", "metadata": {}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07ef6ba2-1212-4620-9a64-d97e427b2584", "node_type": "1", "metadata": {}, "hash": "f3247709ab321d44b6b7a07d17f82dfee0b9c97b4bb76c56fb711efb52ba079b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f326b656-9b2d-4837-abd3-67e6877c7439", "node_type": "1", "metadata": {}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "class_name": "RelatedNodeInfo"}}, "hash": "7b3c6398de10eedb5e0eafb7b891fb7a75b9e63e2164add8238655e59a0d90f1", "text": "We first build an `ObjectIndex` over an existing set of Tools.\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/define an \"object\" index over these tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\nfrom llama_index.objects import ObjectIndex, SimpleToolNodeMapping\n\ntool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\nobj_index = ObjectIndex.from_objects(\n    all_tools,\n    tool_mapping,\n    VectorStoreIndex,\n)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/define an \"object\" index over these tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe then define our `FnRetrieverOpenAIAgent`:\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/define an \"object\" index over these tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.agent import FnRetrieverOpenAIAgent\n\nagent = FnRetrieverOpenAIAgent.from_retriever(obj_index.as_retriever(),", "start_char_idx": 0, "end_char_idx": 1779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07ef6ba2-1212-4620-9a64-d97e427b2584": {"__data__": {"id_": "07ef6ba2-1212-4620-9a64-d97e427b2584", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f326b656-9b2d-4837-abd3-67e6877c7439", "node_type": "1", "metadata": {}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db5a729d-92c2-412b-a5a1-a622ee0f629d", "node_type": "1", "metadata": {}, "hash": "7b3c6398de10eedb5e0eafb7b891fb7a75b9e63e2164add8238655e59a0d90f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cce549ec-b777-4de0-a696-011295bbe324", "node_type": "1", "metadata": {}, "hash": "d415f1114829514bf9b5b631c9faefe43a68d68d2bd31cb54f04fffc75cd0638", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f326b656-9b2d-4837-abd3-67e6877c7439", "node_type": "1", "metadata": {}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "class_name": "RelatedNodeInfo"}}, "hash": "f3247709ab321d44b6b7a07d17f82dfee0b9c97b4bb76c56fb711efb52ba079b", "text": "from_retriever(obj_index.as_retriever(), verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/Context Retrieval Agents\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOur context-augmented OpenAI Agent will always perform retrieval before calling any tools.\n\nThis helps to provide additional context that can help the agent better pick Tools, versus\njust trying to make a decision without any context.", "start_char_idx": 1739, "end_char_idx": 2438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cce549ec-b777-4de0-a696-011295bbe324": {"__data__": {"id_": "cce549ec-b777-4de0-a696-011295bbe324", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f326b656-9b2d-4837-abd3-67e6877c7439", "node_type": "1", "metadata": {}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07ef6ba2-1212-4620-9a64-d97e427b2584", "node_type": "1", "metadata": {}, "hash": "f3247709ab321d44b6b7a07d17f82dfee0b9c97b4bb76c56fb711efb52ba079b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64124226-5ec9-4b0e-a4cf-82c44b8a7e08", "node_type": "1", "metadata": {}, "hash": "3f21d108f04f55a7efc7f5793956639e8f609d7e0126052c1f830c85221771bd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f326b656-9b2d-4837-abd3-67e6877c7439", "node_type": "1", "metadata": {}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "class_name": "RelatedNodeInfo"}}, "hash": "d415f1114829514bf9b5b631c9faefe43a68d68d2bd31cb54f04fffc75cd0638", "text": "File Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/Context Retrieval Agents\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import Document\nfrom llama_index.agent import ContextRetrieverOpenAIAgent\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/toy index - stores a list of abbreviations\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntexts = [\n    \"Abbrevation: X = Revenue\",\n    \"Abbrevation: YZ = Risk Factors\",\n    \"Abbreviation: Z = Costs\",\n]\ndocs = [Document(text=t) for t in texts]\ncontext_index = VectorStoreIndex.from_documents(docs)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/add context agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncontext_agent = ContextRetrieverOpenAIAgent.from_tools_and_retriever(\n    query_engine_tools, context_index.as_retriever(similarity_top_k=1), verbose=True\n)\nresponse = context_agent.chat(\"What is the YZ of March 2022?\")", "start_char_idx": 2440, "end_char_idx": 4205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64124226-5ec9-4b0e-a4cf-82c44b8a7e08": {"__data__": {"id_": "64124226-5ec9-4b0e-a4cf-82c44b8a7e08", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f326b656-9b2d-4837-abd3-67e6877c7439", "node_type": "1", "metadata": {}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cce549ec-b777-4de0-a696-011295bbe324", "node_type": "1", "metadata": {}, "hash": "d415f1114829514bf9b5b631c9faefe43a68d68d2bd31cb54f04fffc75cd0638", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f326b656-9b2d-4837-abd3-67e6877c7439", "node_type": "1", "metadata": {}, "hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "class_name": "RelatedNodeInfo"}}, "hash": "3f21d108f04f55a7efc7f5793956639e8f609d7e0126052c1f830c85221771bd", "text": "File Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/Query Planning\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOpenAI Function Agents can be capable of advanced query planning. The trick is to provide the agent\nwith a `QueryPlanTool` - if the agent calls the QueryPlanTool, it is forced to infer a full Pydantic schema representing a query\nplan over a set of subtools.", "start_char_idx": 4207, "end_char_idx": 4863, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19811015-7b6e-4227-a8e1-71300febfdec": {"__data__": {"id_": "19811015-7b6e-4227-a8e1-71300febfdec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07802572-8cc3-4488-a4fd-6135f0cc3045", "node_type": "1", "metadata": {}, "hash": "f44f2532ad9efd4160a02b65b972bea2c66a323ebc0e40e94781c65978c6bd85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e42e97d4-89d8-42fb-aab2-a01085e26abf", "node_type": "1", "metadata": {}, "hash": "7dfd0da7f19f501659c295cd94cb24b9a1f714967c9334fd48c182851faa1c2a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "07802572-8cc3-4488-a4fd-6135f0cc3045", "node_type": "1", "metadata": {}, "hash": "f44f2532ad9efd4160a02b65b972bea2c66a323ebc0e40e94781c65978c6bd85", "class_name": "RelatedNodeInfo"}}, "hash": "bf8a006929ca2e2595a32510816dc65c87d32e2efcf7d27485fbf6188c51fe6b", "text": "File Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/define query plan tool\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.tools import QueryPlanTool\nfrom llama_index import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(service_context=service_context)\nquery_plan_tool = QueryPlanTool.from_defaults(\n    query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march],\n    response_synthesizer=response_synthesizer,\n)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/initialize agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent = OpenAIAgent.from_tools(\n    [query_plan_tool],\n    max_function_calls=10,\n    llm=OpenAI(temperature=0, model=\"gpt-4-0613\"),\n    verbose=True,\n)\n\nFile Name: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Advanced Concepts (for `OpenAIAgent`, in beta)/should output a query plan to call march, june, and september tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\agents\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 6171\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = agent.", "start_char_idx": 0, "end_char_idx": 1780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e42e97d4-89d8-42fb-aab2-a01085e26abf": {"__data__": {"id_": "e42e97d4-89d8-42fb-aab2-a01085e26abf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07802572-8cc3-4488-a4fd-6135f0cc3045", "node_type": "1", "metadata": {}, "hash": "f44f2532ad9efd4160a02b65b972bea2c66a323ebc0e40e94781c65978c6bd85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19811015-7b6e-4227-a8e1-71300febfdec", "node_type": "1", "metadata": {}, "hash": "bf8a006929ca2e2595a32510816dc65c87d32e2efcf7d27485fbf6188c51fe6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82277ba7-e9d4-451e-8936-bdf713e2781f", "node_type": "1", "metadata": {}, "hash": "ecb0ea18a34cc3f2a2ca1f227e32caa4c65585a1b607fa66070bccc5dd631d04", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "07802572-8cc3-4488-a4fd-6135f0cc3045", "node_type": "1", "metadata": {}, "hash": "f44f2532ad9efd4160a02b65b972bea2c66a323ebc0e40e94781c65978c6bd85", "class_name": "RelatedNodeInfo"}}, "hash": "7dfd0da7f19f501659c295cd94cb24b9a1f714967c9334fd48c182851faa1c2a", "text": "query(\"Analyze Uber revenue growth in March, June, and September\")\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe offer a rich set of Tool Specs that are offered through LlamaHub \ud83e\udd99. \n!\n\nThese tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions. \n\nWe also provide a list of **utility tools** that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Tool Specs\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nComing soon!\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using).", "start_char_idx": 1780, "end_char_idx": 3605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82277ba7-e9d4-451e-8936-bdf713e2781f": {"__data__": {"id_": "82277ba7-e9d4-451e-8936-bdf713e2781f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07802572-8cc3-4488-a4fd-6135f0cc3045", "node_type": "1", "metadata": {}, "hash": "f44f2532ad9efd4160a02b65b972bea2c66a323ebc0e40e94781c65978c6bd85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e42e97d4-89d8-42fb-aab2-a01085e26abf", "node_type": "1", "metadata": {}, "hash": "7dfd0da7f19f501659c295cd94cb24b9a1f714967c9334fd48c182851faa1c2a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "07802572-8cc3-4488-a4fd-6135f0cc3045", "node_type": "1", "metadata": {}, "hash": "f44f2532ad9efd4160a02b65b972bea2c66a323ebc0e40e94781c65978c6bd85", "class_name": "RelatedNodeInfo"}}, "hash": "ecb0ea18a34cc3f2a2ca1f227e32caa4c65585a1b607fa66070bccc5dd631d04", "text": "To tackle this, we\u2019ve provided an initial set of \u201cutility tools\u201d in LlamaHub Tools - utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that\u2019s returned from any API request.\n\nLet\u2019s walk through our two main utility tools below.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/OnDemandLoaderTool\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis tool turns any existing LlamaIndex data loader ( `BaseReader` class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it \u201con-demand\u201d. All three of these steps happen in a single tool call.\n\nOftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call.", "start_char_idx": 3608, "end_char_idx": 5120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44b5d802-dbc2-48bb-9f6a-3ceb0c3dcc93": {"__data__": {"id_": "44b5d802-dbc2-48bb-9f6a-3ceb0c3dcc93", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd", "node_type": "1", "metadata": {}, "hash": "a7c902e17e614210a0e6aa290102eca75a25b2cdf0f7753c249ee78c27c37520", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce343639-2638-4fc0-8fc4-b83afb10564a", "node_type": "1", "metadata": {}, "hash": "beaa9f5d8b34d3227c0b6d6d46094e3b10b43dc76e253be1cadbe14a8ba49624", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd", "node_type": "1", "metadata": {}, "hash": "a7c902e17e614210a0e6aa290102eca75a25b2cdf0f7753c249ee78c27c37520", "class_name": "RelatedNodeInfo"}}, "hash": "cd4aa6a6bfd3808825408c4eb726b068daf9e59adbec4d641f63ba48adba4245", "text": "A usage example is given below:\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/OnDemandLoaderTool\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_hub.wikipedia.base import WikipediaReader\nfrom llama_index.tools.on_demand_loader_tool import OnDemandLoaderTool\n\ntool = OnDemandLoaderTool.from_defaults(\n\treader,\n\tname=\"Wikipedia Tool\",\n\tdescription=\"A tool for loading data and querying articles from Wikipedia\"\n)\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/LoadAndSearchToolSpec\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements `to_tool_list` , and when that function is called, two tools are returned: a `load` tool and then a `search` tool.\n\nThe `load` Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The `search` Tool execution would take in a query string as input and call the underlying index.\n\nThis is helpful for any API endpoint that will by default return large volumes of data - for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows.\n\nExample usage is shown below:\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.", "start_char_idx": 0, "end_char_idx": 1856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce343639-2638-4fc0-8fc4-b83afb10564a": {"__data__": {"id_": "ce343639-2638-4fc0-8fc4-b83afb10564a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd", "node_type": "1", "metadata": {}, "hash": "a7c902e17e614210a0e6aa290102eca75a25b2cdf0f7753c249ee78c27c37520", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44b5d802-dbc2-48bb-9f6a-3ceb0c3dcc93", "node_type": "1", "metadata": {}, "hash": "cd4aa6a6bfd3808825408c4eb726b068daf9e59adbec4d641f63ba48adba4245", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e49b319-be00-40e6-af4b-5c0a8d2ad0f3", "node_type": "1", "metadata": {}, "hash": "5bba92a70333a3659924a1231f0b2a742999cfbd79adacbc238959294e21e26f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd", "node_type": "1", "metadata": {}, "hash": "a7c902e17e614210a0e6aa290102eca75a25b2cdf0f7753c249ee78c27c37520", "class_name": "RelatedNodeInfo"}}, "hash": "beaa9f5d8b34d3227c0b6d6d46094e3b10b43dc76e253be1cadbe14a8ba49624", "text": "md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/LoadAndSearchToolSpec\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_hub.tools.wikipedia.base import WikipediaToolSpec\nfrom llama_index.tools.tool_spec.load_and_search import LoadAndSearchToolSpec\n\nwiki_spec = WikipediaToolSpec()\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/Get the search wikipedia tool\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntool = wiki_spec.to_tool_list()[1]\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nContent Type: text\nHeader Path: LlamaHub Tools Guide/Utility Tools/Create the Agent with load/search tools\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md\nfile_name: llamahub_tools_guide.md\nfile_type: None\nfile_size: 3435\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent = OpenAIAgent.from_tools(\n LoadAndSearchToolSpec.from_defaults(\n    tool\n ).to_tool_list(), verbose=True\n)\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.", "start_char_idx": 1856, "end_char_idx": 3516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e49b319-be00-40e6-af4b-5c0a8d2ad0f3": {"__data__": {"id_": "6e49b319-be00-40e6-af4b-5c0a8d2ad0f3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd", "node_type": "1", "metadata": {}, "hash": "a7c902e17e614210a0e6aa290102eca75a25b2cdf0f7753c249ee78c27c37520", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce343639-2638-4fc0-8fc4-b83afb10564a", "node_type": "1", "metadata": {}, "hash": "beaa9f5d8b34d3227c0b6d6d46094e3b10b43dc76e253be1cadbe14a8ba49624", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd", "node_type": "1", "metadata": {}, "hash": "a7c902e17e614210a0e6aa290102eca75a25b2cdf0f7753c249ee78c27c37520", "class_name": "RelatedNodeInfo"}}, "hash": "5bba92a70333a3659924a1231f0b2a742999cfbd79adacbc238959294e21e26f", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHaving proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a **Tool** as well as a **ToolSpec** containing a series of functions under the hood. \n\nA Tool implements a very generic interface - simply define `__call__` and also return some basic metadata (name, description, function schema).\n\nA Tool Spec defines a full API specification of any service that can be converted into a list of Tools.\n\nWe offer a few different types of Tools:\n- `FunctionTool`: A function tool allows users to easily convert any user-defined function into a Tool. It can also auto-infer the function schema.\n- `QueryEngineTool`: A tool that wraps an existing query engine. Note: since our agent abstractions inherit from `BaseQueryEngine`, these tools can also wrap other agents.\n\nWe offer a rich set of Tools and Tool Specs through LlamaHub \ud83e\udd99.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Blog Post\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor full details, please check out our detailed blog post.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOur Tool Specs and Tools can be imported from the `llama-hub` package.", "start_char_idx": 3497, "end_char_idx": 5422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29988d7a-5ee7-4ad8-81c9-c83f972d0697": {"__data__": {"id_": "29988d7a-5ee7-4ad8-81c9-c83f972d0697", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "652a770b-edf5-4968-bf1c-1e7289800dce", "node_type": "1", "metadata": {}, "hash": "76ed119e7fb6d90417a3ec226117314d7d2d067ad641aaf7b2fe72d61e421851", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e24cf0ac-8186-4727-84ed-6c07832eab80", "node_type": "1", "metadata": {}, "hash": "63b75e57436987c21b6ccbe1b8b0e89cda2fc39a9528584f85994083a5a60e10", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "652a770b-edf5-4968-bf1c-1e7289800dce", "node_type": "1", "metadata": {}, "hash": "76ed119e7fb6d90417a3ec226117314d7d2d067ad641aaf7b2fe72d61e421851", "class_name": "RelatedNodeInfo"}}, "hash": "cd4e9c5bf64d00f506f5b73b75977590c3face787bd666ee217b7bc677cabe12", "text": "To use with our agent,\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.agent import OpenAIAgent\nfrom llama_hub.tools.gmail.base import GmailToolSpec\n\ntool_spec = GmailToolSpec()\nagent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee our Usage Pattern Guide for more details.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/LlamaHub Tools Guide \ud83d\udee0\ufe0f\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCheck out our guide for a full overview of the Tools/Tool Specs in LlamaHub!", "start_char_idx": 0, "end_char_idx": 1679, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e24cf0ac-8186-4727-84ed-6c07832eab80": {"__data__": {"id_": "e24cf0ac-8186-4727-84ed-6c07832eab80", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "652a770b-edf5-4968-bf1c-1e7289800dce", "node_type": "1", "metadata": {}, "hash": "76ed119e7fb6d90417a3ec226117314d7d2d067ad641aaf7b2fe72d61e421851", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29988d7a-5ee7-4ad8-81c9-c83f972d0697", "node_type": "1", "metadata": {}, "hash": "cd4e9c5bf64d00f506f5b73b75977590c3face787bd666ee217b7bc677cabe12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c2c25f4-13ca-43ed-8f97-c793f711ea4b", "node_type": "1", "metadata": {}, "hash": "633e28bd176c37a75988779638f42aa5c2463cec129fb99a8f1bba1ff36e1e32", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "652a770b-edf5-4968-bf1c-1e7289800dce", "node_type": "1", "metadata": {}, "hash": "76ed119e7fb6d90417a3ec226117314d7d2d067ad641aaf7b2fe72d61e421851", "class_name": "RelatedNodeInfo"}}, "hash": "63b75e57436987c21b6ccbe1b8b0e89cda2fc39a9528584f85994083a5a60e10", "text": "File Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/LlamaHub Tools Guide \ud83d\udee0\ufe0f\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nllamahub_tools_guide.md\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/LlamaHub Tools Guide \ud83d\udee0\ufe0f\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n<!-- We offer a rich set of Tool Specs that are offered through LlamaHub \ud83e\udd99. \nThese tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions. \n\n! -->", "start_char_idx": 1681, "end_char_idx": 2625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c2c25f4-13ca-43ed-8f97-c793f711ea4b": {"__data__": {"id_": "0c2c25f4-13ca-43ed-8f97-c793f711ea4b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "652a770b-edf5-4968-bf1c-1e7289800dce", "node_type": "1", "metadata": {}, "hash": "76ed119e7fb6d90417a3ec226117314d7d2d067ad641aaf7b2fe72d61e421851", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e24cf0ac-8186-4727-84ed-6c07832eab80", "node_type": "1", "metadata": {}, "hash": "63b75e57436987c21b6ccbe1b8b0e89cda2fc39a9528584f85994083a5a60e10", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "652a770b-edf5-4968-bf1c-1e7289800dce", "node_type": "1", "metadata": {}, "hash": "76ed119e7fb6d90417a3ec226117314d7d2d067ad641aaf7b2fe72d61e421851", "class_name": "RelatedNodeInfo"}}, "hash": "633e28bd176c37a75988779638f42aa5c2463cec129fb99a8f1bba1ff36e1e32", "text": "! -->\n\n\n<!-- ## Module Guides\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/LlamaHub Tools Guide \ud83d\udee0\ufe0f\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\root.md\nContent Type: text\nHeader Path: Tools/Concept/Tool Example Notebooks\nfile_path: Docs\\core_modules\\agent_modules\\tools\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2318\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nComing soon!  -->\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaHub Tool Specs and Tools can be imported from the `llama-hub` package. They can be plugged into our native agents, or LangChain agents.", "start_char_idx": 2620, "end_char_idx": 3832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35952454-60da-4b15-868b-24884a0f8a0d": {"__data__": {"id_": "35952454-60da-4b15-868b-24884a0f8a0d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1330cb2-d00f-4890-857d-9b0aac7ead05", "node_type": "1", "metadata": {}, "hash": "2428f1e6dd08348a2f2253b78efd023bb2dfd2d686d4b7066949412c888c3322", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}}, "hash": "d8535206f8b6117400bd9773dfc7ef06929be30946b773af97b3c641902bcde9", "text": "They can be plugged into our native agents, or LangChain agents.", "start_char_idx": 0, "end_char_idx": 64, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1330cb2-d00f-4890-857d-9b0aac7ead05": {"__data__": {"id_": "d1330cb2-d00f-4890-857d-9b0aac7ead05", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35952454-60da-4b15-868b-24884a0f8a0d", "node_type": "1", "metadata": {}, "hash": "d8535206f8b6117400bd9773dfc7ef06929be30946b773af97b3c641902bcde9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c5741a9-c6fc-47a9-a2cd-3c45be6dc465", "node_type": "1", "metadata": {}, "hash": "b63d31f2813e0fc9d4f5d2467a38a7ffd119a7f15f22483f872bb4fb6521be10", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}}, "hash": "2428f1e6dd08348a2f2253b78efd023bb2dfd2d686d4b7066949412c888c3322", "text": "They can be plugged into our native agents, or LangChain agents.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with our Agents\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo use with our OpenAIAgent,\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with our Agents\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.agent import OpenAIAgent\nfrom llama_hub.tools.gmail.base import GmailToolSpec\n\ntool_spec = GmailToolSpec()\nagent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), verbose=True)\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/use agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent.chat(\"Can you create a new email to helpdesk and support @example.com about a service outage\")\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/use agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFull Tool details can be found on our LlamaHub page.", "start_char_idx": 0, "end_char_idx": 1850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c5741a9-c6fc-47a9-a2cd-3c45be6dc465": {"__data__": {"id_": "1c5741a9-c6fc-47a9-a2cd-3c45be6dc465", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1330cb2-d00f-4890-857d-9b0aac7ead05", "node_type": "1", "metadata": {}, "hash": "2428f1e6dd08348a2f2253b78efd023bb2dfd2d686d4b7066949412c888c3322", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffe26393-c9d4-4612-bbfc-391cdb7af831", "node_type": "1", "metadata": {}, "hash": "5aacfcd9628a7441d56a1a5bd09a744a45f53154a38d603a024097dc65ace382", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}}, "hash": "b63d31f2813e0fc9d4f5d2467a38a7ffd119a7f15f22483f872bb4fb6521be10", "text": "Each tool contains a \"Usage\" section showing how that tool can be used.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with LangChain\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo use with a LangChain agent, simply convert tools to LangChain tools with `to_langchain_tool()`.\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with LangChain\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntools = tool_spec.to_tool_list()\nlangchain_tools = [t.to_langchain_tool() for t in tools]\n\nFile Name: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/plug into LangChain agent\nLinks: \nfile_path: Docs\\core_modules\\agent_modules\\tools\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1136\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom langchain.agents import initialize_agent\n\nagent_executor = initialize_agent(\n    langchain_tools, llm, agent=\"conversational-react-description\", memory=memory\n)\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\modules.md\nContent Type: code\nHeader Path: Module Guides\nfile_path: Docs\\core_modules\\data_modules\\connector\\modules.md\nfile_name: modules.", "start_char_idx": 1851, "end_char_idx": 3546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffe26393-c9d4-4612-bbfc-391cdb7af831": {"__data__": {"id_": "ffe26393-c9d4-4612-bbfc-391cdb7af831", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c5741a9-c6fc-47a9-a2cd-3c45be6dc465", "node_type": "1", "metadata": {}, "hash": "b63d31f2813e0fc9d4f5d2467a38a7ffd119a7f15f22483f872bb4fb6521be10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3d6a034-cbd5-43c5-9022-1715c05ef1ff", "node_type": "1", "metadata": {}, "hash": "6e1ee1923377b895a9ad3d3a7eab6c9dd9052d228b4f5e6817a4c5c7cd5acac5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}}, "hash": "5aacfcd9628a7441d56a1a5bd09a744a45f53154a38d603a024097dc65ace382", "text": "md\nfile_name: modules.md\nfile_type: None\nfile_size: 1278\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n./././examples/data_connectors/PsychicDemo.ipynb\n./././examples/data_connectors/DeepLakeReader.ipynb\n./././examples/data_connectors/QdrantDemo.ipynb\n./././examples/data_connectors/DiscordDemo.ipynb\n./././examples/data_connectors/MongoDemo.ipynb\n./././examples/data_connectors/ChromaDemo.ipynb\n./././examples/data_connectors/MyScaleReaderDemo.ipynb\n./././examples/data_connectors/FaissDemo.ipynb\n./././examples/data_connectors/ObsidianReaderDemo.ipynb\n./././examples/data_connectors/SlackDemo.ipynb\n./././examples/data_connectors/WebPageDemo.ipynb\n./././examples/data_connectors/PineconeDemo.ipynb\n./././examples/data_connectors/MboxReaderDemo.ipynb\n./././examples/data_connectors/MilvusReaderDemo.ipynb\n./././examples/data_connectors/NotionDemo.ipynb\n./././examples/data_connectors/GithubRepositoryReaderDemo.ipynb\n./././examples/data_connectors/GoogleDocsDemo.ipynb\n./././examples/data_connectors/DatabaseReaderDemo.ipynb\n./././examples/data_connectors/TwitterDemo.ipynb\n./././examples/data_connectors/WeaviateDemo.ipynb\n./././examples/data_connectors/MakeDemo.ipynb\n./././examples/data_connectors/deplot/DeplotReader.ipynb\n```\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3d6a034-cbd5-43c5-9022-1715c05ef1ff": {"__data__": {"id_": "b3d6a034-cbd5-43c5-9022-1715c05ef1ff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffe26393-c9d4-4612-bbfc-391cdb7af831", "node_type": "1", "metadata": {}, "hash": "5aacfcd9628a7441d56a1a5bd09a744a45f53154a38d603a024097dc65ace382", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "node_type": "1", "metadata": {}, "hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "class_name": "RelatedNodeInfo"}}, "hash": "6e1ee1923377b895a9ad3d3a7eab6c9dd9052d228b4f5e6817a4c5c7cd5acac5", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).", "start_char_idx": 5091, "end_char_idx": 5391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4263475-b3a1-4f60-84a0-ca4ccdc7482c": {"__data__": {"id_": "f4263475-b3a1-4f60-84a0-ca4ccdc7482c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "node_type": "1", "metadata": {}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a54d98eb-d4ad-4125-90ec-4a38c95483e7", "node_type": "1", "metadata": {}, "hash": "c016d9c06ffa72224ac57a9c0d227b433f0debadf9fb172394981c26aa4025aa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "node_type": "1", "metadata": {}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "class_name": "RelatedNodeInfo"}}, "hash": "d85b8f3d96526ad0bc43cc481b1e148eb65ca97d6ee5aa519b550adb32144e60", "text": "File Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOnce you've ingested your data, you can build an Index on top, ask questions using a Query Engine, and have a conversation using a Chat Engine.\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/LlamaHub\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOur data connectors are offered through LlamaHub \ud83e\udd99. \nLlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.\n\n!", "start_char_idx": 0, "end_char_idx": 1000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a54d98eb-d4ad-4125-90ec-4a38c95483e7": {"__data__": {"id_": "a54d98eb-d4ad-4125-90ec-4a38c95483e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "node_type": "1", "metadata": {}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4263475-b3a1-4f60-84a0-ca4ccdc7482c", "node_type": "1", "metadata": {}, "hash": "d85b8f3d96526ad0bc43cc481b1e148eb65ca97d6ee5aa519b550adb32144e60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4acfe805-ad5c-4dd3-86e4-3ed23d752c4e", "node_type": "1", "metadata": {}, "hash": "c84ac99be091b050e2a31a0e234a8896ded86084f8f3a4fb3c438ea43c8a1bcd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "node_type": "1", "metadata": {}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "class_name": "RelatedNodeInfo"}}, "hash": "c016d9c06ffa72224ac57a9c0d227b433f0debadf9fb172394981c26aa4025aa", "text": "!\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet started with:\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import download_loader\n\nGoogleDocsReader = download_loader('GoogleDocsReader')\nloader = GoogleDocsReader()\ndocuments = loader.load_data(document_ids=[...])\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: code\nHeader Path: Data Connectors (LlamaHub)/Usage Pattern\nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Modules\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSome sample data connectors:\n- local file directory (`SimpleDirectoryReader`). Can support parsing a wide range of file types: `.pdf`, `.jpg`, `.png`, `.docx`, etc.", "start_char_idx": 999, "end_char_idx": 2764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4acfe805-ad5c-4dd3-86e4-3ed23d752c4e": {"__data__": {"id_": "4acfe805-ad5c-4dd3-86e4-3ed23d752c4e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "node_type": "1", "metadata": {}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a54d98eb-d4ad-4125-90ec-4a38c95483e7", "node_type": "1", "metadata": {}, "hash": "c016d9c06ffa72224ac57a9c0d227b433f0debadf9fb172394981c26aa4025aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6af4dded-1e29-4b14-9289-f2cf8af37bbf", "node_type": "1", "metadata": {}, "hash": "e2b6cd9e2a9e3821b58ae7fec74132b4e5d446fb0e44bc0200439e4deb4fe304", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "node_type": "1", "metadata": {}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "class_name": "RelatedNodeInfo"}}, "hash": "c84ac99be091b050e2a31a0e234a8896ded86084f8f3a4fb3c438ea43c8a1bcd", "text": "- Notion (`NotionPageReader`)\n- Google Docs (`GoogleDocsReader`)\n- Slack (`SlackReader`)\n- Discord (`DiscordReader`)\n- Apify Actors (`ApifyActor`). Can crawl the web, scrape webpages, extract text content, download files including `.pdf`, `.jpg`, `.png`, `.docx`, etc.\n\nSee below for detailed guides.\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\root.md\nContent Type: text\nHeader Path: Data Connectors (LlamaHub)/Modules\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1814\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nmodules.rst\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 720\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEach data loader contains a \"Usage\" section showing how that loader can be used. At the core of using each loader is a `download_loader` function, which\ndownloads the loader file into a module that you can use within your application.", "start_char_idx": 2765, "end_char_idx": 4021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6af4dded-1e29-4b14-9289-f2cf8af37bbf": {"__data__": {"id_": "6af4dded-1e29-4b14-9289-f2cf8af37bbf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "node_type": "1", "metadata": {}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4acfe805-ad5c-4dd3-86e4-3ed23d752c4e", "node_type": "1", "metadata": {}, "hash": "c84ac99be091b050e2a31a0e234a8896ded86084f8f3a4fb3c438ea43c8a1bcd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "node_type": "1", "metadata": {}, "hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "class_name": "RelatedNodeInfo"}}, "hash": "e2b6cd9e2a9e3821b58ae7fec74132b4e5d446fb0e44bc0200439e4deb4fe304", "text": "Example usage:\n\nFile Name: Docs\\core_modules\\data_modules\\connector\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\connector\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 720\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, download_loader\n\nGoogleDocsReader = download_loader('GoogleDocsReader')\n\ngdoc_ids = ['1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec']\nloader = GoogleDocsReader()\ndocuments = loader.load_data(document_ids=gdoc_ids)\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nquery_engine.query('Where did the author go to school?')\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: text\nHeader Path: Documents / Nodes/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDocument and Node objects are core abstractions within LlamaIndex.\n\nA **Document** is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. They can be constructed manually, or created automatically via our data loaders. By default, a Document stores text along with some other attributes. Some of these are listed below.", "start_char_idx": 4023, "end_char_idx": 5523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "467759b4-efac-438a-9c9c-d79a54126a79": {"__data__": {"id_": "467759b4-efac-438a-9c9c-d79a54126a79", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "node_type": "1", "metadata": {}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32f45229-cca8-4dd5-a6e0-7ebf62badeaf", "node_type": "1", "metadata": {}, "hash": "e09cbf435c77025b37993dae0915f6ccb23dd2664a3c19cf17fb173c0f4376b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "node_type": "1", "metadata": {}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "class_name": "RelatedNodeInfo"}}, "hash": "2d79dd0295430e816caaf76780d3f5704ed0ad3e97757cfcfb92b58dd78f7de1", "text": "Some of these are listed below.\n- `metadata` - a dictionary of annotations that can be appended to the text.\n- `relationships` - a dictionary containing relationships to other Documents/Nodes.\n\n*Note*: We have beta support for allowing Documents to store images, and are actively working on improving its multimodal capabilities.\n\nA **Node** represents a \"chunk\" of a source Document, whether that is a text chunk, an image, or other. Similar to Documents, they contain metadata and relationship information with other nodes.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes. By default every Node derived from a Document will inherit the same metadata from that Document (e.g. a \"file_name\" filed in the Document is propagated to every Node).\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: text\nHeader Path: Documents / Nodes/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere are some simple snippets to get started with Documents and Nodes.", "start_char_idx": 0, "end_char_idx": 1322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32f45229-cca8-4dd5-a6e0-7ebf62badeaf": {"__data__": {"id_": "32f45229-cca8-4dd5-a6e0-7ebf62badeaf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "node_type": "1", "metadata": {}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "467759b4-efac-438a-9c9c-d79a54126a79", "node_type": "1", "metadata": {}, "hash": "2d79dd0295430e816caaf76780d3f5704ed0ad3e97757cfcfb92b58dd78f7de1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f014092-2ab1-476d-ba96-e5fd449c45e3", "node_type": "1", "metadata": {}, "hash": "9ce7b3078731baccbf62890878b2c451660f59cd355584fb461921298ed1ad14", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "node_type": "1", "metadata": {}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "class_name": "RelatedNodeInfo"}}, "hash": "e09cbf435c77025b37993dae0915f6ccb23dd2664a3c19cf17fb173c0f4376b2", "text": "File Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: code\nHeader Path: Documents / Nodes/Usage Pattern/build index\nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index import Document, VectorStoreIndex\n\ntext_list = [text1, text2, ...]\ndocuments = [Document(text=t) for t in text_list]\n\nindex = VectorStoreIndex.from_documents(documents)\n\n```\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: code\nHeader Path: Documents / Nodes/Usage Pattern/build index\nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\n\nfrom llama_index.node_parser import SimpleNodeParser\n\n...\n\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(documents)\n\nindex = VectorStoreIndex(nodes)\n\n```\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: text\nHeader Path: Documents / Nodes/Usage Pattern/Document/Node Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTake a look at our in-depth guides for more details on how to use Documents/Nodes.", "start_char_idx": 1324, "end_char_idx": 2880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f014092-2ab1-476d-ba96-e5fd449c45e3": {"__data__": {"id_": "9f014092-2ab1-476d-ba96-e5fd449c45e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "node_type": "1", "metadata": {}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32f45229-cca8-4dd5-a6e0-7ebf62badeaf", "node_type": "1", "metadata": {}, "hash": "e09cbf435c77025b37993dae0915f6ccb23dd2664a3c19cf17fb173c0f4376b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1fc7dc8-1fe2-4476-b2e9-7e2134603e7c", "node_type": "1", "metadata": {}, "hash": "240837e08b10a0b91de30f4e5eb52d9a9192ac27c1bf76968e6dd9e20b033c17", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "node_type": "1", "metadata": {}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "class_name": "RelatedNodeInfo"}}, "hash": "9ce7b3078731baccbf62890878b2c451660f59cd355584fb461921298ed1ad14", "text": "File Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nContent Type: text\nHeader Path: Documents / Nodes/Usage Pattern/Document/Node Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2125\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_documents.md\nusage_nodes.md\nusage_metadata_extractor.md\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDocuments can either be created automatically via data loaders, or constructed manually.\n\nBy default, all of our data loaders (including those offered on LlamaHub) return `Document` objects through the `load_data` function.", "start_char_idx": 2882, "end_char_idx": 3967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1fc7dc8-1fe2-4476-b2e9-7e2134603e7c": {"__data__": {"id_": "c1fc7dc8-1fe2-4476-b2e9-7e2134603e7c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "node_type": "1", "metadata": {}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f014092-2ab1-476d-ba96-e5fd449c45e3", "node_type": "1", "metadata": {}, "hash": "9ce7b3078731baccbf62890878b2c451660f59cd355584fb461921298ed1ad14", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "node_type": "1", "metadata": {}, "hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "class_name": "RelatedNodeInfo"}}, "hash": "240837e08b10a0b91de30f4e5eb52d9a9192ac27c1bf76968e6dd9e20b033c17", "text": "File Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('./data').load_data()\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to construct documents manually. LlamaIndex exposes the `Document` struct.", "start_char_idx": 3969, "end_char_idx": 4981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdebdcf2-d10d-48f8-bed0-afdc55ddbffa": {"__data__": {"id_": "bdebdcf2-d10d-48f8-bed0-afdc55ddbffa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97474367-637c-42cb-bdbe-82a982b8ddac", "node_type": "1", "metadata": {}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7554fb30-8d54-4acd-a4de-dbbe5da79084", "node_type": "1", "metadata": {}, "hash": "c7f2b5a8787b41210ee5d2ef3d7f647c81f04aa93c5b76a8cb9c48c14a5cf714", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "97474367-637c-42cb-bdbe-82a982b8ddac", "node_type": "1", "metadata": {}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "class_name": "RelatedNodeInfo"}}, "hash": "1485ccea8bec3f956bf5be3314acc728117260cdf85080d7b0c7bb57167e2563", "text": "LlamaIndex exposes the `Document` struct.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Document\n\ntext_list = [text1, text2, .]\ndocuments = [Document(text=t) for t in text_list]\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo speed up prototyping and development, you can also quickly create a document using some default text:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Defining Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument = Document.example()\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7554fb30-8d54-4acd-a4de-dbbe5da79084": {"__data__": {"id_": "7554fb30-8d54-4acd-a4de-dbbe5da79084", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97474367-637c-42cb-bdbe-82a982b8ddac", "node_type": "1", "metadata": {}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdebdcf2-d10d-48f8-bed0-afdc55ddbffa", "node_type": "1", "metadata": {}, "hash": "1485ccea8bec3f956bf5be3314acc728117260cdf85080d7b0c7bb57167e2563", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "898e0035-4dc6-4af1-87c5-b5a3f8436aaf", "node_type": "1", "metadata": {}, "hash": "aa092b08e564d7a8dc2a9d8a26d6680b96789d907c12c7cb1de28fe65203f2f5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "97474367-637c-42cb-bdbe-82a982b8ddac", "node_type": "1", "metadata": {}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "class_name": "RelatedNodeInfo"}}, "hash": "c7f2b5a8787b41210ee5d2ef3d7f647c81f04aa93c5b76a8cb9c48c14a5cf714", "text": "md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis section covers various ways to customize `Document` objects. Since the `Document` object is a subclass of our `TextNode` object, all these settings and details apply to the `TextNode` object class as well.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDocuments also offer the chance to include useful metadata. Using the `metadata` dictionary on each document, additional information can be included to help inform responses and track down sources for query responses. This information can be anything, such as filenames or categories. If you are intergrating with a vector database, keep in mind that some vector databases require that the keys must be strings, and the values must be flat (either `str`, `float`, or `int`).\n\nAny information set in the `metadata` dictionary of each document will show up in the `metadata` of each source node created from the document. Additionaly, this information is included in the nodes, enabling the index to utilize it on queries and responses. By default, the metadata is injected into the text for both embedding and LLM model calls.\n\nThere are a few ways to set up this dictionary:\n\n1.", "start_char_idx": 1774, "end_char_idx": 3437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "898e0035-4dc6-4af1-87c5-b5a3f8436aaf": {"__data__": {"id_": "898e0035-4dc6-4af1-87c5-b5a3f8436aaf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97474367-637c-42cb-bdbe-82a982b8ddac", "node_type": "1", "metadata": {}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7554fb30-8d54-4acd-a4de-dbbe5da79084", "node_type": "1", "metadata": {}, "hash": "c7f2b5a8787b41210ee5d2ef3d7f647c81f04aa93c5b76a8cb9c48c14a5cf714", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47a8ccc3-e483-403c-87ab-2fd898383d67", "node_type": "1", "metadata": {}, "hash": "aa6f7d4cabd4656fba93bfda137b64338a03e65487b2a3f0ac2a51bfece825c7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "97474367-637c-42cb-bdbe-82a982b8ddac", "node_type": "1", "metadata": {}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "class_name": "RelatedNodeInfo"}}, "hash": "aa092b08e564d7a8dc2a9d8a26d6680b96789d907c12c7cb1de28fe65203f2f5", "text": "There are a few ways to set up this dictionary:\n\n1. In the document constructor:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument = Document(\n    text='text', \n    metadata={\n        'filename': '<doc_file_name>', \n        'category': '<category>'\n    }\n)\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n2.", "start_char_idx": 3386, "end_char_idx": 4444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47a8ccc3-e483-403c-87ab-2fd898383d67": {"__data__": {"id_": "47a8ccc3-e483-403c-87ab-2fd898383d67", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97474367-637c-42cb-bdbe-82a982b8ddac", "node_type": "1", "metadata": {}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "898e0035-4dc6-4af1-87c5-b5a3f8436aaf", "node_type": "1", "metadata": {}, "hash": "aa092b08e564d7a8dc2a9d8a26d6680b96789d907c12c7cb1de28fe65203f2f5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "97474367-637c-42cb-bdbe-82a982b8ddac", "node_type": "1", "metadata": {}, "hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "class_name": "RelatedNodeInfo"}}, "hash": "aa6f7d4cabd4656fba93bfda137b64338a03e65487b2a3f0ac2a51bfece825c7", "text": "After the document is created:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument.metadata = {'filename': '<doc_file_name>'}\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n3. Set the filename automatically using the `SimpleDirectoryReader` and `file_metadata` hook.", "start_char_idx": 4445, "end_char_idx": 5461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dec0a73-1cc9-4602-a065-f9142880083e": {"__data__": {"id_": "3dec0a73-1cc9-4602-a065-f9142880083e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad6dd30e-799b-4385-b420-d59372953192", "node_type": "1", "metadata": {}, "hash": "a7e433526e10b0bd8ddf7cc047e74cb5f2dd09378f9f522c2414f4b3f52cfe0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47ef4582-31ec-46fe-8d19-566262c354e5", "node_type": "1", "metadata": {}, "hash": "b2fb198eac297fdf122a23477e11cc31011859148a9ce71a1b94ea1925376ac9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ad6dd30e-799b-4385-b420-d59372953192", "node_type": "1", "metadata": {}, "hash": "a7e433526e10b0bd8ddf7cc047e74cb5f2dd09378f9f522c2414f4b3f52cfe0e", "class_name": "RelatedNodeInfo"}}, "hash": "bb0200e54267e5cfa2b7d8f9d3eabe762ecd6d476575ca05f30fdb9cdab06957", "text": "Set the filename automatically using the `SimpleDirectoryReader` and `file_metadata` hook. This will automatically run the hook on each document to set the `metadata` field:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Metadata\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\nfilename_fn = lambda filename: {'file_name': filename}\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/automatically sets the metadata of each document according to filename_fn\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('./data', file_metadata=filename_fn)\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Customizing the id\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAs detailed in the section Document Management, the doc `id_` is used to enable effecient refreshing of documents in the index.", "start_char_idx": 0, "end_char_idx": 1808, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47ef4582-31ec-46fe-8d19-566262c354e5": {"__data__": {"id_": "47ef4582-31ec-46fe-8d19-566262c354e5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad6dd30e-799b-4385-b420-d59372953192", "node_type": "1", "metadata": {}, "hash": "a7e433526e10b0bd8ddf7cc047e74cb5f2dd09378f9f522c2414f4b3f52cfe0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dec0a73-1cc9-4602-a065-f9142880083e", "node_type": "1", "metadata": {}, "hash": "bb0200e54267e5cfa2b7d8f9d3eabe762ecd6d476575ca05f30fdb9cdab06957", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9322d884-94e3-45db-a0e1-ff6d3602ccf6", "node_type": "1", "metadata": {}, "hash": "f85e4db56f2f5d713240df54682e831531dcd4101ed8160ddea1bb7f7d293f05", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ad6dd30e-799b-4385-b420-d59372953192", "node_type": "1", "metadata": {}, "hash": "a7e433526e10b0bd8ddf7cc047e74cb5f2dd09378f9f522c2414f4b3f52cfe0e", "class_name": "RelatedNodeInfo"}}, "hash": "b2fb198eac297fdf122a23477e11cc31011859148a9ce71a1b94ea1925376ac9", "text": "When using the `SimpleDirectoryReader`, you can automatically set the doc `id_` to be the full path to each document:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Customizing the id\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data\", filename_as_id=True).load_data()\nprint([x.doc_id for x in documents])\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Customizing the id\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also set the `id_` of any `Document` or `TextNode` directly!\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Customizing the id\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument.id_ = \"My new document id!\"", "start_char_idx": 1809, "end_char_idx": 3483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9322d884-94e3-45db-a0e1-ff6d3602ccf6": {"__data__": {"id_": "9322d884-94e3-45db-a0e1-ff6d3602ccf6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad6dd30e-799b-4385-b420-d59372953192", "node_type": "1", "metadata": {}, "hash": "a7e433526e10b0bd8ddf7cc047e74cb5f2dd09378f9f522c2414f4b3f52cfe0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47ef4582-31ec-46fe-8d19-566262c354e5", "node_type": "1", "metadata": {}, "hash": "b2fb198eac297fdf122a23477e11cc31011859148a9ce71a1b94ea1925376ac9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ad6dd30e-799b-4385-b420-d59372953192", "node_type": "1", "metadata": {}, "hash": "a7e433526e10b0bd8ddf7cc047e74cb5f2dd09378f9f522c2414f4b3f52cfe0e", "class_name": "RelatedNodeInfo"}}, "hash": "f85e4db56f2f5d713240df54682e831531dcd4101ed8160ddea1bb7f7d293f05", "text": "File Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA key detail mentioned above is that by default, any metadata you set is included in the embeddings generation and LLM.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing LLM Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTypically, a document might have many metadata keys, but you might not want all of them visibile to the LLM during response synthesis. In the above examples, we may not want the LLM to read the `file_name` of our document. However, the `file_name` might include information that will help generate better embeddings. A key advantage of doing this is to bias the embeddings for retrieval without changing what the LLM ends up reading.", "start_char_idx": 3485, "end_char_idx": 4957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de6ba78a-53e3-4297-967c-9d0490004a20": {"__data__": {"id_": "de6ba78a-53e3-4297-967c-9d0490004a20", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9", "node_type": "1", "metadata": {}, "hash": "cf0dcf293c895e4b6aae5368bf837f89c99af86f74ebf04689f0a2f6b57b923e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e853cb59-60ac-4f2a-99d1-828b877e5ab3", "node_type": "1", "metadata": {}, "hash": "9cf0d2ea3ee005f558102927c271090077993d7170f9c93690c6dcc58a14e720", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9", "node_type": "1", "metadata": {}, "hash": "cf0dcf293c895e4b6aae5368bf837f89c99af86f74ebf04689f0a2f6b57b923e", "class_name": "RelatedNodeInfo"}}, "hash": "886a8f54155e24d7d6ad38fd59a990663c6bb2d2b4b28a8baccb6a039db5b2b4", "text": "We can exclude it like so:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing LLM Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument.excluded_llm_metadata_keys = ['file_name']\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing LLM Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen, we can test what the LLM will actually end up reading using the `get_content()` function and specifying `MetadataMode.LLM`:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing LLM Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import MetadataMode\nprint(document.get_content(metadata_mode=MetadataMode.LLM))\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.", "start_char_idx": 0, "end_char_idx": 1817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e853cb59-60ac-4f2a-99d1-828b877e5ab3": {"__data__": {"id_": "e853cb59-60ac-4f2a-99d1-828b877e5ab3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9", "node_type": "1", "metadata": {}, "hash": "cf0dcf293c895e4b6aae5368bf837f89c99af86f74ebf04689f0a2f6b57b923e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de6ba78a-53e3-4297-967c-9d0490004a20", "node_type": "1", "metadata": {}, "hash": "886a8f54155e24d7d6ad38fd59a990663c6bb2d2b4b28a8baccb6a039db5b2b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecfd65a4-f974-4554-ba86-4805cab833b1", "node_type": "1", "metadata": {}, "hash": "f602a0d021d6c79350be9a199e05aa6772eca1228b6be3da61a53466900b8faf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9", "node_type": "1", "metadata": {}, "hash": "cf0dcf293c895e4b6aae5368bf837f89c99af86f74ebf04689f0a2f6b57b923e", "class_name": "RelatedNodeInfo"}}, "hash": "9cf0d2ea3ee005f558102927c271090077993d7170f9c93690c6dcc58a14e720", "text": "LLM))\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing Embedding Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSimilar to customing the metadata visibile to the LLM, we can also customize the metadata visible to emebddings. In this case, you can specifically exclude metadata visible to the embedding model, in case you DON'T want particular text to bias the embeddings.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing Embedding Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument.excluded_embed_metadata_keys = ['file_name']\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing Embedding Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen, we can test what the embedding model will actually end up reading using the `get_content()` function and specifying `MetadataMode.EMBED`:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.", "start_char_idx": 1732, "end_char_idx": 3718, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecfd65a4-f974-4554-ba86-4805cab833b1": {"__data__": {"id_": "ecfd65a4-f974-4554-ba86-4805cab833b1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9", "node_type": "1", "metadata": {}, "hash": "cf0dcf293c895e4b6aae5368bf837f89c99af86f74ebf04689f0a2f6b57b923e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e853cb59-60ac-4f2a-99d1-828b877e5ab3", "node_type": "1", "metadata": {}, "hash": "9cf0d2ea3ee005f558102927c271090077993d7170f9c93690c6dcc58a14e720", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9", "node_type": "1", "metadata": {}, "hash": "cf0dcf293c895e4b6aae5368bf837f89c99af86f74ebf04689f0a2f6b57b923e", "class_name": "RelatedNodeInfo"}}, "hash": "f602a0d021d6c79350be9a199e05aa6772eca1228b6be3da61a53466900b8faf", "text": "EMBED`:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing Embedding Metadata Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import MetadataMode\nprint(document.get_content(metadata_mode=MetadataMode.EMBED))\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Customizing Metadata Format\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAs you know by now, metadata is injected into the actual text of each document/node when sent to the LLM or embedding model. By default, the format of this metadata is controlled by three attributes:\n\n1. `Document.metadata_seperator` -> default = `\"\\n\"`\n\nWhen concatenating all key/value fields of your metadata, this field controls the seperator bewtween each key/value pair.\n\n2. `Document.metadata_template` -> default = `\"{key}: {value}\"`\n\nThis attribute controls how each key/value pair in your metadata is formatted. The two variables `key` and `value` string keys are required.\n\n3. `Document.text_template` -> default = `{metadata_str}\\n\\n{content}`\n\nOnce your metadata is converted into a string using `metadata_seperator` and `metadata_template`, this templates controls what that metadata looks like when joined with the text content of your document/node. The `metadata` and `content` string keys are required.", "start_char_idx": 3631, "end_char_idx": 5619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e846f65e-f37d-46c8-868e-ea192deecbb1": {"__data__": {"id_": "e846f65e-f37d-46c8-868e-ea192deecbb1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa8855d7-51de-40ee-842f-c3acdf2ce809", "node_type": "1", "metadata": {}, "hash": "ec60c4d7677f32291466d5d8fb00d539015b65a4ccfc4e5fd9c7003ff4dba02a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64b66bb3-6599-463d-b7d6-5913638e26e0", "node_type": "1", "metadata": {}, "hash": "c3e2a8a853b3f5232854222fec32b8db36afa3012336710aeb0aa8be3aaeb754", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fa8855d7-51de-40ee-842f-c3acdf2ce809", "node_type": "1", "metadata": {}, "hash": "ec60c4d7677f32291466d5d8fb00d539015b65a4ccfc4e5fd9c7003ff4dba02a", "class_name": "RelatedNodeInfo"}}, "hash": "3170ea2caffc5b4fe183246f09635145b055b4a41960a0f4f67bdf1e29dd0590", "text": "The `metadata` and `content` string keys are required.\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Summary\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nKnowing all this, let's create a short example using all this power:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Summary\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Document\nfrom llama_index.schema import MetadataMode\n\ndocument = Document(\n    text=\"This is a super-customized document\",\n    metadata={\n        \"file_name\": \"super_secret_document.txt\",\n        \"category\": \"finance\",\n        \"author\": \"LlamaIndex\"    \n    },\n    excluded_llm_metadata_keys=['file_name'],\n    metadata_seperator=\"::\",\n    metadata_template=\"{key}=>{value}\",\n    text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n)\n\nprint(\"The LLM sees this: \\n\", document.get_content(metadata_mode=MetadataMode.LLM))\nprint(\"The Embedding model sees this: \\n\", document.get_content(metadata_mode=MetadataMode.EMBED))\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Advanced - Automatic Metadata Extraction\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.", "start_char_idx": 0, "end_char_idx": 2022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64b66bb3-6599-463d-b7d6-5913638e26e0": {"__data__": {"id_": "64b66bb3-6599-463d-b7d6-5913638e26e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa8855d7-51de-40ee-842f-c3acdf2ce809", "node_type": "1", "metadata": {}, "hash": "ec60c4d7677f32291466d5d8fb00d539015b65a4ccfc4e5fd9c7003ff4dba02a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e846f65e-f37d-46c8-868e-ea192deecbb1", "node_type": "1", "metadata": {}, "hash": "3170ea2caffc5b4fe183246f09635145b055b4a41960a0f4f67bdf1e29dd0590", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fa8855d7-51de-40ee-842f-c3acdf2ce809", "node_type": "1", "metadata": {}, "hash": "ec60c4d7677f32291466d5d8fb00d539015b65a4ccfc4e5fd9c7003ff4dba02a", "class_name": "RelatedNodeInfo"}}, "hash": "c3e2a8a853b3f5232854222fec32b8db36afa3012336710aeb0aa8be3aaeb754", "text": "md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe have initial examples of using LLMs themselves to perform metadata extraction.\n\nTake a look here!\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nContent Type: text\nHeader Path: Defining and Customizing Documents/Customizing Documents/Advanced - Metadata Customization/Advanced - Automatic Metadata Extraction\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md\nfile_name: usage_documents.md\nfile_type: None\nfile_size: 7039\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\n/examples/metadata_extraction/MetadataExtractionSEC.ipynb\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nContent Type: text\nHeader Path: Automated Metadata Extraction for Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nfile_name: usage_metadata_extractor.md\nfile_type: None\nfile_size: 1424\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use LLMs to automate metadata extraction with our `MetadataExtractor` modules.\n\nOur metadata extractor modules include the following \"feature extractors\":\n- `SummaryExtractor` - automatically extracts a summary over a set of Nodes\n- `QuestionsAnsweredExtractor` - extracts a set of questions that each Node can answer\n- `TitleExtractor` - extracts a title over the context of each Node\n\nYou can use these feature extractors within our overall `MetadataExtractor` class.", "start_char_idx": 2022, "end_char_idx": 3739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e87c606-e67e-4afb-b738-5a26b6e05584": {"__data__": {"id_": "4e87c606-e67e-4afb-b738-5a26b6e05584", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7", "node_type": "1", "metadata": {}, "hash": "26f84ce32d42a77c94f10e0aec56543bdd5b5aaf79f3d51e97b5103a638f71d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00d0fc93-663f-4feb-8c25-03e5d2998334", "node_type": "1", "metadata": {}, "hash": "3dd9daf956dec3cfddd70c33c78ee2de61360f4021bf80a9ce06513df20bbbed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7", "node_type": "1", "metadata": {}, "hash": "26f84ce32d42a77c94f10e0aec56543bdd5b5aaf79f3d51e97b5103a638f71d6", "class_name": "RelatedNodeInfo"}}, "hash": "5c0bc19293900a5a1b909a87af247bf297325d27f8bf0d34120bde066c022819", "text": "Then you can plug in the `MetadataExtractor` into our node parser:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nContent Type: text\nHeader Path: Automated Metadata Extraction for Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nfile_name: usage_metadata_extractor.md\nfile_type: None\nfile_size: 1424\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser.extractors import (\n    MetadataExtractor,\n    TitleExtractor,\n    QuestionsAnsweredExtractor\n)\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)\nmetadata_extractor = MetadataExtractor(\n    extractors=[\n        TitleExtractor(nodes=5),\n        QuestionsAnsweredExtractor(questions=3),\n    ],\n)\n\nnode_parser = SimpleNodeParser(\n    text_splitter=text_splitter,\n    metadata_extractor=metadata_extractor,\n)\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nContent Type: text\nHeader Path: assume documents are defined -> extract nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nfile_name: usage_metadata_extractor.md\nfile_type: None\nfile_size: 1424\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnodes = node_parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nContent Type: code\nHeader Path: assume documents are defined -> extract nodes\nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md\nfile_name: usage_metadata_extractor.", "start_char_idx": 0, "end_char_idx": 1809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00d0fc93-663f-4feb-8c25-03e5d2998334": {"__data__": {"id_": "00d0fc93-663f-4feb-8c25-03e5d2998334", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7", "node_type": "1", "metadata": {}, "hash": "26f84ce32d42a77c94f10e0aec56543bdd5b5aaf79f3d51e97b5103a638f71d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e87c606-e67e-4afb-b738-5a26b6e05584", "node_type": "1", "metadata": {}, "hash": "5c0bc19293900a5a1b909a87af247bf297325d27f8bf0d34120bde066c022819", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd4513ae-9145-41ff-8297-a26e911309ff", "node_type": "1", "metadata": {}, "hash": "b028581e5325709c8bd179b89d320db983aba6306d5583330c12f1da2c1a60cf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7", "node_type": "1", "metadata": {}, "hash": "26f84ce32d42a77c94f10e0aec56543bdd5b5aaf79f3d51e97b5103a638f71d6", "class_name": "RelatedNodeInfo"}}, "hash": "3dd9daf956dec3cfddd70c33c78ee2de61360f4021bf80a9ce06513df20bbbed", "text": "md\nfile_name: usage_metadata_extractor.md\nfile_type: None\nfile_size: 1424\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\ncaption: Metadata Extraction Guides\nmaxdepth: 1\n---\n/examples/metadata_extraction/MetadataExtractionSEC.ipynb\n```\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: Defining and Customizing Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNodes represent \"chunks\" of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information\nwith other nodes and index structures.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes.", "start_char_idx": 1770, "end_char_idx": 2831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd4513ae-9145-41ff-8297-a26e911309ff": {"__data__": {"id_": "bd4513ae-9145-41ff-8297-a26e911309ff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7", "node_type": "1", "metadata": {}, "hash": "26f84ce32d42a77c94f10e0aec56543bdd5b5aaf79f3d51e97b5103a638f71d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00d0fc93-663f-4feb-8c25-03e5d2998334", "node_type": "1", "metadata": {}, "hash": "3dd9daf956dec3cfddd70c33c78ee2de61360f4021bf80a9ce06513df20bbbed", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7", "node_type": "1", "metadata": {}, "hash": "26f84ce32d42a77c94f10e0aec56543bdd5b5aaf79f3d51e97b5103a638f71d6", "class_name": "RelatedNodeInfo"}}, "hash": "b028581e5325709c8bd179b89d320db983aba6306d5583330c12f1da2c1a60cf", "text": "You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes.\n\nFor instance, you can do\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: Defining and Customizing Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser import SimpleNodeParser\n\nparser = SimpleNodeParser()\n\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: Defining and Customizing Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to construct Node objects manually and skip the first section.", "start_char_idx": 2739, "end_char_idx": 3820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8aa81e5-9c1a-4994-9576-f420e4c763a4": {"__data__": {"id_": "d8aa81e5-9c1a-4994-9576-f420e4c763a4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b07194c-2acc-4baa-8e31-a367741156c1", "node_type": "1", "metadata": {}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "889388cb-178c-4046-9c58-3b9e2a38b89c", "node_type": "1", "metadata": {}, "hash": "ae6e486b71a0f5eb295639db51f837d5e1d0cc7ada0b4e792edc5df69d82c3ef", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0b07194c-2acc-4baa-8e31-a367741156c1", "node_type": "1", "metadata": {}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "class_name": "RelatedNodeInfo"}}, "hash": "b1645a4fab6a91a09f8e9ad50a08dff831181affbab7b249dc5143e739f78a20", "text": "For instance,\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: Defining and Customizing Nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n\nnode1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\nnode2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: set relationships\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnode1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id)\nnode2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id)\nnodes = [node1, node2]\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: set relationships\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `RelatedNodeInfo` class can also store additional `metadata` if needed:\n\nFile Name: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nContent Type: text\nHeader Path: set relationships\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md\nfile_name: usage_nodes.", "start_char_idx": 0, "end_char_idx": 1800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "889388cb-178c-4046-9c58-3b9e2a38b89c": {"__data__": {"id_": "889388cb-178c-4046-9c58-3b9e2a38b89c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b07194c-2acc-4baa-8e31-a367741156c1", "node_type": "1", "metadata": {}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8aa81e5-9c1a-4994-9576-f420e4c763a4", "node_type": "1", "metadata": {}, "hash": "b1645a4fab6a91a09f8e9ad50a08dff831181affbab7b249dc5143e739f78a20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2f975fc-d497-40fc-83df-27ee09d7dc64", "node_type": "1", "metadata": {}, "hash": "e685d78d2f666b77e2c958ccfd8832729c84efafeecc43e0115f34fa44074ce4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0b07194c-2acc-4baa-8e31-a367741156c1", "node_type": "1", "metadata": {}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "class_name": "RelatedNodeInfo"}}, "hash": "ae6e486b71a0f5eb295639db51f837d5e1d0cc7ada0b4e792edc5df69d82c3ef", "text": "md\nfile_name: usage_nodes.md\nfile_type: None\nfile_size: 1354\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnode2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=node1.node_id, metadata={\"key\": \"val\"})\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex offers **composability** of your indices, meaning that you can build indices on top of other indices. This allows you to more effectively index your entire document tree in order to feed custom knowledge to GPT.\n\nComposability allows you to to define lower-level indices for each document, and higher-order indices over a collection of documents. To see how this works, imagine defining 1) a tree index for the text within each document, and 2) a list index over each tree index (one document) within your collection.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Subindices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo see how this works, imagine you have 3 documents: `doc1`, `doc2`, and `doc3`.", "start_char_idx": 1774, "end_char_idx": 3332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2f975fc-d497-40fc-83df-27ee09d7dc64": {"__data__": {"id_": "f2f975fc-d497-40fc-83df-27ee09d7dc64", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b07194c-2acc-4baa-8e31-a367741156c1", "node_type": "1", "metadata": {}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "889388cb-178c-4046-9c58-3b9e2a38b89c", "node_type": "1", "metadata": {}, "hash": "ae6e486b71a0f5eb295639db51f837d5e1d0cc7ada0b4e792edc5df69d82c3ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1335bd5a-3236-4891-9db8-379da4c1ca05", "node_type": "1", "metadata": {}, "hash": "7190196b248df9825743b9d1aea45c516bc4b8a483cecd2d819d934eb10831c6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0b07194c-2acc-4baa-8e31-a367741156c1", "node_type": "1", "metadata": {}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "class_name": "RelatedNodeInfo"}}, "hash": "e685d78d2f666b77e2c958ccfd8832729c84efafeecc43e0115f34fa44074ce4", "text": "File Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Subindices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\n\ndoc1 = SimpleDirectoryReader('data1').load_data()\ndoc2 = SimpleDirectoryReader('data2').load_data()\ndoc3 = SimpleDirectoryReader('data3').load_data()\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Subindices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nNow let's define a tree index for each document. In order to persist the graph later, each index should share the same storage context.", "start_char_idx": 3334, "end_char_idx": 4376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1335bd5a-3236-4891-9db8-379da4c1ca05": {"__data__": {"id_": "1335bd5a-3236-4891-9db8-379da4c1ca05", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b07194c-2acc-4baa-8e31-a367741156c1", "node_type": "1", "metadata": {}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2f975fc-d497-40fc-83df-27ee09d7dc64", "node_type": "1", "metadata": {}, "hash": "e685d78d2f666b77e2c958ccfd8832729c84efafeecc43e0115f34fa44074ce4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0b07194c-2acc-4baa-8e31-a367741156c1", "node_type": "1", "metadata": {}, "hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "class_name": "RelatedNodeInfo"}}, "hash": "7190196b248df9825743b9d1aea45c516bc4b8a483cecd2d819d934eb10831c6", "text": "In order to persist the graph later, each index should share the same storage context.\n\nIn Python, we have:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Subindices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import TreeIndex\n\nstorage_context = storage_context.from_defaults()\n\nindex1 = TreeIndex.from_documents(doc1, storage_context=storage_context)\nindex2 = TreeIndex.from_documents(doc2, storage_context=storage_context)\nindex3 = TreeIndex.from_documents(doc3, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Subindices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!", "start_char_idx": 4290, "end_char_idx": 5412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd274941-455a-48b8-b720-cee0ae61075d": {"__data__": {"id_": "fd274941-455a-48b8-b720-cee0ae61075d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9504f2b8-a7b0-4ab2-960a-0c704dcf9bee", "node_type": "1", "metadata": {}, "hash": "3f94fa8b77888623c16a8e1bd1b4a48100e8b448942e5e88cdb37e7b3c54941f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}}, "hash": "326cb317ee85019efd15db2f6c1c97ab31a14a4cf8737fa0af2a17c9a1161b5c", "text": "File Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Summary Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou then need to explicitly define *summary text* for each subindex. This allows  \nthe subindices to be used as Documents for higher-level indices.", "start_char_idx": 0, "end_char_idx": 502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9504f2b8-a7b0-4ab2-960a-0c704dcf9bee": {"__data__": {"id_": "9504f2b8-a7b0-4ab2-960a-0c704dcf9bee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd274941-455a-48b8-b720-cee0ae61075d", "node_type": "1", "metadata": {}, "hash": "326cb317ee85019efd15db2f6c1c97ab31a14a4cf8737fa0af2a17c9a1161b5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfed0e6f-d2bd-4f6d-8cb6-f200ee70a83a", "node_type": "1", "metadata": {}, "hash": "5f701fa1370144d71e782ff0921f921a7173268acca95e8b083f5f13d3871e63", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}}, "hash": "3f94fa8b77888623c16a8e1bd1b4a48100e8b448942e5e88cdb37e7b3c54941f", "text": "This allows  \nthe subindices to be used as Documents for higher-level indices.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Summary Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex1_summary = \"<summary1>\"\nindex2_summary = \"<summary2>\"\nindex3_summary = \"<summary3>\"\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Summary Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou may choose to manually specify the summary text, or use LlamaIndex itself to generate\na summary, for instance with the following:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Summary Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nsummary = index1.query(\n    \"What is a summary of this document?", "start_char_idx": 424, "end_char_idx": 1859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfed0e6f-d2bd-4f6d-8cb6-f200ee70a83a": {"__data__": {"id_": "bfed0e6f-d2bd-4f6d-8cb6-f200ee70a83a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9504f2b8-a7b0-4ab2-960a-0c704dcf9bee", "node_type": "1", "metadata": {}, "hash": "3f94fa8b77888623c16a8e1bd1b4a48100e8b448942e5e88cdb37e7b3c54941f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0321e342-6af1-4315-a0e3-a299b091a7e1", "node_type": "1", "metadata": {}, "hash": "cdd6be7e959b5f7c33012810258816f86b7f1175a87e66d8b69afe665148b9cd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}}, "hash": "5f701fa1370144d71e782ff0921f921a7173268acca95e8b083f5f13d3871e63", "text": "\", retriever_mode=\"all_leaf\"\n)\nindex1_summary = str(summary)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Defining Summary Text\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**If specified**, this summary text for each subindex can be used to refine the answer during query-time.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Creating a Graph with a Top-Level Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can then create a graph with a list index on top of these 3 tree indices:\nWe can query, save, and load the graph to/from disk as any other index.", "start_char_idx": 1859, "end_char_idx": 2904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0321e342-6af1-4315-a0e3-a299b091a7e1": {"__data__": {"id_": "0321e342-6af1-4315-a0e3-a299b091a7e1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfed0e6f-d2bd-4f6d-8cb6-f200ee70a83a", "node_type": "1", "metadata": {}, "hash": "5f701fa1370144d71e782ff0921f921a7173268acca95e8b083f5f13d3871e63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "686fb86d-ffbb-46f0-819e-3495e7ff4547", "node_type": "1", "metadata": {}, "hash": "71d602335d61014dd592ed125e398a0bf56eb87e2cf789c13e28c0f17f7568f8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}}, "hash": "cdd6be7e959b5f7c33012810258816f86b7f1175a87e66d8b69afe665148b9cd", "text": "File Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Creating a Graph with a Top-Level Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.composability import ComposableGraph\n\ngraph = ComposableGraph.from_indices(\n    ListIndex,\n    [index1, index2, index3],\n    index_summaries=[index1_summary, index2_summary, index3_summary],\n    storage_context=storage_context,\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Creating a Graph with a Top-Level Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/Querying the Graph\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDuring a query, we would start with the top-level list index. Each node in the list corresponds to an underlying tree index. \nThe query will be executed recursively, starting from the root index, then the sub-indices.\nThe default query engine for each index is called under the hood (i.e. `index.as_query_engine()`), unless otherwise configured by passing `custom_query_engines` to the `ComposableGraphQueryEngine`.", "start_char_idx": 2906, "end_char_idx": 4678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "686fb86d-ffbb-46f0-819e-3495e7ff4547": {"__data__": {"id_": "686fb86d-ffbb-46f0-819e-3495e7ff4547", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0321e342-6af1-4315-a0e3-a299b091a7e1", "node_type": "1", "metadata": {}, "hash": "cdd6be7e959b5f7c33012810258816f86b7f1175a87e66d8b69afe665148b9cd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611", "node_type": "1", "metadata": {}, "hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "class_name": "RelatedNodeInfo"}}, "hash": "71d602335d61014dd592ed125e398a0bf56eb87e2cf789c13e28c0f17f7568f8", "text": "Below we show an example that configure the tree index retrievers to use `child_branch_factor=2` (instead of the default `child_branch_factor=1`).\n\n\nMore detail on how to configure `ComposableGraphQueryEngine` can be found here.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/set custom retrievers.", "start_char_idx": 4679, "end_char_idx": 5042, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1843d26d-92bb-4f9c-ba21-bfb76b1ed4b0": {"__data__": {"id_": "1843d26d-92bb-4f9c-ba21-bfb76b1ed4b0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "node_type": "1", "metadata": {}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d49d111-1bfc-4311-bc44-8a6c151f6156", "node_type": "1", "metadata": {}, "hash": "149efcb818412eb826620a2b3b86afee8e7af8be25b984d482201d9cec1e3ba5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "node_type": "1", "metadata": {}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "class_name": "RelatedNodeInfo"}}, "hash": "65aaf6ff6fdc8312600f6dbb238a14543558917669b03787fde1bc83765b179b", "text": "An example is provided below\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncustom_query_engines = {\n    index.index_id: index.as_query_engine(\n        child_branch_factor=2\n    ) \n    for index in [index1, index2, index3]\n}\nquery_engine = graph.as_query_engine(\n    custom_query_engines=custom_query_engines\n)\nresponse = query_engine.query(\"Where did the author grow up?\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/set custom retrievers. An example is provided below\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note that specifying custom retriever for index by id\n> might require you to inspect e.g., `index1.index_id`.\n> Alternatively, you can explicitly set it as follows:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/set custom retrievers. An example is provided below\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex1.set_index_id(\"<index_id_1>\")\nindex2.set_index_id(\"<index_id_2>\")\nindex3.set_index_id(\"<index_id_3>\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/set custom retrievers.", "start_char_idx": 0, "end_char_idx": 1730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d49d111-1bfc-4311-bc44-8a6c151f6156": {"__data__": {"id_": "3d49d111-1bfc-4311-bc44-8a6c151f6156", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "node_type": "1", "metadata": {}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1843d26d-92bb-4f9c-ba21-bfb76b1ed4b0", "node_type": "1", "metadata": {}, "hash": "65aaf6ff6fdc8312600f6dbb238a14543558917669b03787fde1bc83765b179b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1abf506b-8336-41bc-b55b-f13581d2a7da", "node_type": "1", "metadata": {}, "hash": "72c321a9cbf19b5de7c8f00ea84c98cb073d1d871f8cdff28bfa0574c73b3090", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "node_type": "1", "metadata": {}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "class_name": "RelatedNodeInfo"}}, "hash": "149efcb818412eb826620a2b3b86afee8e7af8be25b984d482201d9cec1e3ba5", "text": "An example is provided below\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nSo within a node, instead of fetching the text, we would recursively query the stored tree index to retrieve our answer.\n\n!\n\nNOTE: You can stack indices as many times as you want, depending on the hierarchies of your knowledge base!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/[Optional] Persisting the Graph\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe graph can also be persisted to storage, and then loaded again when needed. Note that you'll need to set the \nID of the root index, or keep track of the default.", "start_char_idx": 1731, "end_char_idx": 2748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1abf506b-8336-41bc-b55b-f13581d2a7da": {"__data__": {"id_": "1abf506b-8336-41bc-b55b-f13581d2a7da", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "node_type": "1", "metadata": {}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d49d111-1bfc-4311-bc44-8a6c151f6156", "node_type": "1", "metadata": {}, "hash": "149efcb818412eb826620a2b3b86afee8e7af8be25b984d482201d9cec1e3ba5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "406cd354-54c5-4304-bbfd-b3d99e3b37a1", "node_type": "1", "metadata": {}, "hash": "43c525080a6f4dc102501ee14e1544b395e0d0b5aa10ca87d19bb24c4ff42324", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "node_type": "1", "metadata": {}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "class_name": "RelatedNodeInfo"}}, "hash": "72c321a9cbf19b5de7c8f00ea84c98cb073d1d871f8cdff28bfa0574c73b3090", "text": "File Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/set the ID\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph.root_index.set_index_id(\"my_id\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/persist to storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph.root_index.storage_context.persist(persist_dir=\"./storage\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/load\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import StorageContext, load_graph_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\ngraph = load_graph_from_storage(storage_context, root_id=\"my_id\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/load\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can take a look at a code example below as well.", "start_char_idx": 2750, "end_char_idx": 4484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "406cd354-54c5-4304-bbfd-b3d99e3b37a1": {"__data__": {"id_": "406cd354-54c5-4304-bbfd-b3d99e3b37a1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "node_type": "1", "metadata": {}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1abf506b-8336-41bc-b55b-f13581d2a7da", "node_type": "1", "metadata": {}, "hash": "72c321a9cbf19b5de7c8f00ea84c98cb073d1d871f8cdff28bfa0574c73b3090", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "node_type": "1", "metadata": {}, "hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "class_name": "RelatedNodeInfo"}}, "hash": "43c525080a6f4dc102501ee14e1544b395e0d0b5aa10ca87d19bb24c4ff42324", "text": "We first build two tree indices, one over the Wikipedia NYC page, and the other over Paul Graham's essay. We then define a keyword extractor index over the two tree indices.\n\nHere is an example notebook.", "start_char_idx": 4485, "end_char_idx": 4688, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffbf1723-e016-411a-86f7-05f2442a7283": {"__data__": {"id_": "ffbf1723-e016-411a-86f7-05f2442a7283", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f", "node_type": "1", "metadata": {}, "hash": "61a26eec879d891a2a323e46275b58c8696878d3ddbcc8c9c858d6f0055e7ff0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f03a32ac-46e0-444a-8581-738e7d5e040b", "node_type": "1", "metadata": {}, "hash": "f10c217862d994157a2c8675c5052ca6fdf1c1226adfc88b0c63cb3d9978f215", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f", "node_type": "1", "metadata": {}, "hash": "61a26eec879d891a2a323e46275b58c8696878d3ddbcc8c9c858d6f0055e7ff0", "class_name": "RelatedNodeInfo"}}, "hash": "2971877059a96b230ab45d2853d131c473d842e80fc9b1c560a0cf5173fb48ac", "text": "We then define a keyword extractor index over the two tree indices.\n\nHere is an example notebook.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\composability.md\nContent Type: text\nHeader Path: Composability/load\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\composability.md\nfile_name: composability.md\nfile_type: None\nfile_size: 5670\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\ncaption: Examples\nmaxdepth: 1\n---\n../../../../examples/composable_indices/ComposableIndices-Prior.ipynb\n../../../../examples/composable_indices/ComposableIndices-Weaviate.ipynb\n../../../../examples/composable_indices/ComposableIndices.ipynb\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMost LlamaIndex index structures allow for **insertion**, **deletion**, **update**, and **refresh** operations.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Insertion\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can \"insert\" a new Document into any index data structure, after building the index initially. This document will be broken down into nodes and ingested into the index.\n\nThe underlying mechanism behind insertion depends on the index structure. For instance, for the list index, a new Document is inserted as additional node(s) in the list.\nFor the vector store index, a new Document (and embeddings) is inserted into the underlying document/embedding store.\n\nAn example notebook showcasing our insert capabilities is given here.", "start_char_idx": 0, "end_char_idx": 2052, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f03a32ac-46e0-444a-8581-738e7d5e040b": {"__data__": {"id_": "f03a32ac-46e0-444a-8581-738e7d5e040b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f", "node_type": "1", "metadata": {}, "hash": "61a26eec879d891a2a323e46275b58c8696878d3ddbcc8c9c858d6f0055e7ff0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffbf1723-e016-411a-86f7-05f2442a7283", "node_type": "1", "metadata": {}, "hash": "2971877059a96b230ab45d2853d131c473d842e80fc9b1c560a0cf5173fb48ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9146699-a47a-4169-b3fc-d029991dec71", "node_type": "1", "metadata": {}, "hash": "0a55c006619d879cb395c69932aee7785b537c954c21543732368ca881810e99", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f", "node_type": "1", "metadata": {}, "hash": "61a26eec879d891a2a323e46275b58c8696878d3ddbcc8c9c858d6f0055e7ff0", "class_name": "RelatedNodeInfo"}}, "hash": "f10c217862d994157a2c8675c5052ca6fdf1c1226adfc88b0c63cb3d9978f215", "text": "An example notebook showcasing our insert capabilities is given here.\nIn this notebook we showcase how to construct an empty index, manually create Document objects, and add those to our index data structures.\n\nAn example code snippet is given below:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Insertion\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ListIndex, Document\n\nindex = ListIndex([])\ntext_chunks = ['text_chunk_1', 'text_chunk_2', 'text_chunk_3']\n\ndoc_chunks = []\nfor i, text in enumerate(text_chunks):\n    doc = Document(text=text, id_=f\"doc_id_{i}\")\n    doc_chunks.append(doc)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/insert\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfor doc_chunk in doc_chunks:\n    index.insert(doc_chunk)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Deletion\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can \"delete\" a Document from most index data structures by specifying a document_id. (**NOTE**: the tree index currently does not support deletion). All nodes corresponding to the document will be deleted.", "start_char_idx": 1983, "end_char_idx": 3862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9146699-a47a-4169-b3fc-d029991dec71": {"__data__": {"id_": "b9146699-a47a-4169-b3fc-d029991dec71", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f", "node_type": "1", "metadata": {}, "hash": "61a26eec879d891a2a323e46275b58c8696878d3ddbcc8c9c858d6f0055e7ff0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f03a32ac-46e0-444a-8581-738e7d5e040b", "node_type": "1", "metadata": {}, "hash": "f10c217862d994157a2c8675c5052ca6fdf1c1226adfc88b0c63cb3d9978f215", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f", "node_type": "1", "metadata": {}, "hash": "61a26eec879d891a2a323e46275b58c8696878d3ddbcc8c9c858d6f0055e7ff0", "class_name": "RelatedNodeInfo"}}, "hash": "0a55c006619d879cb395c69932aee7785b537c954c21543732368ca881810e99", "text": "All nodes corresponding to the document will be deleted.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Deletion\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.delete_ref_doc(\"doc_id_0\", delete_from_docstore=True)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Deletion\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n`delete_from_docstore` will default to `False` in case you are sharing nodes betweeen indexes using the same docstore. However, these nodes will not be used when querying when this is set to `False` as they will be deleted from the `index_struct` of the index, which keeps track of which nodes can be used for querying.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Update\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf a Document is already present within an index, you can \"update\" a Document with the same doc `id_` (for instance, if the information in the Document has changed).", "start_char_idx": 3806, "end_char_idx": 5507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d3e1f2a-7c5e-4655-8fb6-f3e37c9f09a4": {"__data__": {"id_": "2d3e1f2a-7c5e-4655-8fb6-f3e37c9f09a4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "node_type": "1", "metadata": {}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88e956ba-bb12-4885-8d75-71185ec887e6", "node_type": "1", "metadata": {}, "hash": "795dd27439477e0564863c08bd786dd04dc1c4ead75dc1cb64157ec47b3d5a27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "node_type": "1", "metadata": {}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "class_name": "RelatedNodeInfo"}}, "hash": "9dc5064226ecdb9b6da472fcd2fc87e53124c0b4a9682a8cfb1ce9ed28bb1fd7", "text": "File Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/NOTE: the document has a `doc_id` specified\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndoc_chunks[0].text = \"Brand new document text\"\nindex.update_ref_doc(\n    doc_chunks[0], \n    update_kwargs={\"delete_kwargs\": {'delete_from_docstore': True}}\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/NOTE: the document has a `doc_id` specified\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Refresh\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you set the doc `id_` of each document when loading your data, you can also automatically refresh the index.\n\nThe `refresh()` function will only update documents who have the same doc `id_`, but different text contents. Any documents not present in the index at all will also be inserted.\n\n`refresh()` also returns a boolean list, indicating which documents in the input have been refreshed in the index.", "start_char_idx": 0, "end_char_idx": 1850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88e956ba-bb12-4885-8d75-71185ec887e6": {"__data__": {"id_": "88e956ba-bb12-4885-8d75-71185ec887e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "node_type": "1", "metadata": {}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d3e1f2a-7c5e-4655-8fb6-f3e37c9f09a4", "node_type": "1", "metadata": {}, "hash": "9dc5064226ecdb9b6da472fcd2fc87e53124c0b4a9682a8cfb1ce9ed28bb1fd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96f11461-5762-44d2-b517-888dcc2de650", "node_type": "1", "metadata": {}, "hash": "d0a43e75fb20a32de7b78439400a06c47bafc145d32d2726678a7c5aac0aaa45", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "node_type": "1", "metadata": {}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "class_name": "RelatedNodeInfo"}}, "hash": "795dd27439477e0564863c08bd786dd04dc1c4ead75dc1cb64157ec47b3d5a27", "text": "File Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/modify first document, with the same doc_id\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndoc_chunks[0] = Document(text='Super new document text', id_=\"doc_id_0\")\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/add a new document\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndoc_chunks.append(Document(text=\"This isn't in the index yet, but it will be soon!", "start_char_idx": 1852, "end_char_idx": 2785, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96f11461-5762-44d2-b517-888dcc2de650": {"__data__": {"id_": "96f11461-5762-44d2-b517-888dcc2de650", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "node_type": "1", "metadata": {}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88e956ba-bb12-4885-8d75-71185ec887e6", "node_type": "1", "metadata": {}, "hash": "795dd27439477e0564863c08bd786dd04dc1c4ead75dc1cb64157ec47b3d5a27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35cb2b8d-cc7f-49ca-b85b-6ce401dc2336", "node_type": "1", "metadata": {}, "hash": "204f109aa3b3b0f49a1703fdcced28733ba2381b5341b7d4cb86376f509d9227", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "node_type": "1", "metadata": {}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "class_name": "RelatedNodeInfo"}}, "hash": "d0a43e75fb20a32de7b78439400a06c47bafc145d32d2726678a7c5aac0aaa45", "text": "\", id_=\"doc_id_3\"))\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/refresh the index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nrefreshed_docs = index.refresh_ref_docs(\n    doc_chunks,\n    update_kwargs={\"delete_kwargs\": {'delete_from_docstore': True}}\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: code\nHeader Path: Document Management/refreshed_docs[0] and refreshed_docs[-1] should be true\nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```\n\nAgain, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.", "start_char_idx": 2785, "end_char_idx": 3834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35cb2b8d-cc7f-49ca-b85b-6ce401dc2336": {"__data__": {"id_": "35cb2b8d-cc7f-49ca-b85b-6ce401dc2336", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "node_type": "1", "metadata": {}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96f11461-5762-44d2-b517-888dcc2de650", "node_type": "1", "metadata": {}, "hash": "d0a43e75fb20a32de7b78439400a06c47bafc145d32d2726678a7c5aac0aaa45", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "node_type": "1", "metadata": {}, "hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "class_name": "RelatedNodeInfo"}}, "hash": "204f109aa3b3b0f49a1703fdcced28733ba2381b5341b7d4cb86376f509d9227", "text": "This is of course optional.\n\nIf you `print()` the output of `refresh()`, you would see which input documents were refreshed:\n\n```python\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/refreshed_docs[0] and refreshed_docs[-1] should be true\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprint(refreshed_docs)\n> [True, False, False, True]\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/refreshed_docs[0] and refreshed_docs[-1] should be true\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis is most useful when you are reading from a directory that is constantly updating with new information.\n\nTo autmatically set the doc `id_` when using the `SimpleDirectoryReader`, you can set the `filename_as_id` flag. More details can be found here.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Document Tracking\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAny index that uses the docstore (i.e. all indexes except for most vector store integrations), you can also see which documents you have inserted into the docstore.", "start_char_idx": 3807, "end_char_idx": 5616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f083448-5542-4404-acee-af96835a9db8": {"__data__": {"id_": "6f083448-5542-4404-acee-af96835a9db8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "node_type": "1", "metadata": {}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48a96332-cc2f-4081-8f7d-c36963ea928b", "node_type": "1", "metadata": {}, "hash": "df28d3af2a029afb525d37c57809c4ce64933699a89686c95996c177058acca6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "node_type": "1", "metadata": {}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "class_name": "RelatedNodeInfo"}}, "hash": "899357273069354443b6dfd7f76c9b99e692a24d0b5266f75dd79d21b6bd9a4c", "text": "File Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Document Tracking\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprint(index.ref_doc_info)\n> {'doc_id_1': RefDocInfo(node_ids=['071a66a8-3c47-49ad-84fa-7010c6277479'], metadata={}), \n   'doc_id_2': RefDocInfo(node_ids=['9563e84b-f934-41c3-acfd-22e88492c869'], metadata={}), \n   'doc_id_0': RefDocInfo(node_ids=['b53e6c2f-16f7-4024-af4c-42890e945f36'], metadata={}), \n   'doc_id_3': RefDocInfo(node_ids=['6bedb29f-15db-4c7c-9885-7490e10aa33f'], metadata={})}\n\nFile Name: Docs\\core_modules\\data_modules\\index\\document_management.md\nContent Type: text\nHeader Path: Document Management/Document Tracking\nfile_path: Docs\\core_modules\\data_modules\\index\\document_management.md\nfile_name: document_management.md\nfile_type: None\nfile_size: 4938\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEach entry in the output shows the ingested doc `id_`s as keys, and their associated `node_ids` of the nodes they were split into. \n\nLastly, the orignal `metadata` dictionary of each input document is also tracked. You can read more about the `metadata` attribute in Customizing Documents.", "start_char_idx": 0, "end_char_idx": 1425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48a96332-cc2f-4081-8f7d-c36963ea928b": {"__data__": {"id_": "48a96332-cc2f-4081-8f7d-c36963ea928b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "node_type": "1", "metadata": {}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f083448-5542-4404-acee-af96835a9db8", "node_type": "1", "metadata": {}, "hash": "899357273069354443b6dfd7f76c9b99e692a24d0b5266f75dd79d21b6bd9a4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1227e1e5-4a48-4130-9f78-43623f4c1c86", "node_type": "1", "metadata": {}, "hash": "adaec6e390e15c9c88b6aae2c11bd52f64cac5030268bb3c0324a3cf1162755b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "node_type": "1", "metadata": {}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "class_name": "RelatedNodeInfo"}}, "hash": "df28d3af2a029afb525d37c57809c4ce64933699a89686c95996c177058acca6", "text": "You can read more about the `metadata` attribute in Customizing Documents.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis guide describes how each index works with diagrams. \n\nSome terminology:\n- **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects.\n- **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to \n    specify different response modes here.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe list index simply stores Nodes as a sequential chain.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Querying\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDuring query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\nour Response Synthesis module.\n\n!\n\nThe list index does offer numerous ways of querying a list index, from an embedding-based query which \nwill fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:\n\n!", "start_char_idx": 1351, "end_char_idx": 3248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1227e1e5-4a48-4130-9f78-43623f4c1c86": {"__data__": {"id_": "1227e1e5-4a48-4130-9f78-43623f4c1c86", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "node_type": "1", "metadata": {}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48a96332-cc2f-4081-8f7d-c36963ea928b", "node_type": "1", "metadata": {}, "hash": "df28d3af2a029afb525d37c57809c4ce64933699a89686c95996c177058acca6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e16341dd-63c3-4997-81b2-e69224f7c731", "node_type": "1", "metadata": {}, "hash": "3abfc045f326987c76114346f2bbf82b90b06dabad4ca3924a3571e5665428af", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "node_type": "1", "metadata": {}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "class_name": "RelatedNodeInfo"}}, "hash": "adaec6e390e15c9c88b6aae2c11bd52f64cac5030268bb3c0324a3cf1162755b", "text": "File Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Vector Store Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe vector store index stores each Node and a corresponding embedding in a Vector Store.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Querying\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuerying a vector store index involves fetching the top-k most similar Nodes, and passing\nthose into our Response Synthesis module.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Tree Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Querying\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuerying a tree index involves traversing from root nodes down \nto leaf nodes.", "start_char_idx": 3250, "end_char_idx": 5091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e16341dd-63c3-4997-81b2-e69224f7c731": {"__data__": {"id_": "e16341dd-63c3-4997-81b2-e69224f7c731", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "node_type": "1", "metadata": {}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1227e1e5-4a48-4130-9f78-43623f4c1c86", "node_type": "1", "metadata": {}, "hash": "adaec6e390e15c9c88b6aae2c11bd52f64cac5030268bb3c0324a3cf1162755b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af", "node_type": "1", "metadata": {}, "hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "class_name": "RelatedNodeInfo"}}, "hash": "3abfc045f326987c76114346f2bbf82b90b06dabad4ca3924a3571e5665428af", "text": "By default, (`child_branch_factor=1`), a query\nchooses one child node given a parent node. If `child_branch_factor=2`, a query\nchooses two child nodes per level.\n\n!", "start_char_idx": 5092, "end_char_idx": 5256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "663fb4ad-8fa4-4850-b686-345f93efcd36": {"__data__": {"id_": "663fb4ad-8fa4-4850-b686-345f93efcd36", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "node_type": "1", "metadata": {}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5cc27566-ccb0-4f57-a305-15c89e85721d", "node_type": "1", "metadata": {}, "hash": "7cee225c44e41cd18bcd5c577026c328ef378dc6a294e0e2add55adf428bf410", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "node_type": "1", "metadata": {}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "class_name": "RelatedNodeInfo"}}, "hash": "dc7e700360a7f0d3a00b1183494e05abcc251c8c67f4f04369258d8bef6e3348", "text": "If `child_branch_factor=2`, a query\nchooses two child nodes per level.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Keyword Table Index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe keyword table index extracts keywords from each Node and builds a mapping from \neach keyword to the corresponding Nodes of that keyword.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\index_guide.md\nContent Type: text\nHeader Path: How Each Index Works/List Index/Querying\nfile_path: Docs\\core_modules\\data_modules\\index\\index_guide.md\nfile_name: index_guide.md\nfile_type: None\nfile_size: 2327\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDuring query time, we extract relevant keywords from the query, and match those with pre-extracted\nNode keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our \nResponse Synthesis module.\n\n!\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Introduction\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn many cases, especially with long documents, a chunk of text may lack the context necessary to disambiguate the chunk from other similar chunks of text. \n\nTo combat this, we use LLMs to extract certain contextual information relevant to the document to better help the retrieval and language models disambiguate similar-looking passages.\n\nWe show this in an example notebook and demonstrate its effectiveness in processing long documents.", "start_char_idx": 0, "end_char_idx": 1958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5cc27566-ccb0-4f57-a305-15c89e85721d": {"__data__": {"id_": "5cc27566-ccb0-4f57-a305-15c89e85721d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "node_type": "1", "metadata": {}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "663fb4ad-8fa4-4850-b686-345f93efcd36", "node_type": "1", "metadata": {}, "hash": "dc7e700360a7f0d3a00b1183494e05abcc251c8c67f4f04369258d8bef6e3348", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "842b2d1f-a046-4f59-9141-9f44f22b7120", "node_type": "1", "metadata": {}, "hash": "f51c1f2c0b5d82cb92870c81aa9c2450061c8ebf56adb8594a1595238d82cad1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "node_type": "1", "metadata": {}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "class_name": "RelatedNodeInfo"}}, "hash": "7cee225c44e41cd18bcd5c577026c328ef378dc6a294e0e2add55adf428bf410", "text": "We show this in an example notebook and demonstrate its effectiveness in processing long documents.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFirst, we define a metadata extractor that takes in a list of feature extractors that will be processed in sequence.\n\nWe then feed this to the node parser, which will add the additional metadata to each node.", "start_char_idx": 1859, "end_char_idx": 2531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "842b2d1f-a046-4f59-9141-9f44f22b7120": {"__data__": {"id_": "842b2d1f-a046-4f59-9141-9f44f22b7120", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "node_type": "1", "metadata": {}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cc27566-ccb0-4f57-a305-15c89e85721d", "node_type": "1", "metadata": {}, "hash": "7cee225c44e41cd18bcd5c577026c328ef378dc6a294e0e2add55adf428bf410", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72843296-d34d-46e7-afc0-7dfd104a6dc5", "node_type": "1", "metadata": {}, "hash": "81e62fe5696cc1e43d8bdadf02519521ad9ea2632944153e74037cae7ebaaa1b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "node_type": "1", "metadata": {}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "class_name": "RelatedNodeInfo"}}, "hash": "f51c1f2c0b5d82cb92870c81aa9c2450061c8ebf56adb8594a1595238d82cad1", "text": "We then feed this to the node parser, which will add the additional metadata to each node.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index.node_parser.extractors import (\n    MetadataExtractor,\n    SummaryExtractor,\n    QuestionsAnsweredExtractor,\n    TitleExtractor,\n    KeywordExtractor,\n)\n\nmetadata_extractor = MetadataExtractor(\n    extractors=[\n        TitleExtractor(nodes=5),\n        QuestionsAnsweredExtractor(questions=3),\n        SummaryExtractor(summaries=[\"prev\", \"self\"]),\n        KeywordExtractor(keywords=10),\n    ],\n)\n\nnode_parser = SimpleNodeParser(\n    metadata_extractor=metadata_extractor,\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere is an sample of extracted metadata:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Usage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n{'page_label': '2',\n 'file_name': '10k-132.pdf',\n 'document_title': 'Uber Technologies, Inc. 2019 Annual Report: Revolutionizing Mobility and Logistics Across 69 Countries and 111 Million MAPCs with $65 Billion in Gross Bookings',\n 'questions_this_excerpt_can_answer': '\\n\\n1.", "start_char_idx": 2441, "end_char_idx": 4484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72843296-d34d-46e7-afc0-7dfd104a6dc5": {"__data__": {"id_": "72843296-d34d-46e7-afc0-7dfd104a6dc5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "node_type": "1", "metadata": {}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "842b2d1f-a046-4f59-9141-9f44f22b7120", "node_type": "1", "metadata": {}, "hash": "f51c1f2c0b5d82cb92870c81aa9c2450061c8ebf56adb8594a1595238d82cad1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "node_type": "1", "metadata": {}, "hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "class_name": "RelatedNodeInfo"}}, "hash": "81e62fe5696cc1e43d8bdadf02519521ad9ea2632944153e74037cae7ebaaa1b", "text": "How many countries does Uber Technologies, Inc. operate in?\\n2. What is the total number of MAPCs served by Uber Technologies, Inc.?\\n3. How much gross bookings did Uber Technologies, Inc. generate in 2019?',\n 'prev_section_summary': \"\\n\\nThe 2019 Annual Report provides an overview of the key topics and entities that have been important to the organization over the past year. These include financial performance, operational highlights, customer satisfaction, employee engagement, and sustainability initiatives. It also provides an overview of the organization's strategic objectives and goals for the upcoming year.\",\n 'section_summary': '\\nThis section discusses a global tech platform that serves multiple multi-trillion dollar markets with products leveraging core technology and infrastructure. It enables consumers and drivers to tap a button and get a ride or work. The platform has revolutionized personal mobility with ridesharing and is now leveraging its platform to redefine the massive meal delivery and logistics industries. The foundation of the platform is its massive network, leading technology, operational excellence, and product expertise.", "start_char_idx": 4485, "end_char_idx": 5649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32e90751-5bfc-459b-8414-aa67fa4a00c1": {"__data__": {"id_": "32e90751-5bfc-459b-8414-aa67fa4a00c1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "156f0050-4cf2-4295-8684-c1d16f36c709", "node_type": "1", "metadata": {}, "hash": "39e6a05c133d33dae399ddb62ba9101934294dab2a9578a00b26ba8107b37ec4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21da08f4-4d26-4ac5-9798-5c10f3b8333c", "node_type": "1", "metadata": {}, "hash": "266f1b1a4687ac0541d1b3e7707efaebe8bf37a974ff73f35f83b1c45d2b7190", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "156f0050-4cf2-4295-8684-c1d16f36c709", "node_type": "1", "metadata": {}, "hash": "39e6a05c133d33dae399ddb62ba9101934294dab2a9578a00b26ba8107b37ec4", "class_name": "RelatedNodeInfo"}}, "hash": "de244f1bb4a5f5e43bd48ef8d9ac0fbac11ab4526a5b7e0c221581b4dd58a504", "text": "The foundation of the platform is its massive network, leading technology, operational excellence, and product expertise.',\n 'excerpt_keywords': '\\nRidesharing, Mobility, Meal Delivery, Logistics, Network, Technology, Operational Excellence, Product Expertise, Point A, Point B'}\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Custom Extractors\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf the provided extractors do not fit your needs, you can also define a custom extractor like so:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Custom Extractors\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser.extractors import MetadataFeatureExtractor\n\nclass CustomExtractor(MetadataFeatureExtractor):\n    def extract(self, nodes) -> List[Dict]:\n        metadata_list = [\n            {\n                \"custom\": node.metadata[\"document_title\"]\n                + \"\\n\"\n                + node.metadata[\"excerpt_keywords\"]\n            }\n            for node in nodes\n        ]\n        return metadata_list\n\nFile Name: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nContent Type: text\nHeader Path: Metadata Extraction/Custom Extractors\nfile_path: Docs\\core_modules\\data_modules\\index\\metadata_extraction.md\nfile_name: metadata_extraction.md\nfile_type: None\nfile_size: 3978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn a more advanced example, it can also make use of an `llm_predictor` to extract features from the node content and the existing metadata. Refer to the source code of the provided metadata extractors for more details.", "start_char_idx": 0, "end_char_idx": 2138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21da08f4-4d26-4ac5-9798-5c10f3b8333c": {"__data__": {"id_": "21da08f4-4d26-4ac5-9798-5c10f3b8333c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "156f0050-4cf2-4295-8684-c1d16f36c709", "node_type": "1", "metadata": {}, "hash": "39e6a05c133d33dae399ddb62ba9101934294dab2a9578a00b26ba8107b37ec4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32e90751-5bfc-459b-8414-aa67fa4a00c1", "node_type": "1", "metadata": {}, "hash": "de244f1bb4a5f5e43bd48ef8d9ac0fbac11ab4526a5b7e0c221581b4dd58a504", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "156f0050-4cf2-4295-8684-c1d16f36c709", "node_type": "1", "metadata": {}, "hash": "39e6a05c133d33dae399ddb62ba9101934294dab2a9578a00b26ba8107b37ec4", "class_name": "RelatedNodeInfo"}}, "hash": "266f1b1a4687ac0541d1b3e7707efaebe8bf37a974ff73f35f83b1c45d2b7190", "text": "Refer to the source code of the provided metadata extractors for more details.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\modules.md\nContent Type: code\nHeader Path: Module Guides\nfile_path: Docs\\core_modules\\data_modules\\index\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 552\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\nvector_store_guide.ipynb\nList Index <./index_guide.md>\nTree Index <./index_guide.md>\nKeyword Table Index <./index_guide.md>\n/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.ipynb\n/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.ipynb\nSQL Index </examples/index_structs/struct_indices/SQLIndexDemo.ipynb>\n/examples/index_structs/struct_indices/duckdb_sql_query.ipynb\n/examples/index_structs/doc_summary/DocSummary.ipynb\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: text\nHeader Path: Indexes/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn `Index` is a data structure that allows us to quickly retrieve relevant context for a user query.\nFor LlamaIndex, it's the core foundation for retrieval-augmented generation (RAG) use-cases.\n\n\nAt a high-level, `Indices` are built from Documents.\nThey are used to build Query Engines and Chat Engines\nwhich enables question & answer and chat over your data.  \n\nUnder the hood, `Indices` store data in `Node` objects (which represent chunks of the original documents), and expose a Retriever interface that supports additional configuration and automation.", "start_char_idx": 2060, "end_char_idx": 3833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fee806fd-9986-4970-aa40-62f2682f5601": {"__data__": {"id_": "fee806fd-9986-4970-aa40-62f2682f5601", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93d40a64-cd24-4423-9b76-38270c080399", "node_type": "1", "metadata": {}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f4d8270-900d-42eb-9090-f93304ecac87", "node_type": "1", "metadata": {}, "hash": "49bdd696a8dd831f0875c4b30a6ac779e0a4496c507fbbb06ecbc9de536c2d7c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "93d40a64-cd24-4423-9b76-38270c080399", "node_type": "1", "metadata": {}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "class_name": "RelatedNodeInfo"}}, "hash": "73bd043351bab60887420a6612dc1cd170f99f6845042d3d1ec993d81ed8d4d5", "text": "For a more in-depth explanation, check out our guide below:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: text\nHeader Path: Indexes/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nindex_guide.md\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: text\nHeader Path: Indexes/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet started with:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: text\nHeader Path: Indexes/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(docs)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: code\nHeader Path: Indexes/Usage Pattern\nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.", "start_char_idx": 0, "end_char_idx": 1555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f4d8270-900d-42eb-9090-f93304ecac87": {"__data__": {"id_": "4f4d8270-900d-42eb-9090-f93304ecac87", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93d40a64-cd24-4423-9b76-38270c080399", "node_type": "1", "metadata": {}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fee806fd-9986-4970-aa40-62f2682f5601", "node_type": "1", "metadata": {}, "hash": "73bd043351bab60887420a6612dc1cd170f99f6845042d3d1ec993d81ed8d4d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3a4aeb7-f12f-44fb-bb8b-206d05e237b0", "node_type": "1", "metadata": {}, "hash": "a86052b9a40eed5699529d04d8f3e61be99c3d8a84adc1c9db3e8f736efdd6f2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "93d40a64-cd24-4423-9b76-38270c080399", "node_type": "1", "metadata": {}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "class_name": "RelatedNodeInfo"}}, "hash": "49bdd696a8dd831f0875c4b30a6ac779e0a4496c507fbbb06ecbc9de536c2d7c", "text": "md\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: code\nHeader Path: Indexes/Modules\nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\root.md\nContent Type: code\nHeader Path: Indexes/Advanced Concepts\nfile_path: Docs\\core_modules\\data_modules\\index\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1294\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\ncomposability.md\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBuild an index from documents:\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(docs)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.", "start_char_idx": 1494, "end_char_idx": 3089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3a4aeb7-f12f-44fb-bb8b-206d05e237b0": {"__data__": {"id_": "b3a4aeb7-f12f-44fb-bb8b-206d05e237b0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93d40a64-cd24-4423-9b76-38270c080399", "node_type": "1", "metadata": {}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f4d8270-900d-42eb-9090-f93304ecac87", "node_type": "1", "metadata": {}, "hash": "49bdd696a8dd831f0875c4b30a6ac779e0a4496c507fbbb06ecbc9de536c2d7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7c25916-5a5c-4dff-8eb6-0e33191ae75e", "node_type": "1", "metadata": {}, "hash": "cf54d456d9c4a0a91438169886481c6c8b799e8c728bf8e82d956f3ba2eeee75", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "93d40a64-cd24-4423-9b76-38270c080399", "node_type": "1", "metadata": {}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "class_name": "RelatedNodeInfo"}}, "hash": "a86052b9a40eed5699529d04d8f3e61be99c3d8a84adc1c9db3e8f736efdd6f2", "text": "from_documents(docs)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nTo learn how to load documents, see Data Connectors\n```\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/What is happening under the hood?\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n1. Documents are chunked up and parsed into `Node` objects (which are lightweight abstraction over text str that additional keep track of metadata and relationships).\n2. Additional computation is performed to add `Node` into index data structure\n   > Note: the computation is index-specific.\n   >\n   > - For a vector store index, this means calling an embedding model (via API or locally) to compute embedding for the `Node` objects\n   > - For a document summary index, this means calling an LLM to generate a summary\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Configuring Document Parsing\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe most common configuration you might want to change is how to parse document into `Node` objects.", "start_char_idx": 3005, "end_char_idx": 4802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7c25916-5a5c-4dff-8eb6-0e33191ae75e": {"__data__": {"id_": "f7c25916-5a5c-4dff-8eb6-0e33191ae75e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93d40a64-cd24-4423-9b76-38270c080399", "node_type": "1", "metadata": {}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3a4aeb7-f12f-44fb-bb8b-206d05e237b0", "node_type": "1", "metadata": {}, "hash": "a86052b9a40eed5699529d04d8f3e61be99c3d8a84adc1c9db3e8f736efdd6f2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "93d40a64-cd24-4423-9b76-38270c080399", "node_type": "1", "metadata": {}, "hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "class_name": "RelatedNodeInfo"}}, "hash": "cf54d456d9c4a0a91438169886481c6c8b799e8c728bf8e82d956f3ba2eeee75", "text": "File Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can configure our service context to use the desired chunk size and set `show_progress` to display a progress bar during index construction.", "start_char_idx": 4804, "end_char_idx": 5307, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6aeac0a-9ebc-4847-8c85-8cbbee3e663b": {"__data__": {"id_": "f6aeac0a-9ebc-4847-8c85-8cbbee3e663b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c521edf8-0e9f-40d7-a411-07c241fb6733", "node_type": "1", "metadata": {}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31da94f6-90b8-4595-8f2e-88d702dac292", "node_type": "1", "metadata": {}, "hash": "1a579ecaba612f5beaf6b3b398740c015052f070368442c7f4a66db4c11749b4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c521edf8-0e9f-40d7-a411-07c241fb6733", "node_type": "1", "metadata": {}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "class_name": "RelatedNodeInfo"}}, "hash": "eaebff7ca74ac43bdae8e016fef5595a602ffff7f234b033696d5a792138d5f6", "text": "File Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, VectorStoreIndex\n\nservice_context = ServiceContext.from_defaults(chunk_size=512)\nindex = VectorStoreIndex.from_documents(\n    docs,\n    service_context=service_context,\n    show_progress=True\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note: While the high-level API optimizes for ease-of-use, it does _NOT_ expose full range of configurability.\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use the low-level composition API if you need more granular control.\n\nHere we show an example where you want to both modify the text chunk size, disable injecting metadata, and disable creating `Node` relationships.  \nThe steps are:\n\n1.", "start_char_idx": 0, "end_char_idx": 1671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31da94f6-90b8-4595-8f2e-88d702dac292": {"__data__": {"id_": "31da94f6-90b8-4595-8f2e-88d702dac292", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c521edf8-0e9f-40d7-a411-07c241fb6733", "node_type": "1", "metadata": {}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6aeac0a-9ebc-4847-8c85-8cbbee3e663b", "node_type": "1", "metadata": {}, "hash": "eaebff7ca74ac43bdae8e016fef5595a602ffff7f234b033696d5a792138d5f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5cbe7b5-d45d-474b-bfcc-10fe061780a2", "node_type": "1", "metadata": {}, "hash": "1a833459ccf16f1e2c55e4f89267d4bf797f82165a6e1e2dc2c04afe8ddeb498", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c521edf8-0e9f-40d7-a411-07c241fb6733", "node_type": "1", "metadata": {}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "class_name": "RelatedNodeInfo"}}, "hash": "1a579ecaba612f5beaf6b3b398740c015052f070368442c7f4a66db4c11749b4", "text": "The steps are:\n\n1. Configure a node parser\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser import SimpleNodeParser\n\nparser = SimpleNodeParser.from_defaults(\n    chunk_size=512,\n    include_extra_info=False,\n    include_prev_next_rel=False,\n)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n2.", "start_char_idx": 1653, "end_char_idx": 2598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5cbe7b5-d45d-474b-bfcc-10fe061780a2": {"__data__": {"id_": "b5cbe7b5-d45d-474b-bfcc-10fe061780a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c521edf8-0e9f-40d7-a411-07c241fb6733", "node_type": "1", "metadata": {}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31da94f6-90b8-4595-8f2e-88d702dac292", "node_type": "1", "metadata": {}, "hash": "1a579ecaba612f5beaf6b3b398740c015052f070368442c7f4a66db4c11749b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7053c6b9-a453-48d7-aefc-2353774c4bbc", "node_type": "1", "metadata": {}, "hash": "bc18fe21506dfd58ed0494542fac1314ba3acd82a8a84fdac905e3be7d4ea003", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c521edf8-0e9f-40d7-a411-07c241fb6733", "node_type": "1", "metadata": {}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "class_name": "RelatedNodeInfo"}}, "hash": "1a833459ccf16f1e2c55e4f89267d4bf797f82165a6e1e2dc2c04afe8ddeb498", "text": "Parse document into `Node` objects\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n3. build index from `Node` objects\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes)\n\nFile Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Handling Document Update\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRead more about how to deal with data sources that change over time with `Index` **insertion**, **deletion**, **update**, and **refresh** operations.", "start_char_idx": 2599, "end_char_idx": 4352, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7053c6b9-a453-48d7-aefc-2353774c4bbc": {"__data__": {"id_": "7053c6b9-a453-48d7-aefc-2353774c4bbc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c521edf8-0e9f-40d7-a411-07c241fb6733", "node_type": "1", "metadata": {}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5cbe7b5-d45d-474b-bfcc-10fe061780a2", "node_type": "1", "metadata": {}, "hash": "1a833459ccf16f1e2c55e4f89267d4bf797f82165a6e1e2dc2c04afe8ddeb498", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c521edf8-0e9f-40d7-a411-07c241fb6733", "node_type": "1", "metadata": {}, "hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "class_name": "RelatedNodeInfo"}}, "hash": "bc18fe21506dfd58ed0494542fac1314ba3acd82a8a84fdac905e3be7d4ea003", "text": "File Name: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started/Handling Document Update\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\index\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2525\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nmetadata_extraction.md\ndocument_management.md\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nContent Type: text\nHeader Path: Node Parser/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1000\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNode parsers are a simple abstraction that take a list of documents, and chunk them into `Node` objects, such that each node is a specific size. When a document is broken into nodes, all of it's attributes are inherited to the children nodes (i.e. `metadata`, text and metadata templates, etc.). You can read more about `Node` and `Document` properies here.\n\nA node parser can configure the chunk size (in tokens) as well as any overlap between chunked nodes.", "start_char_idx": 4354, "end_char_idx": 5576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68cd4ecc-145c-4942-a673-54b67b2ca4c5": {"__data__": {"id_": "68cd4ecc-145c-4942-a673-54b67b2ca4c5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6", "node_type": "1", "metadata": {}, "hash": "0b63e68ac1162371428a3f1cad68e21b77b247d754a513b382d57787363ec247", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad95958f-2a96-4b93-be43-2c1a8a62ca06", "node_type": "1", "metadata": {}, "hash": "842da23bd464499fcd5d82052e7c87bd3459254b09471826660624e91f224a8f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6", "node_type": "1", "metadata": {}, "hash": "0b63e68ac1162371428a3f1cad68e21b77b247d754a513b382d57787363ec247", "class_name": "RelatedNodeInfo"}}, "hash": "ecc749583cacaa8dd70b6c69d326f70903f4cfa784adcba48d3474bfb22ce0a4", "text": "The chunking is done by using a `TokenTextSplitter`, which default to a chunk size of 1024 and a default chunk overlap of 20 tokens.\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nContent Type: code\nHeader Path: Node Parser/Usage Pattern\nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1000\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index.node_parser import SimpleNodeParser\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n```\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nContent Type: text\nHeader Path: Node Parser/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1000\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can find more usage details and availbale customization options below.\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nContent Type: text\nHeader Path: Node Parser/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1000\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.", "start_char_idx": 0, "end_char_idx": 1624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad95958f-2a96-4b93-be43-2c1a8a62ca06": {"__data__": {"id_": "ad95958f-2a96-4b93-be43-2c1a8a62ca06", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6", "node_type": "1", "metadata": {}, "hash": "0b63e68ac1162371428a3f1cad68e21b77b247d754a513b382d57787363ec247", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68cd4ecc-145c-4942-a673-54b67b2ca4c5", "node_type": "1", "metadata": {}, "hash": "ecc749583cacaa8dd70b6c69d326f70903f4cfa784adcba48d3474bfb22ce0a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0a628c5-0903-4d58-ab2f-171f2f0b930e", "node_type": "1", "metadata": {}, "hash": "04c885c289aff7aa1f418911466ca64120ec5c762d462bddb5b6fedf321eadaf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6", "node_type": "1", "metadata": {}, "hash": "0b63e68ac1162371428a3f1cad68e21b77b247d754a513b382d57787363ec247", "class_name": "RelatedNodeInfo"}}, "hash": "842da23bd464499fcd5d82052e7c87bd3459254b09471826660624e91f224a8f", "text": "md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNode parsers can be used on their own:\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Document\nfrom llama_index.node_parser import SimpleNodeParser\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n\nnodes = node_parser.get_nodes_from_documents([Document(text=\"long text\")], show_progress=False)\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOr set inside a `ServiceContext` to be used automatically when an index is constructed using `.from_documents()`:\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext\nfrom llama_index.node_parser import SimpleNodeParser\n\ndocuments = SimpleDirectoryReader(\"./data\").", "start_char_idx": 1596, "end_char_idx": 3435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0a628c5-0903-4d58-ab2f-171f2f0b930e": {"__data__": {"id_": "f0a628c5-0903-4d58-ab2f-171f2f0b930e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6", "node_type": "1", "metadata": {}, "hash": "0b63e68ac1162371428a3f1cad68e21b77b247d754a513b382d57787363ec247", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad95958f-2a96-4b93-be43-2c1a8a62ca06", "node_type": "1", "metadata": {}, "hash": "842da23bd464499fcd5d82052e7c87bd3459254b09471826660624e91f224a8f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6", "node_type": "1", "metadata": {}, "hash": "0b63e68ac1162371428a3f1cad68e21b77b247d754a513b382d57787363ec247", "class_name": "RelatedNodeInfo"}}, "hash": "04c885c289aff7aa1f418911466ca64120ec5c762d462bddb5b6fedf321eadaf", "text": "node_parser import SimpleNodeParser\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\nservice_context = ServiceContext.from_defaults(node_parser=node_parser)\n\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThere are several options available to customize:\n\n- `text_spliiter` (defaults to `TokenTextSplitter`) - the text splitter used to split text into chunks.\n- `include_metadata` (defaults to `True`) - whether or not `Node`s should inherit the document metadata.\n- `include_prev_next_rel` (defaults to `True`) - whether or not to include previous/next relationships between chunked `Node`s\n- `metadata_extractor` (defaults to `None`) - extra processing to extract helpful metadata. See here for details.\n\nIf you don't want to change the `text_splitter`, you can use `SimpleNodeParser.from_defaults()` to easily change the chunk size and chunk overlap. The defaults are 1024 and 20 respectively.", "start_char_idx": 3354, "end_char_idx": 4738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ce373c6-b769-4ae8-9dc6-74d32bee6687": {"__data__": {"id_": "0ce373c6-b769-4ae8-9dc6-74d32bee6687", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "node_type": "1", "metadata": {}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89724f49-aa7d-4b16-a4ce-d074cdf4fb9d", "node_type": "1", "metadata": {}, "hash": "edbfd0cac5f3dc4426dfb6247ae65858b3697f7a65c02f34476bbad97fd52059", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "node_type": "1", "metadata": {}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "class_name": "RelatedNodeInfo"}}, "hash": "8b90438801cc9dbd84fb3e50a212b4650e2b07903df9e9402019bf589f49c4aa", "text": "The defaults are 1024 and 20 respectively.\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser import SimpleNodeParser\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Text Splitter Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you do customize the `text_splitter` from the default `TokenTextSplitter`, you can use any splitter from langchain, or optionally our `SentenceSplitter`. Each text splitter has options for the default seperator, as well as options for backup seperators. These are useful for languages that are sufficiently different from English.\n\n`TokenTextSplitter` configuration:\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Text Splitter Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(\n  seperator=\" \",\n  chunk_size=1024,\n  chunk_overlap=20,", "start_char_idx": 0, "end_char_idx": 1854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89724f49-aa7d-4b16-a4ce-d074cdf4fb9d": {"__data__": {"id_": "89724f49-aa7d-4b16-a4ce-d074cdf4fb9d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "node_type": "1", "metadata": {}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ce373c6-b769-4ae8-9dc6-74d32bee6687", "node_type": "1", "metadata": {}, "hash": "8b90438801cc9dbd84fb3e50a212b4650e2b07903df9e9402019bf589f49c4aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ccbc9fc-beff-42f9-a50d-7ff02530331a", "node_type": "1", "metadata": {}, "hash": "9116082ff49e02072354f70761a66a65d3d24dc933cd82674fef4cfeb21b06e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "node_type": "1", "metadata": {}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "class_name": "RelatedNodeInfo"}}, "hash": "edbfd0cac5f3dc4426dfb6247ae65858b3697f7a65c02f34476bbad97fd52059", "text": "chunk_size=1024,\n  chunk_overlap=20,\n  backup_seperators=[\"\\n\"]\n)\n\nnode_parser = SimpleNodeParser(text_splitter=text_splitter)\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Text Splitter Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n`SentenceSplitter` configuration:\n\nFile Name: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Text Splitter Customization\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2942\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.langchain_helpers.text_splitter import SentenceSplitter\n\ntext_splitter = SentenceSplitter(\n  seperator=\" \",\n  chunk_size=1024,\n  chunk_overlap=20,\n  backup_seperators=[\"\\n\"],\n  paragraph_seperator=\"\\n\\n\\n\"\n)\n\nnode_parser = SimpleNodeParser(text_splitter=text_splitter)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LlamaIndex hides away the complexities and let you query your data in under 5 lines of code:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.", "start_char_idx": 1818, "end_char_idx": 3559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ccbc9fc-beff-42f9-a50d-7ff02530331a": {"__data__": {"id_": "0ccbc9fc-beff-42f9-a50d-7ff02530331a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "node_type": "1", "metadata": {}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89724f49-aa7d-4b16-a4ce-d074cdf4fb9d", "node_type": "1", "metadata": {}, "hash": "edbfd0cac5f3dc4426dfb6247ae65858b3697f7a65c02f34476bbad97fd52059", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3629e574-e58e-476f-9354-2a8b1649d84b", "node_type": "1", "metadata": {}, "hash": "401b008930bd9609833297090654f8db126973a44ee380bb5cacc798d16b44fa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "node_type": "1", "metadata": {}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "class_name": "RelatedNodeInfo"}}, "hash": "9116082ff49e02072354f70761a66a65d3d24dc933cd82674fef4cfeb21b06e8", "text": "md\nContent Type: text\nHeader Path: Customizing Storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Summarize the documents.\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUnder the hood, LlamaIndex also supports a swappable **storage layer** that allows you to customize where ingested documents (i.e., `Node` objects), embedding vectors, and index metadata are stored.", "start_char_idx": 3559, "end_char_idx": 4647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3629e574-e58e-476f-9354-2a8b1649d84b": {"__data__": {"id_": "3629e574-e58e-476f-9354-2a8b1649d84b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "node_type": "1", "metadata": {}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ccbc9fc-beff-42f9-a50d-7ff02530331a", "node_type": "1", "metadata": {}, "hash": "9116082ff49e02072354f70761a66a65d3d24dc933cd82674fef4cfeb21b06e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "node_type": "1", "metadata": {}, "hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "class_name": "RelatedNodeInfo"}}, "hash": "401b008930bd9609833297090654f8db126973a44ee380bb5cacc798d16b44fa", "text": "!\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo do this, instead of the high-level API,\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.", "start_char_idx": 4650, "end_char_idx": 5696, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08c56a70-26b6-41d3-bee8-2e0e3e0f81de": {"__data__": {"id_": "08c56a70-26b6-41d3-bee8-2e0e3e0f81de", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "node_type": "1", "metadata": {}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd3420d8-9621-49b1-acfc-52208e8049bf", "node_type": "1", "metadata": {}, "hash": "483a3ec6e425186e688ef85dbffff201b59c1f10119fc004fa3acac12c906aa2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "node_type": "1", "metadata": {}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "class_name": "RelatedNodeInfo"}}, "hash": "1609017a897c407245e9b4f340c0dbe47028394db382e54a486e2a563ed56276", "text": "md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwe use a lower-level API that gives more granular control:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Low-Level API\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.docstore import SimpleDocumentStore\nfrom llama_index.storage.index_store import SimpleIndexStore\nfrom llama_index.vector_stores import SimpleVectorStore\nfrom llama_index.node_parser import SimpleNodeParser\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create parser and parse document into nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create storage context using default stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore(),\n    vector_store=SimpleVectorStore(),\n    index_store=SimpleIndexStore(),\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.", "start_char_idx": 0, "end_char_idx": 1880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd3420d8-9621-49b1-acfc-52208e8049bf": {"__data__": {"id_": "bd3420d8-9621-49b1-acfc-52208e8049bf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "node_type": "1", "metadata": {}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08c56a70-26b6-41d3-bee8-2e0e3e0f81de", "node_type": "1", "metadata": {}, "hash": "1609017a897c407245e9b4f340c0dbe47028394db382e54a486e2a563ed56276", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bcc33da-c757-4943-a190-3856c64392f1", "node_type": "1", "metadata": {}, "hash": "099c091dda7abb29b801b9b1aac6151d186fa4db49063e905c23b07b22027e77", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "node_type": "1", "metadata": {}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "class_name": "RelatedNodeInfo"}}, "hash": "483a3ec6e425186e688ef85dbffff201b59c1f10119fc004fa3acac12c906aa2", "text": ")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create (or load) docstore and add nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context.docstore.add_documents(nodes)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/save index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/can also set index_id to save multiple indexes to the same folder\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.set_index_id = \"<index_id>\"\nindex.storage_context.", "start_char_idx": 1813, "end_char_idx": 3546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bcc33da-c757-4943-a190-3856c64392f1": {"__data__": {"id_": "5bcc33da-c757-4943-a190-3856c64392f1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "node_type": "1", "metadata": {}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd3420d8-9621-49b1-acfc-52208e8049bf", "node_type": "1", "metadata": {}, "hash": "483a3ec6e425186e688ef85dbffff201b59c1f10119fc004fa3acac12c906aa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1e23c7e-3eb7-4888-a2b8-fe8975b68a8c", "node_type": "1", "metadata": {}, "hash": "0cd1b9efdace84bb7f5e30d274e06a0a4390855ce59b70e1df84c3dad331e0e1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "node_type": "1", "metadata": {}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "class_name": "RelatedNodeInfo"}}, "hash": "099c091dda7abb29b801b9b1aac6151d186fa4db49063e905c23b07b22027e77", "text": "set_index_id = \"<index_id>\"\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/this will loaded the persisted stores from persist_dir\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(\n    persist_dir=\"<persist_dir>\"\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/then load the index object\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import load_index_from_storage\nloaded_index = load_index_from_storage(storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/if loading an index from a persist_dir containing multiple indexes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nloaded_index = load_index_from_storage(storage_context, index_id=\"<index_id>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/if loading multiple indexes from a persist dir\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.", "start_char_idx": 3496, "end_char_idx": 5296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1e23c7e-3eb7-4888-a2b8-fe8975b68a8c": {"__data__": {"id_": "f1e23c7e-3eb7-4888-a2b8-fe8975b68a8c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "node_type": "1", "metadata": {}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bcc33da-c757-4943-a190-3856c64392f1", "node_type": "1", "metadata": {}, "hash": "099c091dda7abb29b801b9b1aac6151d186fa4db49063e905c23b07b22027e77", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "node_type": "1", "metadata": {}, "hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "class_name": "RelatedNodeInfo"}}, "hash": "0cd1b9efdace84bb7f5e30d274e06a0a4390855ce59b70e1df84c3dad331e0e1", "text": "md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nloaded_indicies = load_index_from_storage(storage_context, index_ids=[\"<index_id>\", .])\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.", "start_char_idx": 5268, "end_char_idx": 5573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e805909e-8d09-43df-a9c6-dc53f9d770b6": {"__data__": {"id_": "e805909e-8d09-43df-a9c6-dc53f9d770b6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8779a8b9-760f-4de7-b49c-0df95d1bcd8b", "node_type": "1", "metadata": {}, "hash": "21215dc15b2e69800bb35d61983507f66418ee4c56d71e753d07733816b50eab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8779a8b9-760f-4de7-b49c-0df95d1bcd8b", "node_type": "1", "metadata": {}, "hash": "21215dc15b2e69800bb35d61983507f66418ee4c56d71e753d07733816b50eab", "class_name": "RelatedNodeInfo"}}, "hash": "21215dc15b2e69800bb35d61983507f66418ee4c56d71e753d07733816b50eab", "text": ".])\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/if loading multiple indexes from a persist dir\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can customize the underlying storage with a one-line change to instantiate different document stores, index stores, and vector stores.\nSee Document Stores, Vector Stores, Index Stores guides for more details.\n\nFor saving and loading a graph/composable index, see the full guide here.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Vector Store Integrations and Storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMost of our vector store integrations store the entire index (vectors + text) in the vector store itself. This comes with the major benefit of not having to exlicitly persist the index as shown above, since the vector store is already hosted and persisting the data in our index.", "start_char_idx": 0, "end_char_idx": 1344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f89aa930-dca9-40e5-b5ef-8f084a3ac5e4": {"__data__": {"id_": "f89aa930-dca9-40e5-b5ef-8f084a3ac5e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "node_type": "1", "metadata": {}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28fd1901-5401-42bb-8b1d-26ae73c1afad", "node_type": "1", "metadata": {}, "hash": "82d8b41e6aaa0ec8aa7d6c9b746e4804da099500ec01dc32de606c1a6a2c1724", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "node_type": "1", "metadata": {}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "class_name": "RelatedNodeInfo"}}, "hash": "c3d3c2035f941135406c79f196342970c8cdf29e09d9b663dad708de1de443c5", "text": "The vector stores that support this practice are:\n\n- ChatGPTRetrievalPluginClient\n- ChromaVectorStore\n- DocArrayHnswVectorStore\n- DocArrayInMemoryVectorStore\n- LanceDBVectorStore\n- MetalVectorStore\n- MilvusVectorStore\n- MyScaleVectorStore\n- OpensearchVectorStore\n- PineconeVectorStore\n- QdrantVectorStore\n- RedisVectorStore\n- WeaviateVectorStore\n\nA small example using Pinecone is below:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Vector Store Integrations and Storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport pinecone\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores import PineconeVectorStore\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/Creating a Pinecone index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\napi_key = \"api_key\"\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\npinecone.create_index(\n    \"quickstart\",\n    dimension=1536,\n    metric=\"euclidean\",\n    pod_type=\"p1\"\n)\nindex = pinecone.Index(\"quickstart\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/construct vector store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.", "start_char_idx": 0, "end_char_idx": 1742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28fd1901-5401-42bb-8b1d-26ae73c1afad": {"__data__": {"id_": "28fd1901-5401-42bb-8b1d-26ae73c1afad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "node_type": "1", "metadata": {}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f89aa930-dca9-40e5-b5ef-8f084a3ac5e4", "node_type": "1", "metadata": {}, "hash": "c3d3c2035f941135406c79f196342970c8cdf29e09d9b663dad708de1de443c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ed1fe30-5069-4a0e-89cf-347e756b4cdb", "node_type": "1", "metadata": {}, "hash": "145145b4260658c2dc929ff9eb6687246cbcad9c9120024d2d72246e4ee7fb2e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "node_type": "1", "metadata": {}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "class_name": "RelatedNodeInfo"}}, "hash": "82d8b41e6aaa0ec8aa7d6c9b746e4804da099500ec01dc32de606c1a6a2c1724", "text": "md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = PineconeVectorStore(pinecone_index=index)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/load documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create index, which will insert documents/vectors to pinecone\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create index, which will insert documents/vectors to pinecone\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.", "start_char_idx": 1714, "end_char_idx": 3551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ed1fe30-5069-4a0e-89cf-347e756b4cdb": {"__data__": {"id_": "2ed1fe30-5069-4a0e-89cf-347e756b4cdb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "node_type": "1", "metadata": {}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28fd1901-5401-42bb-8b1d-26ae73c1afad", "node_type": "1", "metadata": {}, "hash": "82d8b41e6aaa0ec8aa7d6c9b746e4804da099500ec01dc32de606c1a6a2c1724", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ed20e50-d408-40db-b36b-8488d813aafd", "node_type": "1", "metadata": {}, "hash": "93697686d45eb7ea3455180d3dd74b356d0e2f13a39e92d4c01e8d6d4a22b450", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "node_type": "1", "metadata": {}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "class_name": "RelatedNodeInfo"}}, "hash": "145145b4260658c2dc929ff9eb6687246cbcad9c9120024d2d72246e4ee7fb2e", "text": "md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you have an existing vector store with data already loaded in, \nyou can connect to it and directly create a `VectorStoreIndex` as follows:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\customization.md\nContent Type: text\nHeader Path: Customizing Storage/create index, which will insert documents/vectors to pinecone\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\customization.md\nfile_name: customization.md\nfile_type: None\nfile_size: 4804\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = pinecone.Index(\"quickstart\")\nvector_store = PineconeVectorStore(pinecone_index=index)\nloaded_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDocument stores contain ingested document chunks, which we call `Node` objects.\n\nSee the API Reference for more details.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/Simple Document Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, the `SimpleDocumentStore` stores `Node` objects in-memory.", "start_char_idx": 3523, "end_char_idx": 5263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ed20e50-d408-40db-b36b-8488d813aafd": {"__data__": {"id_": "3ed20e50-d408-40db-b36b-8488d813aafd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "node_type": "1", "metadata": {}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ed1fe30-5069-4a0e-89cf-347e756b4cdb", "node_type": "1", "metadata": {}, "hash": "145145b4260658c2dc929ff9eb6687246cbcad9c9120024d2d72246e4ee7fb2e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "node_type": "1", "metadata": {}, "hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "class_name": "RelatedNodeInfo"}}, "hash": "93697686d45eb7ea3455180d3dd74b356d0e2f13a39e92d4c01e8d6d4a22b450", "text": "They can be persisted to (and loaded from) disk by calling `docstore.persist()` (and `SimpleDocumentStore.from_persist_path(...)` respectively).", "start_char_idx": 5265, "end_char_idx": 5409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20101a59-a024-4649-bac5-9059bfdd5587": {"__data__": {"id_": "20101a59-a024-4649-bac5-9059bfdd5587", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd1ce237-0bf3-493e-b318-08c55024cba4", "node_type": "1", "metadata": {}, "hash": "f4c3c478c1f46a44b06ef0f2776cc3dc4bdd684b0676026672b09fdb6e79e7fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9e67033-f226-4fd3-8b78-793a4ff61056", "node_type": "1", "metadata": {}, "hash": "50b422e96aa5564e04be9d5c3027f84ff7deb1b32cda192a744a140a95e103ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cd1ce237-0bf3-493e-b318-08c55024cba4", "node_type": "1", "metadata": {}, "hash": "f4c3c478c1f46a44b06ef0f2776cc3dc4bdd684b0676026672b09fdb6e79e7fc", "class_name": "RelatedNodeInfo"}}, "hash": "c9c3a3f9648edb351c0e1196a12a68322c4655efbbf17f1fbb6f833f6b0dc5d9", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/MongoDB Document Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support MongoDB as an alternative document store backend that persists data as `Node` objects are ingested.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/MongoDB Document Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.docstore import MongoDocumentStore\nfrom llama_index.node_parser import SimpleNodeParser\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create parser and parse document into nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create (or load) docstore and add nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocstore = MongoDocumentStore.from_uri(uri=\"<mongodb+srv://.", "start_char_idx": 0, "end_char_idx": 1804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9e67033-f226-4fd3-8b78-793a4ff61056": {"__data__": {"id_": "b9e67033-f226-4fd3-8b78-793a4ff61056", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd1ce237-0bf3-493e-b318-08c55024cba4", "node_type": "1", "metadata": {}, "hash": "f4c3c478c1f46a44b06ef0f2776cc3dc4bdd684b0676026672b09fdb6e79e7fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20101a59-a024-4649-bac5-9059bfdd5587", "node_type": "1", "metadata": {}, "hash": "c9c3a3f9648edb351c0e1196a12a68322c4655efbbf17f1fbb6f833f6b0dc5d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa47c07e-5519-415a-98b9-998b1eb33242", "node_type": "1", "metadata": {}, "hash": "08dc7f2bba0677767e8d5bede45f4dbb7f1f0a0c01bf5d802bb228e868adbaab", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cd1ce237-0bf3-493e-b318-08c55024cba4", "node_type": "1", "metadata": {}, "hash": "f4c3c478c1f46a44b06ef0f2776cc3dc4bdd684b0676026672b09fdb6e79e7fc", "class_name": "RelatedNodeInfo"}}, "hash": "50b422e96aa5564e04be9d5c3027f84ff7deb1b32cda192a744a140a95e103ac", "text": "from_uri(uri=\"<mongodb+srv://.>\")\ndocstore.add_documents(nodes)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUnder the hood, `MongoDocumentStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your nodes.\n> Note: You can configure the `db_name` and `namespace` when instantiating `MongoDocumentStore`, otherwise they default to `db_name=\"db_docstore\"` and `namespace=\"docstore\"`.\n\nNote that it's not necessary to call `storage_context.persist()` (or `docstore.persist()`) when using an `MongoDocumentStore`\nsince data is persisted by default. \n\nYou can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoDocumentStore` with an existing `db_name` and `collection_name`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa47c07e-5519-415a-98b9-998b1eb33242": {"__data__": {"id_": "aa47c07e-5519-415a-98b9-998b1eb33242", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd1ce237-0bf3-493e-b318-08c55024cba4", "node_type": "1", "metadata": {}, "hash": "f4c3c478c1f46a44b06ef0f2776cc3dc4bdd684b0676026672b09fdb6e79e7fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9e67033-f226-4fd3-8b78-793a4ff61056", "node_type": "1", "metadata": {}, "hash": "50b422e96aa5564e04be9d5c3027f84ff7deb1b32cda192a744a140a95e103ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cd1ce237-0bf3-493e-b318-08c55024cba4", "node_type": "1", "metadata": {}, "hash": "f4c3c478c1f46a44b06ef0f2776cc3dc4bdd684b0676026672b09fdb6e79e7fc", "class_name": "RelatedNodeInfo"}}, "hash": "08dc7f2bba0677767e8d5bede45f4dbb7f1f0a0c01bf5d802bb228e868adbaab", "text": "A more complete example can be found here\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/Redis Document Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support Redis as an alternative document store backend that persists data as `Node` objects are ingested.", "start_char_idx": 3662, "end_char_idx": 4161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4e81f8d-42e5-4864-b424-4c84f2be118a": {"__data__": {"id_": "e4e81f8d-42e5-4864-b424-4c84f2be118a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45256c7f-e83f-4f1b-9943-5d93df44beef", "node_type": "1", "metadata": {}, "hash": "c43ba2531e2c721dee02361971b76eac954ea5f5ce1a137be507d9353e9dbff2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf6f149b-9c66-4a34-bdbd-10d460b9a92d", "node_type": "1", "metadata": {}, "hash": "3ff483d831c053b457f0da395f385f56345d1a3e5226f6ae1fd633b55e53ea7e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "45256c7f-e83f-4f1b-9943-5d93df44beef", "node_type": "1", "metadata": {}, "hash": "c43ba2531e2c721dee02361971b76eac954ea5f5ce1a137be507d9353e9dbff2", "class_name": "RelatedNodeInfo"}}, "hash": "0c608f25c41cb97ed1dc3672bd65676fa06a1a7710e8cd62a25af0d2f5d2dc0c", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/Redis Document Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.docstore import RedisDocumentStore\nfrom llama_index.node_parser import SimpleNodeParser\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create parser and parse document into nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create (or load) docstore and add nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocstore = RedisDocumentStore.from_host_and_port(\n  host=\"127.0.0.1\", \n  port=\"6379\", \n  namespace='llama_index'\n)\ndocstore.add_documents(nodes)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/create storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.", "start_char_idx": 0, "end_char_idx": 1652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf6f149b-9c66-4a34-bdbd-10d460b9a92d": {"__data__": {"id_": "cf6f149b-9c66-4a34-bdbd-10d460b9a92d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45256c7f-e83f-4f1b-9943-5d93df44beef", "node_type": "1", "metadata": {}, "hash": "c43ba2531e2c721dee02361971b76eac954ea5f5ce1a137be507d9353e9dbff2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4e81f8d-42e5-4864-b424-4c84f2be118a", "node_type": "1", "metadata": {}, "hash": "0c608f25c41cb97ed1dc3672bd65676fa06a1a7710e8cd62a25af0d2f5d2dc0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab467314-fca3-4e77-9f26-7cb7378e39c7", "node_type": "1", "metadata": {}, "hash": "ea85eff7d1ae394d2af2b37bd26b6f9566410dab2de06dbdba5735864aa432c0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "45256c7f-e83f-4f1b-9943-5d93df44beef", "node_type": "1", "metadata": {}, "hash": "c43ba2531e2c721dee02361971b76eac954ea5f5ce1a137be507d9353e9dbff2", "class_name": "RelatedNodeInfo"}}, "hash": "3ff483d831c053b457f0da395f385f56345d1a3e5226f6ae1fd633b55e53ea7e", "text": "md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\docstores.md\nContent Type: text\nHeader Path: Document Stores/build index\nfile_path: Docs\\core_modules\\data_modules\\storage\\docstores.md\nfile_name: docstores.md\nfile_type: None\nfile_size: 3234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUnder the hood, `RedisDocumentStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/docs`.\n> Note: You can configure the `namespace` when instantiating `RedisDocumentStore`, otherwise it defaults `namespace=\"docstore\"`.\n\nYou can easily reconnect to your Redis client and reload the index by re-initializing a `RedisDocumentStore` with an existing `host`, `port`, and `namespace`.\n\nA more complete example can be found here\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIndex stores contains lightweight index metadata (i.e.", "start_char_idx": 1628, "end_char_idx": 3433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab467314-fca3-4e77-9f26-7cb7378e39c7": {"__data__": {"id_": "ab467314-fca3-4e77-9f26-7cb7378e39c7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45256c7f-e83f-4f1b-9943-5d93df44beef", "node_type": "1", "metadata": {}, "hash": "c43ba2531e2c721dee02361971b76eac954ea5f5ce1a137be507d9353e9dbff2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf6f149b-9c66-4a34-bdbd-10d460b9a92d", "node_type": "1", "metadata": {}, "hash": "3ff483d831c053b457f0da395f385f56345d1a3e5226f6ae1fd633b55e53ea7e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "45256c7f-e83f-4f1b-9943-5d93df44beef", "node_type": "1", "metadata": {}, "hash": "c43ba2531e2c721dee02361971b76eac954ea5f5ce1a137be507d9353e9dbff2", "class_name": "RelatedNodeInfo"}}, "hash": "ea85eff7d1ae394d2af2b37bd26b6f9566410dab2de06dbdba5735864aa432c0", "text": "additional state information created when building an index).\n\nSee the API Reference for more details.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/Simple Index Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LlamaIndex uses a simple index store backed by an in-memory key-value store.\nThey can be persisted to (and loaded from) disk by calling `index_store.persist()` (and `SimpleIndexStore.from_persist_path(...)` respectively).\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/MongoDB Index Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSimilarly to document stores, we can also use `MongoDB` as the storage backend of the index store.", "start_char_idx": 3434, "end_char_idx": 4576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e66c707a-63e2-4e3d-9b6d-99e26513393e": {"__data__": {"id_": "e66c707a-63e2-4e3d-9b6d-99e26513393e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538", "node_type": "1", "metadata": {}, "hash": "d86b971b5a48e95f197d11500c39125c644077c162add32d9ebf22631f3252da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f07022f-8d04-4293-a052-3d0640606654", "node_type": "1", "metadata": {}, "hash": "0e9a7c16c2b7c49b360f12c6c69bc040388b7b31425491fc2a5a969ff0b5fa0b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538", "node_type": "1", "metadata": {}, "hash": "d86b971b5a48e95f197d11500c39125c644077c162add32d9ebf22631f3252da", "class_name": "RelatedNodeInfo"}}, "hash": "4496384e8762fd1d55a0f21749b63b5ceb62450032112f0f944aee7a09984405", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/MongoDB Index Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.index_store import MongoIndexStore\nfrom llama_index import VectorStoreIndex\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/create (or load) index store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_store = MongoIndexStore.from_uri(uri=\"<mongodb+srv://.>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/create storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(index_store=index_store)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f07022f-8d04-4293-a052-3d0640606654": {"__data__": {"id_": "7f07022f-8d04-4293-a052-3d0640606654", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538", "node_type": "1", "metadata": {}, "hash": "d86b971b5a48e95f197d11500c39125c644077c162add32d9ebf22631f3252da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e66c707a-63e2-4e3d-9b6d-99e26513393e", "node_type": "1", "metadata": {}, "hash": "4496384e8762fd1d55a0f21749b63b5ceb62450032112f0f944aee7a09984405", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11f1ac58-f0cb-4b48-a6bc-3a23d4f90a0f", "node_type": "1", "metadata": {}, "hash": "6d7af20b737f1fddf06b00b5da26923b741f5b7b89d3d9ede38dd89302720aee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538", "node_type": "1", "metadata": {}, "hash": "d86b971b5a48e95f197d11500c39125c644077c162add32d9ebf22631f3252da", "class_name": "RelatedNodeInfo"}}, "hash": "0e9a7c16c2b7c49b360f12c6c69bc040388b7b31425491fc2a5a969ff0b5fa0b", "text": "storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/or alternatively, load index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import load_index_from_storage\nindex = load_index_from_storage(storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/or alternatively, load index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUnder the hood, `MongoIndexStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your index metadata.\n> Note: You can configure the `db_name` and `namespace` when instantiating `MongoIndexStore`, otherwise they default to `db_name=\"db_docstore\"` and `namespace=\"docstore\"`.\n\nNote that it's not necessary to call `storage_context.persist()` (or `index_store.persist()`) when using an `MongoIndexStore`\nsince data is persisted by default. \n\nYou can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoIndexStore` with an existing `db_name` and `collection_name`.", "start_char_idx": 1690, "end_char_idx": 3204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11f1ac58-f0cb-4b48-a6bc-3a23d4f90a0f": {"__data__": {"id_": "11f1ac58-f0cb-4b48-a6bc-3a23d4f90a0f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538", "node_type": "1", "metadata": {}, "hash": "d86b971b5a48e95f197d11500c39125c644077c162add32d9ebf22631f3252da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f07022f-8d04-4293-a052-3d0640606654", "node_type": "1", "metadata": {}, "hash": "0e9a7c16c2b7c49b360f12c6c69bc040388b7b31425491fc2a5a969ff0b5fa0b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538", "node_type": "1", "metadata": {}, "hash": "d86b971b5a48e95f197d11500c39125c644077c162add32d9ebf22631f3252da", "class_name": "RelatedNodeInfo"}}, "hash": "6d7af20b737f1fddf06b00b5da26923b741f5b7b89d3d9ede38dd89302720aee", "text": "A more complete example can be found here\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/Redis Index Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support Redis as an alternative document store backend that persists data as `Node` objects are ingested.", "start_char_idx": 3206, "end_char_idx": 3708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5838bac-77c4-4377-b9ab-07ea8231d1fe": {"__data__": {"id_": "c5838bac-77c4-4377-b9ab-07ea8231d1fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1075c947-32af-4c99-8c75-4f87b980f909", "node_type": "1", "metadata": {}, "hash": "f143b93edef9fa1e87134984f4e10697b652de6b5dba7b3abdb8d8c8a0ec8224", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f6091dc-cb54-4eb4-9763-bc6001f95072", "node_type": "1", "metadata": {}, "hash": "467cd5acd425894745fb0986d379da6fd98cd1da8036e9b703e38146dc3adf2d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1075c947-32af-4c99-8c75-4f87b980f909", "node_type": "1", "metadata": {}, "hash": "f143b93edef9fa1e87134984f4e10697b652de6b5dba7b3abdb8d8c8a0ec8224", "class_name": "RelatedNodeInfo"}}, "hash": "221a064b4876faef97a9ce2e3adf707e7f3b00e8663733cc56195a2892ccee07", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/Redis Index Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.index_store import RedisIndexStore\nfrom llama_index import VectorStoreIndex\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/create (or load) docstore and add nodes\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_store = RedisIndexStore.from_host_and_port(\n  host=\"127.0.0.1\", \n  port=\"6379\", \n  namespace='llama_index'\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/create storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(index_store=index_store)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.", "start_char_idx": 0, "end_char_idx": 1592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f6091dc-cb54-4eb4-9763-bc6001f95072": {"__data__": {"id_": "0f6091dc-cb54-4eb4-9763-bc6001f95072", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1075c947-32af-4c99-8c75-4f87b980f909", "node_type": "1", "metadata": {}, "hash": "f143b93edef9fa1e87134984f4e10697b652de6b5dba7b3abdb8d8c8a0ec8224", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5838bac-77c4-4377-b9ab-07ea8231d1fe", "node_type": "1", "metadata": {}, "hash": "221a064b4876faef97a9ce2e3adf707e7f3b00e8663733cc56195a2892ccee07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f721465-dba3-4b27-b3ea-531a2aedb1f4", "node_type": "1", "metadata": {}, "hash": "6b000d10e55e8419055786c0ce8a3099b218896e7ed3f03ec048824ce65aba75", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1075c947-32af-4c99-8c75-4f87b980f909", "node_type": "1", "metadata": {}, "hash": "f143b93edef9fa1e87134984f4e10697b652de6b5dba7b3abdb8d8c8a0ec8224", "class_name": "RelatedNodeInfo"}}, "hash": "467cd5acd425894745fb0986d379da6fd98cd1da8036e9b703e38146dc3adf2d", "text": "md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/or alternatively, load index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import load_index_from_storage\nindex = load_index_from_storage(storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nContent Type: text\nHeader Path: Index Stores/or alternatively, load index\nfile_path: Docs\\core_modules\\data_modules\\storage\\index_stores.md\nfile_name: index_stores.md\nfile_type: None\nfile_size: 3187\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUnder the hood, `RedisIndexStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/index`.\n> Note: You can configure the `namespace` when instantiating `RedisIndexStore`, otherwise it defaults `namespace=\"index_store\"`.\n\nYou can easily reconnect to your Redis client and reload the index by re-initializing a `RedisIndexStore` with an existing `host`, `port`, and `namespace`.", "start_char_idx": 1565, "end_char_idx": 3013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f721465-dba3-4b27-b3ea-531a2aedb1f4": {"__data__": {"id_": "8f721465-dba3-4b27-b3ea-531a2aedb1f4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1075c947-32af-4c99-8c75-4f87b980f909", "node_type": "1", "metadata": {}, "hash": "f143b93edef9fa1e87134984f4e10697b652de6b5dba7b3abdb8d8c8a0ec8224", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f6091dc-cb54-4eb4-9763-bc6001f95072", "node_type": "1", "metadata": {}, "hash": "467cd5acd425894745fb0986d379da6fd98cd1da8036e9b703e38146dc3adf2d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "1075c947-32af-4c99-8c75-4f87b980f909", "node_type": "1", "metadata": {}, "hash": "f143b93edef9fa1e87134984f4e10697b652de6b5dba7b3abdb8d8c8a0ec8224", "class_name": "RelatedNodeInfo"}}, "hash": "6b000d10e55e8419055786c0ce8a3099b218896e7ed3f03ec048824ce65aba75", "text": "A more complete example can be found here\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\kv_stores.md\nContent Type: text\nHeader Path: Key-Value Stores\nfile_path: Docs\\core_modules\\data_modules\\storage\\kv_stores.md\nfile_name: kv_stores.md\nfile_type: None\nfile_size: 560\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nKey-Value stores are the underlying storage abstractions that power our Document Stores and Index Stores.\n\nWe provide the following key-value stores:\n- **Simple Key-Value Store**: An in-memory KV store. The user can choose to call `persist` on this kv store to persist data to disk.\n- **MongoDB Key-Value Store**: A MongoDB KV store.\n\nSee the API Reference for more details.\n\nNote: At the moment, these storage abstractions are not externally facing.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/Concept\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides a high-level interface for ingesting, indexing, and querying your external data.\n\nUnder the hood, LlamaIndex also supports swappable **storage components** that allows you to customize:\n\n- **Document stores**: where ingested documents (i.e., `Node` objects) are stored,\n- **Index stores**: where index metadata are stored,\n- **Vector stores**: where embedding vectors are stored.\n\nThe Document/Index stores rely on a common Key-Value store abstraction, which is also detailed below.\n\nLlamaIndex supports persisting data to any storage backend supported by fsspec. \nWe have confirmed support for the following storage backends:\n\n- Local filesystem\n- AWS S3\n- Cloudflare R2", "start_char_idx": 3015, "end_char_idx": 4832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fcf52bd-c5be-4e21-a4fb-93e2af5e06c9": {"__data__": {"id_": "7fcf52bd-c5be-4e21-a4fb-93e2af5e06c9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cccb9e7-9a10-4487-a24a-550528d54562", "node_type": "1", "metadata": {}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f20e29cb-25e5-4d56-a2e2-e2d612ff07f1", "node_type": "1", "metadata": {}, "hash": "ca758cda474e739667ca84f065284d5a83e76b553a932e7aef72925e3776a72b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2cccb9e7-9a10-4487-a24a-550528d54562", "node_type": "1", "metadata": {}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "class_name": "RelatedNodeInfo"}}, "hash": "5ba4a14802321b14532588b0d18816fc4438723e1de064828bcf4d6f74c43539", "text": "!\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMany vector stores (except FAISS) will store both the data as well as the index (embeddings). This means that you will not need to use a separate document store or index store. This *also* means that you will not need to explicitly persist this data - this happens automatically. Usage would look something like the following to build a new index / reload an existing one.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/build a new index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, StorageContext\nfrom llama_index.vector_stores import DeepLakeVectorStore\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/construct vector store and customize storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_store = DeepLakeVectorStore(dataset_path=\"<dataset_path>\")\nstorage_context = StorageContext.from_defaults(\n    vector_store = vector_store\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/Load documents and build index\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.", "start_char_idx": 0, "end_char_idx": 1850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f20e29cb-25e5-4d56-a2e2-e2d612ff07f1": {"__data__": {"id_": "f20e29cb-25e5-4d56-a2e2-e2d612ff07f1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cccb9e7-9a10-4487-a24a-550528d54562", "node_type": "1", "metadata": {}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fcf52bd-c5be-4e21-a4fb-93e2af5e06c9", "node_type": "1", "metadata": {}, "hash": "5ba4a14802321b14532588b0d18816fc4438723e1de064828bcf4d6f74c43539", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a8f23cd-6466-4b33-97f8-d52f17345761", "node_type": "1", "metadata": {}, "hash": "673ef040f393b4588ff5efb750a6b54b27832c6565f969fd3b8c7ab59120222f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2cccb9e7-9a10-4487-a24a-550528d54562", "node_type": "1", "metadata": {}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "class_name": "RelatedNodeInfo"}}, "hash": "ca758cda474e739667ca84f065284d5a83e76b553a932e7aef72925e3776a72b", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/reload an existing one\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/reload an existing one\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee our Vector Store Module Guide below for more details.", "start_char_idx": 1831, "end_char_idx": 2841, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a8f23cd-6466-4b33-97f8-d52f17345761": {"__data__": {"id_": "5a8f23cd-6466-4b33-97f8-d52f17345761", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cccb9e7-9a10-4487-a24a-550528d54562", "node_type": "1", "metadata": {}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f20e29cb-25e5-4d56-a2e2-e2d612ff07f1", "node_type": "1", "metadata": {}, "hash": "ca758cda474e739667ca84f065284d5a83e76b553a932e7aef72925e3776a72b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e27bbfc-12a0-446d-b4e2-feaf0b9c1066", "node_type": "1", "metadata": {}, "hash": "7adbce3a0a26bb7cdb559a38b4f31a24f0839acae29d5a9ead5a84156f03e6b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2cccb9e7-9a10-4487-a24a-550528d54562", "node_type": "1", "metadata": {}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "class_name": "RelatedNodeInfo"}}, "hash": "673ef040f393b4588ff5efb750a6b54b27832c6565f969fd3b8c7ab59120222f", "text": "Note that in general to use storage abstractions, you need to define a `StorageContext` object:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/reload an existing one\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.storage.docstore import SimpleDocumentStore\nfrom llama_index.storage.index_store import SimpleIndexStore\nfrom llama_index.vector_stores import SimpleVectorStore\nfrom llama_index.storage import StorageContext\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/create storage context using default stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore(),\n    vector_store=SimpleVectorStore(),\n    index_store=SimpleIndexStore(),\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/create storage context using default stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMore details on customization/persistence can be found in the guides below.", "start_char_idx": 2844, "end_char_idx": 4426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e27bbfc-12a0-446d-b4e2-feaf0b9c1066": {"__data__": {"id_": "6e27bbfc-12a0-446d-b4e2-feaf0b9c1066", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cccb9e7-9a10-4487-a24a-550528d54562", "node_type": "1", "metadata": {}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a8f23cd-6466-4b33-97f8-d52f17345761", "node_type": "1", "metadata": {}, "hash": "673ef040f393b4588ff5efb750a6b54b27832c6565f969fd3b8c7ab59120222f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2cccb9e7-9a10-4487-a24a-550528d54562", "node_type": "1", "metadata": {}, "hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "class_name": "RelatedNodeInfo"}}, "hash": "7adbce3a0a26bb7cdb559a38b4f31a24f0839acae29d5a9ead5a84156f03e6b3", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/create storage context using default stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\ncustomization.md\nsave_load.md\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/Modules\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe offer in-depth guides on the different storage components.", "start_char_idx": 4428, "end_char_idx": 5200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a1d7807-aa41-47c8-9dd7-084d4097483a": {"__data__": {"id_": "8a1d7807-aa41-47c8-9dd7-084d4097483a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96b953dd-0f15-42fa-a00b-ecc0e554907c", "node_type": "1", "metadata": {}, "hash": "df94adb5aa2b2f90688857b542f2b2b10c554a3e169fc64642073cdeeb466041", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9344c360-d43a-4ece-a321-d6e98c311d4b", "node_type": "1", "metadata": {}, "hash": "751da0c2085d592c95fea605117ed1bef7a545b5ba3e4320496041c35eebe61a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "96b953dd-0f15-42fa-a00b-ecc0e554907c", "node_type": "1", "metadata": {}, "hash": "df94adb5aa2b2f90688857b542f2b2b10c554a3e169fc64642073cdeeb466041", "class_name": "RelatedNodeInfo"}}, "hash": "e5af6cc85b86529fa88691ce6a8899e69674b33a0776939b355b60d8f761bb3d", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\root.md\nContent Type: text\nHeader Path: Storage/Modules\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2811\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nvector_stores.md\ndocstores.md\nindex_stores.md\nkv_stores.md\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Persisting Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LlamaIndex stores data in-memory, and this data can be explicitly persisted if desired:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Persisting Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context.persist(persist_dir=\"<persist_dir>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Persisting Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis will persist data to disk, under the specified `persist_dir` (or `./storage` by default).", "start_char_idx": 0, "end_char_idx": 1700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9344c360-d43a-4ece-a321-d6e98c311d4b": {"__data__": {"id_": "9344c360-d43a-4ece-a321-d6e98c311d4b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96b953dd-0f15-42fa-a00b-ecc0e554907c", "node_type": "1", "metadata": {}, "hash": "df94adb5aa2b2f90688857b542f2b2b10c554a3e169fc64642073cdeeb466041", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a1d7807-aa41-47c8-9dd7-084d4097483a", "node_type": "1", "metadata": {}, "hash": "e5af6cc85b86529fa88691ce6a8899e69674b33a0776939b355b60d8f761bb3d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "96b953dd-0f15-42fa-a00b-ecc0e554907c", "node_type": "1", "metadata": {}, "hash": "df94adb5aa2b2f90688857b542f2b2b10c554a3e169fc64642073cdeeb466041", "class_name": "RelatedNodeInfo"}}, "hash": "751da0c2085d592c95fea605117ed1bef7a545b5ba3e4320496041c35eebe61a", "text": "Multiple indexes can be persisted and loaded from the same directory, assuming you keep track of index ID's for loading.\n\nUser can also configure alternative storage backends (e.g. `MongoDB`) that persist data by default.\nIn this case, calling `storage_context.persist()` will do nothing.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Loading Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo load data, user simply needs to re-create the storage context using the same configuration (e.g. pass in the same `persist_dir` or vector store client).\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Loading Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n    vector_store=SimpleVectorStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Loading Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can then load specific indices from the `StorageContext` through some convenience functions below.", "start_char_idx": 1702, "end_char_idx": 3593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c0a5cf8-a554-4b45-8532-aa25ceb3d911": {"__data__": {"id_": "5c0a5cf8-a554-4b45-8532-aa25ceb3d911", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd7538f0-7953-4038-8b82-42031ff86fb1", "node_type": "1", "metadata": {}, "hash": "a137a1c4a08fb5334a8f0c6127e5140526090e2d54fb96acf6916bc5447361f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a11f277d-1983-4066-aace-321e61588680", "node_type": "1", "metadata": {}, "hash": "3d6ccc61dd186370a04136a0d91cd64600da5e16f14ee8d151ce165654aaf185", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fd7538f0-7953-4038-8b82-42031ff86fb1", "node_type": "1", "metadata": {}, "hash": "a137a1c4a08fb5334a8f0c6127e5140526090e2d54fb96acf6916bc5447361f1", "class_name": "RelatedNodeInfo"}}, "hash": "8e32897bbc6a48ce40450a1c0b1f9070ce5fc9595c6332ac90cb1cd725bb834c", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Loading Data\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import load_index_from_storage, load_indices_from_storage, load_graph_from_storage\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/need to specify index_id if multiple indexes are persisted to the same directory\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = load_index_from_storage(storage_context, index_id=\"<index_id>\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/don't need to specify index_id if there's only one index in storage context\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = load_index_from_storage(storage_context)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load multiple indices\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.", "start_char_idx": 0, "end_char_idx": 1640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a11f277d-1983-4066-aace-321e61588680": {"__data__": {"id_": "a11f277d-1983-4066-aace-321e61588680", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd7538f0-7953-4038-8b82-42031ff86fb1", "node_type": "1", "metadata": {}, "hash": "a137a1c4a08fb5334a8f0c6127e5140526090e2d54fb96acf6916bc5447361f1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c0a5cf8-a554-4b45-8532-aa25ceb3d911", "node_type": "1", "metadata": {}, "hash": "8e32897bbc6a48ce40450a1c0b1f9070ce5fc9595c6332ac90cb1cd725bb834c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "711f400f-f144-4aba-bbd7-f0ef888b36bf", "node_type": "1", "metadata": {}, "hash": "172771c06ba470d8001018cf84a886ba8c5ad955e1a4231210204044ab802636", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fd7538f0-7953-4038-8b82-42031ff86fb1", "node_type": "1", "metadata": {}, "hash": "a137a1c4a08fb5334a8f0c6127e5140526090e2d54fb96acf6916bc5447361f1", "class_name": "RelatedNodeInfo"}}, "hash": "3d6ccc61dd186370a04136a0d91cd64600da5e16f14ee8d151ce165654aaf185", "text": "md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindices = load_indices_from_storage(storage_context) # loads all indices\nindices = load_indices_from_storage(storage_context, index_ids=[index_id1, .]) # loads specific indices\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load composable graph\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph = load_graph_from_storage(storage_context, root_id=\"<root_id>\") # loads graph with the specified root_id\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load composable graph\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere's the full API Reference on saving and loading.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Using a remote backend\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LlamaIndex uses a local filesystem to load and save files. However, you can override this by passing a `fsspec.AbstractFileSystem` object.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "711f400f-f144-4aba-bbd7-f0ef888b36bf": {"__data__": {"id_": "711f400f-f144-4aba-bbd7-f0ef888b36bf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd7538f0-7953-4038-8b82-42031ff86fb1", "node_type": "1", "metadata": {}, "hash": "a137a1c4a08fb5334a8f0c6127e5140526090e2d54fb96acf6916bc5447361f1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a11f277d-1983-4066-aace-321e61588680", "node_type": "1", "metadata": {}, "hash": "3d6ccc61dd186370a04136a0d91cd64600da5e16f14ee8d151ce165654aaf185", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fd7538f0-7953-4038-8b82-42031ff86fb1", "node_type": "1", "metadata": {}, "hash": "a137a1c4a08fb5334a8f0c6127e5140526090e2d54fb96acf6916bc5447361f1", "class_name": "RelatedNodeInfo"}}, "hash": "172771c06ba470d8001018cf84a886ba8c5ad955e1a4231210204044ab802636", "text": "However, you can override this by passing a `fsspec.AbstractFileSystem` object.\n\nHere's a simple example, instantiating a vector store:\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/Using a remote backend\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport dotenv\nimport s3fs\nimport os\ndotenv.load_dotenv(\"../../../.env\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../../../examples/paul_graham_essay/data/').load_data()\nprint(len(documents))\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load documents\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAt this point, everything has been the same. Now - let's instantiate a S3 filesystem and save / load from there.", "start_char_idx": 3259, "end_char_idx": 4810, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7165700b-0795-438a-9394-5943ce730da2": {"__data__": {"id_": "7165700b-0795-438a-9394-5943ce730da2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537", "node_type": "1", "metadata": {}, "hash": "3c4e3f57d8d8eeb02c6f8c3bfc094d56481ac66edba4b1acee241552182c8444", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d438c52-7882-4870-b7d7-9786dad54f7f", "node_type": "1", "metadata": {}, "hash": "6bb05212b99c7498a2402e8bf260fc253186bf99ab38924b8b4cd83c714b381b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537", "node_type": "1", "metadata": {}, "hash": "3c4e3f57d8d8eeb02c6f8c3bfc094d56481ac66edba4b1acee241552182c8444", "class_name": "RelatedNodeInfo"}}, "hash": "6dc836a2c7f279ee3fa90b5adac3c30d676bcce909efb814fdff44b3f7e7dc82", "text": "Now - let's instantiate a S3 filesystem and save / load from there.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/set up s3fs\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAWS_KEY = os.environ['AWS_ACCESS_KEY_ID']\nAWS_SECRET = os.environ['AWS_SECRET_ACCESS_KEY']\nR2_ACCOUNT_ID = os.environ['R2_ACCOUNT_ID']\n\nassert AWS_KEY is not None and AWS_KEY != \"\"\n\ns3 = s3fs.S3FileSystem(\n   key=AWS_KEY,\n   secret=AWS_SECRET,\n   endpoint_url=f'https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com',\n   s3_additional_kwargs={'ACL': 'public-read'}\n)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/save index to remote blob storage\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.set_index_id(\"vector_index\")\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/this is {bucket_name}/{index_name}\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.storage_context.persist('llama-index/storage_demo', fs=s3)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.", "start_char_idx": 0, "end_char_idx": 1686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d438c52-7882-4870-b7d7-9786dad54f7f": {"__data__": {"id_": "4d438c52-7882-4870-b7d7-9786dad54f7f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537", "node_type": "1", "metadata": {}, "hash": "3c4e3f57d8d8eeb02c6f8c3bfc094d56481ac66edba4b1acee241552182c8444", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7165700b-0795-438a-9394-5943ce730da2", "node_type": "1", "metadata": {}, "hash": "6dc836a2c7f279ee3fa90b5adac3c30d676bcce909efb814fdff44b3f7e7dc82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa59b324-bced-42a5-bfc5-4d9e93f4e35d", "node_type": "1", "metadata": {}, "hash": "05f331d9ad376f3460e7c4795db651d3024011e60ec7d580a16401d97ede21a3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537", "node_type": "1", "metadata": {}, "hash": "3c4e3f57d8d8eeb02c6f8c3bfc094d56481ac66edba4b1acee241552182c8444", "class_name": "RelatedNodeInfo"}}, "hash": "6bb05212b99c7498a2402e8bf260fc253186bf99ab38924b8b4cd83c714b381b", "text": "fs=s3)\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load index from s3\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nsc = StorageContext.from_defaults(persist_dir='llama-index/storage_demo', fs=s3)\nindex2 = load_index_from_storage(sc, 'vector_index')\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\save_load.md\nContent Type: text\nHeader Path: Persisting & Loading Data/load index from s3\nfile_path: Docs\\core_modules\\data_modules\\storage\\save_load.md\nfile_name: save_load.md\nfile_type: None\nfile_size: 3559\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, if you do not pass a filesystem, we will assume a local filesystem.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nVector stores contain embedding vectors of ingested document chunks \n(and sometimes the document chunks as well).", "start_char_idx": 1618, "end_char_idx": 2996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa59b324-bced-42a5-bfc5-4d9e93f4e35d": {"__data__": {"id_": "aa59b324-bced-42a5-bfc5-4d9e93f4e35d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537", "node_type": "1", "metadata": {}, "hash": "3c4e3f57d8d8eeb02c6f8c3bfc094d56481ac66edba4b1acee241552182c8444", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d438c52-7882-4870-b7d7-9786dad54f7f", "node_type": "1", "metadata": {}, "hash": "6bb05212b99c7498a2402e8bf260fc253186bf99ab38924b8b4cd83c714b381b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537", "node_type": "1", "metadata": {}, "hash": "3c4e3f57d8d8eeb02c6f8c3bfc094d56481ac66edba4b1acee241552182c8444", "class_name": "RelatedNodeInfo"}}, "hash": "05f331d9ad376f3460e7c4795db651d3024011e60ec7d580a16401d97ede21a3", "text": "File Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Simple Vector Store\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LlamaIndex uses a simple in-memory vector store that's great for quick experimentation.\nThey can be persisted to (and loaded from) disk by calling `vector_store.persist()` (and `SimpleVectorStore.from_persist_path(...)` respectively).\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Third-Party Vector Store Integrations\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe also integrate with a wide range of vector store implementations. \nThey mainly differ in 2 aspects:\n1. in-memory vs. hosted\n2. stores only vector embeddings vs.", "start_char_idx": 2998, "end_char_idx": 4141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e11970d-2737-408a-9880-82efb9f30500": {"__data__": {"id_": "4e11970d-2737-408a-9880-82efb9f30500", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cae4deaf-d284-4a2f-a529-c51f448188a0", "node_type": "1", "metadata": {}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b02504ef-9496-4b5d-9803-34e6bb4c4b20", "node_type": "1", "metadata": {}, "hash": "f7df85472e7839c85ffd3bfc64fd2fd59a5ec4e80b0ab781a50d1378e298025a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cae4deaf-d284-4a2f-a529-c51f448188a0", "node_type": "1", "metadata": {}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "class_name": "RelatedNodeInfo"}}, "hash": "4420f98742af652388587db35a0cf3531de04a164b805fdd4ae86ca6fcbf5d70", "text": "hosted\n2. stores only vector embeddings vs. also stores documents\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Third-Party Vector Store Integrations/In-Memory Vector Stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* Faiss\n* Chroma\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Third-Party Vector Store Integrations/(Self) Hosted Vector Stores\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* Pinecone\n* Weaviate\n* Milvus/Zilliz\n* Qdrant\n* Chroma\n* Opensearch\n* DeepLake\n* MyScale\n* Tair\n* DocArray\n* MongoDB Atlas\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Third-Party Vector Store Integrations/Others\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* ChatGPTRetrievalPlugin\n\nFor more details, see Vector Store Integrations.\n\nFile Name: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nContent Type: text\nHeader Path: Vector Stores/Third-Party Vector Store Integrations/Others\nLinks: \nfile_path: Docs\\core_modules\\data_modules\\storage\\vector_stores.md\nfile_name: vector_stores.", "start_char_idx": 0, "end_char_idx": 1728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b02504ef-9496-4b5d-9803-34e6bb4c4b20": {"__data__": {"id_": "b02504ef-9496-4b5d-9803-34e6bb4c4b20", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cae4deaf-d284-4a2f-a529-c51f448188a0", "node_type": "1", "metadata": {}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e11970d-2737-408a-9880-82efb9f30500", "node_type": "1", "metadata": {}, "hash": "4420f98742af652388587db35a0cf3531de04a164b805fdd4ae86ca6fcbf5d70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "665b86e0-639d-4207-a483-3a874ea9d39a", "node_type": "1", "metadata": {}, "hash": "a2cac16372549a7ad03186064cd451e496bcdd13ba07a467b4df9c17dc3fb87b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cae4deaf-d284-4a2f-a529-c51f448188a0", "node_type": "1", "metadata": {}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "class_name": "RelatedNodeInfo"}}, "hash": "f7df85472e7839c85ffd3bfc64fd2fd59a5ec4e80b0ab781a50d1378e298025a", "text": "md\nfile_name: vector_stores.md\nfile_type: None\nfile_size: 2112\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\ncaption: Examples\nmaxdepth: 1\n---\n/examples/vector_stores/SimpleIndexDemo.ipynb\n/examples/vector_stores/QdrantIndexDemo.ipynb\n/examples/vector_stores/FaissIndexDemo.ipynb\n/examples/vector_stores/DeepLakeIndexDemo.ipynb\n/examples/vector_stores/MyScaleIndexDemo.ipynb\n/examples/vector_stores/MetalIndexDemo.ipynb\n/examples/vector_stores/WeaviateIndexDemo.ipynb\n/examples/vector_stores/OpensearchDemo.ipynb\n/examples/vector_stores/PineconeIndexDemo.ipynb\n/examples/vector_stores/ChromaIndexDemo.ipynb\n/examples/vector_stores/LanceDBIndexDemo.ipynb\n/examples/vector_stores/MilvusIndexDemo.ipynb\n/examples/vector_stores/RedisIndexDemo.ipynb\n/examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb\n/examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb\n/examples/vector_stores/AsyncIndexCreationDemo.ipynb\n/examples/vector_stores/TairIndexDemo.ipynb\n/examples/vector_stores/SupabaseVectorIndexDemo.ipynb\n/examples/vector_stores/DocArrayHnswIndexDemo.ipynb\n/examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb\n/examples/vector_stores/MongoDBAtlasVectorSearch.ipynb\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\modules.md\nContent Type: text\nHeader Path: Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 300\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support integrations with OpenAI, Azure, and anything LangChain offers.", "start_char_idx": 1700, "end_char_idx": 3311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "665b86e0-639d-4207-a483-3a874ea9d39a": {"__data__": {"id_": "665b86e0-639d-4207-a483-3a874ea9d39a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cae4deaf-d284-4a2f-a529-c51f448188a0", "node_type": "1", "metadata": {}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b02504ef-9496-4b5d-9803-34e6bb4c4b20", "node_type": "1", "metadata": {}, "hash": "f7df85472e7839c85ffd3bfc64fd2fd59a5ec4e80b0ab781a50d1378e298025a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fc3f90d-1a37-403c-a257-0f84b3e46ea0", "node_type": "1", "metadata": {}, "hash": "8e601881c37eb7ae0a17b019926cd4e4f4fbff7c4e1223659c4ead5870598b1c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cae4deaf-d284-4a2f-a529-c51f448188a0", "node_type": "1", "metadata": {}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "class_name": "RelatedNodeInfo"}}, "hash": "a2cac16372549a7ad03186064cd451e496bcdd13ba07a467b4df9c17dc3fb87b", "text": "Azure, and anything LangChain offers.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\modules.md\nContent Type: text\nHeader Path: Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 300\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\n/examples/embeddings/OpenAI.ipynb\n/examples/embeddings/Langchain.ipynb\n/examples/customization/llms/AzureOpenAI.ipynb\n/examples/embeddings/custom_embeddings.ipynb\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Concept\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEmbeddings are used in LlamaIndex to represent your documents using a sophistacted numerical representation. Embedding models take text as input, and return a long list of numbers used to caputre the semantics of the text. These embedding models have been trained to represent text this way, and help enable many applications, including search!\n\nAt a high level, if a user asks a question about dogs, then the embedding for that question will be highly similar to text that talks about dogs.\n\nWhen calculating the similarity between embeddings, there are many methods to use (dot product, cosine similarity, etc.). By default, LlamaIndex uses cosine similarity when comparing embeddings.\n\nThere are many embedding models to pick from. By default, LlamaIndex uses `text-embedding-ada-002` from OpenAI. We also support any embedding model offered by Langchain here, as well as providing an easy to extend base class for implementing your own embeddings.", "start_char_idx": 3274, "end_char_idx": 5091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fc3f90d-1a37-403c-a257-0f84b3e46ea0": {"__data__": {"id_": "3fc3f90d-1a37-403c-a257-0f84b3e46ea0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cae4deaf-d284-4a2f-a529-c51f448188a0", "node_type": "1", "metadata": {}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "665b86e0-639d-4207-a483-3a874ea9d39a", "node_type": "1", "metadata": {}, "hash": "a2cac16372549a7ad03186064cd451e496bcdd13ba07a467b4df9c17dc3fb87b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cae4deaf-d284-4a2f-a529-c51f448188a0", "node_type": "1", "metadata": {}, "hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "class_name": "RelatedNodeInfo"}}, "hash": "8e601881c37eb7ae0a17b019926cd4e4f4fbff7c4e1223659c4ead5870598b1c", "text": "File Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMost commonly in LlamaIndex, embedding models will be specified in the `ServiceContext` object, and then used in a vector index.", "start_char_idx": 5093, "end_char_idx": 5550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97faf9c1-4ef6-43f9-b68d-dc9eb12cace0": {"__data__": {"id_": "97faf9c1-4ef6-43f9-b68d-dc9eb12cace0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "node_type": "1", "metadata": {}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6537a827-6b22-4ea0-909d-d40605457dc8", "node_type": "1", "metadata": {}, "hash": "e0e5afa98659fa8ab13de585b76ce8cb3eab601c2759f74736c2ad2ac9e19b0d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "node_type": "1", "metadata": {}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "class_name": "RelatedNodeInfo"}}, "hash": "784c063b454a5de45458286de31877fd6071cd7a3e02e187630c4568c274ea6d", "text": "The embedding model will be used to embed the documents used during index construction, as well as embedding any queries you make using the query engine later on.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext\nfrom llama_index.embeddings import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding()\nservice_context = serviceContext.from_defaults(embed_model=embed_model)\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can find more usage details and availbale customization options below.", "start_char_idx": 0, "end_char_idx": 1092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6537a827-6b22-4ea0-909d-d40605457dc8": {"__data__": {"id_": "6537a827-6b22-4ea0-909d-d40605457dc8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "node_type": "1", "metadata": {}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97faf9c1-4ef6-43f9-b68d-dc9eb12cace0", "node_type": "1", "metadata": {}, "hash": "784c063b454a5de45458286de31877fd6071cd7a3e02e187630c4568c274ea6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d74b1e06-4f5d-43f4-9481-acbad0dc7f1a", "node_type": "1", "metadata": {}, "hash": "725f97fcdc31ec7b0903c1e238efd0f669bc136291df750fdb1a5d1d2b295433", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "node_type": "1", "metadata": {}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "class_name": "RelatedNodeInfo"}}, "hash": "e0e5afa98659fa8ab13de585b76ce8cb3eab601c2759f74736c2ad2ac9e19b0d", "text": "File Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support integrations with OpenAI, Azure, and anything LangChain offers. Details below.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\root.md\nContent Type: text\nHeader Path: Embeddings/Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1900\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe most common usage for an embedding model will be setting it in the service context object, and then using it to construct an index and query. The input documents will be broken into nodes, and the emedding model will generate an embedding for each node.", "start_char_idx": 1094, "end_char_idx": 2848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d74b1e06-4f5d-43f4-9481-acbad0dc7f1a": {"__data__": {"id_": "d74b1e06-4f5d-43f4-9481-acbad0dc7f1a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "node_type": "1", "metadata": {}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6537a827-6b22-4ea0-909d-d40605457dc8", "node_type": "1", "metadata": {}, "hash": "e0e5afa98659fa8ab13de585b76ce8cb3eab601c2759f74736c2ad2ac9e19b0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b1b51ae-3158-40ed-8b5e-0d17adac50ec", "node_type": "1", "metadata": {}, "hash": "7d01f9c6ad9afdac055dc9bd736169a3df140c124903502169c356163f100df5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "node_type": "1", "metadata": {}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "class_name": "RelatedNodeInfo"}}, "hash": "725f97fcdc31ec7b0903c1e238efd0f669bc136291df750fdb1a5d1d2b295433", "text": "By default, LlamaIndex will use `text-embedding-ada-002`, which is what the example below manually sets up for you.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Getting Started\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding()\nservice_context = serviceContext.from_defaults(embed_model=embed_model)\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/optionally set a global service context to avoid passing it into other objects every time\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import set_global_service_context\nset_global_service_context(service_context)\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/optionally set a global service context to avoid passing it into other objects every time\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen, at query time, the embedding model will be used again to embed the query text.", "start_char_idx": 2850, "end_char_idx": 4724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b1b51ae-3158-40ed-8b5e-0d17adac50ec": {"__data__": {"id_": "2b1b51ae-3158-40ed-8b5e-0d17adac50ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "node_type": "1", "metadata": {}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d74b1e06-4f5d-43f4-9481-acbad0dc7f1a", "node_type": "1", "metadata": {}, "hash": "725f97fcdc31ec7b0903c1e238efd0f669bc136291df750fdb1a5d1d2b295433", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "node_type": "1", "metadata": {}, "hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "class_name": "RelatedNodeInfo"}}, "hash": "7d01f9c6ad9afdac055dc9bd736169a3df140c124903502169c356163f100df5", "text": "File Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/optionally set a global service context to avoid passing it into other objects every time\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"query string\")\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Batch Size\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, embeddings requests are sent to OpenAI in batches of 10. For some users, this may (rarely) incur a rate limit.", "start_char_idx": 4726, "end_char_idx": 5740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c67d148-7cec-4ba2-8970-d6584100e4d9": {"__data__": {"id_": "1c67d148-7cec-4ba2-8970-d6584100e4d9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf87247e-da28-4d97-827f-4beaef959aa9", "node_type": "1", "metadata": {}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b00879d8-1aa8-4cbb-ac15-51f6e436f2e6", "node_type": "1", "metadata": {}, "hash": "6a81ce7d9beaea315c3ef86b11acc06a0a3426535f56d1f01d631a4412e6c88e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cf87247e-da28-4d97-827f-4beaef959aa9", "node_type": "1", "metadata": {}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "class_name": "RelatedNodeInfo"}}, "hash": "5191e763e3d59e63fe90a7e4481e9f32306062f82a27ed0f900ef9859324de59", "text": "For some users, this may (rarely) incur a rate limit. For other users embedding many documents, this batch size may be too small.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/set the batch size to 42\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nembed_model = OpenAIEmbedding(embed_batch_size=42)\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Embedding Model Integrations\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe also support any embeddings offered by Langchain here, using our `LangchainEmbedding` wrapper class.\n\nThe example below loads a model from Hugging Face, using Langchain's embedding class.", "start_char_idx": 0, "end_char_idx": 1145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b00879d8-1aa8-4cbb-ac15-51f6e436f2e6": {"__data__": {"id_": "b00879d8-1aa8-4cbb-ac15-51f6e436f2e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf87247e-da28-4d97-827f-4beaef959aa9", "node_type": "1", "metadata": {}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c67d148-7cec-4ba2-8970-d6584100e4d9", "node_type": "1", "metadata": {}, "hash": "5191e763e3d59e63fe90a7e4481e9f32306062f82a27ed0f900ef9859324de59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f14ce69-f3dd-4132-84b0-206a96d9de9b", "node_type": "1", "metadata": {}, "hash": "cd493143f16f2bd00085287f100bef08839972dc19881af1b5e715725a7591d3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cf87247e-da28-4d97-827f-4beaef959aa9", "node_type": "1", "metadata": {}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "class_name": "RelatedNodeInfo"}}, "hash": "6a81ce7d9beaea315c3ef86b11acc06a0a3426535f56d1f01d631a4412e6c88e", "text": "The example below loads a model from Hugging Face, using Langchain's embedding class.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Embedding Model Integrations\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom llama_index import LangchainEmbedding, ServiceContext\n\nembed_model = LangchainEmbedding(\n  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n)\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Custom Embedding Model\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you wanted to use embeddings not offered by LlamaIndex or Langchain, you can also extend our base embeddings class and implement your own!\n\nThe example below uses Instructor Embeddings (install/setup details here), and implements a custom embeddings class. Instructor embeddings work by providing text, as well as \"instructions\" on the domain of the text to embed. This is helpful when embedding text from a very specific and specialized topic.", "start_char_idx": 1060, "end_char_idx": 2678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f14ce69-f3dd-4132-84b0-206a96d9de9b": {"__data__": {"id_": "7f14ce69-f3dd-4132-84b0-206a96d9de9b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf87247e-da28-4d97-827f-4beaef959aa9", "node_type": "1", "metadata": {}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b00879d8-1aa8-4cbb-ac15-51f6e436f2e6", "node_type": "1", "metadata": {}, "hash": "6a81ce7d9beaea315c3ef86b11acc06a0a3426535f56d1f01d631a4412e6c88e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d098b1ae-02c9-4f62-bb8d-f5ed9e288914", "node_type": "1", "metadata": {}, "hash": "7928ced9d521975bdecb52fdf8a980f27dca5e7400724d9e541457bf65a58a0a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cf87247e-da28-4d97-827f-4beaef959aa9", "node_type": "1", "metadata": {}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "class_name": "RelatedNodeInfo"}}, "hash": "cd493143f16f2bd00085287f100bef08839972dc19881af1b5e715725a7591d3", "text": "This is helpful when embedding text from a very specific and specialized topic.\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Custom Embedding Model\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom typing import Any, List\nfrom InstructorEmbedding import INSTRUCTOR\nfrom llama_index.embeddings.base import BaseEmbedding\n\nclass InstructorEmbeddings(BaseEmbedding):\n  def __init__(\n    self, \n    instructor_model_name: str = \"hkunlp/instructor-large\",\n    instruction: str = \"Represent the Computer Science documentation or question:\",\n    **kwargs: Any,\n  ) -> None:\n    self._model = INSTRUCTOR(instructor_model_name)\n    self._instruction = instruction\n    super().__init__(**kwargs)\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n      embeddings = self._model.encode([[self._instruction, query]])\n      return embeddings[0]\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n      embeddings = self._model.encode([[self._instruction, text]])\n      return embeddings[0] \n\n    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n      embeddings = self._model.encode([[self._instruction, text] for text in texts])\n      return embeddings\n\nFile Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Standalone Usage\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also use embeddings as a standalone module for your project, existing application, or general testing and exploration.", "start_char_idx": 2599, "end_char_idx": 4558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d098b1ae-02c9-4f62-bb8d-f5ed9e288914": {"__data__": {"id_": "d098b1ae-02c9-4f62-bb8d-f5ed9e288914", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf87247e-da28-4d97-827f-4beaef959aa9", "node_type": "1", "metadata": {}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f14ce69-f3dd-4132-84b0-206a96d9de9b", "node_type": "1", "metadata": {}, "hash": "cd493143f16f2bd00085287f100bef08839972dc19881af1b5e715725a7591d3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cf87247e-da28-4d97-827f-4beaef959aa9", "node_type": "1", "metadata": {}, "hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "class_name": "RelatedNodeInfo"}}, "hash": "7928ced9d521975bdecb52fdf8a980f27dca5e7400724d9e541457bf65a58a0a", "text": "File Name: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Customization/Standalone Usage\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3986\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nembeddings = embed_model.get_text_embedding(\"It is raining cats and dogs here!\")\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\modules.md\nContent Type: text\nHeader Path: Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 677\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support integrations with OpenAI, Anthropic, Hugging Face, PaLM, and more.", "start_char_idx": 4560, "end_char_idx": 5403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60447b3a-237a-4a20-bdf4-02d73d9c5b70": {"__data__": {"id_": "60447b3a-237a-4a20-bdf4-02d73d9c5b70", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "node_type": "1", "metadata": {}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8502831f-6b0c-4d66-ae71-6a5e4f0b9158", "node_type": "1", "metadata": {}, "hash": "a8be8269f220de0aab2524e4346305eb52b9ec9a31ef1a0494d5dfb7c4bf1b94", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "node_type": "1", "metadata": {}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "class_name": "RelatedNodeInfo"}}, "hash": "ffcdd0847d257ba804898c0b2fa7140e035f04baa3c896200d1326b932c69bef", "text": "File Name: Docs\\core_modules\\model_modules\\llms\\modules.md\nContent Type: code\nHeader Path: Modules/OpenAI\nfile_path: Docs\\core_modules\\model_modules\\llms\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 677\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/openai.ipynb\n/examples/llm/azure_openai.ipynb\n\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\modules.md\nContent Type: code\nHeader Path: Modules/Anthropic\nfile_path: Docs\\core_modules\\model_modules\\llms\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 677\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/anthropic.ipynb\n\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\modules.md\nContent Type: code\nHeader Path: Modules/Hugging Face\nfile_path: Docs\\core_modules\\model_modules\\llms\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 677\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/customization/llms/SimpleIndexDemo-Huggingface_camel.ipynb\n/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb\n\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\modules.md\nContent Type: code\nHeader Path: Modules/PaLM\nfile_path: Docs\\core_modules\\model_modules\\llms\\modules.md\nfile_name: modules.", "start_char_idx": 0, "end_char_idx": 1460, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8502831f-6b0c-4d66-ae71-6a5e4f0b9158": {"__data__": {"id_": "8502831f-6b0c-4d66-ae71-6a5e4f0b9158", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "node_type": "1", "metadata": {}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60447b3a-237a-4a20-bdf4-02d73d9c5b70", "node_type": "1", "metadata": {}, "hash": "ffcdd0847d257ba804898c0b2fa7140e035f04baa3c896200d1326b932c69bef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ca2153a-bb41-4cd3-8eba-f9d3efec4352", "node_type": "1", "metadata": {}, "hash": "8d92950d7529071c6f77909e063c182be4240fe75bbbc65c1ee91dc3fc720e6d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "node_type": "1", "metadata": {}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "class_name": "RelatedNodeInfo"}}, "hash": "a8be8269f220de0aab2524e4346305eb52b9ec9a31ef1a0494d5dfb7c4bf1b94", "text": "md\nfile_name: modules.md\nfile_type: None\nfile_size: 677\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/palm.ipynb\n\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/Concept\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nPicking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.\n\nLLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\n\nLlamaIndex provides a unified interface for defining LLM modules, whether it's from OpenAI, Hugging Face, or LangChain, so that you \ndon't have to write the boilerplate code of defining the LLM interface yourself.", "start_char_idx": 1438, "end_char_idx": 2677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ca2153a-bb41-4cd3-8eba-f9d3efec4352": {"__data__": {"id_": "4ca2153a-bb41-4cd3-8eba-f9d3efec4352", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "node_type": "1", "metadata": {}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8502831f-6b0c-4d66-ae71-6a5e4f0b9158", "node_type": "1", "metadata": {}, "hash": "a8be8269f220de0aab2524e4346305eb52b9ec9a31ef1a0494d5dfb7c4bf1b94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "877e0a46-7397-4bfc-a924-0ac24e0c8eb7", "node_type": "1", "metadata": {}, "hash": "e747b0eba6d307af451ff80eb13761f1ad94165fb7748b5c8237f970f80267b3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "node_type": "1", "metadata": {}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "class_name": "RelatedNodeInfo"}}, "hash": "8d92950d7529071c6f77909e063c182be4240fe75bbbc65c1ee91dc3fc720e6d", "text": "This interface consists of the following (more details below):\n- Support for **text completion** and **chat** endpoints (details below)\n- Support for **streaming** and **non-streaming** endpoints\n- Support for **synchronous** and **asynchronous** endpoints\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe following code snippet shows how you can get started using LLMs.", "start_char_idx": 2678, "end_char_idx": 3314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "877e0a46-7397-4bfc-a924-0ac24e0c8eb7": {"__data__": {"id_": "877e0a46-7397-4bfc-a924-0ac24e0c8eb7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "node_type": "1", "metadata": {}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ca2153a-bb41-4cd3-8eba-f9d3efec4352", "node_type": "1", "metadata": {}, "hash": "8d92950d7529071c6f77909e063c182be4240fe75bbbc65c1ee91dc3fc720e6d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8", "node_type": "1", "metadata": {}, "hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "class_name": "RelatedNodeInfo"}}, "hash": "e747b0eba6d307af451ff80eb13761f1ad94165fb7748b5c8237f970f80267b3", "text": "File Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.llms import OpenAI\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/non-streaming\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresp = OpenAI().complete('Paul Graham is ')\nprint(resp)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/non-streaming\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use the LLM as a standalone module or with other LlamaIndex abstractions. Check out our guide below.", "start_char_idx": 3316, "end_char_idx": 4448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f55e0d52-7922-46fc-adca-747d0dbdb6f5": {"__data__": {"id_": "f55e0d52-7922-46fc-adca-747d0dbdb6f5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "node_type": "1", "metadata": {}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "066d5c34-faad-4abb-a00f-6a9ccdf982d7", "node_type": "1", "metadata": {}, "hash": "5d3b290f6b30b050356fe8f6f853d8acd6641f1d19d1ae65477ee6a1143e2d07", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "node_type": "1", "metadata": {}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "class_name": "RelatedNodeInfo"}}, "hash": "f361664e23d522a282e75224507b3b5c3aab078fd252e667e1955d86c6f28b29", "text": "Check out our guide below.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/non-streaming\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_standalone.md\nusage_custom.md\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe support integrations with OpenAI, Hugging Face, PaLM, and more.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\root.md\nContent Type: text\nHeader Path: LLM/Modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1580\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.", "start_char_idx": 0, "end_char_idx": 1648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "066d5c34-faad-4abb-a00f-6a9ccdf982d7": {"__data__": {"id_": "066d5c34-faad-4abb-a00f-6a9ccdf982d7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "node_type": "1", "metadata": {}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f55e0d52-7922-46fc-adca-747d0dbdb6f5", "node_type": "1", "metadata": {}, "hash": "f361664e23d522a282e75224507b3b5c3aab078fd252e667e1955d86c6f28b29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4eba65f0-8080-4260-8cdf-c806124d8292", "node_type": "1", "metadata": {}, "hash": "b89ec8b6f91dad05f2c0cd5c9a1cce0a1026a979c2b5146cc3a09000d50f3c7e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "node_type": "1", "metadata": {}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "class_name": "RelatedNodeInfo"}}, "hash": "5d3b290f6b30b050356fe8f6f853d8acd6641f1d19d1ae65477ee6a1143e2d07", "text": "By default, we use OpenAI's `text-davinci-003` model. But you may choose to customize\nthe underlying LLM being used.\n\nBelow we show a few examples of LLM customization. This includes\n\n- changing the underlying LLM\n- changing the number of output tokens (for OpenAI, Cohere, or AI21)\n- having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Changing the underlying LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn example snippet of customizing the LLM being used is shown below.\nIn this example, we use `text-davinci-002` instead of `text-davinci-003`. Available models include `text-davinci-003`,`text-curie-001`,`text-babbage-001`,`text-ada-001`, `code-davinci-002`,`code-cushman-001`. \n\nNote that\nyou may also plug in any LLM shown on Langchain's\nLLM page.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Changing the underlying LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext\n)\nfrom llama_index.llms import OpenAI\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.", "start_char_idx": 1650, "end_char_idx": 3405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4eba65f0-8080-4260-8cdf-c806124d8292": {"__data__": {"id_": "4eba65f0-8080-4260-8cdf-c806124d8292", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "node_type": "1", "metadata": {}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "066d5c34-faad-4abb-a00f-6a9ccdf982d7", "node_type": "1", "metadata": {}, "hash": "5d3b290f6b30b050356fe8f6f853d8acd6641f1d19d1ae65477ee6a1143e2d07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dc13219-c223-4f21-9482-cdebddf4b7e3", "node_type": "1", "metadata": {}, "hash": "90417415a65efa77c4f9937a7056d6c1864f203c190b075b96e64035af8d6e9f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "node_type": "1", "metadata": {}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "class_name": "RelatedNodeInfo"}}, "hash": "b89ec8b6f91dad05f2c0cd5c9a1cce0a1026a979c2b5146cc3a09000d50f3c7e", "text": "md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/from langchain.llms import .\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/define LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(temperature=0, model=\"text-davinci-002\")\nservice_context = ServiceContext.from_defaults(llm=llm)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/build index\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = KeywordTableIndex.from_documents(documents, service_context=service_context)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/get response from query\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dc13219-c223-4f21-9482-cdebddf4b7e3": {"__data__": {"id_": "8dc13219-c223-4f21-9482-cdebddf4b7e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "node_type": "1", "metadata": {}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4eba65f0-8080-4260-8cdf-c806124d8292", "node_type": "1", "metadata": {}, "hash": "b89ec8b6f91dad05f2c0cd5c9a1cce0a1026a979c2b5146cc3a09000d50f3c7e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "node_type": "1", "metadata": {}, "hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "class_name": "RelatedNodeInfo"}}, "hash": "90417415a65efa77c4f9937a7056d6c1864f203c190b075b96e64035af8d6e9f", "text": "md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do after his time at Y Combinator?\")", "start_char_idx": 4979, "end_char_idx": 5256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1364720d-f74c-454b-9505-be0425082e42": {"__data__": {"id_": "1364720d-f74c-454b-9505-be0425082e42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "node_type": "1", "metadata": {}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a31672c0-029c-49ea-83ef-ebea6bf21349", "node_type": "1", "metadata": {}, "hash": "121dc53a9f2e5499ebeb227e5703fb5453c6bba4220c06e7c8ef739f6f6dfd8c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "node_type": "1", "metadata": {}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "class_name": "RelatedNodeInfo"}}, "hash": "19f415fe0cfd6406f3eaf29d2e4ecd1159855efe0b943ed3e1c37c6db23ef49f", "text": "File Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe number of output tokens is usually set to some low number by default (for instance,\nwith OpenAI the default is 256).\n\nFor OpenAI, Cohere, AI21, you just need to set the `max_tokens` parameter\n(or maxTokens for AI21). We will handle text chunking/calculations under the hood.", "start_char_idx": 0, "end_char_idx": 715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a31672c0-029c-49ea-83ef-ebea6bf21349": {"__data__": {"id_": "a31672c0-029c-49ea-83ef-ebea6bf21349", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "node_type": "1", "metadata": {}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1364720d-f74c-454b-9505-be0425082e42", "node_type": "1", "metadata": {}, "hash": "19f415fe0cfd6406f3eaf29d2e4ecd1159855efe0b943ed3e1c37c6db23ef49f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ed14c17-a43b-4cbf-9b74-74e78cabdade", "node_type": "1", "metadata": {}, "hash": "1207e634db08727ad7d02e5a12a526f21a967a63fb61db3558eb377dbd0a7cdb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "node_type": "1", "metadata": {}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "class_name": "RelatedNodeInfo"}}, "hash": "121dc53a9f2e5499ebeb227e5703fb5453c6bba4220c06e7c8ef739f6f6dfd8c", "text": "We will handle text chunking/calculations under the hood.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama_index.llms import OpenAI\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/define LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(temperature=0, model=\"text-davinci-002\", max_tokens=512)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Explicitly configure `context_window` and `num_output`\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you are using other LLM classes from langchain, you may need to explicitly configure the `context_window` and `num_output` via the `ServiceContext` since the information is not available by default.", "start_char_idx": 658, "end_char_idx": 2474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ed14c17-a43b-4cbf-9b74-74e78cabdade": {"__data__": {"id_": "7ed14c17-a43b-4cbf-9b74-74e78cabdade", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "node_type": "1", "metadata": {}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a31672c0-029c-49ea-83ef-ebea6bf21349", "node_type": "1", "metadata": {}, "hash": "121dc53a9f2e5499ebeb227e5703fb5453c6bba4220c06e7c8ef739f6f6dfd8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb69fa0a-3c79-4a62-bcd7-ea2a95975d9d", "node_type": "1", "metadata": {}, "hash": "10a939e1204b7a00932dcaa4192e624413f25c91ba32beec24a0fb8bf4bc65f0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "node_type": "1", "metadata": {}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "class_name": "RelatedNodeInfo"}}, "hash": "1207e634db08727ad7d02e5a12a526f21a967a63fb61db3558eb377dbd0a7cdb", "text": "File Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Explicitly configure `context_window` and `num_output`\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama_index.llms import OpenAI\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/from langchain.llms import .\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/set context window\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncontext_window = 4096\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/set number of output tokens\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb69fa0a-3c79-4a62-bcd7-ea2a95975d9d": {"__data__": {"id_": "fb69fa0a-3c79-4a62-bcd7-ea2a95975d9d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "node_type": "1", "metadata": {}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ed14c17-a43b-4cbf-9b74-74e78cabdade", "node_type": "1", "metadata": {}, "hash": "1207e634db08727ad7d02e5a12a526f21a967a63fb61db3558eb377dbd0a7cdb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6", "node_type": "1", "metadata": {}, "hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "class_name": "RelatedNodeInfo"}}, "hash": "10a939e1204b7a00932dcaa4192e624413f25c91ba32beec24a0fb8bf4bc65f0", "text": "md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnum_output = 256\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/define LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(\n    temperature=0, \n    model=\"text-davinci-002\", \n    max_tokens=num_output,\n)\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm,\n    context_window=context_window,\n    num_output=num_output,\n)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Using a HuggingFace LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports using LLMs from HuggingFace directly. Note that for a completely private experience, also setup a local embedding model (example here).", "start_char_idx": 4135, "end_char_idx": 5452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d78599f9-78e8-4ebc-b7b7-b1d6d7307702": {"__data__": {"id_": "d78599f9-78e8-4ebc-b7b7-b1d6d7307702", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a", "node_type": "1", "metadata": {}, "hash": "e993cae5ebc3cb0bd0f20104b76fc036c729a07a74ece8f4a7543bee076343b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47f69943-ccf3-4bf2-864b-b7ed32b85e9c", "node_type": "1", "metadata": {}, "hash": "3269ee61e7262ef3eb6b34fb5ca758b6e00cbdb3e6c64245abda640a0521cd58", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a", "node_type": "1", "metadata": {}, "hash": "e993cae5ebc3cb0bd0f20104b76fc036c729a07a74ece8f4a7543bee076343b7", "class_name": "RelatedNodeInfo"}}, "hash": "34ccd23fc0d486dc28930baeac3fb8815a7ebdb016b6e058c515fdfd1ed9bdaa", "text": "Note that for a completely private experience, also setup a local embedding model (example here).\n\nMany open-source models from HuggingFace require either some preamble before before each prompt, which is a `system_prompt`. Additionally, queries themselves may need an additional wrapper around the `query_str` itself. All this information is usually available from the HuggingFace model card for the model you are using.\n\nBelow, this example uses both the `system_prompt` and `query_wrapper_prompt`, using specific prompts from the model card found here.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Using a HuggingFace LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.prompts.prompts import SimpleInputPrompt\n\nsystem_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"", "start_char_idx": 0, "end_char_idx": 1496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47f69943-ccf3-4bf2-864b-b7ed32b85e9c": {"__data__": {"id_": "47f69943-ccf3-4bf2-864b-b7ed32b85e9c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a", "node_type": "1", "metadata": {}, "hash": "e993cae5ebc3cb0bd0f20104b76fc036c729a07a74ece8f4a7543bee076343b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d78599f9-78e8-4ebc-b7b7-b1d6d7307702", "node_type": "1", "metadata": {}, "hash": "34ccd23fc0d486dc28930baeac3fb8815a7ebdb016b6e058c515fdfd1ed9bdaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9876e684-21c6-48a3-bb99-01f70cdb6908", "node_type": "1", "metadata": {}, "hash": "78daf13efc20062afa1ab83d46df286176801cb9177288cb31ed244858c0d8b5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a", "node_type": "1", "metadata": {}, "hash": "e993cae5ebc3cb0bd0f20104b76fc036c729a07a74ece8f4a7543bee076343b7", "class_name": "RelatedNodeInfo"}}, "hash": "3269ee61e7262ef3eb6b34fb5ca758b6e00cbdb3e6c64245abda640a0521cd58", "text": "- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/This will wrap the default prompts that are internal to llama-index\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n\nimport torch\nfrom llama_index.llms import HuggingFaceLLM\nllm = HuggingFaceLLM(\n    context_window=4096, \n    max_new_tokens=256,\n    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n    system_prompt=system_prompt,\n    query_wrapper_prompt=query_wrapper_prompt,\n    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    device_map=\"auto\",\n    stopping_ids=[50278, 50279, 50277, 1, 0],\n    tokenizer_kwargs={\"max_length\": 4096},\n    # uncomment this if using CUDA to reduce memory usage\n    # model_kwargs={\"torch_dtype\": torch.float16}\n)\nservice_context = ServiceContext.from_defaults(\n    chunk_size=1024, \n    llm=llm,\n)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/This will wrap the default prompts that are internal to llama-index\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSome models will raise errors if all the keys from the tokenizer are passed to the model. A common tokenizer output that causes issues is `token_type_ids`.", "start_char_idx": 1418, "end_char_idx": 3286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9876e684-21c6-48a3-bb99-01f70cdb6908": {"__data__": {"id_": "9876e684-21c6-48a3-bb99-01f70cdb6908", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a", "node_type": "1", "metadata": {}, "hash": "e993cae5ebc3cb0bd0f20104b76fc036c729a07a74ece8f4a7543bee076343b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47f69943-ccf3-4bf2-864b-b7ed32b85e9c", "node_type": "1", "metadata": {}, "hash": "3269ee61e7262ef3eb6b34fb5ca758b6e00cbdb3e6c64245abda640a0521cd58", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a", "node_type": "1", "metadata": {}, "hash": "e993cae5ebc3cb0bd0f20104b76fc036c729a07a74ece8f4a7543bee076343b7", "class_name": "RelatedNodeInfo"}}, "hash": "78daf13efc20062afa1ab83d46df286176801cb9177288cb31ed244858c0d8b5", "text": "A common tokenizer output that causes issues is `token_type_ids`. Below is an example of configuring the predictor to remove this before passing the inputs to the model:\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/This will wrap the default prompts that are internal to llama-index\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHuggingFaceLLM(\n    ...\n    tokenizer_outputs_to_remove=[\"token_type_ids\"]\n)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/This will wrap the default prompts that are internal to llama-index\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full API reference can be found here.\n\nSeveral example notebooks are also listed below:\n\n- StableLM\n- Camel\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Using a Custom LLM Model - Advanced\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo use a custom LLM model, you only need to implement the `LLM` class (or `CustomLLM` for a simpler interface)\nYou will be responsible for passing the text to the model and returning the newly generated tokens.\n\nNote that for a completely private experience, also setup a local embedding model (example here).", "start_char_idx": 3221, "end_char_idx": 5163, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ddc107e-02a7-4d69-a976-a15b237b97f8": {"__data__": {"id_": "8ddc107e-02a7-4d69-a976-a15b237b97f8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c", "node_type": "1", "metadata": {}, "hash": "c0641090de36ac0cf8d00d2ab57ef7bdbf433a45416289a3918828aa606a42a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82aac71e-f8f6-41b5-86fc-cc647f68c6b6", "node_type": "1", "metadata": {}, "hash": "326faad75b59cd94ed683f37a7a54cda6f310134e282d7356791cbabbdb58768", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c", "node_type": "1", "metadata": {}, "hash": "c0641090de36ac0cf8d00d2ab57ef7bdbf433a45416289a3918828aa606a42a9", "class_name": "RelatedNodeInfo"}}, "hash": "60333aca05813aa542a9889f82d07a793cbf6e363466fe9efc3daf97ac3fe8ef", "text": "Note that for a completely private experience, also setup a local embedding model (example here).\n\nHere is a small example using locally running facebook/OPT model and Huggingface's pipeline abstraction:\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Example: Using a Custom LLM Model - Advanced\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport torch\nfrom transformers import pipeline\nfrom typing import Optional, List, Mapping, Any\n\nfrom llama_index import (\n    ServiceContext, \n    SimpleDirectoryReader, \n    LangchainEmbedding, \n    ListIndex\n)\nfrom llama_index.llms import CustomLLM, CompletionResponse, LLMMetadata\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/set context window size\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncontext_window = 2048\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/set number of output tokens\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnum_output = 256\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.", "start_char_idx": 0, "end_char_idx": 1781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82aac71e-f8f6-41b5-86fc-cc647f68c6b6": {"__data__": {"id_": "82aac71e-f8f6-41b5-86fc-cc647f68c6b6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c", "node_type": "1", "metadata": {}, "hash": "c0641090de36ac0cf8d00d2ab57ef7bdbf433a45416289a3918828aa606a42a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ddc107e-02a7-4d69-a976-a15b237b97f8", "node_type": "1", "metadata": {}, "hash": "60333aca05813aa542a9889f82d07a793cbf6e363466fe9efc3daf97ac3fe8ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fecefe8c-f135-4ecd-b1f4-bcf27763cbce", "node_type": "1", "metadata": {}, "hash": "bf319d317f552e2c6f9c72efa14daad3e5c627c8e92b2a30184e2d0b3c782053", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c", "node_type": "1", "metadata": {}, "hash": "c0641090de36ac0cf8d00d2ab57ef7bdbf433a45416289a3918828aa606a42a9", "class_name": "RelatedNodeInfo"}}, "hash": "326faad75b59cd94ed683f37a7a54cda6f310134e282d7356791cbabbdb58768", "text": "md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/store the pipeline/model outisde of the LLM class to avoid memory issues\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmodel_name = \"facebook/opt-iml-max-30b\"\npipeline = pipeline(\"text-generation\", model=model_name, device=\"cuda:0\", model_kwargs={\"torch_dtype\":torch.bfloat16})\n\nclass OurLLM(CustomLLM):\n\n    @property\n    def metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        return LLMMetadata(\n            context_window=context_window, num_output=num_output\n        )\n\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        prompt_length = len(prompt)\n        response = pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n\n        # only return newly generated tokens\n        text = response[prompt_length:]\n        return CompletionResponse(text=text)\n    \n    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n        raise NotImplementedError()\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/define our LLM\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OurLLM()\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm, \n    context_window=context_window, \n    num_output=num_output\n)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Load the your data\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.", "start_char_idx": 1781, "end_char_idx": 3737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fecefe8c-f135-4ecd-b1f4-bcf27763cbce": {"__data__": {"id_": "fecefe8c-f135-4ecd-b1f4-bcf27763cbce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c", "node_type": "1", "metadata": {}, "hash": "c0641090de36ac0cf8d00d2ab57ef7bdbf433a45416289a3918828aa606a42a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82aac71e-f8f6-41b5-86fc-cc647f68c6b6", "node_type": "1", "metadata": {}, "hash": "326faad75b59cd94ed683f37a7a54cda6f310134e282d7356791cbabbdb58768", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c", "node_type": "1", "metadata": {}, "hash": "c0641090de36ac0cf8d00d2ab57ef7bdbf433a45416289a3918828aa606a42a9", "class_name": "RelatedNodeInfo"}}, "hash": "bf319d317f552e2c6f9c72efa14daad3e5c627c8e92b2a30184e2d0b3c782053", "text": "md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('./data').load_data()\nindex = ListIndex.from_documents(documents, service_context=service_context)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Query and print response\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"<query_text>\")\nprint(response)\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nContent Type: text\nHeader Path: Customizing LLMs within LlamaIndex Abstractions/Query and print response\nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_custom.md\nfile_name: usage_custom.md\nfile_type: None\nfile_size: 9564\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUsing this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\n\nNote that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it's capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.\n\nA list of all default internal prompts is available here, and chat-specific prompts are listed here. You can also implement your own custom prompts, as described here.", "start_char_idx": 3737, "end_char_idx": 5647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8eefb2ba-49eb-4091-a4a8-5bc8a0765803": {"__data__": {"id_": "8eefb2ba-49eb-4091-a4a8-5bc8a0765803", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0a12723-8a55-44cf-ac43-2688613ee018", "node_type": "1", "metadata": {}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a96e2d52-a0a9-4f0b-b7e8-7cbefefa2ff4", "node_type": "1", "metadata": {}, "hash": "dc021e4c0aa0f17c8de2246ff4d4f1fad7123de8c725e7b631f60b3683ef4eb2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e0a12723-8a55-44cf-ac43-2688613ee018", "node_type": "1", "metadata": {}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "class_name": "RelatedNodeInfo"}}, "hash": "200ad72b96b0d94fc6de636ea312e3bf1a893561ec921b4c30bab66ccaf688e4", "text": "You can also implement your own custom prompts, as described here.\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nContent Type: text\nHeader Path: Using LLMs as standalone modules\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nfile_name: usage_standalone.md\nfile_type: None\nfile_size: 797\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use our LLM modules on their own.", "start_char_idx": 0, "end_char_idx": 469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a96e2d52-a0a9-4f0b-b7e8-7cbefefa2ff4": {"__data__": {"id_": "a96e2d52-a0a9-4f0b-b7e8-7cbefefa2ff4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0a12723-8a55-44cf-ac43-2688613ee018", "node_type": "1", "metadata": {}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8eefb2ba-49eb-4091-a4a8-5bc8a0765803", "node_type": "1", "metadata": {}, "hash": "200ad72b96b0d94fc6de636ea312e3bf1a893561ec921b4c30bab66ccaf688e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ce6af19-6c4e-49aa-8530-527c1776d181", "node_type": "1", "metadata": {}, "hash": "ffb4574b5c3ce9df058b25c71bd2fd49e4a61a3f6d6c80b95a174ca2cf32e030", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e0a12723-8a55-44cf-ac43-2688613ee018", "node_type": "1", "metadata": {}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "class_name": "RelatedNodeInfo"}}, "hash": "dc021e4c0aa0f17c8de2246ff4d4f1fad7123de8c725e7b631f60b3683ef4eb2", "text": "File Name: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nContent Type: code\nHeader Path: Using LLMs as standalone modules/using streaming endpoint\nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nfile_name: usage_standalone.md\nfile_type: None\nfile_size: 797\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index.llms import OpenAI\n\nresp = OpenAI().complete('Paul Graham is ')\nprint(resp)\n\nfrom llama_index.llms import OpenAI\nllm = OpenAI()\nresp = llm.stream_complete('Paul Graham is ')\nfor delta in resp:\n    print(delta, end='')\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nContent Type: code\nHeader Path: Using LLMs as standalone modules/Chat Example\nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nfile_name: usage_standalone.md\nfile_type: None\nfile_size: 797\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index.llms import ChatMessage, OpenAI\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a pirate with a colorful personality\"),\n    ChatMessage(role=\"user\", content=\"What is your name\"),\n]\nresp = OpenAI().chat(messages)\nprint(resp)\n```\n\nFile Name: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nContent Type: text\nHeader Path: Using LLMs as standalone modules/Chat Example\nfile_path: Docs\\core_modules\\model_modules\\llms\\usage_standalone.md\nfile_name: usage_standalone.md\nfile_type: None\nfile_size: 797\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCheck out our modules section for usage guides for each LLM.", "start_char_idx": 471, "end_char_idx": 2158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ce6af19-6c4e-49aa-8530-527c1776d181": {"__data__": {"id_": "5ce6af19-6c4e-49aa-8530-527c1776d181", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0a12723-8a55-44cf-ac43-2688613ee018", "node_type": "1", "metadata": {}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a96e2d52-a0a9-4f0b-b7e8-7cbefefa2ff4", "node_type": "1", "metadata": {}, "hash": "dc021e4c0aa0f17c8de2246ff4d4f1fad7123de8c725e7b631f60b3683ef4eb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "053a299d-174e-4691-be85-64399ab269a1", "node_type": "1", "metadata": {}, "hash": "f81c33c64247c60b01fb32f55770ed732aa5750df6b77c1f7370ab92caa5e4c2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e0a12723-8a55-44cf-ac43-2688613ee018", "node_type": "1", "metadata": {}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "class_name": "RelatedNodeInfo"}}, "hash": "ffb4574b5c3ce9df058b25c71bd2fd49e4a61a3f6d6c80b95a174ca2cf32e030", "text": "File Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Concept\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nPrompting is the fundamental input that gives LLMs their expressive power. LlamaIndex uses prompts to build the index, do insertion, \nperform traversal during querying, and to synthesize the final answer.\n\nLlamaIndex uses a set of default prompt templates that work well out of the box.\n\nIn addition, there are some prompts written and used specifically for chat models like `gpt-3.5-turbo` here.\n\nUsers may also provide their own prompt templates to further customize the behavior of the framework. The best method for customizing is copying the default prompt from the link above, and using that as the base for any modifications.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Defining a custom prompt\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDefining a custom prompt is as simple as creating a format string\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Defining a custom prompt\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Prompt\n\ntemplate = (\n    \"We have provided context information below.", "start_char_idx": 2160, "end_char_idx": 3937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "053a299d-174e-4691-be85-64399ab269a1": {"__data__": {"id_": "053a299d-174e-4691-be85-64399ab269a1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0a12723-8a55-44cf-ac43-2688613ee018", "node_type": "1", "metadata": {}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ce6af19-6c4e-49aa-8530-527c1776d181", "node_type": "1", "metadata": {}, "hash": "ffb4574b5c3ce9df058b25c71bd2fd49e4a61a3f6d6c80b95a174ca2cf32e030", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e0a12723-8a55-44cf-ac43-2688613ee018", "node_type": "1", "metadata": {}, "hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "class_name": "RelatedNodeInfo"}}, "hash": "f81c33c64247c60b01fb32f55770ed732aa5750df6b77c1f7370ab92caa5e4c2", "text": "\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given this information, please answer the question: {query_str}\\n\"\n)\nqa_template = Prompt(template)\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Defining a custom prompt\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note: you may see references to legacy prompt subclasses such as `QuestionAnswerPrompt`, `RefinePrompt`. These have been deprecated (and now are type aliases of `Prompt`). Now you can directly specify `Prompt(template)` to construct custom prompts. But you still have to make sure the template string contains the expected parameters (e.g. `{context_str}` and `{query_str}`) when replacing a default question answer prompt.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSince LlamaIndex is a multi-step pipeline, it's important to identify the operation that you want to modify and pass in the custom prompt at the right place.\n\nAt a high-level, prompts are used in 1) index construction, and 2) query engine execution\n\nThe most commonly used prompts will be the `text_qa_template` and the `refine_template`.", "start_char_idx": 3938, "end_char_idx": 5587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0f715d0-e1de-4622-b474-01547e080bf3": {"__data__": {"id_": "d0f715d0-e1de-4622-b474-01547e080bf3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f3972c2-9ca0-4dc0-9c58-481c82e443d3", "node_type": "1", "metadata": {}, "hash": "a9380cb22cffcc8982cbd6077d9973b3bd04dba78ba07fa37357b1d82d85defb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}}, "hash": "06823781c2c32ab0bc68aa74ba87757fa38f888916aeb09364f84e38508844cd", "text": "- `text_qa_template` - used to get an initial answer to a query using retrieved nodes\n- `refine_tempalate` - used when the retrieved text does not fit into a single LLM call with `response_mode=\"compact\"` (the default), or when more than one node is retrieved using `response_mode=\"refine\"`. The answer from the first query is inserted as an `existing_answer`, and the LLM must update or repeat the existing answer based on the new context.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in index construction\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDifferent indices use different types of prompts during construction (some don't use prompts at all). \nFor instance, `TreeIndex` uses a `SummaryPrompt` to hierarchically\nsummarize the nodes, and `KeywordTableIndex` uses a `KeywordExtractPrompt` to extract keywords.", "start_char_idx": 0, "end_char_idx": 1103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f3972c2-9ca0-4dc0-9c58-481c82e443d3": {"__data__": {"id_": "0f3972c2-9ca0-4dc0-9c58-481c82e443d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0f715d0-e1de-4622-b474-01547e080bf3", "node_type": "1", "metadata": {}, "hash": "06823781c2c32ab0bc68aa74ba87757fa38f888916aeb09364f84e38508844cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19e99400-c7d2-40fb-8687-098392f49704", "node_type": "1", "metadata": {}, "hash": "6223020496945b6321e7968eea6262a58425b2fbae844f1b9fef9cf59f809429", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}}, "hash": "a9380cb22cffcc8982cbd6077d9973b3bd04dba78ba07fa37357b1d82d85defb", "text": "There are two equivalent ways to override the prompts:\n\n1. via the default nodes constructor\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in index construction\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = TreeIndex(nodes, summary_template=<custom_prompt>)\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in index construction\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n2. via the documents constructor.", "start_char_idx": 1105, "end_char_idx": 2084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19e99400-c7d2-40fb-8687-098392f49704": {"__data__": {"id_": "19e99400-c7d2-40fb-8687-098392f49704", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f3972c2-9ca0-4dc0-9c58-481c82e443d3", "node_type": "1", "metadata": {}, "hash": "a9380cb22cffcc8982cbd6077d9973b3bd04dba78ba07fa37357b1d82d85defb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f9963d1-6a38-4220-8617-93a5cb9789bd", "node_type": "1", "metadata": {}, "hash": "6bd034b2de3d298909ba1c958e277076b0f01652d067513d4721b399af3fcd27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}}, "hash": "6223020496945b6321e7968eea6262a58425b2fbae844f1b9fef9cf59f809429", "text": "File Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in index construction\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = TreeIndex.from_documents(docs, summary_template=<custom_prompt>)\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in index construction\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor more details on which index uses which prompts, please visit\nIndex class references.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in query engine\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMore commonly, prompts are used at query-time (i.e. for executing a query against an index and synthesizing the final response). \n\nThere are also two equivalent ways to override the prompts:\n\n1. via the high-level API\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in query engine\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.", "start_char_idx": 2086, "end_char_idx": 3917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f9963d1-6a38-4220-8617-93a5cb9789bd": {"__data__": {"id_": "1f9963d1-6a38-4220-8617-93a5cb9789bd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19e99400-c7d2-40fb-8687-098392f49704", "node_type": "1", "metadata": {}, "hash": "6223020496945b6321e7968eea6262a58425b2fbae844f1b9fef9cf59f809429", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecc26c36-b4b2-4a93-80b2-61fb57afdbd0", "node_type": "1", "metadata": {}, "hash": "2ebeecc3087fabe235965196530db325b8ba31cf13dab190b5fae1cd41590f59", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}}, "hash": "6bd034b2de3d298909ba1c958e277076b0f01652d067513d4721b399af3fcd27", "text": "md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    text_qa_template=<custom_qa_prompt>,\n    refine_template=<custom_refine_prompt>\n)\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in query engine\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n2. via the low-level composition API\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in query engine\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = index.as_retriever()\nsynth = get_response_synthesizer(\n    text_qa_template=<custom_qa_prompt>,\n    refine_template=<custom_refine_prompt>\n)\nquery_engine = RetrieverQueryEngine(retriever, response_synthesizer)\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: text\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modify prompts used in query engine\nLinks: \nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe two approaches above are equivalent,", "start_char_idx": 3895, "end_char_idx": 5637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecc26c36-b4b2-4a93-80b2-61fb57afdbd0": {"__data__": {"id_": "ecc26c36-b4b2-4a93-80b2-61fb57afdbd0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f9963d1-6a38-4220-8617-93a5cb9789bd", "node_type": "1", "metadata": {}, "hash": "6bd034b2de3d298909ba1c958e277076b0f01652d067513d4721b399af3fcd27", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "node_type": "1", "metadata": {}, "hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "class_name": "RelatedNodeInfo"}}, "hash": "2ebeecc3087fabe235965196530db325b8ba31cf13dab190b5fae1cd41590f59", "text": "where 1 is essentially syntactic sugar for 2 and hides away the underlying complexity.", "start_char_idx": 5638, "end_char_idx": 5724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b857577b-3e79-4099-b044-63d4274fbb69": {"__data__": {"id_": "b857577b-3e79-4099-b044-63d4274fbb69", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4", "node_type": "1", "metadata": {}, "hash": "1f2969c151e8e590987b04579d8802598ae3d17b17ded6ea334bec5bf6e71f5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0490741-1793-41f1-a0cf-e480098af30b", "node_type": "1", "metadata": {}, "hash": "960effeef9bc1135375a5bb9a31aa500f85ca2ca983d377e12603a5112e49e83", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4", "node_type": "1", "metadata": {}, "hash": "1f2969c151e8e590987b04579d8802598ae3d17b17ded6ea334bec5bf6e71f5e", "class_name": "RelatedNodeInfo"}}, "hash": "4d7b8806b7407221c2f7901dcfddd52637567150fef24a77cbaeefe87249eefa", "text": "You might want to use 1 to quickly modify some common parameters, and use 2 to have more granular control.\n\n\nFor more details on which classes use which prompts, please visit\nQuery class references.\n\nCheck out the reference documentation for a full set of all prompts.\n\nFile Name: Docs\\core_modules\\model_modules\\prompts.md\nContent Type: code\nHeader Path: Prompts/Usage Pattern/Passing custom prompts into the pipeline/Modules\nfile_path: Docs\\core_modules\\model_modules\\prompts.md\nfile_name: prompts.md\nfile_type: None\nfile_size: 4625\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/customization/prompts/completion_prompts.ipynb\n/examples/customization/prompts/chat_prompts.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 644\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe provide a few simple implementations to start, with more sophisticated modes coming soon!  \n\nMore specifically, the `SimpleChatEngine` does not make use of a knowledge base, \nwhereas `CondenseQuestionChatEngine` and `ReActChatEngine` make use of a query engine over knowledge base.", "start_char_idx": 0, "end_char_idx": 1384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0490741-1793-41f1-a0cf-e480098af30b": {"__data__": {"id_": "c0490741-1793-41f1-a0cf-e480098af30b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4", "node_type": "1", "metadata": {}, "hash": "1f2969c151e8e590987b04579d8802598ae3d17b17ded6ea334bec5bf6e71f5e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b857577b-3e79-4099-b044-63d4274fbb69", "node_type": "1", "metadata": {}, "hash": "4d7b8806b7407221c2f7901dcfddd52637567150fef24a77cbaeefe87249eefa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "618c9782-52ac-4364-b4e9-75de2b313e4f", "node_type": "1", "metadata": {}, "hash": "583a0cfe950528b8bee03b0768b85375db6b699b13f6d153e982f1b331e54dbb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4", "node_type": "1", "metadata": {}, "hash": "1f2969c151e8e590987b04579d8802598ae3d17b17ded6ea334bec5bf6e71f5e", "class_name": "RelatedNodeInfo"}}, "hash": "960effeef9bc1135375a5bb9a31aa500f85ca2ca983d377e12603a5112e49e83", "text": "File Name: Docs\\core_modules\\query_modules\\chat_engines\\modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 644\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nSimple Chat Engine </examples/chat_engine/chat_engine_repl.ipynb>\nReAct Chat Engine </examples/chat_engine/chat_engine_react.ipynb>\nOpenAI Chat Engine </examples/chat_engine/chat_engine_openai.ipynb>\nCondense Question Chat Engine </examples/chat_engine/chat_engine_condense_question.ipynb>\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nChat engine is a high-level interface for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer).\nThink ChatGPT, but augmented with your knowledge base.  \n\nConceptually, it is a **stateful** analogy of a Query Engine. \nBy keeping track of the conversation history, it can answer questions with past context in mind.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you want to ask standalone question over your data (i.e. without keeping track of conversation history), use Query Engine instead.", "start_char_idx": 1386, "end_char_idx": 3179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "618c9782-52ac-4364-b4e9-75de2b313e4f": {"__data__": {"id_": "618c9782-52ac-4364-b4e9-75de2b313e4f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4", "node_type": "1", "metadata": {}, "hash": "1f2969c151e8e590987b04579d8802598ae3d17b17ded6ea334bec5bf6e71f5e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0490741-1793-41f1-a0cf-e480098af30b", "node_type": "1", "metadata": {}, "hash": "960effeef9bc1135375a5bb9a31aa500f85ca2ca983d377e12603a5112e49e83", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4", "node_type": "1", "metadata": {}, "hash": "1f2969c151e8e590987b04579d8802598ae3d17b17ded6ea334bec5bf6e71f5e", "class_name": "RelatedNodeInfo"}}, "hash": "583a0cfe950528b8bee03b0768b85375db6b699b13f6d153e982f1b331e54dbb", "text": "without keeping track of conversation history), use Query Engine instead.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet started with:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine = index.as_chat_engine()\nresponse = chat_engine.chat(\"Tell me a joke.\")\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo stream response:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.\")", "start_char_idx": 3106, "end_char_idx": 4742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cad62ac2-f538-411f-8ab6-d6a0c7dc1afd": {"__data__": {"id_": "cad62ac2-f538-411f-8ab6-d6a0c7dc1afd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec", "node_type": "1", "metadata": {}, "hash": "63458a56aa3fc79bf06244416fc0fdd25738e6a24ac2ffe262277c44e61576bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c969a149-819d-44e5-a53e-fe3b9c7bf3b2", "node_type": "1", "metadata": {}, "hash": "713892e8ad7e4ff6fbdb38aab5caa1547496e599a5d0a65afa716e331926027a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec", "node_type": "1", "metadata": {}, "hash": "63458a56aa3fc79bf06244416fc0fdd25738e6a24ac2ffe262277c44e61576bb", "class_name": "RelatedNodeInfo"}}, "hash": "0a19811f3c89236becd6d45947cb0510b2581e4f2f2d452c2ef2010b8dcaae7f", "text": "for token in streaming_response.response_gen:\n    print(token, end=\"\")\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: code\nHeader Path: Chat Engine/Usage Pattern\nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBelow you can find corresponding tutorials to see the available chat engines in action.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nContent Type: text\nHeader Path: Chat Engine/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1189\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBuild a chat engine from index:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.", "start_char_idx": 0, "end_char_idx": 1694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c969a149-819d-44e5-a53e-fe3b9c7bf3b2": {"__data__": {"id_": "c969a149-819d-44e5-a53e-fe3b9c7bf3b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec", "node_type": "1", "metadata": {}, "hash": "63458a56aa3fc79bf06244416fc0fdd25738e6a24ac2ffe262277c44e61576bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cad62ac2-f538-411f-8ab6-d6a0c7dc1afd", "node_type": "1", "metadata": {}, "hash": "0a19811f3c89236becd6d45947cb0510b2581e4f2f2d452c2ef2010b8dcaae7f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec", "node_type": "1", "metadata": {}, "hash": "63458a56aa3fc79bf06244416fc0fdd25738e6a24ac2ffe262277c44e61576bb", "class_name": "RelatedNodeInfo"}}, "hash": "713892e8ad7e4ff6fbdb38aab5caa1547496e599a5d0a65afa716e331926027a", "text": "md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine = index.as_chat_engine()\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHave a conversation with your data:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = chat_engine.chat(\"Tell me a joke.\")", "start_char_idx": 1694, "end_char_idx": 3237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a7ea6ca-717b-4b09-a88b-6def15e14eef": {"__data__": {"id_": "8a7ea6ca-717b-4b09-a88b-6def15e14eef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8ea7021-af99-4bc8-b505-811562e86599", "node_type": "1", "metadata": {}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba35fd49-1e5b-487c-9605-98df0fbd0e46", "node_type": "1", "metadata": {}, "hash": "e665f6e6619f2a34f78d3a1eb9b34e38364adfe3bed1282c8583ef01e8db046d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e8ea7021-af99-4bc8-b505-811562e86599", "node_type": "1", "metadata": {}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "class_name": "RelatedNodeInfo"}}, "hash": "a9d90ab8757e484cf8365d1070837c35750f46c40bf5ea0ed545bc7f7cdc37ae", "text": "File Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nReset chat history to start a new conversation:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine.reset()\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEnter an interactive chat REPL:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine.chat_repl()\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.", "start_char_idx": 0, "end_char_idx": 1642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba35fd49-1e5b-487c-9605-98df0fbd0e46": {"__data__": {"id_": "ba35fd49-1e5b-487c-9605-98df0fbd0e46", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8ea7021-af99-4bc8-b505-811562e86599", "node_type": "1", "metadata": {}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a7ea6ca-717b-4b09-a88b-6def15e14eef", "node_type": "1", "metadata": {}, "hash": "a9d90ab8757e484cf8365d1070837c35750f46c40bf5ea0ed545bc7f7cdc37ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ba4363f-c41e-4e89-8b36-96ef866d2c2b", "node_type": "1", "metadata": {}, "hash": "24c42c11803af77b67e461a70f092954e86cd52aaa741d245f6a9003f8f44e35", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e8ea7021-af99-4bc8-b505-811562e86599", "node_type": "1", "metadata": {}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "class_name": "RelatedNodeInfo"}}, "hash": "e665f6e6619f2a34f78d3a1eb9b34e38364adfe3bed1282c8583ef01e8db046d", "text": "md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nConfiguring a chat engine is very similar to configuring a query engine.", "start_char_idx": 1642, "end_char_idx": 2019, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ba4363f-c41e-4e89-8b36-96ef866d2c2b": {"__data__": {"id_": "5ba4363f-c41e-4e89-8b36-96ef866d2c2b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8ea7021-af99-4bc8-b505-811562e86599", "node_type": "1", "metadata": {}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba35fd49-1e5b-487c-9605-98df0fbd0e46", "node_type": "1", "metadata": {}, "hash": "e665f6e6619f2a34f78d3a1eb9b34e38364adfe3bed1282c8583ef01e8db046d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "457677b5-d8c1-420f-8ea1-83b56cf3b153", "node_type": "1", "metadata": {}, "hash": "277b8262cc061b136630fc8a9650ddcc287d7be7677c6d6d50d42204f7d6397f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e8ea7021-af99-4bc8-b505-811562e86599", "node_type": "1", "metadata": {}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "class_name": "RelatedNodeInfo"}}, "hash": "24c42c11803af77b67e461a70f092954e86cd52aaa741d245f6a9003f8f44e35", "text": "File Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can directly build and configure a chat engine from an index in 1 line of code:\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nchat_engine = index.as_chat_engine(\n    chat_mode='condense_question', \n    verbose=True\n)\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note: you can access different chat engines by specifying the `chat_mode` as a kwarg. `condense_question` corresponds to `CondenseQuestionChatEngine`, `react` corresponds to `ReActChatEngine`.\n\n> Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.", "start_char_idx": 2021, "end_char_idx": 3675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "457677b5-d8c1-420f-8ea1-83b56cf3b153": {"__data__": {"id_": "457677b5-d8c1-420f-8ea1-83b56cf3b153", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8ea7021-af99-4bc8-b505-811562e86599", "node_type": "1", "metadata": {}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ba4363f-c41e-4e89-8b36-96ef866d2c2b", "node_type": "1", "metadata": {}, "hash": "24c42c11803af77b67e461a70f092954e86cd52aaa741d245f6a9003f8f44e35", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e8ea7021-af99-4bc8-b505-811562e86599", "node_type": "1", "metadata": {}, "hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "class_name": "RelatedNodeInfo"}}, "hash": "277b8262cc061b136630fc8a9650ddcc287d7be7677c6d6d50d42204f7d6397f", "text": "File Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use the low-level composition API if you need more granular control.\nConcretely speaking, you would explicitly construct `ChatEngine` object instead of calling `index.as_chat_engine(...)`.\n> Note: You may need to look at API references or example notebooks.\n\nHere's an example where we configure the following:\n* configure the condense question prompt, \n* initialize the conversation with some existing history,\n* print verbose debug message.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.prompts  import Prompt\n\ncustom_prompt = Prompt(\"\"\"\\\nGiven a conversation (between Human and Assistant) and a follow up message from Human, \\\nrewrite the message to be a standalone question that captures all relevant context \\\nfrom the conversation.", "start_char_idx": 3677, "end_char_idx": 5196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52b153c9-e550-4a6d-8325-254c3a866d9a": {"__data__": {"id_": "52b153c9-e550-4a6d-8325-254c3a866d9a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0d05493-b438-49a8-8171-2da3c170a2a4", "node_type": "1", "metadata": {}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e502ed84-b3d9-4265-af70-95d5738e1807", "node_type": "1", "metadata": {}, "hash": "d92e3f49269f701d59464f14f013005ffa4f2b551313e358046282db5adf353b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d0d05493-b438-49a8-8171-2da3c170a2a4", "node_type": "1", "metadata": {}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "class_name": "RelatedNodeInfo"}}, "hash": "4764f3bad01cf2dcf1d52954345eab86671d018fc97f4d578f7aef76c81c1e36", "text": "<Chat History> \n{chat_history}\n\n<Follow Up Message>\n{question}\n\n<Standalone question>\n\"\"\")\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/list of (human_message, ai_message) tuples\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncustom_chat_history = [\n    (\n        'Hello assistant, we are having a insightful discussion about Paul Graham today.', \n        'Okay, sounds good.'\n    )\n]\n\nquery_engine = index.as_query_engine()\nchat_engine = CondenseQuestionChatEngine.from_defaults(\n    query_engine=query_engine, \n    condense_question_prompt=custom_prompt,\n    chat_history=custom_chat_history,\n    verbose=True\n)\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo enable streaming, you simply need to call the `stream_chat` endpoint instead of the `chat` endpoint.\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis somewhat inconsistent with query engine (where you pass in a `streaming=True` flag). We are working on making the behavior more consistent!", "start_char_idx": 0, "end_char_idx": 1918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e502ed84-b3d9-4265-af70-95d5738e1807": {"__data__": {"id_": "e502ed84-b3d9-4265-af70-95d5738e1807", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0d05493-b438-49a8-8171-2da3c170a2a4", "node_type": "1", "metadata": {}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52b153c9-e550-4a6d-8325-254c3a866d9a", "node_type": "1", "metadata": {}, "hash": "4764f3bad01cf2dcf1d52954345eab86671d018fc97f4d578f7aef76c81c1e36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "055491d4-1246-43bf-8f94-54c2a555705f", "node_type": "1", "metadata": {}, "hash": "584f5f1a814d1e354db54aabcd54a3e1c2511a4536abe63fcbe7f96f6cf96032", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d0d05493-b438-49a8-8171-2da3c170a2a4", "node_type": "1", "metadata": {}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "class_name": "RelatedNodeInfo"}}, "hash": "d92e3f49269f701d59464f14f013005ffa4f2b551313e358046282db5adf353b", "text": "We are working on making the behavior more consistent!\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.\")\nfor token in streaming_response.response_gen:\n    print(token, end=\"\")\n```\n\nFile Name: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nfile_path: Docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 3083\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee an end-to-end tutorial\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/SimilarityPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUsed to remove nodes that are below a similarity score threshold.", "start_char_idx": 1864, "end_char_idx": 3317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "055491d4-1246-43bf-8f94-54c2a555705f": {"__data__": {"id_": "055491d4-1246-43bf-8f94-54c2a555705f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0d05493-b438-49a8-8171-2da3c170a2a4", "node_type": "1", "metadata": {}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e502ed84-b3d9-4265-af70-95d5738e1807", "node_type": "1", "metadata": {}, "hash": "d92e3f49269f701d59464f14f013005ffa4f2b551313e358046282db5adf353b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3aae0939-41d6-4a37-a417-c6da7c0fb284", "node_type": "1", "metadata": {}, "hash": "697fe89f3b35e0f5f8df427d6a7d1b17ecb55ffcb861baeae4140c3495589721", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d0d05493-b438-49a8-8171-2da3c170a2a4", "node_type": "1", "metadata": {}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "class_name": "RelatedNodeInfo"}}, "hash": "584f5f1a814d1e354db54aabcd54a3e1c2511a4536abe63fcbe7f96f6cf96032", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/SimilarityPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\npostprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/KeywordNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUsed to ensure certain keywords are either excluded or included.", "start_char_idx": 3319, "end_char_idx": 4285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3aae0939-41d6-4a37-a417-c6da7c0fb284": {"__data__": {"id_": "3aae0939-41d6-4a37-a417-c6da7c0fb284", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0d05493-b438-49a8-8171-2da3c170a2a4", "node_type": "1", "metadata": {}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "055491d4-1246-43bf-8f94-54c2a555705f", "node_type": "1", "metadata": {}, "hash": "584f5f1a814d1e354db54aabcd54a3e1c2511a4536abe63fcbe7f96f6cf96032", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d0d05493-b438-49a8-8171-2da3c170a2a4", "node_type": "1", "metadata": {}, "hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "class_name": "RelatedNodeInfo"}}, "hash": "697fe89f3b35e0f5f8df427d6a7d1b17ecb55ffcb861baeae4140c3495589721", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/KeywordNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import KeywordNodePostprocessor\n\npostprocessor = KeywordNodePostprocessor(\n  required_keywords=[\"word1\", \"word2\"],\n  exclude_keywords=[\"word3\", \"word4\"]\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis postprocessor optimizes token usage by removing sentences that are not relevant to the query (this is done using embeddings).\n\nThe percentile cutoff is a measure for using the top percentage of relevant sentences.\n\nThe threshold cutoff can be specified instead, which uses a raw similarity cutoff for picking which sentences to keep.", "start_char_idx": 4287, "end_char_idx": 5590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8919ae46-fb0f-4130-bc8d-2cddb2a74f73": {"__data__": {"id_": "8919ae46-fb0f-4130-bc8d-2cddb2a74f73", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "node_type": "1", "metadata": {}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9e43751-02d0-4621-b39d-2e44418f4455", "node_type": "1", "metadata": {}, "hash": "42fb258fbba7e9ccca9ae276e70dd4aeccbbd2aebdfc4b4115d711b679d5f8fc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "node_type": "1", "metadata": {}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "class_name": "RelatedNodeInfo"}}, "hash": "9517ba1eb27e4ce5ecceedee38d93312863fb8654c44dc0be0ef4a13fa7b1c11", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import SentenceEmbeddingOptimizer\n\npostprocessor = SentenceEmbeddingOptimizer(\n  embed_model=service_context.embed_model,\n  percentile_cutoff=0.5,\n  # threshold_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full notebook guide can be found here\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUses the \"Cohere ReRank\" functionality to re-order nodes, and returns the top N nodes.", "start_char_idx": 0, "end_char_idx": 1465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9e43751-02d0-4621-b39d-2e44418f4455": {"__data__": {"id_": "e9e43751-02d0-4621-b39d-2e44418f4455", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "node_type": "1", "metadata": {}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8919ae46-fb0f-4130-bc8d-2cddb2a74f73", "node_type": "1", "metadata": {}, "hash": "9517ba1eb27e4ce5ecceedee38d93312863fb8654c44dc0be0ef4a13fa7b1c11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abf16a18-0c5e-4efd-8898-60b1c0832f83", "node_type": "1", "metadata": {}, "hash": "13105deaa880b1567e2d8c04a7738dff9bbaad99ec9e552cb3ec550f6ba4f660", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "node_type": "1", "metadata": {}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "class_name": "RelatedNodeInfo"}}, "hash": "42fb258fbba7e9ccca9ae276e70dd4aeccbbd2aebdfc4b4115d711b679d5f8fc", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import CohereRerank\n\npostprocessor = CohereRerank(\n  top_n=2\n  model=\"rerank-english-v2.0\",\n  api_key=\"YOUR COHERE API KEY\"\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFull notebook guide is available here.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUses a LLM to re-order nodes by asking the LLM to return the relevant documents and a score of how relevant they are. Returns the top N ranked nodes.", "start_char_idx": 1467, "end_char_idx": 2916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abf16a18-0c5e-4efd-8898-60b1c0832f83": {"__data__": {"id_": "abf16a18-0c5e-4efd-8898-60b1c0832f83", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "node_type": "1", "metadata": {}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9e43751-02d0-4621-b39d-2e44418f4455", "node_type": "1", "metadata": {}, "hash": "42fb258fbba7e9ccca9ae276e70dd4aeccbbd2aebdfc4b4115d711b679d5f8fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05040174-34f9-4d79-a1be-b1cba73de84e", "node_type": "1", "metadata": {}, "hash": "fc339a165d14c367721680b6baa6008a350a854b045e9d18d3fa45f8edf82996", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "node_type": "1", "metadata": {}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "class_name": "RelatedNodeInfo"}}, "hash": "13105deaa880b1567e2d8c04a7738dff9bbaad99ec9e552cb3ec550f6ba4f660", "text": "Returns the top N ranked nodes.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import LLMRerank\n\npostprocessor = LLMRerank(\n  top_n=2\n  service_context=service_context,\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFull notebook guide is available her for Gatsby and here for Lyft 10K documents.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis postproccesor returns the top K nodes sorted by date. This assumes there is a `date` field to parse in the metadata of each node.", "start_char_idx": 2885, "end_char_idx": 4371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05040174-34f9-4d79-a1be-b1cba73de84e": {"__data__": {"id_": "05040174-34f9-4d79-a1be-b1cba73de84e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "node_type": "1", "metadata": {}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abf16a18-0c5e-4efd-8898-60b1c0832f83", "node_type": "1", "metadata": {}, "hash": "13105deaa880b1567e2d8c04a7738dff9bbaad99ec9e552cb3ec550f6ba4f660", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "node_type": "1", "metadata": {}, "hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "class_name": "RelatedNodeInfo"}}, "hash": "fc339a165d14c367721680b6baa6008a350a854b045e9d18d3fa45f8edf82996", "text": "This assumes there is a `date` field to parse in the metadata of each node.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import FixedRecencyPostprocessor\n\npostprocessor = FixedRecencyPostprocessor(\n  tok_k=1,\n  date_key=\"date\"  # the key in the metadata to find the date\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nA full notebook guide is available here.", "start_char_idx": 4296, "end_char_idx": 5378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae435429-cf0c-42e8-8050-c9ac1b8c91df": {"__data__": {"id_": "ae435429-cf0c-42e8-8050-c9ac1b8c91df", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "178e7a4c-422c-4c79-8e54-119e2e05c739", "node_type": "1", "metadata": {}, "hash": "f3542e49f8d2cb1d95f9b322b0e3fcf3894852f6e22c5bf1ac10faf515f29913", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30c86132-8708-43b2-b30b-5fa311f3bac5", "node_type": "1", "metadata": {}, "hash": "077ec10394eabac712658efa27c47b486e21b084aa29a85ae7d398955fb97781", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "178e7a4c-422c-4c79-8e54-119e2e05c739", "node_type": "1", "metadata": {}, "hash": "f3542e49f8d2cb1d95f9b322b0e3fcf3894852f6e22c5bf1ac10faf515f29913", "class_name": "RelatedNodeInfo"}}, "hash": "fb01da5b0c7bdc1c1a1f4d8457662c13a07cfae247b720d88d272c1376372593", "text": "A full notebook guide is available here.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis postproccesor returns the top K nodes after sorting by date and removing older nodes that are too similar after measuring embedding similarity.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import EmbeddingRecencyPostprocessor\n\npostprocessor = EmbeddingRecencyPostprocessor(\n  service_context=service_context,\n  date_key=\"date\",\n  similarity_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full notebook guide is available here.", "start_char_idx": 0, "end_char_idx": 1584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30c86132-8708-43b2-b30b-5fa311f3bac5": {"__data__": {"id_": "30c86132-8708-43b2-b30b-5fa311f3bac5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "178e7a4c-422c-4c79-8e54-119e2e05c739", "node_type": "1", "metadata": {}, "hash": "f3542e49f8d2cb1d95f9b322b0e3fcf3894852f6e22c5bf1ac10faf515f29913", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae435429-cf0c-42e8-8050-c9ac1b8c91df", "node_type": "1", "metadata": {}, "hash": "fb01da5b0c7bdc1c1a1f4d8457662c13a07cfae247b720d88d272c1376372593", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1462f6cf-d6f6-42b4-bd56-1bcc7ab4db0d", "node_type": "1", "metadata": {}, "hash": "a76892c88fd471842c16da77cb4f7c47112f02e620fdd4f18d5d7bc984c21cfe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "178e7a4c-422c-4c79-8e54-119e2e05c739", "node_type": "1", "metadata": {}, "hash": "f3542e49f8d2cb1d95f9b322b0e3fcf3894852f6e22c5bf1ac10faf515f29913", "class_name": "RelatedNodeInfo"}}, "hash": "077ec10394eabac712658efa27c47b486e21b084aa29a85ae7d398955fb97781", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis postproccesor returns the top K nodes applying a time-weighted rerank to each node. Each time a node is retrieved, the time it was retrieved is recorded. This biases search to favor information that has not be returned in a query yet.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import TimeWeightedPostprocessor\n\npostprocessor = TimeWeightedPostprocessor(\n  time_decay=0.99,\n  top_k=1\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full notebook guide is available here.", "start_char_idx": 1586, "end_char_idx": 3150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1462f6cf-d6f6-42b4-bd56-1bcc7ab4db0d": {"__data__": {"id_": "1462f6cf-d6f6-42b4-bd56-1bcc7ab4db0d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "178e7a4c-422c-4c79-8e54-119e2e05c739", "node_type": "1", "metadata": {}, "hash": "f3542e49f8d2cb1d95f9b322b0e3fcf3894852f6e22c5bf1ac10faf515f29913", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30c86132-8708-43b2-b30b-5fa311f3bac5", "node_type": "1", "metadata": {}, "hash": "077ec10394eabac712658efa27c47b486e21b084aa29a85ae7d398955fb97781", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "178e7a4c-422c-4c79-8e54-119e2e05c739", "node_type": "1", "metadata": {}, "hash": "f3542e49f8d2cb1d95f9b322b0e3fcf3894852f6e22c5bf1ac10faf515f29913", "class_name": "RelatedNodeInfo"}}, "hash": "a76892c88fd471842c16da77cb4f7c47112f02e620fdd4f18d5d7bc984c21cfe", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe PII (Personal Identifiable Information) postprocssor removes information that might be a security risk. It does this by using NER (either with a dedicated NER model, or with a local LLM model).\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: code\nHeader Path: Modules/(Beta) PIINodePostprocessor/LLM Version\nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index.indices.postprocessor import PIINodePostprocessor\n\npostprocessor = PIINodePostprocessor(\n  service_context=service_context,  # this should be setup with an LLM you trust\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis version uses the default local model from Hugging Face that is loaded when you run `pipline(\"ner\")`.", "start_char_idx": 3152, "end_char_idx": 4817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "717e1455-26c2-4e94-8828-0aafc65fe714": {"__data__": {"id_": "717e1455-26c2-4e94-8828-0aafc65fe714", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7f4e18c-f12c-4440-bec3-960141c48bea", "node_type": "1", "metadata": {}, "hash": "ca492d2c5c08edadd2435c94f6ac4de35cf5745049a75e1ecfbacf2adb4de0e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43d90206-228a-42f7-8038-dec0902d6fae", "node_type": "1", "metadata": {}, "hash": "28043d4ffd660547c1ecbd08bd6877564c4d823f972fef478904dbcf3a134348", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d7f4e18c-f12c-4440-bec3-960141c48bea", "node_type": "1", "metadata": {}, "hash": "ca492d2c5c08edadd2435c94f6ac4de35cf5745049a75e1ecfbacf2adb4de0e1", "class_name": "RelatedNodeInfo"}}, "hash": "420fc35f633728fbce99d3072a5c858d6397acbd1cd9307f1eba5f48ad2cdbac", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import NERPIINodePostprocessor\n\npostprocessor = NERPIINodePostprocessor()\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full notebook guide for both can be found here.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUses pre-defined settings to read the `Node` relationships and fetch either all nodes that come previously, next, or both.\n\nThis is useful when you know the relationships point to important data (either before, after, or both) that should be sent to the LLM if that node is retrieved.", "start_char_idx": 0, "end_char_idx": 1647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43d90206-228a-42f7-8038-dec0902d6fae": {"__data__": {"id_": "43d90206-228a-42f7-8038-dec0902d6fae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7f4e18c-f12c-4440-bec3-960141c48bea", "node_type": "1", "metadata": {}, "hash": "ca492d2c5c08edadd2435c94f6ac4de35cf5745049a75e1ecfbacf2adb4de0e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "717e1455-26c2-4e94-8828-0aafc65fe714", "node_type": "1", "metadata": {}, "hash": "420fc35f633728fbce99d3072a5c858d6397acbd1cd9307f1eba5f48ad2cdbac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe3b84fa-7467-48fa-969e-b9fba40b30c2", "node_type": "1", "metadata": {}, "hash": "fd51068037a1ecce0a10b24a41c49e9111fe355461036292b366aa3bffd4a874", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d7f4e18c-f12c-4440-bec3-960141c48bea", "node_type": "1", "metadata": {}, "hash": "ca492d2c5c08edadd2435c94f6ac4de35cf5745049a75e1ecfbacf2adb4de0e1", "class_name": "RelatedNodeInfo"}}, "hash": "28043d4ffd660547c1ecbd08bd6877564c4d823f972fef478904dbcf3a134348", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import PrevNextNodePostprocessor\n\npostprocessor = PrevNextNodePostprocessor(\n  docstore=index.docstore,\n  num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards\n  mode=\"next\"   # can be either 'next', 'previous', or 'both'\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe same as PrevNextNodePostprocessor, but lets the LLM decide the mode (next, previous, or both).", "start_char_idx": 1649, "end_char_idx": 3280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe3b84fa-7467-48fa-969e-b9fba40b30c2": {"__data__": {"id_": "fe3b84fa-7467-48fa-969e-b9fba40b30c2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7f4e18c-f12c-4440-bec3-960141c48bea", "node_type": "1", "metadata": {}, "hash": "ca492d2c5c08edadd2435c94f6ac4de35cf5745049a75e1ecfbacf2adb4de0e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43d90206-228a-42f7-8038-dec0902d6fae", "node_type": "1", "metadata": {}, "hash": "28043d4ffd660547c1ecbd08bd6877564c4d823f972fef478904dbcf3a134348", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d7f4e18c-f12c-4440-bec3-960141c48bea", "node_type": "1", "metadata": {}, "hash": "ca492d2c5c08edadd2435c94f6ac4de35cf5745049a75e1ecfbacf2adb4de0e1", "class_name": "RelatedNodeInfo"}}, "hash": "fd51068037a1ecce0a10b24a41c49e9111fe355461036292b366aa3bffd4a874", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import AutoPrevNextNodePostprocessor\n\npostprocessor = AutoPrevNextNodePostprocessor(\n  docstore=index.docstore,\n  service_context=service_context\n  num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA full example notebook is available here.", "start_char_idx": 3282, "end_char_idx": 4437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c485a396-c235-463c-b215-ebde1980ff28": {"__data__": {"id_": "c485a396-c235-463c-b215-ebde1980ff28", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d1ffae8-1523-447e-9879-92deafe8423c", "node_type": "1", "metadata": {}, "hash": "69405ea491b03796819a1192f7395d251090e6c9723db4dd4e9d2a53f3c1cd6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "139ad006-31e5-407f-ac67-e31c4c44b392", "node_type": "1", "metadata": {}, "hash": "ef6d8e81675850480a30ada9bdf5098db266f4dfde9e3d0fa1003fbfeb9819f0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8d1ffae8-1523-447e-9879-92deafe8423c", "node_type": "1", "metadata": {}, "hash": "69405ea491b03796819a1192f7395d251090e6c9723db4dd4e9d2a53f3c1cd6d", "class_name": "RelatedNodeInfo"}}, "hash": "a96c2f4a771b2d1b091e0c8df0413c6d4c1d2aeaaf82989ddda97714c0999640", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nContent Type: code\nHeader Path: Modules/(Beta) PIINodePostprocessor/All Notebooks\nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 6896\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/node_postprocessor/OptimizerDemo.ipynb\n/examples/node_postprocessor/CohereRerank.ipynb\n/examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb\n/examples/node_postprocessor/LLMReranker-Gatsby.ipynb\n/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb\n/examples/node_postprocessor/TimeWeightedPostprocessorDemo.ipynb\n/examples/node_postprocessor/PII.ipynb\n/examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNode postprocessors are a set of modules that take a set of nodes, and apply some kind of transformation or filtering before returning them.\n\nIn LlamaIndex, node postprocessors are most commonly applied within a query engine, after the node retrieval step and before the response synthesis step.\n\nLlamaIndex offers several node postprocessors for immediate use, while also providing a simple API for adding your own custom postprocessors.", "start_char_idx": 0, "end_char_idx": 1630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "139ad006-31e5-407f-ac67-e31c4c44b392": {"__data__": {"id_": "139ad006-31e5-407f-ac67-e31c4c44b392", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d1ffae8-1523-447e-9879-92deafe8423c", "node_type": "1", "metadata": {}, "hash": "69405ea491b03796819a1192f7395d251090e6c9723db4dd4e9d2a53f3c1cd6d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c485a396-c235-463c-b215-ebde1980ff28", "node_type": "1", "metadata": {}, "hash": "a96c2f4a771b2d1b091e0c8df0413c6d4c1d2aeaaf82989ddda97714c0999640", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ec911a2-ad2e-4b5d-acc2-5da8be39c8e5", "node_type": "1", "metadata": {}, "hash": "b6603d3edeb94c85b70ac8f113d6390bc89a1f821bad111e9baa0a1830a5c37c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8d1ffae8-1523-447e-9879-92deafe8423c", "node_type": "1", "metadata": {}, "hash": "69405ea491b03796819a1192f7395d251090e6c9723db4dd4e9d2a53f3c1cd6d", "class_name": "RelatedNodeInfo"}}, "hash": "ef6d8e81675850480a30ada9bdf5098db266f4dfde9e3d0fa1003fbfeb9819f0", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nConfused about where node postprocessor fits in the pipeline? Read about high-level concepts\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn example of using a node postprocessors is below:\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.schema import Node, NodeWithScore\n\nnodes = [\n  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n]\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.", "start_char_idx": 1632, "end_char_idx": 3334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ec911a2-ad2e-4b5d-acc2-5da8be39c8e5": {"__data__": {"id_": "9ec911a2-ad2e-4b5d-acc2-5da8be39c8e5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d1ffae8-1523-447e-9879-92deafe8423c", "node_type": "1", "metadata": {}, "hash": "69405ea491b03796819a1192f7395d251090e6c9723db4dd4e9d2a53f3c1cd6d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "139ad006-31e5-407f-ac67-e31c4c44b392", "node_type": "1", "metadata": {}, "hash": "ef6d8e81675850480a30ada9bdf5098db266f4dfde9e3d0fa1003fbfeb9819f0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8d1ffae8-1523-447e-9879-92deafe8423c", "node_type": "1", "metadata": {}, "hash": "69405ea491b03796819a1192f7395d251090e6c9723db4dd4e9d2a53f3c1cd6d", "class_name": "RelatedNodeInfo"}}, "hash": "b6603d3edeb94c85b70ac8f113d6390bc89a1f821bad111e9baa0a1830a5c37c", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can find more details using post processors and how to build your own below.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBelow you can find guides for each node postprocessor.", "start_char_idx": 3315, "end_char_idx": 4858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "006ee14e-9068-48ac-83c7-dcc4b99c69c9": {"__data__": {"id_": "006ee14e-9068-48ac-83c7-dcc4b99c69c9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88a26da0-22e2-444b-9df6-8f01c0e24203", "node_type": "1", "metadata": {}, "hash": "9e26995e97574a5611b450946caa3a36830ba558282f15189073f61e799d13c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f217501-fdde-4300-8766-590ca19de1e3", "node_type": "1", "metadata": {}, "hash": "aefacc9987210ffa4d9e25744ee6bd560abc011c2efbe9588fa4d47b5bda72b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "88a26da0-22e2-444b-9df6-8f01c0e24203", "node_type": "1", "metadata": {}, "hash": "9e26995e97574a5611b450946caa3a36830ba558282f15189073f61e799d13c5", "class_name": "RelatedNodeInfo"}}, "hash": "3b0b47d35b9193c2b7ca0adc4f8097be111bfb896dd4cf536bc54a25e7089dbe", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nContent Type: text\nHeader Path: Node Postprocessor/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1390\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMost commonly, node-postprocessors will be used in a query engine, where they are applied to the nodes returned from a retriever, and before the response synthesis step.\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/all node post-processors will be applied during each query\nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.indices.postprocessor import TimeWeightedPostprocessor\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(\n  node_postprocessors=[\n    TimeWeightedPostprocessor(\n        time_decay=0.5, time_access_refresh=False, top_k=1\n    )\n  ]\n)\n\nresponse = query_engine.query(\"query string\")\n```\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.", "start_char_idx": 0, "end_char_idx": 1878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f217501-fdde-4300-8766-590ca19de1e3": {"__data__": {"id_": "2f217501-fdde-4300-8766-590ca19de1e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88a26da0-22e2-444b-9df6-8f01c0e24203", "node_type": "1", "metadata": {}, "hash": "9e26995e97574a5611b450946caa3a36830ba558282f15189073f61e799d13c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "006ee14e-9068-48ac-83c7-dcc4b99c69c9", "node_type": "1", "metadata": {}, "hash": "3b0b47d35b9193c2b7ca0adc4f8097be111bfb896dd4cf536bc54a25e7089dbe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38317e39-4b36-458a-bd0a-afe652b6a453", "node_type": "1", "metadata": {}, "hash": "4a6f72cf81eeb5e90d1955af55c20c8603241ce9511c98d05350353c5839b9aa", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "88a26da0-22e2-444b-9df6-8f01c0e24203", "node_type": "1", "metadata": {}, "hash": "9e26995e97574a5611b450946caa3a36830ba558282f15189073f61e799d13c5", "class_name": "RelatedNodeInfo"}}, "hash": "aefacc9987210ffa4d9e25744ee6bd560abc011c2efbe9588fa4d47b5bda72b6", "text": "md\nContent Type: text\nHeader Path: Usage Pattern/Using with Retrieved Nodes\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOr used as a standalone object for filtering retrieved nodes:\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with Retrieved Nodes\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\nnodes = index.as_retriever().query(\"query string\")\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/filter nodes below 0.75 similarity score\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with your own nodes\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAs you may have noticed,", "start_char_idx": 1878, "end_char_idx": 3697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38317e39-4b36-458a-bd0a-afe652b6a453": {"__data__": {"id_": "38317e39-4b36-458a-bd0a-afe652b6a453", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88a26da0-22e2-444b-9df6-8f01c0e24203", "node_type": "1", "metadata": {}, "hash": "9e26995e97574a5611b450946caa3a36830ba558282f15189073f61e799d13c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f217501-fdde-4300-8766-590ca19de1e3", "node_type": "1", "metadata": {}, "hash": "aefacc9987210ffa4d9e25744ee6bd560abc011c2efbe9588fa4d47b5bda72b6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "88a26da0-22e2-444b-9df6-8f01c0e24203", "node_type": "1", "metadata": {}, "hash": "9e26995e97574a5611b450946caa3a36830ba558282f15189073f61e799d13c5", "class_name": "RelatedNodeInfo"}}, "hash": "4a6f72cf81eeb5e90d1955af55c20c8603241ce9511c98d05350353c5839b9aa", "text": "the postprocessors take `NodeWithScore` objects as inputs, which is just a wrapper class with a `Node` and a `score` value.", "start_char_idx": 3698, "end_char_idx": 3821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0e1077f-7df1-4be1-a15e-cf519bc7e007": {"__data__": {"id_": "f0e1077f-7df1-4be1-a15e-cf519bc7e007", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "node_type": "1", "metadata": {}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e3c8471-87b5-4e0b-9653-9ec23b2e6cad", "node_type": "1", "metadata": {}, "hash": "9867512f3ac55930e3a1494cef0ad9984b97e2484586c74c5f1663686a6f3cd0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "node_type": "1", "metadata": {}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "class_name": "RelatedNodeInfo"}}, "hash": "0f4976fa48845f8fd1cb25744ba0f34ff7ae63dbea25eaa7c1bd61ebe5341bd4", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with your own nodes\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.schema import Node, NodeWithScore\n\nnodes = [\n  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n]\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/filter nodes below 0.75 similarity score\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe base class is `BaseNodePostprocessor`, and the API interface is very simple:\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.", "start_char_idx": 0, "end_char_idx": 1852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e3c8471-87b5-4e0b-9653-9ec23b2e6cad": {"__data__": {"id_": "6e3c8471-87b5-4e0b-9653-9ec23b2e6cad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "node_type": "1", "metadata": {}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0e1077f-7df1-4be1-a15e-cf519bc7e007", "node_type": "1", "metadata": {}, "hash": "0f4976fa48845f8fd1cb25744ba0f34ff7ae63dbea25eaa7c1bd61ebe5341bd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c338303-12d9-4c81-b8de-e12c64d598d3", "node_type": "1", "metadata": {}, "hash": "a13fcd75e7966fa6660dae984b810eb2581ea1f1faa490bb3e47b74de1864940", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "node_type": "1", "metadata": {}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "class_name": "RelatedNodeInfo"}}, "hash": "9867512f3ac55930e3a1494cef0ad9984b97e2484586c74c5f1663686a6f3cd0", "text": "md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nclass BaseNodePostprocessor:\n    \"\"\"Node postprocessor.\"\"\"\n\n    @abstractmethod\n    def postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"", "start_char_idx": 1852, "end_char_idx": 2251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c338303-12d9-4c81-b8de-e12c64d598d3": {"__data__": {"id_": "4c338303-12d9-4c81-b8de-e12c64d598d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "node_type": "1", "metadata": {}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e3c8471-87b5-4e0b-9653-9ec23b2e6cad", "node_type": "1", "metadata": {}, "hash": "9867512f3ac55930e3a1494cef0ad9984b97e2484586c74c5f1663686a6f3cd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "886e3f81-c5dc-4b0f-98ef-26b73704602f", "node_type": "1", "metadata": {}, "hash": "e4c25e6e24fdcc151ce355864c2efcf16c46e3474592e613b73faf0a5062023b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "node_type": "1", "metadata": {}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "class_name": "RelatedNodeInfo"}}, "hash": "a13fcd75e7966fa6660dae984b810eb2581ea1f1faa490bb3e47b74de1864940", "text": "File Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA dummy node-postprocessor can be implemented in just a few lines of code:\n\nFile Name: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import QueryBundle\nfrom llama_index.indices.postprocessor.base import BaseNodePostprocessor\nfrom llama_index.schema import NodeWithScore\n\nclass DummyNodePostprocessor:\n\n    def postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \n        # subtracts 1 from the score\n        for n in nodes:\n            n.score -= 1\n\n        return nodes\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex allows you to perform *query transformations* over your index structures.\nQuery transformations are modules that will convert a query into another query. They can be **single-step**, as in the transformation is run once before the query is executed against an index.", "start_char_idx": 2253, "end_char_idx": 4222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "886e3f81-c5dc-4b0f-98ef-26b73704602f": {"__data__": {"id_": "886e3f81-c5dc-4b0f-98ef-26b73704602f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "node_type": "1", "metadata": {}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c338303-12d9-4c81-b8de-e12c64d598d3", "node_type": "1", "metadata": {}, "hash": "a13fcd75e7966fa6660dae984b810eb2581ea1f1faa490bb3e47b74de1864940", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "node_type": "1", "metadata": {}, "hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "class_name": "RelatedNodeInfo"}}, "hash": "e4c25e6e24fdcc151ce355864c2efcf16c46e3474592e613b73faf0a5062023b", "text": "They can also be **multi-step**, as in: \n1. The query is transformed, executed against an index, \n2. The response is retrieved.\n3. Subsequent queries are transformed/executed in a sequential fashion.\n\nWe list some of our query transformations in more detail below.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Use Cases\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuery transformations have multiple use cases:\n- Transforming an initial query into a form that can be more easily embedded (e.g. HyDE)\n- Transforming an initial query into a subquestion that can be more easily answered from the data (single-step query decomposition)\n- Breaking an initial query into multiple subquestions that can be more easily answered on their own. (multi-step query decomposition)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/HyDE (Hypothetical Document Embeddings)\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHyDE is a technique where given a natural language query, a hypothetical document/answer is generated first. This hypothetical document is then used for embedding lookup rather than the raw query.\n\nTo use HyDE, an example code snippet is shown below.", "start_char_idx": 4225, "end_char_idx": 5993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecb74226-c19d-4a62-940b-a190429915a2": {"__data__": {"id_": "ecb74226-c19d-4a62-940b-a190429915a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bbacaf0-99f4-4ae6-9740-b7774c835b80", "node_type": "1", "metadata": {}, "hash": "09c72f3249a95e391f5cb9ba0f0f49ec5645ee86e151ece31a7f14f141158518", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc3772b8-cd45-489d-9821-b1e072034617", "node_type": "1", "metadata": {}, "hash": "d865974a25914e5cd9ff3931e738fb2f6c22323759608ec5e873689d133f88a3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7bbacaf0-99f4-4ae6-9740-b7774c835b80", "node_type": "1", "metadata": {}, "hash": "09c72f3249a95e391f5cb9ba0f0f49ec5645ee86e151ece31a7f14f141158518", "class_name": "RelatedNodeInfo"}}, "hash": "599ed6488ad4438f9d64afe6f9be1661fb6bec7b2904629b20c9e84f757598af", "text": "To use HyDE, an example code snippet is shown below.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/HyDE (Hypothetical Document Embeddings)\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.indices.query.query_transform.base import HyDEQueryTransform\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/load documents, build index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('./paul_graham_essay/data').load_data()\nindex = VectorStoreIndex(documents)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/run query with HyDE query transform\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_str = \"what did paul graham do after going to RISD\"\nhyde = HyDEQueryTransform(include_original=True)\nquery_engine = index.as_query_engine()\nquery_engine = TransformQueryEngine(query_engine, query_transform=hyde)\nresponse = query_engine.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc3772b8-cd45-489d-9821-b1e072034617": {"__data__": {"id_": "bc3772b8-cd45-489d-9821-b1e072034617", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bbacaf0-99f4-4ae6-9740-b7774c835b80", "node_type": "1", "metadata": {}, "hash": "09c72f3249a95e391f5cb9ba0f0f49ec5645ee86e151ece31a7f14f141158518", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecb74226-c19d-4a62-940b-a190429915a2", "node_type": "1", "metadata": {}, "hash": "599ed6488ad4438f9d64afe6f9be1661fb6bec7b2904629b20c9e84f757598af", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7bbacaf0-99f4-4ae6-9740-b7774c835b80", "node_type": "1", "metadata": {}, "hash": "09c72f3249a95e391f5cb9ba0f0f49ec5645ee86e151ece31a7f14f141158518", "class_name": "RelatedNodeInfo"}}, "hash": "d865974a25914e5cd9ff3931e738fb2f6c22323759608ec5e873689d133f88a3", "text": "query_transform=hyde)\nresponse = query_engine.query(query_str)\nprint(response)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/run query with HyDE query transform\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCheck out our example notebook for a full walkthrough.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Single-Step Query Decomposition\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSome recent approaches (e.g. self-ask, ReAct) have suggested that LLM's \nperform better at answering complex questions when they break the question into smaller steps. We have found that this is true for queries that require knowledge augmentation as well.\n\nIf your query is complex, different parts of your knowledge base may answer different \"subqueries\" around the overall query.\n\nOur single-step query decomposition feature transforms a **complicated** question into a simpler one over the data collection to help provide a sub-answer to the original question.\n\nThis is especially helpful over a composed graph. Within a composed graph, a query can be routed to multiple subindexes, each representing a subset of the overall knowledge corpus. Query decomposition allows us to transform the query into a more suitable question over any given index.\n\nAn example image is shown below.\n\n!", "start_char_idx": 1887, "end_char_idx": 3777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c2568d9-9d74-46f8-9494-0f6c1208b1e7": {"__data__": {"id_": "4c2568d9-9d74-46f8-9494-0f6c1208b1e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf", "node_type": "1", "metadata": {}, "hash": "a8e47cc50e6e4a90dc3ecf90a80320bdc25455d894dc0095d9bac38bac7ec2dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff288ad3-f67e-453f-80a2-36e8df06acf8", "node_type": "1", "metadata": {}, "hash": "5477578229295ebd519be0616cac87fef33038a765db12d17856e7af3c3463cf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf", "node_type": "1", "metadata": {}, "hash": "a8e47cc50e6e4a90dc3ecf90a80320bdc25455d894dc0095d9bac38bac7ec2dc", "class_name": "RelatedNodeInfo"}}, "hash": "3686e6638541a01366d76c7c6d1b0bcfa75b59f897b4b892c74835bcc2438604", "text": "An example image is shown below.\n\n!\n\n\nHere's a corresponding example code snippet over a composed graph.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/llm_predictor_chatgpt corresponds to the ChatGPT LLM interface\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor_chatgpt, verbose=True\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/initialize indexes and graph\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/configure retrievers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_query_engine = vector_index.as_query_engine()\nvector_query_engine = TransformQueryEngine(\n    vector_query_engine, \n    query_transform=decompose_transform\n    transform_extra_info={'index_summary': vector_index.index_struct.summary}\n)\ncustom_query_engines = {\n    vector_index.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff288ad3-f67e-453f-80a2-36e8df06acf8": {"__data__": {"id_": "ff288ad3-f67e-453f-80a2-36e8df06acf8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf", "node_type": "1", "metadata": {}, "hash": "a8e47cc50e6e4a90dc3ecf90a80320bdc25455d894dc0095d9bac38bac7ec2dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c2568d9-9d74-46f8-9494-0f6c1208b1e7", "node_type": "1", "metadata": {}, "hash": "3686e6638541a01366d76c7c6d1b0bcfa75b59f897b4b892c74835bcc2438604", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b8d6b4d-877f-4f5a-bc3c-d15e7eaae96f", "node_type": "1", "metadata": {}, "hash": "ad8d8b948996e894668f9b4c1a9ad7c4ffacd2a9b16b5db1324055a4dbd6c0c8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf", "node_type": "1", "metadata": {}, "hash": "a8e47cc50e6e4a90dc3ecf90a80320bdc25455d894dc0095d9bac38bac7ec2dc", "class_name": "RelatedNodeInfo"}}, "hash": "5477578229295ebd519be0616cac87fef33038a765db12d17856e7af3c3463cf", "text": "index_struct.summary}\n)\ncustom_query_engines = {\n    vector_index.index_id: vector_query_engine\n}\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/query\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_str = (\n    \"Compare and contrast the airports in Seattle, Houston, and Toronto. \"\n)\nquery_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\nresponse = query_engine.query(query_str)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/query\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCheck out our example notebook for a full walkthrough.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Multi-Step Query Transformations\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMulti-step query transformations are a generalization on top of existing single-step query transformation approaches.\n\nGiven an initial, complex query, the query is transformed and executed against an index. The response is retrieved from the query. \nGiven the response (along with prior responses) and the query, followup questions may be asked against the index as well.", "start_char_idx": 1813, "end_char_idx": 3795, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b8d6b4d-877f-4f5a-bc3c-d15e7eaae96f": {"__data__": {"id_": "7b8d6b4d-877f-4f5a-bc3c-d15e7eaae96f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf", "node_type": "1", "metadata": {}, "hash": "a8e47cc50e6e4a90dc3ecf90a80320bdc25455d894dc0095d9bac38bac7ec2dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff288ad3-f67e-453f-80a2-36e8df06acf8", "node_type": "1", "metadata": {}, "hash": "5477578229295ebd519be0616cac87fef33038a765db12d17856e7af3c3463cf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf", "node_type": "1", "metadata": {}, "hash": "a8e47cc50e6e4a90dc3ecf90a80320bdc25455d894dc0095d9bac38bac7ec2dc", "class_name": "RelatedNodeInfo"}}, "hash": "ad8d8b948996e894668f9b4c1a9ad7c4ffacd2a9b16b5db1324055a4dbd6c0c8", "text": "This technique allows a query to be run against a single knowledge source until that query has satisfied all questions.\n\nAn example image is shown below.\n\n!\n\n\nHere's a corresponding example code snippet.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Multi-Step Query Transformations\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.query.query_transform.base import StepDecomposeQueryTransform\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstep_decompose_transform = StepDecomposeQueryTransform(\n    llm_predictor, verbose=True\n)\n\nquery_engine = index.as_query_engine()\nquery_engine = MultiStepQueryEngine(query_engine, query_transform=step_decompose_transform)\n\nresponse = query_engine.query(\n    \"Who was in the first batch of the accelerator program the author started?\",\n)\nprint(str(response))\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCheck out our example notebook for a full walkthrough.", "start_char_idx": 3796, "end_char_idx": 5744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc004b59-b2ee-4071-aa1d-845bddf28a4d": {"__data__": {"id_": "dc004b59-b2ee-4071-aa1d-845bddf28a4d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cedbeb90-5626-4faf-ac24-de412008c78c", "node_type": "1", "metadata": {}, "hash": "331b266488459b207e1643ed788f071467f6f531f195502932f3391d94a00ce5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f5d255e-1c3c-45e5-afe0-4bd6b947188d", "node_type": "1", "metadata": {}, "hash": "d62608b9243ec1dc4e382915753d7a7bb4e978f4b4ed076362073018197d1a5f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cedbeb90-5626-4faf-ac24-de412008c78c", "node_type": "1", "metadata": {}, "hash": "331b266488459b207e1643ed788f071467f6f531f195502932f3391d94a00ce5", "class_name": "RelatedNodeInfo"}}, "hash": "8079ebce2d5c235539a4e5f1c2b19f61b99c01cace03a0830928398b1cf74887", "text": "File Name: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md\nfile_name: query_transformations.md\nfile_type: None\nfile_size: 6056\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\ncaption: Examples\nmaxdepth: 1\n---\n/examples/query_transformations/HyDEQueryTransformDemo.ipynb\n/examples/query_transformations/SimpleIndexDemo-multistep.ipynb\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nContent Type: code\nHeader Path: Module Guides/Basic\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1348\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\nRetriever Query Engine </examples/query_engine/CustomRetrievers.ipynb>\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nContent Type: code\nHeader Path: Module Guides/Structured & Semi-Structured Data\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1348\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/json_query_engine.ipynb\n/examples/query_engine/pandas_query_engine.ipynb\n/examples/query_engine/knowledge_graph_query_engine.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nContent Type: code\nHeader Path: Module Guides/Advanced\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nfile_name: modules.", "start_char_idx": 0, "end_char_idx": 1764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f5d255e-1c3c-45e5-afe0-4bd6b947188d": {"__data__": {"id_": "4f5d255e-1c3c-45e5-afe0-4bd6b947188d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cedbeb90-5626-4faf-ac24-de412008c78c", "node_type": "1", "metadata": {}, "hash": "331b266488459b207e1643ed788f071467f6f531f195502932f3391d94a00ce5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc004b59-b2ee-4071-aa1d-845bddf28a4d", "node_type": "1", "metadata": {}, "hash": "8079ebce2d5c235539a4e5f1c2b19f61b99c01cace03a0830928398b1cf74887", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "818cb288-4336-4115-846e-24de9ebecb88", "node_type": "1", "metadata": {}, "hash": "5ff509539b3a2f8b13ad463bc090103df9ad7ce953fed8a059514bee97d2bfcf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cedbeb90-5626-4faf-ac24-de412008c78c", "node_type": "1", "metadata": {}, "hash": "331b266488459b207e1643ed788f071467f6f531f195502932f3391d94a00ce5", "class_name": "RelatedNodeInfo"}}, "hash": "d62608b9243ec1dc4e382915753d7a7bb4e978f4b4ed076362073018197d1a5f", "text": "md\nfile_name: modules.md\nfile_type: None\nfile_size: 1348\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/RouterQueryEngine.ipynb\n/examples/query_engine/RetrieverRouterQueryEngine.ipynb\n/examples/query_engine/JointQASummary.ipynb\n/examples/query_engine/sub_question_query_engine.ipynb\n/examples/query_transformations/SimpleIndexDemo-multistep.ipynb\n/examples/query_engine/SQLRouterQueryEngine.ipynb\n/examples/query_engine/SQLAutoVectorQueryEngine.ipynb\n/examples/query_engine/SQLJoinQueryEngine.ipynb\n/examples/index_structs/struct_indices/duckdb_sql_query.ipynb\nRetry Query Engine </examples/evaluation/RetryQuery.ipynb>\nRetry Source Query Engine </examples/evaluation/RetryQuery.ipynb>\nRetry Guideline Query Engine </examples/evaluation/RetryQuery.ipynb>\n/examples/query_engine/citation_query_engine.ipynb\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nContent Type: code\nHeader Path: Module Guides/Experimental\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1348\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/flare_query_engine.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\response_modes.md\nContent Type: text\nHeader Path: Response Modes\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\response_modes.md\nfile_name: response_modes.md\nfile_type: None\nfile_size: 1347\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRight now,", "start_char_idx": 1742, "end_char_idx": 3488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "818cb288-4336-4115-846e-24de9ebecb88": {"__data__": {"id_": "818cb288-4336-4115-846e-24de9ebecb88", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cedbeb90-5626-4faf-ac24-de412008c78c", "node_type": "1", "metadata": {}, "hash": "331b266488459b207e1643ed788f071467f6f531f195502932f3391d94a00ce5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f5d255e-1c3c-45e5-afe0-4bd6b947188d", "node_type": "1", "metadata": {}, "hash": "d62608b9243ec1dc4e382915753d7a7bb4e978f4b4ed076362073018197d1a5f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "cedbeb90-5626-4faf-ac24-de412008c78c", "node_type": "1", "metadata": {}, "hash": "331b266488459b207e1643ed788f071467f6f531f195502932f3391d94a00ce5", "class_name": "RelatedNodeInfo"}}, "hash": "5ff509539b3a2f8b13ad463bc090103df9ad7ce953fed8a059514bee97d2bfcf", "text": "we support the following options:\n- `default`: \"create and refine\" an answer by sequentially going through each retrieved `Node`; \n    This makes a separate LLM call per Node. Good for more detailed answers.\n- `compact`: \"compact\" the prompt during each LLM call by stuffing as \n    many `Node` text chunks that can fit within the maximum prompt size. If there are \n    too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n    multiple prompts.\n- `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree \n    and return the root node as the response. Good for summarization purposes.\n- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n    without actually sending them. Then can be inspected by checking `response.source_nodes`.\n    The response object is covered in more detail in Section 5.\n- `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text\n    chunk while accumulating the responses into an array. Returns a concatenated string of all\n    responses. Good for when you need to run the same query separately against each text\n    chunk.\n\nSee Response Synthesizer to learn more.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuery engine is a generic interface that allows you to ask question over your data.\n\nA query engine takes in a natural language query, and returns a rich response.\nIt is most often (but not always) built on one or many Indices via Retrievers.\nYou can compose multiple query engines to achieve more advanced capability.", "start_char_idx": 3489, "end_char_idx": 5376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2172d386-222e-4632-979d-c43cf53eef8a": {"__data__": {"id_": "2172d386-222e-4632-979d-c43cf53eef8a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "node_type": "1", "metadata": {}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "496f6e3d-6497-4e4d-8f64-7bc846728080", "node_type": "1", "metadata": {}, "hash": "51b79ba348a14ad62ad28b7fcff4d62ce1fe52ecf0c3bd2aabb96f23edf7f687", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "node_type": "1", "metadata": {}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "class_name": "RelatedNodeInfo"}}, "hash": "ba2cd19dc540177ae35f4b61e5c80240ad0f1acf2ab6618b0c3d325317fee17b", "text": "You can compose multiple query engines to achieve more advanced capability.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you want to have a conversation with your data (multiple back-and-forth instead of a single question & answer), take a look at Chat Engine\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet started with:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Who is Paul Graham.\")", "start_char_idx": 0, "end_char_idx": 1329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "496f6e3d-6497-4e4d-8f64-7bc846728080": {"__data__": {"id_": "496f6e3d-6497-4e4d-8f64-7bc846728080", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "node_type": "1", "metadata": {}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2172d386-222e-4632-979d-c43cf53eef8a", "node_type": "1", "metadata": {}, "hash": "ba2cd19dc540177ae35f4b61e5c80240ad0f1acf2ab6618b0c3d325317fee17b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64c581be-ffaf-4820-9cbe-60e7faa0d31d", "node_type": "1", "metadata": {}, "hash": "4043b1d3cdb38733215bd235a9e96ce0381ccaa897b1211803816e6c6976efd6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "node_type": "1", "metadata": {}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "class_name": "RelatedNodeInfo"}}, "hash": "51b79ba348a14ad62ad28b7fcff4d62ce1fe52ecf0c3bd2aabb96f23edf7f687", "text": "File Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo stream response:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(streaming=True)\nstreaming_response = query_engine.query(\"Who is Paul Graham.\")", "start_char_idx": 1331, "end_char_idx": 2137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64c581be-ffaf-4820-9cbe-60e7faa0d31d": {"__data__": {"id_": "64c581be-ffaf-4820-9cbe-60e7faa0d31d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "node_type": "1", "metadata": {}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "496f6e3d-6497-4e4d-8f64-7bc846728080", "node_type": "1", "metadata": {}, "hash": "51b79ba348a14ad62ad28b7fcff4d62ce1fe52ecf0c3bd2aabb96f23edf7f687", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60564498-bab5-47c1-9a8c-b991f672b4e4", "node_type": "1", "metadata": {}, "hash": "6a9f3c15c9a1072d0c9e066aa62427dd0b7992d765cde196e62b64521a5dda2d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "node_type": "1", "metadata": {}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "class_name": "RelatedNodeInfo"}}, "hash": "4043b1d3cdb38733215bd235a9e96ce0381ccaa897b1211803816e6c6976efd6", "text": "streaming_response.print_response_stream()\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: code\nHeader Path: Query Engine/Usage Pattern\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: code\nHeader Path: Query Engine/Modules\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 3\n---\nmodules.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\root.md\nContent Type: code\nHeader Path: Query Engine/Supporting Modules\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1234\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nsupporting_modules.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports streaming the response as it's being generated.\nThis allows you to start printing or processing the beginning of the response before the full response is finished.\nThis can drastically reduce the perceived latency of queries.", "start_char_idx": 2138, "end_char_idx": 3904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60564498-bab5-47c1-9a8c-b991f672b4e4": {"__data__": {"id_": "60564498-bab5-47c1-9a8c-b991f672b4e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "node_type": "1", "metadata": {}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64c581be-ffaf-4820-9cbe-60e7faa0d31d", "node_type": "1", "metadata": {}, "hash": "4043b1d3cdb38733215bd235a9e96ce0381ccaa897b1211803816e6c6976efd6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514", "node_type": "1", "metadata": {}, "hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "class_name": "RelatedNodeInfo"}}, "hash": "6a9f3c15c9a1072d0c9e066aa62427dd0b7992d765cde196e62b64521a5dda2d", "text": "This can drastically reduce the perceived latency of queries.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo enable streaming, you need to use an LLM that supports streaming.\nRight now, streaming is supported by `OpenAI`, `HuggingFaceLLM`, and most LangChain LLMs (via `LangChainLLM`).\n\nConfigure query engine to use streaming:\n\nIf you are using the high-level API, set `streaming=True` when building a query engine.", "start_char_idx": 3843, "end_char_idx": 4555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dbd9c5f-178b-4ecd-aa3e-41f2824140f4": {"__data__": {"id_": "4dbd9c5f-178b-4ecd-aa3e-41f2824140f4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f99b44ac-ef51-4064-b11f-376000a8d227", "node_type": "1", "metadata": {}, "hash": "56da6f91d81b42c0bcdd08a6fbb6456a7f243f2ae32d5b910461d8ccb5a6c72e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d66814f-bd9e-4e5b-a7be-fe9801064c3e", "node_type": "1", "metadata": {}, "hash": "ee020d95fe29d768d51f7d6bf4681f1b47b65f98380c2b53172a7dd14f2abc37", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f99b44ac-ef51-4064-b11f-376000a8d227", "node_type": "1", "metadata": {}, "hash": "56da6f91d81b42c0bcdd08a6fbb6456a7f243f2ae32d5b910461d8ccb5a6c72e", "class_name": "RelatedNodeInfo"}}, "hash": "05476a86e0c33954b5d3cb140dc4540548ff16afd96c5600b73c78cab406a6ae", "text": "File Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    streaming=True,\n    similarity_top_k=1\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you are using the low-level API to compose the query engine,\npass `streaming=True` when constructing the `Response Synthesizer`:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import get_response_synthesizer\nsynth = get_response_synthesizer(streaming=True, .)\nquery_engine = RetrieverQueryEngine(response_synthesizer=synth, .)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d66814f-bd9e-4e5b-a7be-fe9801064c3e": {"__data__": {"id_": "9d66814f-bd9e-4e5b-a7be-fe9801064c3e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f99b44ac-ef51-4064-b11f-376000a8d227", "node_type": "1", "metadata": {}, "hash": "56da6f91d81b42c0bcdd08a6fbb6456a7f243f2ae32d5b910461d8ccb5a6c72e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dbd9c5f-178b-4ecd-aa3e-41f2824140f4", "node_type": "1", "metadata": {}, "hash": "05476a86e0c33954b5d3cb140dc4540548ff16afd96c5600b73c78cab406a6ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f53b62a-6b12-4eb0-ade0-061997ded610", "node_type": "1", "metadata": {}, "hash": "b6f2b7f4a6404d0134ec387a87daf826681e632acc4cb6c670e7d3197cffc5b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f99b44ac-ef51-4064-b11f-376000a8d227", "node_type": "1", "metadata": {}, "hash": "56da6f91d81b42c0bcdd08a6fbb6456a7f243f2ae32d5b910461d8ccb5a6c72e", "class_name": "RelatedNodeInfo"}}, "hash": "ee020d95fe29d768d51f7d6bf4681f1b47b65f98380c2b53172a7dd14f2abc37", "text": "md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAfter properly configuring both the LLM and the query engine,\ncalling `query` now returns a `StreamingResponse` object.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?\", \n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe response is returned immediately when the LLM call *starts*, without having to wait for the full completion.\n\n> Note: In the case where the query engine makes multiple LLM calls, only the last LLM call will be streamed and the response is returned when the last LLM call starts.", "start_char_idx": 1611, "end_char_idx": 2953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f53b62a-6b12-4eb0-ade0-061997ded610": {"__data__": {"id_": "4f53b62a-6b12-4eb0-ade0-061997ded610", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f99b44ac-ef51-4064-b11f-376000a8d227", "node_type": "1", "metadata": {}, "hash": "56da6f91d81b42c0bcdd08a6fbb6456a7f243f2ae32d5b910461d8ccb5a6c72e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d66814f-bd9e-4e5b-a7be-fe9801064c3e", "node_type": "1", "metadata": {}, "hash": "ee020d95fe29d768d51f7d6bf4681f1b47b65f98380c2b53172a7dd14f2abc37", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f99b44ac-ef51-4064-b11f-376000a8d227", "node_type": "1", "metadata": {}, "hash": "56da6f91d81b42c0bcdd08a6fbb6456a7f243f2ae32d5b910461d8ccb5a6c72e", "class_name": "RelatedNodeInfo"}}, "hash": "b6f2b7f4a6404d0134ec387a87daf826681e632acc4cb6c670e7d3197cffc5b2", "text": "You can obtain a `Generator` from the streaming response and iterate over the tokens as they arrive:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfor text in streaming_response.response_gen:\n    # do something with text as they arrive.", "start_char_idx": 2955, "end_char_idx": 3498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c11af20-f2f5-4904-9253-0de11f04df45": {"__data__": {"id_": "3c11af20-f2f5-4904-9253-0de11f04df45", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ef046785-afb0-44f2-b971-a11f2947b864", "node_type": "1", "metadata": {}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91e5b4a4-c742-4dc8-9acb-32493d892ec9", "node_type": "1", "metadata": {}, "hash": "0d6c73e0baf744d2596f42ae3aff4850ae1d7d0073b1fa016cf96f176916c336", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ef046785-afb0-44f2-b971-a11f2947b864", "node_type": "1", "metadata": {}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "class_name": "RelatedNodeInfo"}}, "hash": "0565e80081af0e57493ce657a5c668404adc2bd3f053b13dd369a502a2015f62", "text": "File Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAlternatively, if you just want to print the text as they arrive:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstreaming_response.print_response_stream()\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\streaming.md\nfile_name: streaming.md\nfile_type: None\nfile_size: 2005\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee an end-to-end example\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\supporting_modules.md\nContent Type: code\nHeader Path: Supporting Modules\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\supporting_modules.md\nfile_name: supporting_modules.md\nfile_type: None\nfile_size: 99\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\nadvanced/query_transformations.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.", "start_char_idx": 0, "end_char_idx": 1687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91e5b4a4-c742-4dc8-9acb-32493d892ec9": {"__data__": {"id_": "91e5b4a4-c742-4dc8-9acb-32493d892ec9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ef046785-afb0-44f2-b971-a11f2947b864", "node_type": "1", "metadata": {}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c11af20-f2f5-4904-9253-0de11f04df45", "node_type": "1", "metadata": {}, "hash": "0565e80081af0e57493ce657a5c668404adc2bd3f053b13dd369a502a2015f62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8976cc4-4f5c-40ba-9df1-f095f713aed6", "node_type": "1", "metadata": {}, "hash": "ba8cf6892bad5aef013a5903da34527ee94a97f5f359c08f219bce10256bd18a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ef046785-afb0-44f2-b971-a11f2947b864", "node_type": "1", "metadata": {}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "class_name": "RelatedNodeInfo"}}, "hash": "0d6c73e0baf744d2596f42ae3aff4850ae1d7d0073b1fa016cf96f176916c336", "text": "md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBuild a query engine from index:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAsk a question over your data\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.", "start_char_idx": 1609, "end_char_idx": 3284, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8976cc4-4f5c-40ba-9df1-f095f713aed6": {"__data__": {"id_": "e8976cc4-4f5c-40ba-9df1-f095f713aed6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ef046785-afb0-44f2-b971-a11f2947b864", "node_type": "1", "metadata": {}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91e5b4a4-c742-4dc8-9acb-32493d892ec9", "node_type": "1", "metadata": {}, "hash": "0d6c73e0baf744d2596f42ae3aff4850ae1d7d0073b1fa016cf96f176916c336", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b2690bf-b5cb-41b0-929b-8fd7b832cfee", "node_type": "1", "metadata": {}, "hash": "a9115d3dacb1cff2d46e3710f0b59568d17965dce7961a57060e78b4e9816e81", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ef046785-afb0-44f2-b971-a11f2947b864", "node_type": "1", "metadata": {}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "class_name": "RelatedNodeInfo"}}, "hash": "ba8cf6892bad5aef013a5903da34527ee94a97f5f359c08f219bce10256bd18a", "text": "md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = query_engine.query('Who is Paul Graham?')", "start_char_idx": 3284, "end_char_idx": 3627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b2690bf-b5cb-41b0-929b-8fd7b832cfee": {"__data__": {"id_": "6b2690bf-b5cb-41b0-929b-8fd7b832cfee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ef046785-afb0-44f2-b971-a11f2947b864", "node_type": "1", "metadata": {}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8976cc4-4f5c-40ba-9df1-f095f713aed6", "node_type": "1", "metadata": {}, "hash": "ba8cf6892bad5aef013a5903da34527ee94a97f5f359c08f219bce10256bd18a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ef046785-afb0-44f2-b971-a11f2947b864", "node_type": "1", "metadata": {}, "hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "class_name": "RelatedNodeInfo"}}, "hash": "a9115d3dacb1cff2d46e3710f0b59568d17965dce7961a57060e78b4e9816e81", "text": "query('Who is Paul Graham?')\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can directly build and configure a query engine from an index in 1 line of code:\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    response_mode='tree_summarize',\n    verbose=True,\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.  \n\nSee **Response Modes** for a full list of response modes and what they do.", "start_char_idx": 3599, "end_char_idx": 5172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "872aad94-9da9-4557-97bd-abd802849708": {"__data__": {"id_": "872aad94-9da9-4557-97bd-abd802849708", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e55610d1-f38c-4556-ab21-8baa89ea8746", "node_type": "1", "metadata": {}, "hash": "5a184d2afc52e38ef816783ed5d05f9044cceb3b0107e208121b3a7788b1a4e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5d7c57a-f321-4824-aff1-31e89e24375a", "node_type": "1", "metadata": {}, "hash": "0f1c13e9fad0dfcc0f473116347ed53b2dccecb1ef08b8270592317f4b3e3bdb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e55610d1-f38c-4556-ab21-8baa89ea8746", "node_type": "1", "metadata": {}, "hash": "5a184d2afc52e38ef816783ed5d05f9044cceb3b0107e208121b3a7788b1a4e7", "class_name": "RelatedNodeInfo"}}, "hash": "34861dd5b1749d45e3d1f7c8ccccc1f4e1a4b7ff5026ba661a5cc9798ee08ade", "text": "See **Response Modes** for a full list of response modes and what they do.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\nhidden:\n---\nresponse_modes.md\nstreaming.md\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use the low-level composition API if you need more granular control.\nConcretely speaking, you would explicitly construct a `QueryEngine` object instead of calling `index.as_query_engine(...)`.\n> Note: You may need to look at API references or example notebooks.\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.retrievers import VectorIndexRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.", "start_char_idx": 0, "end_char_idx": 1867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5d7c57a-f321-4824-aff1-31e89e24375a": {"__data__": {"id_": "c5d7c57a-f321-4824-aff1-31e89e24375a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e55610d1-f38c-4556-ab21-8baa89ea8746", "node_type": "1", "metadata": {}, "hash": "5a184d2afc52e38ef816783ed5d05f9044cceb3b0107e208121b3a7788b1a4e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "872aad94-9da9-4557-97bd-abd802849708", "node_type": "1", "metadata": {}, "hash": "34861dd5b1749d45e3d1f7c8ccccc1f4e1a4b7ff5026ba661a5cc9798ee08ade", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "050c6272-35a7-41be-b9c7-a044c487524f", "node_type": "1", "metadata": {}, "hash": "f4c756cbd9d06f8a1c8656e46711ccc5b4c10af36236c5c9f38c2de8f94f69f2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e55610d1-f38c-4556-ab21-8baa89ea8746", "node_type": "1", "metadata": {}, "hash": "5a184d2afc52e38ef816783ed5d05f9044cceb3b0107e208121b3a7788b1a4e7", "class_name": "RelatedNodeInfo"}}, "hash": "0f1c13e9fad0dfcc0f473116347ed53b2dccecb1ef08b8270592317f4b3e3bdb", "text": "md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/build index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/configure retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = VectorIndexRetriever(\n    index=index, \n    similarity_top_k=2,\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/configure response synthesizer\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse_synthesizer = get_response_synthesizer(\n    response_mode=\"tree_summarize\",\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/assemble query engine\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.", "start_char_idx": 1867, "end_char_idx": 3481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "050c6272-35a7-41be-b9c7-a044c487524f": {"__data__": {"id_": "050c6272-35a7-41be-b9c7-a044c487524f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e55610d1-f38c-4556-ab21-8baa89ea8746", "node_type": "1", "metadata": {}, "hash": "5a184d2afc52e38ef816783ed5d05f9044cceb3b0107e208121b3a7788b1a4e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5d7c57a-f321-4824-aff1-31e89e24375a", "node_type": "1", "metadata": {}, "hash": "0f1c13e9fad0dfcc0f473116347ed53b2dccecb1ef08b8270592317f4b3e3bdb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e55610d1-f38c-4556-ab21-8baa89ea8746", "node_type": "1", "metadata": {}, "hash": "5a184d2afc52e38ef816783ed5d05f9044cceb3b0107e208121b3a7788b1a4e7", "class_name": "RelatedNodeInfo"}}, "hash": "f4c756cbd9d06f8a1c8656e46711ccc5b4c10af36236c5c9f38c2de8f94f69f2", "text": "md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/query\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo enable streaming, you simply need to pass in a `streaming=True` flag\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    streaming=True,\n)\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?", "start_char_idx": 3453, "end_char_idx": 5169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c931fa1-b7bc-4665-8296-b69e8ec7022b": {"__data__": {"id_": "3c931fa1-b7bc-4665-8296-b69e8ec7022b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "49f45f3d-b52b-4ba6-94bd-073ae87610c0", "node_type": "1", "metadata": {}, "hash": "741f829103d87723bad5d1f5d01b9ff474001ab22bbec0914af554f35a5c4547", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "49f45f3d-b52b-4ba6-94bd-073ae87610c0", "node_type": "1", "metadata": {}, "hash": "741f829103d87723bad5d1f5d01b9ff474001ab22bbec0914af554f35a5c4547", "class_name": "RelatedNodeInfo"}}, "hash": "741f829103d87723bad5d1f5d01b9ff474001ab22bbec0914af554f35a5c4547", "text": "\", \n)\nstreaming_response.print_response_stream()\n\nFile Name: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\nfile_path: Docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2455\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* Read the full streaming guide\n* See an end-to-end example\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDetailed inputs/outputs for each response synthesizer are found below.\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/API Example\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe following shows the setup for utilizing all kwargs.\n\n- `response_mode` specifies which response synthesizer to use\n- `service_context` defines the LLM and related settings for synthesis\n- `text_qa_template` and `refine_template` are the prompts used at various stages\n- `use_async` is used for only the `tree_summarize` response mode right now, to asynchronously build the summary tree\n- `streaming` configures whether to return a streaming response object or not\n\nIn the `synthesize`/`asyntheszie` functions, you can optionally provide additional source nodes, which will be added to the `response.source_nodes` list.", "start_char_idx": 0, "end_char_idx": 1891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8d66d8d-f99e-4cac-a2ae-208fa8323598": {"__data__": {"id_": "c8d66d8d-f99e-4cac-a2ae-208fa8323598", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19e9220c-9515-4e08-9b2d-339a09be6cba", "node_type": "1", "metadata": {}, "hash": "dee63d8f226759fde5f74ade948b85f36884f06333e49c583c5b56d88d23cf98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d0352ec-83b8-458d-9d7e-dd1dd5e57aff", "node_type": "1", "metadata": {}, "hash": "fdbb91daff0424a9d5c731a8144fca314b80a3e0ab042269f25f9b1672b9ca05", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "19e9220c-9515-4e08-9b2d-339a09be6cba", "node_type": "1", "metadata": {}, "hash": "dee63d8f226759fde5f74ade948b85f36884f06333e49c583c5b56d88d23cf98", "class_name": "RelatedNodeInfo"}}, "hash": "e414025695bfaa10c2ef859de942dd85040b80d00395f49de1c34d197d7e8e01", "text": "File Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/API Example\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import Node, NodeWithScore\nfrom llama_index import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(\n  response_mode=\"refine\",\n  service_context=service_context,\n  text_qa_template=text_qa_template,\n  refine_template=refine_template,\n  use_async=False,\n  streaming=False\n)\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/synchronous\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = response_synthesizer.synthesize(\n  \"query string\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), .],\n  additional_source_nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), .], \n)\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = await response_synthesizer.asynthesize(\n  \"query string\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), .],", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d0352ec-83b8-458d-9d7e-dd1dd5e57aff": {"__data__": {"id_": "1d0352ec-83b8-458d-9d7e-dd1dd5e57aff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19e9220c-9515-4e08-9b2d-339a09be6cba", "node_type": "1", "metadata": {}, "hash": "dee63d8f226759fde5f74ade948b85f36884f06333e49c583c5b56d88d23cf98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8d66d8d-f99e-4cac-a2ae-208fa8323598", "node_type": "1", "metadata": {}, "hash": "e414025695bfaa10c2ef859de942dd85040b80d00395f49de1c34d197d7e8e01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44dbce79-8521-48a7-beea-de14b5c6383c", "node_type": "1", "metadata": {}, "hash": "a363d96b66f0060257e1788bdd4240b36659923a79ca3f8e700ae29b6195cf2b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "19e9220c-9515-4e08-9b2d-339a09be6cba", "node_type": "1", "metadata": {}, "hash": "dee63d8f226759fde5f74ade948b85f36884f06333e49c583c5b56d88d23cf98", "class_name": "RelatedNodeInfo"}}, "hash": "fdbb91daff0424a9d5c731a8144fca314b80a3e0ab042269f25f9b1672b9ca05", "text": "score=1.0), .],\n  additional_source_nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), .], \n)\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also directly return a string, using the lower-level `get_response` and `aget_response` functions\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse_str = response_synthesizer.get_response(\n  \"query string\", \n  text_chunks=[\"text1\", \"text2\", .]\n)\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nContent Type: code\nHeader Path: Module Guide/Example Notebooks\nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1978\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/response_synthesizers/refine.ipynb\n/examples/response_synthesizers/tree_summarize.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44dbce79-8521-48a7-beea-de14b5c6383c": {"__data__": {"id_": "44dbce79-8521-48a7-beea-de14b5c6383c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19e9220c-9515-4e08-9b2d-339a09be6cba", "node_type": "1", "metadata": {}, "hash": "dee63d8f226759fde5f74ade948b85f36884f06333e49c583c5b56d88d23cf98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d0352ec-83b8-458d-9d7e-dd1dd5e57aff", "node_type": "1", "metadata": {}, "hash": "fdbb91daff0424a9d5c731a8144fca314b80a3e0ab042269f25f9b1672b9ca05", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "19e9220c-9515-4e08-9b2d-339a09be6cba", "node_type": "1", "metadata": {}, "hash": "dee63d8f226759fde5f74ade948b85f36884f06333e49c583c5b56d88d23cf98", "class_name": "RelatedNodeInfo"}}, "hash": "a363d96b66f0060257e1788bdd4240b36659923a79ca3f8e700ae29b6195cf2b", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA `Response Synthesizer` is what generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a `Response` object.\n\nThe method for doing this can take many forms, from as simple as iterating over text chunks, to as complex as building a tree. The main idea here is to simplify the process of generating a response using an LLM across your data.\n\nWhen used in a query engine, the response synthesizer is used after nodes are retrieved from a retriever, and after any node-postprocessors are ran.\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nConfused about where response synthesizer fits in the pipeline?", "start_char_idx": 3482, "end_char_idx": 4603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a35a11a-8fd9-45d7-ad98-06a245c10326": {"__data__": {"id_": "1a35a11a-8fd9-45d7-ad98-06a245c10326", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc007a85-6883-4a66-afa8-30af77b854cd", "node_type": "1", "metadata": {}, "hash": "b974c2210558ff9f02865dd3b29d3bb5175893a955e3c16017b0d20712de61ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a1f5112-7bc8-4e99-becc-4cf7fc32a113", "node_type": "1", "metadata": {}, "hash": "c7ac254c0c5548d042fe48378d78076aa2d91f94676f1d20a4298ab414e0215d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc007a85-6883-4a66-afa8-30af77b854cd", "node_type": "1", "metadata": {}, "hash": "b974c2210558ff9f02865dd3b29d3bb5175893a955e3c16017b0d20712de61ed", "class_name": "RelatedNodeInfo"}}, "hash": "a4ea8317fd20061437fd74f75d12fd2555ed37a0a17eaad1c8a417b732e85683", "text": "Read the high-level concepts\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUse a response synthesizer on it's own:\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import Node\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode='compact')\n\nresponse = response_synthesizer.synthesize(\"query text\", nodes=[Node(text=\"text\"), .])\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOr in a query engine after you've created an index:\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a1f5112-7bc8-4e99-becc-4cf7fc32a113": {"__data__": {"id_": "3a1f5112-7bc8-4e99-becc-4cf7fc32a113", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc007a85-6883-4a66-afa8-30af77b854cd", "node_type": "1", "metadata": {}, "hash": "b974c2210558ff9f02865dd3b29d3bb5175893a955e3c16017b0d20712de61ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a35a11a-8fd9-45d7-ad98-06a245c10326", "node_type": "1", "metadata": {}, "hash": "a4ea8317fd20061437fd74f75d12fd2555ed37a0a17eaad1c8a417b732e85683", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bc007a85-6883-4a66-afa8-30af77b854cd", "node_type": "1", "metadata": {}, "hash": "b974c2210558ff9f02865dd3b29d3bb5175893a955e3c16017b0d20712de61ed", "class_name": "RelatedNodeInfo"}}, "hash": "c7ac254c0c5548d042fe48378d78076aa2d91f94676f1d20a4298ab414e0215d", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can find more details on all available response synthesizers, modes, and how to build your own below.\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 2\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBelow you can find detailed API information for each response synthesis module.", "start_char_idx": 1697, "end_char_idx": 3266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41e35ee7-a929-4f68-af37-634800af56ca": {"__data__": {"id_": "41e35ee7-a929-4f68-af37-634800af56ca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "node_type": "1", "metadata": {}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cefcf65a-e0d8-4249-8784-d1b253b6d370", "node_type": "1", "metadata": {}, "hash": "babdd53092261df92ff07b828d983f08dfc44cb128306dad491aa52e0656c52d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "node_type": "1", "metadata": {}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "class_name": "RelatedNodeInfo"}}, "hash": "a96f50d50412a686a498b199dd881097792befa3bc904e996972ae74b944e33f", "text": "File Name: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nContent Type: text\nHeader Path: Response Synthesizer/Modules\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1620\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nConfiguring the response synthesizer for a query engine using `response_mode`:\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import Node, NodeWithScore\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode='compact')\n\nresponse = response_synthesizer.synthesize(\n  \"query text\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), .]\n)\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cefcf65a-e0d8-4249-8784-d1b253b6d370": {"__data__": {"id_": "cefcf65a-e0d8-4249-8784-d1b253b6d370", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "node_type": "1", "metadata": {}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41e35ee7-a929-4f68-af37-634800af56ca", "node_type": "1", "metadata": {}, "hash": "a96f50d50412a686a498b199dd881097792befa3bc904e996972ae74b944e33f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46d530a0-a244-4710-9007-f0264c1aa5e2", "node_type": "1", "metadata": {}, "hash": "b2f83f426c617440838f1f3873b87d404fddd36a8f694327015e131490fcb642", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "node_type": "1", "metadata": {}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "class_name": "RelatedNodeInfo"}}, "hash": "babdd53092261df92ff07b828d983f08dfc44cb128306dad491aa52e0656c52d", "text": "md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOr, more commonly, in a query engine after you've created an index:\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring the Response Mode\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nResponse synthesizers are typically specified through a `response_mode` kwarg setting.\n\nSeveral response synthesizers are implemented already in LlamaIndex:\n\n- `refine`: \"create and refine\" an answer by sequentially going through each retrieved text chunk.", "start_char_idx": 1776, "end_char_idx": 3581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46d530a0-a244-4710-9007-f0264c1aa5e2": {"__data__": {"id_": "46d530a0-a244-4710-9007-f0264c1aa5e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "node_type": "1", "metadata": {}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cefcf65a-e0d8-4249-8784-d1b253b6d370", "node_type": "1", "metadata": {}, "hash": "babdd53092261df92ff07b828d983f08dfc44cb128306dad491aa52e0656c52d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32eef4f1-7aa6-4f39-b7b0-c6dcc26185f7", "node_type": "1", "metadata": {}, "hash": "f04fe1c00907969ac63902208a054b58a5eee434dd63ae98240f7bda60455f33", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "node_type": "1", "metadata": {}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "class_name": "RelatedNodeInfo"}}, "hash": "b2f83f426c617440838f1f3873b87d404fddd36a8f694327015e131490fcb642", "text": "This makes a separate LLM call per Node. Good for more detailed answers.\n- `compact` (default): \"compact\" the prompt during each LLM call by stuffing as \n    many text chunks that can fit within the maximum prompt size. If there are \n    too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n    multiple compact prompts. The same as `refine`, but should result in less LLM calls.\n- `tree_summarize`: Given a set of text chunks and the query, recursively construct a tree \n    and return the root node as the response. Good for summarization purposes.\n- `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt. Good for quick\n    summarization purposes, but may lose detail due to truncation.\n- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n    without actually sending them. Then can be inspected by checking `response.source_nodes`.\n- `accumulate`: Given a set of text chunks and the query, apply the query to each text\n    chunk while accumulating the responses into an array. Returns a concatenated string of all\n    responses. Good for when you need to run the same query separately against each text\n    chunk.\n- `compact_accumulate`: The same as accumulate, but will \"compact\" each LLM prompt similar to\n    `compact`, and run the same query against each text chunk.\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Response Synthesizers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEach response synthesizer inherits from `llama_index.response_synthesizers.base.BaseSynthesizer`. The base API is extremely simple, which makes it easy to create your own response synthesizer.", "start_char_idx": 3587, "end_char_idx": 5552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32eef4f1-7aa6-4f39-b7b0-c6dcc26185f7": {"__data__": {"id_": "32eef4f1-7aa6-4f39-b7b0-c6dcc26185f7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "node_type": "1", "metadata": {}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46d530a0-a244-4710-9007-f0264c1aa5e2", "node_type": "1", "metadata": {}, "hash": "b2f83f426c617440838f1f3873b87d404fddd36a8f694327015e131490fcb642", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "node_type": "1", "metadata": {}, "hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "class_name": "RelatedNodeInfo"}}, "hash": "f04fe1c00907969ac63902208a054b58a5eee434dd63ae98240f7bda60455f33", "text": "The base API is extremely simple, which makes it easy to create your own response synthesizer.\n\nMaybe you want to customize which template is used at each step in `tree_summarize`, or maybe a new research paper came out detailing a new way to generate a response to a query, you can create your own response synthesizer and plug it into any query engine or use it on it's own.", "start_char_idx": 5458, "end_char_idx": 5834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9291fcf4-5c26-46d4-8daf-f632e94482a6": {"__data__": {"id_": "9291fcf4-5c26-46d4-8daf-f632e94482a6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcd9a011-a035-41e2-87a7-23f1405a9dfc", "node_type": "1", "metadata": {}, "hash": "67d6a49a3605f68a4dc05d6ae7e5e3f596b72391c637b340510e824dbf901f51", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fcd9a011-a035-41e2-87a7-23f1405a9dfc", "node_type": "1", "metadata": {}, "hash": "67d6a49a3605f68a4dc05d6ae7e5e3f596b72391c637b340510e824dbf901f51", "class_name": "RelatedNodeInfo"}}, "hash": "67d6a49a3605f68a4dc05d6ae7e5e3f596b72391c637b340510e824dbf901f51", "text": "Below we show the `__init__()` function, as well as the two abstract methods that every response synthesizer must implement. The basic requirements are to process a query and text chunks, and return a string (or string generator) response.\n\nFile Name: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Response Synthesizers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4151\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nclass BaseSynthesizer(ABC):\n    \"\"\"Response builder class.\"\"\"\n\n    def __init__(\n        self,\n        service_context: Optional[ServiceContext] = None,\n        streaming: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._service_context = service_context or ServiceContext.from_defaults()\n        self._callback_manager = self._service_context.callback_manager\n        self._streaming = streaming\n\n    @abstractmethod\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"\n        ...\n\n    @abstractmethod\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"\n        ...\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe are adding more module guides soon!\nIn the meanwhile, please take a look at the API References.", "start_char_idx": 0, "end_char_idx": 1922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8bff74e-ae63-4b2f-84a2-10cd31bcef79": {"__data__": {"id_": "b8bff74e-ae63-4b2f-84a2-10cd31bcef79", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b", "node_type": "1", "metadata": {}, "hash": "de8679944de3bdb9de7f5a3dc0360ecc21480fcb375a28cf0b1e1671e3a1c17d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "298e40fd-0964-4fca-930f-cf5b053eb630", "node_type": "1", "metadata": {}, "hash": "7906a00b38356f84c3193f0b599d37b6573662a6d937ef3c03d029fa3f4eeee3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b", "node_type": "1", "metadata": {}, "hash": "de8679944de3bdb9de7f5a3dc0360ecc21480fcb375a28cf0b1e1671e3a1c17d", "class_name": "RelatedNodeInfo"}}, "hash": "c2b96f437a911cb039627eab097318134ab140296b8af1be602263c27df4b19f", "text": "In the meanwhile, please take a look at the API References.\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Vector Index Retrievers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* VectorIndexRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Vector Index Retrievers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nVectorIndexAutoRetriever </examples/vector_stores/chroma_auto_retriever.ipynb>\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/List Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* ListIndexRetriever \n* ListIndexEmbeddingRetriever \n* ListIndexLLMRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Tree Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.", "start_char_idx": 0, "end_char_idx": 1509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "298e40fd-0964-4fca-930f-cf5b053eb630": {"__data__": {"id_": "298e40fd-0964-4fca-930f-cf5b053eb630", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b", "node_type": "1", "metadata": {}, "hash": "de8679944de3bdb9de7f5a3dc0360ecc21480fcb375a28cf0b1e1671e3a1c17d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8bff74e-ae63-4b2f-84a2-10cd31bcef79", "node_type": "1", "metadata": {}, "hash": "c2b96f437a911cb039627eab097318134ab140296b8af1be602263c27df4b19f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09c96574-11c8-4d52-a6f4-0226d2004eb9", "node_type": "1", "metadata": {}, "hash": "1c83d30ea01a86b84c379fa65084817ab640d7edfb817c31fecd6fd89ce8062a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b", "node_type": "1", "metadata": {}, "hash": "de8679944de3bdb9de7f5a3dc0360ecc21480fcb375a28cf0b1e1671e3a1c17d", "class_name": "RelatedNodeInfo"}}, "hash": "7906a00b38356f84c3193f0b599d37b6573662a6d937ef3c03d029fa3f4eeee3", "text": "md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* TreeSelectLeafRetriever\n* TreeSelectLeafEmbeddingRetriever\n* TreeAllLeafRetriever\n* TreeRootRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Keyword Table Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* KeywordTableGPTRetriever\n* KeywordTableSimpleRetriever\n* KeywordTableRAKERetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: code\nHeader Path: Module Guides/Knowledge Graph Index\nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\nCustom Retriever (KG Index and Vector Store Index) </examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.ipynb>\n```\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Knowledge Graph Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* KGTableRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.", "start_char_idx": 1487, "end_char_idx": 3137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09c96574-11c8-4d52-a6f4-0226d2004eb9": {"__data__": {"id_": "09c96574-11c8-4d52-a6f4-0226d2004eb9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b", "node_type": "1", "metadata": {}, "hash": "de8679944de3bdb9de7f5a3dc0360ecc21480fcb375a28cf0b1e1671e3a1c17d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "298e40fd-0964-4fca-930f-cf5b053eb630", "node_type": "1", "metadata": {}, "hash": "7906a00b38356f84c3193f0b599d37b6573662a6d937ef3c03d029fa3f4eeee3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b", "node_type": "1", "metadata": {}, "hash": "de8679944de3bdb9de7f5a3dc0360ecc21480fcb375a28cf0b1e1671e3a1c17d", "class_name": "RelatedNodeInfo"}}, "hash": "1c83d30ea01a86b84c379fa65084817ab640d7edfb817c31fecd6fd89ce8062a", "text": "md\nContent Type: text\nHeader Path: Module Guides/Document Summary Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* DocumentSummaryIndexRetriever\n* DocumentSummaryIndexEmbeddingRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Composed Retrievers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* TransformRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\modules.md\nContent Type: text\nHeader Path: Module Guides/Composed Retrievers\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 1182\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere we show the mapping from `retriever_mode` configuration to the selected retriever class.\n> Note that `retriever_mode` can mean different thing for different index classes.", "start_char_idx": 3137, "end_char_idx": 4818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00b09109-7e4e-4405-81d9-eac4ceb4bbe1": {"__data__": {"id_": "00b09109-7e4e-4405-81d9-eac4ceb4bbe1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c7685ea-8047-4290-ac9a-498322fa81da", "node_type": "1", "metadata": {}, "hash": "950e12c152d44fd3ff8a79a58274466279aae9513e19d2c2d87eb400f47ddb89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1626bfc-e490-40e1-9a72-b01ed133699d", "node_type": "1", "metadata": {}, "hash": "4efeb17a5358f02c9012041326babfc41640dc46505551700ad9d2a9319266ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6c7685ea-8047-4290-ac9a-498322fa81da", "node_type": "1", "metadata": {}, "hash": "950e12c152d44fd3ff8a79a58274466279aae9513e19d2c2d87eb400f47ddb89", "class_name": "RelatedNodeInfo"}}, "hash": "bfb9050957beedcf8a107173953b2cd6c1afe7f7667023dc8d3ae910032967dd", "text": "> Note that `retriever_mode` can mean different thing for different index classes.\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Vector Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSpecifying `retriever_mode` has no effect (silently ignored).\n`vector_index.as_retriever(...)` always returns a VectorIndexRetriever.\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/List Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* `default`: ListIndexRetriever \n* `embedding`: ListIndexEmbeddingRetriever \n* `llm`: ListIndexLLMRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Tree Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* `select_leaf`: TreeSelectLeafRetriever\n* `select_leaf_embedding`: TreeSelectLeafEmbeddingRetriever\n* `all_leaf`: TreeAllLeafRetriever\n* `root`: TreeRootRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.", "start_char_idx": 0, "end_char_idx": 1650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1626bfc-e490-40e1-9a72-b01ed133699d": {"__data__": {"id_": "b1626bfc-e490-40e1-9a72-b01ed133699d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c7685ea-8047-4290-ac9a-498322fa81da", "node_type": "1", "metadata": {}, "hash": "950e12c152d44fd3ff8a79a58274466279aae9513e19d2c2d87eb400f47ddb89", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00b09109-7e4e-4405-81d9-eac4ceb4bbe1", "node_type": "1", "metadata": {}, "hash": "bfb9050957beedcf8a107173953b2cd6c1afe7f7667023dc8d3ae910032967dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d38fd5d-8264-45fc-8417-3cce56944380", "node_type": "1", "metadata": {}, "hash": "421b15082007f3833143e12bc469f19ed9f7e94a8d056707d9b303015c38c643", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6c7685ea-8047-4290-ac9a-498322fa81da", "node_type": "1", "metadata": {}, "hash": "950e12c152d44fd3ff8a79a58274466279aae9513e19d2c2d87eb400f47ddb89", "class_name": "RelatedNodeInfo"}}, "hash": "4efeb17a5358f02c9012041326babfc41640dc46505551700ad9d2a9319266ee", "text": "md\nContent Type: text\nHeader Path: Retriever Modes/Keyword Table Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* `default`: KeywordTableGPTRetriever\n* `simple`: KeywordTableSimpleRetriever\n* `rake`: KeywordTableRAKERetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Knowledge Graph Index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* `keyword`: KGTableRetriever\n* `embedding`: KGTableRetriever\n* `hybrid`: KGTableRetriever\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Document Summary Index\nfile_path: Docs\\core_modules\\query_modules\\retriever\\retriever_modes.md\nfile_name: retriever_modes.md\nfile_type: None\nfile_size: 1065\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* `default`: DocumentSummaryIndexRetriever\n* `embedding`: DocumentSummaryIndexEmbeddingRetrievers\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: text\nHeader Path: Retriever/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.", "start_char_idx": 1650, "end_char_idx": 3193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d38fd5d-8264-45fc-8417-3cce56944380": {"__data__": {"id_": "8d38fd5d-8264-45fc-8417-3cce56944380", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c7685ea-8047-4290-ac9a-498322fa81da", "node_type": "1", "metadata": {}, "hash": "950e12c152d44fd3ff8a79a58274466279aae9513e19d2c2d87eb400f47ddb89", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1626bfc-e490-40e1-9a72-b01ed133699d", "node_type": "1", "metadata": {}, "hash": "4efeb17a5358f02c9012041326babfc41640dc46505551700ad9d2a9319266ee", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6c7685ea-8047-4290-ac9a-498322fa81da", "node_type": "1", "metadata": {}, "hash": "950e12c152d44fd3ff8a79a58274466279aae9513e19d2c2d87eb400f47ddb89", "class_name": "RelatedNodeInfo"}}, "hash": "421b15082007f3833143e12bc469f19ed9f7e94a8d056707d9b303015c38c643", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRetrievers are responsible for fetching the most relevant context given a user query (or chat message).  \n\nIt can be built on top of Indices, but can also be defined independently.\nIt is used as a key building block in Query Engines (and Chat Engines) for retrieving relevant context.\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: text\nHeader Path: Retriever/Concept\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nConfused about where retriever fits in the pipeline? Read about high-level concepts\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: text\nHeader Path: Retriever/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet started with:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: text\nHeader Path: Retriever/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = index.as_retriever()\nnodes = retriever.retrieve(\"Who is Paul Graham?\")", "start_char_idx": 3174, "end_char_idx": 4757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac33dca7-0115-4896-97d2-a1da715c057f": {"__data__": {"id_": "ac33dca7-0115-4896-97d2-a1da715c057f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e", "node_type": "1", "metadata": {}, "hash": "718e2dcdd8d1aa111011d3ce529dc7010abf815229ba66d9395fcf924bf0f38a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85014ae4-1598-4cbc-adb0-282440392bb0", "node_type": "1", "metadata": {}, "hash": "aeaa2ed4d2e72c53c149d6c106703533379ee0167666fc448a5308f7e295e682", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e", "node_type": "1", "metadata": {}, "hash": "718e2dcdd8d1aa111011d3ce529dc7010abf815229ba66d9395fcf924bf0f38a", "class_name": "RelatedNodeInfo"}}, "hash": "a0f39a6c8260bd69ac2e2499a4242bb0ab1d2abf49a59d3a6bf11789c7a6b22f", "text": "File Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: code\nHeader Path: Retriever/Usage Pattern\nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\root.md\nContent Type: code\nHeader Path: Retriever/Modules\nfile_path: Docs\\core_modules\\query_modules\\retriever\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGet a retriever from index:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = index.as_retriever()\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.", "start_char_idx": 0, "end_char_idx": 1572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85014ae4-1598-4cbc-adb0-282440392bb0": {"__data__": {"id_": "85014ae4-1598-4cbc-adb0-282440392bb0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e", "node_type": "1", "metadata": {}, "hash": "718e2dcdd8d1aa111011d3ce529dc7010abf815229ba66d9395fcf924bf0f38a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac33dca7-0115-4896-97d2-a1da715c057f", "node_type": "1", "metadata": {}, "hash": "a0f39a6c8260bd69ac2e2499a4242bb0ab1d2abf49a59d3a6bf11789c7a6b22f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94979dcc-a9c3-41a5-a731-31d54ad85a5b", "node_type": "1", "metadata": {}, "hash": "4cc770f0f039c3cec2aeaa19fe18b624314cd789cdca471ce33d9dca9d41efa0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e", "node_type": "1", "metadata": {}, "hash": "718e2dcdd8d1aa111011d3ce529dc7010abf815229ba66d9395fcf924bf0f38a", "class_name": "RelatedNodeInfo"}}, "hash": "aeaa2ed4d2e72c53c149d6c106703533379ee0167666fc448a5308f7e295e682", "text": "md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRetrieve relevant context for a question:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnodes = retriever.retrieve('Who is Paul Graham?')\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n> Note: To learn how to build an index, see Index\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can select the index-specific retriever class via `retriever_mode`.", "start_char_idx": 1572, "end_char_idx": 3166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94979dcc-a9c3-41a5-a731-31d54ad85a5b": {"__data__": {"id_": "94979dcc-a9c3-41a5-a731-31d54ad85a5b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e", "node_type": "1", "metadata": {}, "hash": "718e2dcdd8d1aa111011d3ce529dc7010abf815229ba66d9395fcf924bf0f38a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85014ae4-1598-4cbc-adb0-282440392bb0", "node_type": "1", "metadata": {}, "hash": "aeaa2ed4d2e72c53c149d6c106703533379ee0167666fc448a5308f7e295e682", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e", "node_type": "1", "metadata": {}, "hash": "718e2dcdd8d1aa111011d3ce529dc7010abf815229ba66d9395fcf924bf0f38a", "class_name": "RelatedNodeInfo"}}, "hash": "4cc770f0f039c3cec2aeaa19fe18b624314cd789cdca471ce33d9dca9d41efa0", "text": "For example, with a `ListIndex`:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = list_index.as_retriever(\n    retriever_mode='llm',\n)\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis creates a ListIndexLLMRetriever on top of the list index.\n\nSee **Retriever Modes** for a full list of (index-specific) retriever modes\nand the retriever classes they map to.", "start_char_idx": 3168, "end_char_idx": 4206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cc59327-43fd-46e2-b2d0-ec9c077beac5": {"__data__": {"id_": "2cc59327-43fd-46e2-b2d0-ec9c077beac5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "node_type": "1", "metadata": {}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1cc8bdd-50ee-41c8-8b3a-ab587e4cb9d5", "node_type": "1", "metadata": {}, "hash": "d6454df9562c24cb4a9084e8644ab5cccdba2c71e684eb46ac48b6cb888cb5c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "node_type": "1", "metadata": {}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "class_name": "RelatedNodeInfo"}}, "hash": "38e240a498f979439f24d35f2224ae467159772f41ab3e449146cc00e78e46b4", "text": "File Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\nhidden:\n---\nretriever_modes.md\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Configuring a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn the same way, you can pass kwargs to configure the selected retriever.\n> Note: take a look at the API reference for the selected retriever class' constructor parameters for a list of valid kwargs.", "start_char_idx": 0, "end_char_idx": 1009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1cc8bdd-50ee-41c8-8b3a-ab587e4cb9d5": {"__data__": {"id_": "b1cc8bdd-50ee-41c8-8b3a-ab587e4cb9d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "node_type": "1", "metadata": {}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cc59327-43fd-46e2-b2d0-ec9c077beac5", "node_type": "1", "metadata": {}, "hash": "38e240a498f979439f24d35f2224ae467159772f41ab3e449146cc00e78e46b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ef03dc7-3c9d-40db-9e66-e4473620b4b1", "node_type": "1", "metadata": {}, "hash": "ca1354f8e51336ccb860f8be13ea8217d1b6108b5931aa19caee2e546aa373af", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "node_type": "1", "metadata": {}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "class_name": "RelatedNodeInfo"}}, "hash": "d6454df9562c24cb4a9084e8644ab5cccdba2c71e684eb46ac48b6cb888cb5c3", "text": "For example, if we selected the \"llm\" retriever mode, we might do the following:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Configuring a Retriever\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = list_index.as_retriever(\n    retriever_mode='llm',\n    choice_batch_size=5,\n)\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can use the low-level composition API if you need more granular control.", "start_char_idx": 1011, "end_char_idx": 2026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ef03dc7-3c9d-40db-9e66-e4473620b4b1": {"__data__": {"id_": "2ef03dc7-3c9d-40db-9e66-e4473620b4b1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "node_type": "1", "metadata": {}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1cc8bdd-50ee-41c8-8b3a-ab587e4cb9d5", "node_type": "1", "metadata": {}, "hash": "d6454df9562c24cb4a9084e8644ab5cccdba2c71e684eb46ac48b6cb888cb5c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a14e5911-54bd-4998-aac3-9591f2f97113", "node_type": "1", "metadata": {}, "hash": "5da99b9720b492903aa20ebff30c68ff22faaa0d44b2198700d3bc908b56f54e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "node_type": "1", "metadata": {}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "class_name": "RelatedNodeInfo"}}, "hash": "ca1354f8e51336ccb860f8be13ea8217d1b6108b5931aa19caee2e546aa373af", "text": "To achieve the same outcome as above, you can directly import and construct the desired retriever class:\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Low-Level Composition API\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.list import ListIndexLLMRetriever\n\nretriever = ListIndexLLMRetriever(\n    index=list_index,\n    choice_batch_size=5,\n)\n\nFile Name: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/High-Level API/Advanced\nfile_path: Docs\\core_modules\\query_modules\\retriever\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 1874\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\nDefine Custom Retriever </examples/query_engine/CustomRetrievers.ipynb>\n```\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports integrations with output parsing modules offered\nby other frameworks.", "start_char_idx": 2030, "end_char_idx": 3585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a14e5911-54bd-4998-aac3-9591f2f97113": {"__data__": {"id_": "a14e5911-54bd-4998-aac3-9591f2f97113", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "node_type": "1", "metadata": {}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ef03dc7-3c9d-40db-9e66-e4473620b4b1", "node_type": "1", "metadata": {}, "hash": "ca1354f8e51336ccb860f8be13ea8217d1b6108b5931aa19caee2e546aa373af", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "node_type": "1", "metadata": {}, "hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "class_name": "RelatedNodeInfo"}}, "hash": "5da99b9720b492903aa20ebff30c68ff22faaa0d44b2198700d3bc908b56f54e", "text": "These output parsing modules can be used in the following ways:\n- To provide formatting instructions for any prompt / query (through `output_parser.format`)\n- To provide \"parsing\" for LLM outputs (through `output_parser.parse`)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Guardrails\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGuardrails is an open-source Python package for specification/validation/correction of output schemas. See below for a code example.", "start_char_idx": 3586, "end_char_idx": 4320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "038a69b4-043e-45c7-9ce7-aea38a9c30ae": {"__data__": {"id_": "038a69b4-043e-45c7-9ce7-aea38a9c30ae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9450061-cd10-4103-8133-a4093c11c82f", "node_type": "1", "metadata": {}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30d4603a-906a-457a-8787-b958f30381a3", "node_type": "1", "metadata": {}, "hash": "e83a7c0c248e85447e8aa15198840ce72d531d1604ae2a1d6a2ea0f9983bf5de", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f9450061-cd10-4103-8133-a4093c11c82f", "node_type": "1", "metadata": {}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "class_name": "RelatedNodeInfo"}}, "hash": "bf85aace88fc80e444b181f21ca8f49aa42fcff35ea72d60849a1e2db3ac2847", "text": "See below for a code example.\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Guardrails\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers import GuardrailsOutputParser\nfrom llama_index.llm_predictor import StructuredLLMPredictor\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/load documents, build index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex(documents, chunk_size=512)\nllm_predictor = StructuredLLMPredictor()\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define query / output spec\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nrail_spec = (\"\"\"\n<rail version=\"0.1\">\n\n<output>\n    <list name=\"points\" description=\"Bullet points regarding events in the author's life.", "start_char_idx": 0, "end_char_idx": 1856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30d4603a-906a-457a-8787-b958f30381a3": {"__data__": {"id_": "30d4603a-906a-457a-8787-b958f30381a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9450061-cd10-4103-8133-a4093c11c82f", "node_type": "1", "metadata": {}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "038a69b4-043e-45c7-9ce7-aea38a9c30ae", "node_type": "1", "metadata": {}, "hash": "bf85aace88fc80e444b181f21ca8f49aa42fcff35ea72d60849a1e2db3ac2847", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a99b73bc-0a2a-487c-b417-566d106bfbe3", "node_type": "1", "metadata": {}, "hash": "998a0393e6dcb8811fe8619711877c83c1eeaa67ecd585face46046d1019662f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f9450061-cd10-4103-8133-a4093c11c82f", "node_type": "1", "metadata": {}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "class_name": "RelatedNodeInfo"}}, "hash": "e83a7c0c248e85447e8aa15198840ce72d531d1604ae2a1d6a2ea0f9983bf5de", "text": "\">\n        <object>\n            <string name=\"explanation\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation2\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation3\" format=\"one-line\" on-fail-one-line=\"noop\" />\n        </object>\n    </list>\n</output>\n\n<prompt>\n\nQuery string here.\n\n@xml_prefix_prompt\n\n{output_schema}\n\n@json_suffix_prompt_v2_wo_none\n</prompt>\n</rail>\n\"\"\")\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output parser\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\noutput_parser = GuardrailsOutputParser.from_rail_string(rail_spec, llm=llm_predictor.llm)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/format each prompt with output parser instructions\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\n\nqa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.", "start_char_idx": 1856, "end_char_idx": 3730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a99b73bc-0a2a-487c-b417-566d106bfbe3": {"__data__": {"id_": "a99b73bc-0a2a-487c-b417-566d106bfbe3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9450061-cd10-4103-8133-a4093c11c82f", "node_type": "1", "metadata": {}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30d4603a-906a-457a-8787-b958f30381a3", "node_type": "1", "metadata": {}, "hash": "e83a7c0c248e85447e8aa15198840ce72d531d1604ae2a1d6a2ea0f9983bf5de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14292a30-78f8-4036-bb75-12df73a72eed", "node_type": "1", "metadata": {}, "hash": "7453d184385fad3c9a03355ba1e752ec1f284207db45229ef0e333bcdc0c1066", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f9450061-cd10-4103-8133-a4093c11c82f", "node_type": "1", "metadata": {}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "class_name": "RelatedNodeInfo"}}, "hash": "998a0393e6dcb8811fe8619711877c83c1eeaa67ecd585face46046d1019662f", "text": "md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    service_context=ServiceContext.from_defaults(\n        llm_predictor=llm_predictor\n    ),\n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n)\nresponse = query_engine.query(\n    \"What are the three items the author did growing up?", "start_char_idx": 3702, "end_char_idx": 4143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14292a30-78f8-4036-bb75-12df73a72eed": {"__data__": {"id_": "14292a30-78f8-4036-bb75-12df73a72eed", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9450061-cd10-4103-8133-a4093c11c82f", "node_type": "1", "metadata": {}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a99b73bc-0a2a-487c-b417-566d106bfbe3", "node_type": "1", "metadata": {}, "hash": "998a0393e6dcb8811fe8619711877c83c1eeaa67ecd585face46046d1019662f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f9450061-cd10-4103-8133-a4093c11c82f", "node_type": "1", "metadata": {}, "hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "class_name": "RelatedNodeInfo"}}, "hash": "7453d184385fad3c9a03355ba1e752ec1f284207db45229ef0e333bcdc0c1066", "text": "query(\n    \"What are the three items the author did growing up?\", \n)\nprint(response)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOutput:\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n{'points': [{'explanation': 'Writing short stories', 'explanation2': 'Programming on an IBM 1401', 'explanation3': 'Using microcomputers'}]}\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Langchain\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLangchain also offers output parsing modules that you can use within LlamaIndex.", "start_char_idx": 4080, "end_char_idx": 5551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2e09c03-26b5-4fad-8415-c20bd10525d2": {"__data__": {"id_": "f2e09c03-26b5-4fad-8415-c20bd10525d2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82d6e145-bda6-4955-8139-0ebd6de31919", "node_type": "1", "metadata": {}, "hash": "1f082b70c7ec1e1691b6db6e13e333bcdccd65d4f36b4af8a1128f1c4c5f9efe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3eb032c3-57ef-4e7a-af31-7b7df20da4a3", "node_type": "1", "metadata": {}, "hash": "64509cc0262dc5a1df2f191b79698a96731c6593e1ce1b46f90a72040afdcc99", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "82d6e145-bda6-4955-8139-0ebd6de31919", "node_type": "1", "metadata": {}, "hash": "1f082b70c7ec1e1691b6db6e13e333bcdccd65d4f36b4af8a1128f1c4c5f9efe", "class_name": "RelatedNodeInfo"}}, "hash": "1a454bc8220d8464335a868b75fd94d5452bf18553415d20b4f354e41693364e", "text": "File Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Langchain\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers import LangchainOutputParser\nfrom llama_index.llm_predictor import StructuredLLMPredictor\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/load documents, build index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nllm_predictor = StructuredLLMPredictor()\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output schema\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse_schemas = [\n    ResponseSchema(name=\"Education\", description=\"Describes the author's educational experience/background.\"),\n    ResponseSchema(name=\"Work\", description=\"Describes the author's work experience/background.\")", "start_char_idx": 0, "end_char_idx": 1984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3eb032c3-57ef-4e7a-af31-7b7df20da4a3": {"__data__": {"id_": "3eb032c3-57ef-4e7a-af31-7b7df20da4a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82d6e145-bda6-4955-8139-0ebd6de31919", "node_type": "1", "metadata": {}, "hash": "1f082b70c7ec1e1691b6db6e13e333bcdccd65d4f36b4af8a1128f1c4c5f9efe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2e09c03-26b5-4fad-8415-c20bd10525d2", "node_type": "1", "metadata": {}, "hash": "1a454bc8220d8464335a868b75fd94d5452bf18553415d20b4f354e41693364e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ae5f656-cd55-48e5-abc6-78b2ff30365c", "node_type": "1", "metadata": {}, "hash": "65e778f862369d124bd7ba3ab08b0dbbc23eecceecf1486fa611829de250eb57", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "82d6e145-bda6-4955-8139-0ebd6de31919", "node_type": "1", "metadata": {}, "hash": "1f082b70c7ec1e1691b6db6e13e333bcdccd65d4f36b4af8a1128f1c4c5f9efe", "class_name": "RelatedNodeInfo"}}, "hash": "64509cc0262dc5a1df2f191b79698a96731c6593e1ce1b46f90a72040afdcc99", "text": "\"),\n    ResponseSchema(name=\"Work\", description=\"Describes the author's work experience/background.\")\n]\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output parser\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nlc_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\noutput_parser = LangchainOutputParser(lc_output_parser)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/format each prompt with output parser instructions\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\nqa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(\n    service_context=ServiceContext.from_defaults(\n        llm_predictor=llm_predictor\n    ),\n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n)\nresponse = query_engine.query(\n    \"What are a few things the author did growing up?", "start_char_idx": 1883, "end_char_idx": 3865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ae5f656-cd55-48e5-abc6-78b2ff30365c": {"__data__": {"id_": "4ae5f656-cd55-48e5-abc6-78b2ff30365c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82d6e145-bda6-4955-8139-0ebd6de31919", "node_type": "1", "metadata": {}, "hash": "1f082b70c7ec1e1691b6db6e13e333bcdccd65d4f36b4af8a1128f1c4c5f9efe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3eb032c3-57ef-4e7a-af31-7b7df20da4a3", "node_type": "1", "metadata": {}, "hash": "64509cc0262dc5a1df2f191b79698a96731c6593e1ce1b46f90a72040afdcc99", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "82d6e145-bda6-4955-8139-0ebd6de31919", "node_type": "1", "metadata": {}, "hash": "1f082b70c7ec1e1691b6db6e13e333bcdccd65d4f36b4af8a1128f1c4c5f9efe", "class_name": "RelatedNodeInfo"}}, "hash": "65e778f862369d124bd7ba3ab08b0dbbc23eecceecf1486fa611829de250eb57", "text": "\", \n)\nprint(str(response))\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOutput:\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n{'Education': 'Before college, the author wrote short stories and experimented with programming on an IBM 1401.', 'Work': 'The author worked on writing and programming outside of school.'}", "start_char_idx": 3865, "end_char_idx": 4838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88eba848-fa30-49f3-8783-7ae042d90552": {"__data__": {"id_": "88eba848-fa30-49f3-8783-7ae042d90552", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "node_type": "1", "metadata": {}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98f295d9-66cf-4771-88ea-20417403d118", "node_type": "1", "metadata": {}, "hash": "cc3298f6f7b7039b9a5b3fd9e41cfb4ae2ab7ed95f464759afb93f8ee976a5dd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "node_type": "1", "metadata": {}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "class_name": "RelatedNodeInfo"}}, "hash": "e7aa315076f0e4b6a745878458d312703806d58c7bef4c832cd2c9e1a44056be", "text": "', 'Work': 'The author worked on writing and programming outside of school.'}\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nContent Type: code\nHeader Path: Output Parsing/Guides\nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md\nfile_name: output_parser.md\nfile_type: None\nfile_size: 5259\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\n\n/examples/output_parsing/GuardrailsDemo.ipynb\n/examples/output_parsing/LangchainOutputParserDemo.ipynb\n/examples/output_parsing/guidance_pydantic_program.ipynb\n/examples/output_parsing/guidance_sub_question.ipynb\n/examples/output_parsing/openai_pydantic_program.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nContent Type: text\nHeader Path: Pydantic Program\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nfile_name: pydantic_program.md\nfile_type: None\nfile_size: 1271\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA pydantic program is a generic abstraction that takes in an input string and converts it to a structured Pydantic object type.\n\nBecause this abstraction is so generic, it encompasses a broad range of LLM workflows. The programs are composable and be for more generic or specific use cases.\n\nThere's a few general types of Pydantic Programs:\n- **LLM Text Completion Pydantic Programs**: These convert input text into a user-specified structured object through a text completion API + output parsing.\n- **LLM Function Calling Pydantic Program**: These convert input text into a user-specified structured object through an LLM function calling API.\n- **Prepackaged Pydantic Programs**: These convert input text into prespecified structured objects.", "start_char_idx": 0, "end_char_idx": 1884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98f295d9-66cf-4771-88ea-20417403d118": {"__data__": {"id_": "98f295d9-66cf-4771-88ea-20417403d118", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "node_type": "1", "metadata": {}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88eba848-fa30-49f3-8783-7ae042d90552", "node_type": "1", "metadata": {}, "hash": "e7aa315076f0e4b6a745878458d312703806d58c7bef4c832cd2c9e1a44056be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60c02106-6a4a-482c-ab2a-8df3532b474b", "node_type": "1", "metadata": {}, "hash": "2b2b42c7e00665eedb74ec681242bc68c59df1bdd4c6de69fb2845805eb80a64", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "node_type": "1", "metadata": {}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "class_name": "RelatedNodeInfo"}}, "hash": "cc3298f6f7b7039b9a5b3fd9e41cfb4ae2ab7ed95f464759afb93f8ee976a5dd", "text": "- **Prepackaged Pydantic Programs**: These convert input text into prespecified structured objects.\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nContent Type: text\nHeader Path: Pydantic Program/LLM Text Completion Pydantic Programs\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nfile_name: pydantic_program.md\nfile_type: None\nfile_size: 1271\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTODO: Coming soon!", "start_char_idx": 1785, "end_char_idx": 2315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60c02106-6a4a-482c-ab2a-8df3532b474b": {"__data__": {"id_": "60c02106-6a4a-482c-ab2a-8df3532b474b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "node_type": "1", "metadata": {}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98f295d9-66cf-4771-88ea-20417403d118", "node_type": "1", "metadata": {}, "hash": "cc3298f6f7b7039b9a5b3fd9e41cfb4ae2ab7ed95f464759afb93f8ee976a5dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "727954f9-af6d-4d68-a515-0e1b00bbc39f", "node_type": "1", "metadata": {}, "hash": "f021cf69754f7233064f912f4acdbf21f0e2e5233c47370548a05784437ed75d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "node_type": "1", "metadata": {}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "class_name": "RelatedNodeInfo"}}, "hash": "2b2b42c7e00665eedb74ec681242bc68c59df1bdd4c6de69fb2845805eb80a64", "text": "File Name: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nContent Type: code\nHeader Path: Pydantic Program/LLM Function Calling Pydantic Programs\nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nfile_name: pydantic_program.md\nfile_type: None\nfile_size: 1271\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/output_parsing/openai_pydantic_program.ipynb\n/examples/output_parsing/guidance_pydantic_program.ipynb\n/examples/output_parsing/guidance_sub_question.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nContent Type: code\nHeader Path: Pydantic Program/Prepackaged Pydantic Programs\nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md\nfile_name: pydantic_program.md\nfile_type: None\nfile_size: 1271\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/output_parsing/df_program.ipynb\n/examples/output_parsing/evaporate_program.ipynb\n```\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nContent Type: text\nHeader Path: Structured Outputs\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2489\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe ability of LLMs to produce structured outputs are important for downstream applications that rely on reliably parsing output values. \nLlamaIndex itself also relies on structured output in the following ways.\n- **Document retrieval**: Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval.", "start_char_idx": 2317, "end_char_idx": 4128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "727954f9-af6d-4d68-a515-0e1b00bbc39f": {"__data__": {"id_": "727954f9-af6d-4d68-a515-0e1b00bbc39f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "node_type": "1", "metadata": {}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60c02106-6a4a-482c-ab2a-8df3532b474b", "node_type": "1", "metadata": {}, "hash": "2b2b42c7e00665eedb74ec681242bc68c59df1bdd4c6de69fb2845805eb80a64", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "node_type": "1", "metadata": {}, "hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "class_name": "RelatedNodeInfo"}}, "hash": "f021cf69754f7233064f912f4acdbf21f0e2e5233c47370548a05784437ed75d", "text": "For instance, the tree index expects LLM calls to be in the format \"ANSWER: (number)\".\n- **Response synthesis**: Users may expect that the final response contains some degree of structure (e.g. a JSON output, a formatted SQL query, etc.)\n\nLlamaIndex provides a variety of modules enabling LLMs to produce outputs in a structured format. We provide modules at different levels of abstraction:\n- **Output Parsers**: These are modules that operate before and after an LLM text completion endpoint. They are not used with LLM function calling endpoints (since those contain structured outputs out of the box).\n- **Pydantic Programs**: These are generic modules that map an input prompt to a structured output, represented by a Pydantic object. They may use function calling APIs or text completion APIs + output parsers.\n- **Pre-defined Pydantic Program**: We have pre-defined Pydantic programs that map inputs to specific output types (like dataframes).\n\nSee the sections below for an overview of output parsers and Pydantic programs.\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nContent Type: text\nHeader Path: Structured Outputs/\ud83d\udd2c Anatomy of a Structured Output Function\nLinks: \nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2489\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere we describe the different components of an LLM-powered structured output function. The pipeline depends on whether you're using a **generic LLM text completion API** or an **LLM function calling API**.\n\n!\n\nWith generic completion APIs, the inputs and outputs are handled by text prompts. The output parser plays a role before and after the LLM call in ensuring structured outputs. Before the LLM call, the output parser can\nappend format instructions to the prompt.", "start_char_idx": 4129, "end_char_idx": 6013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d87db227-b408-45bb-acb5-86b12488fd18": {"__data__": {"id_": "d87db227-b408-45bb-acb5-86b12488fd18", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2", "node_type": "1", "metadata": {}, "hash": "c61b7484b150d732bcb42e7c9074f65e085dce210897002e1e3a04cd47c80ce3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3daabe54-04ed-4728-bff1-58bd74f9cdb9", "node_type": "1", "metadata": {}, "hash": "17f7fdcb725494ecda3a6faa05222bbb7ec97687f17bf6d0e37901a9f565bbf0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2", "node_type": "1", "metadata": {}, "hash": "c61b7484b150d732bcb42e7c9074f65e085dce210897002e1e3a04cd47c80ce3", "class_name": "RelatedNodeInfo"}}, "hash": "3044ef47fd268ca12c96f7f62f9f70dcbc0567e9a6d4149de8bc9c0391c193ec", "text": "Before the LLM call, the output parser can\nappend format instructions to the prompt. After the LLM call, the output parser can parse the output to the specified instructions.\n\nWith function calling APIs, the output is inherently in a structured format, and the input can take in the signature of the desired object. The structured output just needs to be cast in the right object format (e.g. Pydantic).\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nContent Type: code\nHeader Path: Structured Outputs/Output Parser Modules\nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2489\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\noutput_parser.md\n```\n\nFile Name: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nContent Type: code\nHeader Path: Structured Outputs/Pydantic Program Modules\nfile_path: Docs\\core_modules\\query_modules\\structured_outputs\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2489\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 2\n---\npydantic_program.md\n```\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nContent Type: text\nHeader Path: Callbacks/Concept\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2492\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library. \nUsing the callback manager, as many callbacks as needed can be added.\n\nIn addition to logging data related to events, you can also track the duration and number of occurances\nof each event.", "start_char_idx": 0, "end_char_idx": 1844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3daabe54-04ed-4728-bff1-58bd74f9cdb9": {"__data__": {"id_": "3daabe54-04ed-4728-bff1-58bd74f9cdb9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2", "node_type": "1", "metadata": {}, "hash": "c61b7484b150d732bcb42e7c9074f65e085dce210897002e1e3a04cd47c80ce3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d87db227-b408-45bb-acb5-86b12488fd18", "node_type": "1", "metadata": {}, "hash": "3044ef47fd268ca12c96f7f62f9f70dcbc0567e9a6d4149de8bc9c0391c193ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8120e5b1-0e04-4361-acc5-40389a3d4a23", "node_type": "1", "metadata": {}, "hash": "a5107b2740f71571e7963e40b36c293d0db2b13b35cbfd0d5ef13d6467cb5bdb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2", "node_type": "1", "metadata": {}, "hash": "c61b7484b150d732bcb42e7c9074f65e085dce210897002e1e3a04cd47c80ce3", "class_name": "RelatedNodeInfo"}}, "hash": "17f7fdcb725494ecda3a6faa05222bbb7ec97687f17bf6d0e37901a9f565bbf0", "text": "Furthermore, a trace map of events is also recorded, and callbacks can use this data\nhowever they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events\nafter most operations.\n\n**Callback Event Types**  \nWhile each callback may not leverage each event type, the following events are available to be tracked:\n\n- `CHUNKING` -> Logs for the before and after of text splitting.\n- `NODE_PARSING` -> Logs for the documents and the nodes that they are parsed into.\n- `EMBEDDING` -> Logs for the number of texts embedded.\n- `LLM` -> Logs for the template and response of LLM calls.\n- `QUERY` -> Keeps track of the start and end of each query.\n- `RETRIEVE` -> Logs for the nodes retrieved for a query.\n- `SYNTHESIZE` -> Logs for the result for synthesize calls.\n- `TREE` -> Logs for the summary and level of summaries generated.\n- `SUB_QUESTIONS` -> Logs for the sub questions and answers generated.\n\nYou can implement your own callback to track and trace these events, or use an existing callback.\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nContent Type: text\nHeader Path: Callbacks/Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2492\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCurrently supported callbacks are as follows:\n\n- TokenCountingHandler -> Flexible token counting for prompt, completion, and embedding token usage. See the migration details here\n- LlamaDebugHanlder -> Basic tracking and tracing for events. Example usage can be found in the notebook below.\n- WandbCallbackHandler -> Tracking of events and traces using the Wandb Prompts frontend. More details are in the notebook below or at Wandb\n- AimCallback -> Tracking of LLM inputs and outputs. Example usage can be found in the notebook below.", "start_char_idx": 1847, "end_char_idx": 3734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8120e5b1-0e04-4361-acc5-40389a3d4a23": {"__data__": {"id_": "8120e5b1-0e04-4361-acc5-40389a3d4a23", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2", "node_type": "1", "metadata": {}, "hash": "c61b7484b150d732bcb42e7c9074f65e085dce210897002e1e3a04cd47c80ce3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3daabe54-04ed-4728-bff1-58bd74f9cdb9", "node_type": "1", "metadata": {}, "hash": "17f7fdcb725494ecda3a6faa05222bbb7ec97687f17bf6d0e37901a9f565bbf0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2", "node_type": "1", "metadata": {}, "hash": "c61b7484b150d732bcb42e7c9074f65e085dce210897002e1e3a04cd47c80ce3", "class_name": "RelatedNodeInfo"}}, "hash": "a5107b2740f71571e7963e40b36c293d0db2b13b35cbfd0d5ef13d6467cb5bdb", "text": "Example usage can be found in the notebook below.\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nContent Type: text\nHeader Path: Callbacks/Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2492\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\nhidden:\n---\n/examples/callbacks/TokenCountingHandler.ipynb\n/examples/callbacks/LlamaDebugHandler.ipynb\n/examples/callbacks/WandbCallbackHandler.ipynb\n/examples/callbacks/AimCallback.ipynb\ntoken_counting_migration.md\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: Token Counting - Migration Guide\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe existing token counting implementation has been __deprecated__. \n\nWe know token counting is important to many users, so this guide was created to walkthrough a (hopefully painless) transition. \n\nPreviously, token counting was kept track of on the `llm_predictor` and `embed_model` objects directly, and optionally printed to the console. This implementation used a static tokenizer for token counting (gpt-2), and the `last_token_usage` and `total_token_usage` attributes were not always kept track of properly.\n\nGoing forward, token counting as moved into a callback. Using the `TokenCountingHandler` callback, you now have more options for how tokens are counted, the lifetime of the token counts, and even creating separete token counters for different indexes.", "start_char_idx": 3685, "end_char_idx": 5472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70840c13-c0a8-4197-acfc-40bea23adbe0": {"__data__": {"id_": "70840c13-c0a8-4197-acfc-40bea23adbe0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9", "node_type": "1", "metadata": {}, "hash": "189ae8fea1540694ea3ba1fda1c13a9104e66df13d9d11990c18ae003ada3c9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83722899-33b6-4e87-9f1a-884ed7627d55", "node_type": "1", "metadata": {}, "hash": "98de344efb058cb74b019867ee8ceb9432e31dc2596805e3fa33c27e447658a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9", "node_type": "1", "metadata": {}, "hash": "189ae8fea1540694ea3ba1fda1c13a9104e66df13d9d11990c18ae003ada3c9d", "class_name": "RelatedNodeInfo"}}, "hash": "954c6c872ea77ad559408f33e34bb054714d6d7ac7233ef1460bef441ea17ddc", "text": "Here is a minimum example of using the new `TokenCountingHandler` with an OpenAI model:\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: Token Counting - Migration Guide\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport tiktoken\nfrom llama_index.callbacks import CallbackManager, TokenCountingHandler\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: NOTE: The tokenizer should be a function that takes in text and returns a list of tokens\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\"text-davinci-003\").encode\n    verbose=False  # set to true to see usage printed to the console\n)\n\ncallback_manager = CallbackManager([token_counter])\n\nservice_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n\ndocument = SimpleDirectoryReader(\"./data\").load_data()\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: if verbose is turned on, you will see embedding token usage printed\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents,", "start_char_idx": 0, "end_char_idx": 1985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83722899-33b6-4e87-9f1a-884ed7627d55": {"__data__": {"id_": "83722899-33b6-4e87-9f1a-884ed7627d55", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9", "node_type": "1", "metadata": {}, "hash": "189ae8fea1540694ea3ba1fda1c13a9104e66df13d9d11990c18ae003ada3c9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70840c13-c0a8-4197-acfc-40bea23adbe0", "node_type": "1", "metadata": {}, "hash": "954c6c872ea77ad559408f33e34bb054714d6d7ac7233ef1460bef441ea17ddc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddd83ffd-eabe-4b99-90af-124ba80a5438", "node_type": "1", "metadata": {}, "hash": "2a75f2713160344c4f73e1ccad485b3d6b9f6246342db2da46f018dd935d94d6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9", "node_type": "1", "metadata": {}, "hash": "189ae8fea1540694ea3ba1fda1c13a9104e66df13d9d11990c18ae003ada3c9d", "class_name": "RelatedNodeInfo"}}, "hash": "98de344efb058cb74b019867ee8ceb9432e31dc2596805e3fa33c27e447658a5", "text": "from_documents(documents, service_context=service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: otherwise, you can access the count directly\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprint(token_counter.total_embedding_token_count)\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: reset the counts at your discretion!\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntoken_counter.reset_counts()\n\nFile Name: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nContent Type: text\nHeader Path: also track prompt, completion, and total LLM tokens, in addition to embeddings\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md\nfile_name: token_counting_migration.md\nfile_type: None\nfile_size: 2503\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = index.as_query_engine().query(\"What did the author do growing up?\")", "start_char_idx": 1960, "end_char_idx": 3455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddd83ffd-eabe-4b99-90af-124ba80a5438": {"__data__": {"id_": "ddd83ffd-eabe-4b99-90af-124ba80a5438", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9", "node_type": "1", "metadata": {}, "hash": "189ae8fea1540694ea3ba1fda1c13a9104e66df13d9d11990c18ae003ada3c9d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83722899-33b6-4e87-9f1a-884ed7627d55", "node_type": "1", "metadata": {}, "hash": "98de344efb058cb74b019867ee8ceb9432e31dc2596805e3fa33c27e447658a5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9", "node_type": "1", "metadata": {}, "hash": "189ae8fea1540694ea3ba1fda1c13a9104e66df13d9d11990c18ae003ada3c9d", "class_name": "RelatedNodeInfo"}}, "hash": "2a75f2713160344c4f73e1ccad485b3d6b9f6246342db2da46f018dd935d94d6", "text": "print('Embedding Tokens: ', token_counter.total_embedding_token_count, '\\n',\n      'LLM Prompt Tokens: ', token_counter.prompt_llm_token_count, '\\n',\n      'LLM Completion Tokens: ', token_counter.completion_llm_token_count, '\\n',\n      'Total LLM Token Count: ', token_counter.total_llm_token_count)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEach call to an LLM will cost some amount of money - for instance, OpenAI's gpt-3.5-turbo costs $0.002 / 1k tokens. The cost of building an index and querying depends on \n\n- the type of LLM used\n- the type of data structure used\n- parameters used during building \n- parameters used during querying\n\nThe cost of building and querying each index is a TODO in the reference documentation. In the meantime, we provide the following information:\n\n1. A high-level overview of the cost structure of the indices.\n2. A token predictor that you can use directly within LlamaIndex!", "start_char_idx": 3456, "end_char_idx": 4670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8c86f8b-5eb4-45ab-a1ec-52fd2593b30f": {"__data__": {"id_": "b8c86f8b-5eb4-45ab-a1ec-52fd2593b30f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484", "node_type": "1", "metadata": {}, "hash": "80e78fffd7b5fa2cbf81c024a795f520cfdc746c6d7b13f609dcc1ed7ba0e3cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb9484de-8627-4501-906e-291343164f4b", "node_type": "1", "metadata": {}, "hash": "3489a5933e175a2d830ad71908d55e63497c60412eff15669bd9ff629cd9289a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484", "node_type": "1", "metadata": {}, "hash": "80e78fffd7b5fa2cbf81c024a795f520cfdc746c6d7b13f609dcc1ed7ba0e3cb", "class_name": "RelatedNodeInfo"}}, "hash": "c05581f92dfd81f82818e047c3807c9cf33cb3d809c6ae156017f24f987821e5", "text": "2. A token predictor that you can use directly within LlamaIndex!\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Indices with no LLM calls\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe following indices don't require LLM calls at all during building (0 cost):\n- `ListIndex`\n- `SimpleKeywordTableIndex` - uses a regex keyword extractor to extract keywords from each document\n- `RAKEKeywordTableIndex` - uses a RAKE keyword extractor to extract keywords from each document\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Indices with LLM calls\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe following indices do require LLM calls during build time:\n- `TreeIndex` - use LLM to hierarchically summarize the text to build the tree\n- `KeywordTableIndex` - use LLM to extract keywords from each document\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Query Time\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThere will always be >= 1 LLM call during query time, in order to synthesize the final answer. \nSome indices contain cost tradeoffs between index building and querying.", "start_char_idx": 0, "end_char_idx": 1906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb9484de-8627-4501-906e-291343164f4b": {"__data__": {"id_": "cb9484de-8627-4501-906e-291343164f4b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484", "node_type": "1", "metadata": {}, "hash": "80e78fffd7b5fa2cbf81c024a795f520cfdc746c6d7b13f609dcc1ed7ba0e3cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8c86f8b-5eb4-45ab-a1ec-52fd2593b30f", "node_type": "1", "metadata": {}, "hash": "c05581f92dfd81f82818e047c3807c9cf33cb3d809c6ae156017f24f987821e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc8303a3-e113-46f4-a075-42b6a782e8e3", "node_type": "1", "metadata": {}, "hash": "36b62737306258159a4aadf4a41a54dc948042c5f33b9dd3ebe535dcd85dba61", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484", "node_type": "1", "metadata": {}, "hash": "80e78fffd7b5fa2cbf81c024a795f520cfdc746c6d7b13f609dcc1ed7ba0e3cb", "class_name": "RelatedNodeInfo"}}, "hash": "3489a5933e175a2d830ad71908d55e63497c60412eff15669bd9ff629cd9289a", "text": "Some indices contain cost tradeoffs between index building and querying. `ListIndex`, for instance,\nis free to build, but running a query over a list index (without filtering or embedding lookups), will\ncall the LLM {math}`N` times.\n\nHere are some notes regarding each of the indices:\n- `ListIndex`: by default requires {math}`N` LLM calls, where N is the number of nodes.\n- `TreeIndex`: by default requires {math}`\\log (N)` LLM calls, where N is the number of leaf nodes. \n    - Setting `child_branch_factor=2` will be more expensive than the default `child_branch_factor=1` (polynomial vs logarithmic), because we traverse 2 children instead of just 1 for each parent node.\n- `KeywordTableIndex`: by default requires an LLM call to extract query keywords.\n    - Can do `index.as_retriever(retriever_mode=\"simple\")` or `index.as_retriever(retriever_mode=\"rake\")` to also use regex/RAKE keyword extractors on your query text.\n-  `VectorStoreIndex`: by default, requires one LLM call per query. If you increase the `similarity_top_k` or `chunk_size`, or change the `response_mode`, then this number will increase.\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex offers token **predictors** to predict token usage of LLM and embedding calls.\nThis allows you to estimate your costs during 1) index construction, and 2) index querying, before\nany respective LLM calls are made.\n\nTokens are counted using the `TokenCountingHandler` callback. See the example notebook for details on the setup.", "start_char_idx": 1834, "end_char_idx": 3668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc8303a3-e113-46f4-a075-42b6a782e8e3": {"__data__": {"id_": "cc8303a3-e113-46f4-a075-42b6a782e8e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484", "node_type": "1", "metadata": {}, "hash": "80e78fffd7b5fa2cbf81c024a795f520cfdc746c6d7b13f609dcc1ed7ba0e3cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb9484de-8627-4501-906e-291343164f4b", "node_type": "1", "metadata": {}, "hash": "3489a5933e175a2d830ad71908d55e63497c60412eff15669bd9ff629cd9289a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484", "node_type": "1", "metadata": {}, "hash": "80e78fffd7b5fa2cbf81c024a795f520cfdc746c6d7b13f609dcc1ed7ba0e3cb", "class_name": "RelatedNodeInfo"}}, "hash": "36b62737306258159a4aadf4a41a54dc948042c5f33b9dd3ebe535dcd85dba61", "text": "See the example notebook for details on the setup.\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Using MockLLM\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo predict token usage of LLM calls, import and instantiate the MockLLM as shown below. The `max_tokens` parameter is used as a \"worst case\" prediction, where each LLM response will contain exactly that number of tokens. If `max_tokens` is not specified, then it will simply predict back the prompt.", "start_char_idx": 3618, "end_char_idx": 4352, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36616633-4d3b-481b-ac71-c52d704cf5da": {"__data__": {"id_": "36616633-4d3b-481b-ac71-c52d704cf5da", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd01209c-9c3a-4eb0-8594-239574f103e7", "node_type": "1", "metadata": {}, "hash": "7d811f93b41cad65008c0176862140e8e18267816f04ecf320884101d51ea033", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa7f8c98-34f1-4ec6-9841-cbce006ebae2", "node_type": "1", "metadata": {}, "hash": "a527c81ea27be46ad52da3d255cda4ad3689a1b296648093eb684dbbebe0ae8d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bd01209c-9c3a-4eb0-8594-239574f103e7", "node_type": "1", "metadata": {}, "hash": "7d811f93b41cad65008c0176862140e8e18267816f04ecf320884101d51ea033", "class_name": "RelatedNodeInfo"}}, "hash": "8da9dbab48d5b0c6b80240c50c128bf7f49db91cf181bea650653424458d3d84", "text": "If `max_tokens` is not specified, then it will simply predict back the prompt.\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Using MockLLM\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, set_global_service_context\nfrom llama_index.llms import MockLLM\n\nllm = MockLLM(max_tokens=256)\n\nservice_context = ServiceContext.from_defaults(llm=llm)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/optionally set a global service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nset_global_service_context(service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/optionally set a global service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can then use this predictor during both index construction and querying.", "start_char_idx": 0, "end_char_idx": 1595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa7f8c98-34f1-4ec6-9841-cbce006ebae2": {"__data__": {"id_": "aa7f8c98-34f1-4ec6-9841-cbce006ebae2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd01209c-9c3a-4eb0-8594-239574f103e7", "node_type": "1", "metadata": {}, "hash": "7d811f93b41cad65008c0176862140e8e18267816f04ecf320884101d51ea033", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36616633-4d3b-481b-ac71-c52d704cf5da", "node_type": "1", "metadata": {}, "hash": "8da9dbab48d5b0c6b80240c50c128bf7f49db91cf181bea650653424458d3d84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1efbc0eb-eb89-47db-8965-39835610c7bb", "node_type": "1", "metadata": {}, "hash": "bd14bd05180dcc08fa90591e1d213be7b63a90e6b384c6740b283f7089106d79", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bd01209c-9c3a-4eb0-8594-239574f103e7", "node_type": "1", "metadata": {}, "hash": "7d811f93b41cad65008c0176862140e8e18267816f04ecf320884101d51ea033", "class_name": "RelatedNodeInfo"}}, "hash": "a527c81ea27be46ad52da3d255cda4ad3689a1b296648093eb684dbbebe0ae8d", "text": "File Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Using MockEmbedding\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou may also predict the token usage of embedding calls with `MockEmbedding`.\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Using MockEmbedding\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, set_global_service_context\nfrom llama_index import MockEmbedding\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/specify a MockLLMPredictor\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nembed_model = MockEmbedding(embed_dim=1536)\n\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/optionally set a global service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.", "start_char_idx": 1597, "end_char_idx": 3359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1efbc0eb-eb89-47db-8965-39835610c7bb": {"__data__": {"id_": "1efbc0eb-eb89-47db-8965-39835610c7bb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd01209c-9c3a-4eb0-8594-239574f103e7", "node_type": "1", "metadata": {}, "hash": "7d811f93b41cad65008c0176862140e8e18267816f04ecf320884101d51ea033", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa7f8c98-34f1-4ec6-9841-cbce006ebae2", "node_type": "1", "metadata": {}, "hash": "a527c81ea27be46ad52da3d255cda4ad3689a1b296648093eb684dbbebe0ae8d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bd01209c-9c3a-4eb0-8594-239574f103e7", "node_type": "1", "metadata": {}, "hash": "7d811f93b41cad65008c0176862140e8e18267816f04ecf320884101d51ea033", "class_name": "RelatedNodeInfo"}}, "hash": "bd14bd05180dcc08fa90591e1d213be7b63a90e6b384c6740b283f7089106d79", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nset_global_service_context(service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRead about the full usage pattern below!\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nContent Type: text\nHeader Path: Cost Analysis/Concept/Overview of Cost Structure/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 4217\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\ncaption: Examples\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn order to measure LLM and Embedding token counts, you'll need to\n\n1.", "start_char_idx": 3340, "end_char_idx": 4865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c420b97-581e-4f93-a7da-d3b136554c80": {"__data__": {"id_": "2c420b97-581e-4f93-a7da-d3b136554c80", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39f0cb49-3072-4e99-824e-39b89862f502", "node_type": "1", "metadata": {}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f60cd002-f417-4f3f-a2bc-1a1666a150ec", "node_type": "1", "metadata": {}, "hash": "f64fe6946e6e33356ce362b42b0127d8c1fcd85d23e41f8a6e50cafc9c130b08", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "39f0cb49-3072-4e99-824e-39b89862f502", "node_type": "1", "metadata": {}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "class_name": "RelatedNodeInfo"}}, "hash": "f5fc8393dcd64e38a48252f96ead2f2d2aef6bacbeb13af5d86e1b78bde52963", "text": "Setup `MockLLM` and `MockEmbedding` objects\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.llms import MockLLM\nfrom llama_index import MockEmbedding\n\nllm = MockLLM(max_tokens=256)\nembed_model = MockEmbedding(embed_dim=1536)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n2.", "start_char_idx": 0, "end_char_idx": 1004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f60cd002-f417-4f3f-a2bc-1a1666a150ec": {"__data__": {"id_": "f60cd002-f417-4f3f-a2bc-1a1666a150ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39f0cb49-3072-4e99-824e-39b89862f502", "node_type": "1", "metadata": {}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c420b97-581e-4f93-a7da-d3b136554c80", "node_type": "1", "metadata": {}, "hash": "f5fc8393dcd64e38a48252f96ead2f2d2aef6bacbeb13af5d86e1b78bde52963", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b2f6c81-0a0f-4da3-8a2c-12c0ac7d710f", "node_type": "1", "metadata": {}, "hash": "6adca613739d7bbe325eb877c7165ae7761f46d27bcdaf0b6e55554a2c8f9180", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "39f0cb49-3072-4e99-824e-39b89862f502", "node_type": "1", "metadata": {}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "class_name": "RelatedNodeInfo"}}, "hash": "f64fe6946e6e33356ce362b42b0127d8c1fcd85d23e41f8a6e50cafc9c130b08", "text": "Setup the `TokenCountingCallback` handler\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport tiktoken\nfrom llama_index.callbacks import CallbackManager, TokenCountingHandler\n\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n)\n\ncallback_manager = CallbackManager([token_counter])\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n3.", "start_char_idx": 1005, "end_char_idx": 2105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b2f6c81-0a0f-4da3-8a2c-12c0ac7d710f": {"__data__": {"id_": "7b2f6c81-0a0f-4da3-8a2c-12c0ac7d710f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39f0cb49-3072-4e99-824e-39b89862f502", "node_type": "1", "metadata": {}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f60cd002-f417-4f3f-a2bc-1a1666a150ec", "node_type": "1", "metadata": {}, "hash": "f64fe6946e6e33356ce362b42b0127d8c1fcd85d23e41f8a6e50cafc9c130b08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c629b58-2558-4626-87d6-aca4236b12da", "node_type": "1", "metadata": {}, "hash": "7ab613def70f57af6de2c2ff0fafb53e2fdaf011e1dde6bd38d2fc7942259ef3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "39f0cb49-3072-4e99-824e-39b89862f502", "node_type": "1", "metadata": {}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "class_name": "RelatedNodeInfo"}}, "hash": "6adca613739d7bbe325eb877c7165ae7761f46d27bcdaf0b6e55554a2c8f9180", "text": "Add them to the global `ServiceContext`\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, set_global_service_context\n\nset_global_service_context(\n    ServiceContext.from_defaults(\n        llm=llm, \n        embed_model=embed_model, \n        callback_manager=callback_manager\n    )\n)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n4.", "start_char_idx": 2106, "end_char_idx": 3188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c629b58-2558-4626-87d6-aca4236b12da": {"__data__": {"id_": "3c629b58-2558-4626-87d6-aca4236b12da", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39f0cb49-3072-4e99-824e-39b89862f502", "node_type": "1", "metadata": {}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b2f6c81-0a0f-4da3-8a2c-12c0ac7d710f", "node_type": "1", "metadata": {}, "hash": "6adca613739d7bbe325eb877c7165ae7761f46d27bcdaf0b6e55554a2c8f9180", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "39f0cb49-3072-4e99-824e-39b89862f502", "node_type": "1", "metadata": {}, "hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "class_name": "RelatedNodeInfo"}}, "hash": "7ab613def70f57af6de2c2ff0fafb53e2fdaf011e1dde6bd38d2fc7942259ef3", "text": "Construct an Index\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./docs/examples/data/paul_graham\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n5. Measure the counts!", "start_char_idx": 3189, "end_char_idx": 4237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ea2aa38-021e-4b6b-89c7-527a836a0c27": {"__data__": {"id_": "2ea2aa38-021e-4b6b-89c7-527a836a0c27", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "node_type": "1", "metadata": {}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a023095-45e1-4385-b446-c75328fdfe72", "node_type": "1", "metadata": {}, "hash": "5d72397fc462605e7d8731096b836d16830770844602c4799267196811638f00", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "node_type": "1", "metadata": {}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "class_name": "RelatedNodeInfo"}}, "hash": "18295918124cf1396f321d852db952f16a9b5a3515aca0aeea70ef8958b257f4", "text": "Measure the counts!\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Estimating LLM and Embedding Token Counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nprint(\n    \"Embedding Tokens: \",\n    token_counter.total_embedding_token_count,\n    \"\\n\",\n    \"LLM Prompt Tokens: \",\n    token_counter.prompt_llm_token_count,\n    \"\\n\",\n    \"LLM Completion Tokens: \",\n    token_counter.completion_llm_token_count,\n    \"\\n\",\n    \"Total LLM Token Count: \",\n    token_counter.total_llm_token_count,\n    \"\\n\",\n)\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/reset counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntoken_counter.reset_counts()\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/reset counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n6.", "start_char_idx": 0, "end_char_idx": 1545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a023095-45e1-4385-b446-c75328fdfe72": {"__data__": {"id_": "5a023095-45e1-4385-b446-c75328fdfe72", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "node_type": "1", "metadata": {}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ea2aa38-021e-4b6b-89c7-527a836a0c27", "node_type": "1", "metadata": {}, "hash": "18295918124cf1396f321d852db952f16a9b5a3515aca0aeea70ef8958b257f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc476e0c-e354-4a0e-8eca-6c98817c9859", "node_type": "1", "metadata": {}, "hash": "e1e9e1563a4a9134fe8095fe40af4eb19f2c6611a697301ca8c786873a3a6e8b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "node_type": "1", "metadata": {}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "class_name": "RelatedNodeInfo"}}, "hash": "5d72397fc462605e7d8731096b836d16830770844602c4799267196811638f00", "text": "Run a query, mesaure again\n\nFile Name: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/reset counts\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 2168\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"query\")\n\nprint(\n    \"Embedding Tokens: \",\n    token_counter.total_embedding_token_count,\n    \"\\n\",\n    \"LLM Prompt Tokens: \",\n    token_counter.prompt_llm_token_count,\n    \"\\n\",\n    \"LLM Completion Tokens: \",\n    token_counter.completion_llm_token_count,\n    \"\\n\",\n    \"Total LLM Token Count: \",\n    token_counter.total_llm_token_count,\n    \"\\n\",\n)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\modules.md\nContent Type: text\nHeader Path: Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 290\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNotebooks with usage of these components can be found below.", "start_char_idx": 1546, "end_char_idx": 2759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc476e0c-e354-4a0e-8eca-6c98817c9859": {"__data__": {"id_": "fc476e0c-e354-4a0e-8eca-6c98817c9859", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "node_type": "1", "metadata": {}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a023095-45e1-4385-b446-c75328fdfe72", "node_type": "1", "metadata": {}, "hash": "5d72397fc462605e7d8731096b836d16830770844602c4799267196811638f00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d7fb08f-830e-48d5-bf7b-256fb6afa197", "node_type": "1", "metadata": {}, "hash": "98b337a1974c9d61d35f8fce9585dcb55350650da09de460e86daed6d8f63da0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "node_type": "1", "metadata": {}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "class_name": "RelatedNodeInfo"}}, "hash": "e1e9e1563a4a9134fe8095fe40af4eb19f2c6611a697301ca8c786873a3a6e8b", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\modules.md\nContent Type: text\nHeader Path: Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\modules.md\nfile_name: modules.md\nfile_type: None\nfile_size: 290\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\n\n../../../examples/evaluation/TestNYC-Evaluation.ipynb\n../../../examples/evaluation/TestNYC-Evaluation-Query.ipynb\n../../../examples/evaluation/QuestionGeneration.ipynb\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEvaluation in generative AI and retrieval is a difficult task. Due to the unpredictable nature of text, and a general lack of \"expected\" outcomes to compare against, there are many blockers to getting started with evaluation.\n\nHowever, LlamaIndex offers a few key modules for evaluating the quality of both Document retrieval and response synthesis.\nHere are some key questions for each component:\n\n- **Document retrieval**: Are the sources relevant to the query?\n- **Response synthesis**: Does the response match the retrieved context? Does it also match the query? \n\nThis guide describes how the evaluation components within LlamaIndex work. Note that our current evaluation modules\ndo *not* require ground-truth labels. Evaluation can be done with some combination of the query, context, response,\nand combine these with LLM calls.", "start_char_idx": 2761, "end_char_idx": 4448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d7fb08f-830e-48d5-bf7b-256fb6afa197": {"__data__": {"id_": "8d7fb08f-830e-48d5-bf7b-256fb6afa197", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "node_type": "1", "metadata": {}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc476e0c-e354-4a0e-8eca-6c98817c9859", "node_type": "1", "metadata": {}, "hash": "e1e9e1563a4a9134fe8095fe40af4eb19f2c6611a697301ca8c786873a3a6e8b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "node_type": "1", "metadata": {}, "hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "class_name": "RelatedNodeInfo"}}, "hash": "98b337a1974c9d61d35f8fce9585dcb55350650da09de460e86daed6d8f63da0", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Evaluation of the Response + Context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEach response from a `query_engine.query` calls returns both the synthesized response as well as source documents.\n\nWe can evaluate the response against the retrieved sources - without taking into account the query!\n\nThis allows you to measure hallucination - if the response does not match the retrieved sources, this means that the model may be \"hallucinating\" an answer since it is not rooting the answer in the context provided to it in the prompt.\n\nThere are two sub-modes of evaluation here. We can either get a binary response \"YES\"/\"NO\" on whether response matches *any* source context,\nand also get a response list across sources to see which sources match.\n\nThe `ResponseEvaluator` handles both modes for evaluating in this context.", "start_char_idx": 4450, "end_char_idx": 5562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62b532e5-e99c-4d07-a113-8924ff067a71": {"__data__": {"id_": "62b532e5-e99c-4d07-a113-8924ff067a71", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec5952e9-f4da-452a-9210-1509bb76996d", "node_type": "1", "metadata": {}, "hash": "7c9418cadbe2a894f1312ac989c2464a536fc0613af17f8cfd2383eeac54332c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e11533d-8fd3-461d-83fc-83f699cbd98c", "node_type": "1", "metadata": {}, "hash": "f52fc51af468c6671811dceac293ff82fa12351c69a72f99a203e3836c723a35", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ec5952e9-f4da-452a-9210-1509bb76996d", "node_type": "1", "metadata": {}, "hash": "7c9418cadbe2a894f1312ac989c2464a536fc0613af17f8cfd2383eeac54332c", "class_name": "RelatedNodeInfo"}}, "hash": "7a86e404b5db1a8bb8bf375748623f77928bb5deece337f7841e111873803413", "text": "The `ResponseEvaluator` handles both modes for evaluating in this context.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Evaluation of the Query + Response + Source Context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis is similar to the above section, except now we also take into account the query. The goal is to determine if\nthe response + source context answers the query.\n\nAs with the above, there are two submodes of evaluation. \n- We can either get a binary response \"YES\"/\"NO\" on whether\nthe response matches the query, and whether any source node also matches the query.\n- We can also ignore the synthesized response, and check every source node to see\nif it matches the query.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Question Generation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn addition to evaluating queries, LlamaIndex can also use your data to generate questions to evaluate on. This means that you can automatically generate questions, and then run an evaluation pipeline to test if the LLM can actually answer questions accurately using your data.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor full usage details, see the usage pattern below.", "start_char_idx": 0, "end_char_idx": 1966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e11533d-8fd3-461d-83fc-83f699cbd98c": {"__data__": {"id_": "9e11533d-8fd3-461d-83fc-83f699cbd98c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec5952e9-f4da-452a-9210-1509bb76996d", "node_type": "1", "metadata": {}, "hash": "7c9418cadbe2a894f1312ac989c2464a536fc0613af17f8cfd2383eeac54332c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62b532e5-e99c-4d07-a113-8924ff067a71", "node_type": "1", "metadata": {}, "hash": "7a86e404b5db1a8bb8bf375748623f77928bb5deece337f7841e111873803413", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ec5952e9-f4da-452a-9210-1509bb76996d", "node_type": "1", "metadata": {}, "hash": "7c9418cadbe2a894f1312ac989c2464a536fc0613af17f8cfd2383eeac54332c", "class_name": "RelatedNodeInfo"}}, "hash": "f52fc51af468c6671811dceac293ff82fa12351c69a72f99a203e3836c723a35", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nusage_pattern.md\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNotebooks with usage of these components can be found below.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nContent Type: text\nHeader Path: Evaluation/Concept/Modules\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 2793\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\nmodules.md\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Binary Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis mode of evaluation will return \"YES\"/\"NO\" if the synthesized response matches any source context.", "start_char_idx": 1968, "end_char_idx": 3642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1df1131e-791b-4c13-b2b9-fac7e3f0fbe7": {"__data__": {"id_": "1df1131e-791b-4c13-b2b9-fac7e3f0fbe7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07e1b2c7-9b3f-4256-93a1-db834bab04d8", "node_type": "1", "metadata": {}, "hash": "b94644963a0d9e7a5ec557bfe45d6d47f59bd8badc004a01e279445195d2b350", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06c9b143-e5af-4e8e-8354-26cdab4080ca", "node_type": "1", "metadata": {}, "hash": "99cce85546cc279317abb9f54cc43ea56a6c6f603926d90628acfbfd4e4fc6e1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "07e1b2c7-9b3f-4256-93a1-db834bab04d8", "node_type": "1", "metadata": {}, "hash": "b94644963a0d9e7a5ec557bfe45d6d47f59bd8badc004a01e279445195d2b350", "class_name": "RelatedNodeInfo"}}, "hash": "44460d72f5bf74b319952500b633d4896bd081e14c47841b528803c0268ac65f", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Binary Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import ResponseEvaluator\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/define evaluator\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06c9b143-e5af-4e8e-8354-26cdab4080ca": {"__data__": {"id_": "06c9b143-e5af-4e8e-8354-26cdab4080ca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07e1b2c7-9b3f-4256-93a1-db834bab04d8", "node_type": "1", "metadata": {}, "hash": "b94644963a0d9e7a5ec557bfe45d6d47f59bd8badc004a01e279445195d2b350", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1df1131e-791b-4c13-b2b9-fac7e3f0fbe7", "node_type": "1", "metadata": {}, "hash": "44460d72f5bf74b319952500b633d4896bd081e14c47841b528803c0268ac65f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "07e1b2c7-9b3f-4256-93a1-db834bab04d8", "node_type": "1", "metadata": {}, "hash": "b94644963a0d9e7a5ec557bfe45d6d47f59bd8badc004a01e279445195d2b350", "class_name": "RelatedNodeInfo"}}, "hash": "99cce85546cc279317abb9f54cc43ea56a6c6f603926d90628acfbfd4e4fc6e1", "text": "md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nevaluator = ResponseEvaluator(service_context=service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\"What battles took place in New York City in the American Revolution?\")\neval_result = evaluator.evaluate(response)\nprint(str(eval_result))\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou'll get back either a `YES` or `NO` response.\n\n!\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Sources Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis mode of evaluation will return \"YES\"/\"NO\" for every source node.", "start_char_idx": 1727, "end_char_idx": 3503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbe39360-85bc-48ea-bb0b-5cabbdf5a707": {"__data__": {"id_": "bbe39360-85bc-48ea-bb0b-5cabbdf5a707", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "422218a9-2dc5-4297-8cb4-280ebd599a91", "node_type": "1", "metadata": {}, "hash": "47fc36050e751d1f22daaeb938476c167d4ec84f7d9872fb689621df57b80c3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1be8183-5f77-4bac-9853-97f8c646e8e0", "node_type": "1", "metadata": {}, "hash": "5b5c37b170c00a6cdb7f78839cf01e36bff96fa5c0d89f2d95bfd6ba6f311790", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "422218a9-2dc5-4297-8cb4-280ebd599a91", "node_type": "1", "metadata": {}, "hash": "47fc36050e751d1f22daaeb938476c167d4ec84f7d9872fb689621df57b80c3d", "class_name": "RelatedNodeInfo"}}, "hash": "b360eb2d395ab7be20489da4a654901825bc229e2cfe94339d14c677930f4b1d", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Sources Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\nfrom llama_index.evaluation import ResponseEvaluator\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/define evaluator\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1be8183-5f77-4bac-9853-97f8c646e8e0": {"__data__": {"id_": "f1be8183-5f77-4bac-9853-97f8c646e8e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "422218a9-2dc5-4297-8cb4-280ebd599a91", "node_type": "1", "metadata": {}, "hash": "47fc36050e751d1f22daaeb938476c167d4ec84f7d9872fb689621df57b80c3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbe39360-85bc-48ea-bb0b-5cabbdf5a707", "node_type": "1", "metadata": {}, "hash": "b360eb2d395ab7be20489da4a654901825bc229e2cfe94339d14c677930f4b1d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "422218a9-2dc5-4297-8cb4-280ebd599a91", "node_type": "1", "metadata": {}, "hash": "47fc36050e751d1f22daaeb938476c167d4ec84f7d9872fb689621df57b80c3d", "class_name": "RelatedNodeInfo"}}, "hash": "5b5c37b170c00a6cdb7f78839cf01e36bff96fa5c0d89f2d95bfd6ba6f311790", "text": "md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nevaluator = ResponseEvaluator(service_context=service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\"What battles took place in New York City in the American Revolution?\")\neval_result = evaluator.evaluate_source_nodes(response)\nprint(str(eval_result))\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou'll get back a list of \"YES\"/\"NO\", corresponding to each source node in `response.source_nodes`.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Binary Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis mode of evaluation will return \"YES\"/\"NO\" if the synthesized response matches the query + any source context.", "start_char_idx": 1747, "end_char_idx": 3628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9f022c4-2428-4af6-8818-20c27f67d2c7": {"__data__": {"id_": "c9f022c4-2428-4af6-8818-20c27f67d2c7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7a933fe9-3bf3-469b-b381-0fde2c77d1af", "node_type": "1", "metadata": {}, "hash": "a82967ac4a25c0427e823e185c9427d37d2b3985459baae4a693fa91b2763329", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "220062a2-05f1-4532-97b9-c9e378f4eda5", "node_type": "1", "metadata": {}, "hash": "1cf7a1db25ceb5a1e66c309e2a07d817c595531a34bcf000430b8431c042aec3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7a933fe9-3bf3-469b-b381-0fde2c77d1af", "node_type": "1", "metadata": {}, "hash": "a82967ac4a25c0427e823e185c9427d37d2b3985459baae4a693fa91b2763329", "class_name": "RelatedNodeInfo"}}, "hash": "f4f3547cd228572f88bca22fdb76dab36f12267a923684d20c75df1317d9e860", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Binary Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import QueryResponseEvaluator\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/define evaluator\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "220062a2-05f1-4532-97b9-c9e378f4eda5": {"__data__": {"id_": "220062a2-05f1-4532-97b9-c9e378f4eda5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7a933fe9-3bf3-469b-b381-0fde2c77d1af", "node_type": "1", "metadata": {}, "hash": "a82967ac4a25c0427e823e185c9427d37d2b3985459baae4a693fa91b2763329", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9f022c4-2428-4af6-8818-20c27f67d2c7", "node_type": "1", "metadata": {}, "hash": "f4f3547cd228572f88bca22fdb76dab36f12267a923684d20c75df1317d9e860", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7a933fe9-3bf3-469b-b381-0fde2c77d1af", "node_type": "1", "metadata": {}, "hash": "a82967ac4a25c0427e823e185c9427d37d2b3985459baae4a693fa91b2763329", "class_name": "RelatedNodeInfo"}}, "hash": "1cf7a1db25ceb5a1e66c309e2a07d817c595531a34bcf000430b8431c042aec3", "text": "md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nevaluator = QueryResponseEvaluator(service_context=service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\"What battles took place in New York City in the American Revolution?\")\neval_result = evaluator.evaluate(response)\nprint(str(eval_result))\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Sources Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis mode of evaluation will look at each source node, and see if each source node contains an answer to the query.", "start_char_idx": 1732, "end_char_idx": 3509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6eed3470-65ae-4a30-94e4-cefb8b399503": {"__data__": {"id_": "6eed3470-65ae-4a30-94e4-cefb8b399503", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c58fa53f-1bec-4b92-b3c0-28261fffc47e", "node_type": "1", "metadata": {}, "hash": "a72b0fb285709b9aa500c22f866cf120574de7e8a743ce66828e299d8ed999e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "617168b3-3ae1-40f2-9e9a-0e2410dbd36a", "node_type": "1", "metadata": {}, "hash": "e326b3024b1dd9ea3d35d01fc2dcf2d0b1ae403947149dac811583900cce1d0f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c58fa53f-1bec-4b92-b3c0-28261fffc47e", "node_type": "1", "metadata": {}, "hash": "a72b0fb285709b9aa500c22f866cf120574de7e8a743ce66828e299d8ed999e6", "class_name": "RelatedNodeInfo"}}, "hash": "1783a6eb1cc1b6b987c4ef83f1ef53e62d143a3c29b5506f150a81f95869cf53", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Sources Evaluation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\nfrom llama_index.evaluation import QueryResponseEvaluator\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n.\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/define evaluator\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "617168b3-3ae1-40f2-9e9a-0e2410dbd36a": {"__data__": {"id_": "617168b3-3ae1-40f2-9e9a-0e2410dbd36a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c58fa53f-1bec-4b92-b3c0-28261fffc47e", "node_type": "1", "metadata": {}, "hash": "a72b0fb285709b9aa500c22f866cf120574de7e8a743ce66828e299d8ed999e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6eed3470-65ae-4a30-94e4-cefb8b399503", "node_type": "1", "metadata": {}, "hash": "1783a6eb1cc1b6b987c4ef83f1ef53e62d143a3c29b5506f150a81f95869cf53", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c58fa53f-1bec-4b92-b3c0-28261fffc47e", "node_type": "1", "metadata": {}, "hash": "a72b0fb285709b9aa500c22f866cf120574de7e8a743ce66828e299d8ed999e6", "class_name": "RelatedNodeInfo"}}, "hash": "e326b3024b1dd9ea3d35d01fc2dcf2d0b1ae403947149dac811583900cce1d0f", "text": "md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nevaluator = QueryResponseEvaluator(service_context=service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\"What battles took place in New York City in the American Revolution?\")\neval_result = evaluator.evaluate_source_nodes(response)\nprint(str(eval_result))\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/query index\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Question Generation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can also generate questions to answer using your data. Using in combination with the above evaluators, you can create a fully automated evaluation pipeline over your data.", "start_char_idx": 1752, "end_char_idx": 3610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c1b9aa9-2e93-4c24-9677-b9fe35aecb55": {"__data__": {"id_": "4c1b9aa9-2e93-4c24-9677-b9fe35aecb55", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "node_type": "1", "metadata": {}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff7170af-6d0f-4f65-8b68-6e8b25416e3b", "node_type": "1", "metadata": {}, "hash": "818ddb8090dcfa4137cea2e7eb896774696c95c0dc203d5497c418832ebd9bf8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "node_type": "1", "metadata": {}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "class_name": "RelatedNodeInfo"}}, "hash": "e5601d3637ebe625b425eebe7449bb42aa333fb878a6b0a7590bcdc9c0d2af02", "text": "File Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/Question Generation\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\nfrom llama_index.evaluation import ResponseEvaluator\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/build documents\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nFile Name: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Evaluating Response for Hallucination/define genertor, generate questions\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md\nfile_name: usage_pattern.", "start_char_idx": 0, "end_char_idx": 1856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff7170af-6d0f-4f65-8b68-6e8b25416e3b": {"__data__": {"id_": "ff7170af-6d0f-4f65-8b68-6e8b25416e3b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "node_type": "1", "metadata": {}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c1b9aa9-2e93-4c24-9677-b9fe35aecb55", "node_type": "1", "metadata": {}, "hash": "e5601d3637ebe625b425eebe7449bb42aa333fb878a6b0a7590bcdc9c0d2af02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37e90aa5-f3f4-4fdb-904a-f7e2546c2cdc", "node_type": "1", "metadata": {}, "hash": "c13d1964eeb1e406a8d921b3decf181839a5ad5e7d5c6ba8eba105c99f0ef5b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "node_type": "1", "metadata": {}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "class_name": "RelatedNodeInfo"}}, "hash": "818ddb8090dcfa4137cea2e7eb896774696c95c0dc203d5497c418832ebd9bf8", "text": "md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 4302\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndata_generator = DatasetGenerator.from_documents(documents)\n\neval_questions = data_generator.generate_questions_from_nodes()\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/Concept\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe Playground module in LlamaIndex is a way to automatically test your data (i.e. documents) across a diverse combination of indices, models, embeddings, modes, etc. to decide which ones are best for your purposes. More options will continue to be added.\n\nFor each combination, you'll be able to compare the results for any query and compare the answers, latency, tokens used, and so on.\n\nYou may initialize a Playground with a list of pre-built indices, or initialize one from a list of Documents using the preset indices.\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA sample usage is given below.\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/Usage Pattern\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.", "start_char_idx": 1828, "end_char_idx": 3551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37e90aa5-f3f4-4fdb-904a-f7e2546c2cdc": {"__data__": {"id_": "37e90aa5-f3f4-4fdb-904a-f7e2546c2cdc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "node_type": "1", "metadata": {}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff7170af-6d0f-4f65-8b68-6e8b25416e3b", "node_type": "1", "metadata": {}, "hash": "818ddb8090dcfa4137cea2e7eb896774696c95c0dc203d5497c418832ebd9bf8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c2ae263-8111-4b0a-aa1a-8499e26a67f5", "node_type": "1", "metadata": {}, "hash": "d0c03c0a05b47571ee21ddde5ec85c9f1b3bf05eb62e9dc6e2cf5d45e55feb4c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "node_type": "1", "metadata": {}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "class_name": "RelatedNodeInfo"}}, "hash": "c13d1964eeb1e406a8d921b3decf181839a5ad5e7d5c6ba8eba105c99f0ef5b2", "text": "md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import download_loader\nfrom llama_index.indices.vector_store import VectorStoreIndex\nfrom llama_index.indices.tree.base import TreeIndex\nfrom llama_index.playground import Playground\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/load data\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWikipediaReader = download_loader(\"WikipediaReader\")\nloader = WikipediaReader()\ndocuments = loader.load_data(pages=['Berlin'])\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/define multiple index data structures (vector index, list index)\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindices = [VectorStoreIndex(documents), TreeIndex(documents)]\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: text\nHeader Path: Playground/initialize playground\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nplayground = Playground(indices=indices)\n\nFile Name: Docs\\core_modules\\supporting_modules\\playground\\root.", "start_char_idx": 3532, "end_char_idx": 5245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c2ae263-8111-4b0a-aa1a-8499e26a67f5": {"__data__": {"id_": "9c2ae263-8111-4b0a-aa1a-8499e26a67f5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "node_type": "1", "metadata": {}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37e90aa5-f3f4-4fdb-904a-f7e2546c2cdc", "node_type": "1", "metadata": {}, "hash": "c13d1964eeb1e406a8d921b3decf181839a5ad5e7d5c6ba8eba105c99f0ef5b2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "node_type": "1", "metadata": {}, "hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "class_name": "RelatedNodeInfo"}}, "hash": "d0c03c0a05b47571ee21ddde5ec85c9f1b3bf05eb62e9dc6e2cf5d45e55feb4c", "text": "md\nContent Type: text\nHeader Path: Playground/playground compare\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nplayground.compare(\"What is the population of Berlin?\")", "start_char_idx": 5245, "end_char_idx": 5580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5f910fc-19c2-4e85-b99c-cb02ca9f570a": {"__data__": {"id_": "a5f910fc-19c2-4e85-b99c-cb02ca9f570a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25f0891c-e28f-439f-b61f-356860391fab", "node_type": "1", "metadata": {}, "hash": "7b32858201113b02d3a93d6a3fb16e6913c7a9e842583cdd811c7c0aeb26e9ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "037cd9ea-c523-48fd-a581-855307260406", "node_type": "1", "metadata": {}, "hash": "88200e44b9c1f5ddc07862d48b7ba279ce7645bd9cfb5cf6fef24ac3860bfaca", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25f0891c-e28f-439f-b61f-356860391fab", "node_type": "1", "metadata": {}, "hash": "7b32858201113b02d3a93d6a3fb16e6913c7a9e842583cdd811c7c0aeb26e9ba", "class_name": "RelatedNodeInfo"}}, "hash": "35d12bbf9c32bfaf6ede5b951cc1b9ab8ace9eb0f3ab27bf0c2106a17705583c", "text": "File Name: Docs\\core_modules\\supporting_modules\\playground\\root.md\nContent Type: code\nHeader Path: Playground/Modules\nfile_path: Docs\\core_modules\\supporting_modules\\playground\\root.md\nfile_name: root.md\nfile_type: None\nfile_size: 1372\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n../../../examples/analysis/PlaygroundDemo.ipynb\n```\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Concept\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `ServiceContext` is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application.\nYou can use it to set the global configuration, as well as local configurations at specific parts of the pipeline.\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Configuring the service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `ServiceContext` is a simple python dataclass that you can directly construct by passing in the desired components.", "start_char_idx": 0, "end_char_idx": 1522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "037cd9ea-c523-48fd-a581-855307260406": {"__data__": {"id_": "037cd9ea-c523-48fd-a581-855307260406", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25f0891c-e28f-439f-b61f-356860391fab", "node_type": "1", "metadata": {}, "hash": "7b32858201113b02d3a93d6a3fb16e6913c7a9e842583cdd811c7c0aeb26e9ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5f910fc-19c2-4e85-b99c-cb02ca9f570a", "node_type": "1", "metadata": {}, "hash": "35d12bbf9c32bfaf6ede5b951cc1b9ab8ace9eb0f3ab27bf0c2106a17705583c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f22326b-f3fd-4295-b2b1-41c05122c2c3", "node_type": "1", "metadata": {}, "hash": "582ad4b5686adbf2ee3f2fcdcee5169a67bc82caf51a41a29f0da663de2a8b25", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25f0891c-e28f-439f-b61f-356860391fab", "node_type": "1", "metadata": {}, "hash": "7b32858201113b02d3a93d6a3fb16e6913c7a9e842583cdd811c7c0aeb26e9ba", "class_name": "RelatedNodeInfo"}}, "hash": "88200e44b9c1f5ddc07862d48b7ba279ce7645bd9cfb5cf6fef24ac3860bfaca", "text": "File Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Configuring the service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@dataclass\nclass ServiceContext:\n    # The LLM used to generate natural language responses to queries.\n    llm_predictor: BaseLLMPredictor\n\n    # The PromptHelper object that helps with truncating and repacking text chunks to fit in the LLM's context window.\n    prompt_helper: PromptHelper\n\n    # The embedding model used to generate vector representations of text.\n    embed_model: BaseEmbedding\n\n    # The parser that converts documents into nodes.\n    node_parser: NodeParser\n\n    # The callback manager object that calls it's handlers on events. Provides basic logging and tracing capabilities.", "start_char_idx": 1524, "end_char_idx": 2509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f22326b-f3fd-4295-b2b1-41c05122c2c3": {"__data__": {"id_": "2f22326b-f3fd-4295-b2b1-41c05122c2c3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25f0891c-e28f-439f-b61f-356860391fab", "node_type": "1", "metadata": {}, "hash": "7b32858201113b02d3a93d6a3fb16e6913c7a9e842583cdd811c7c0aeb26e9ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "037cd9ea-c523-48fd-a581-855307260406", "node_type": "1", "metadata": {}, "hash": "88200e44b9c1f5ddc07862d48b7ba279ce7645bd9cfb5cf6fef24ac3860bfaca", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25f0891c-e28f-439f-b61f-356860391fab", "node_type": "1", "metadata": {}, "hash": "7b32858201113b02d3a93d6a3fb16e6913c7a9e842583cdd811c7c0aeb26e9ba", "class_name": "RelatedNodeInfo"}}, "hash": "582ad4b5686adbf2ee3f2fcdcee5169a67bc82caf51a41a29f0da663de2a8b25", "text": "Provides basic logging and tracing capabilities.\n    callback_manager: CallbackManager\n\n    @classmethod\n    def from_defaults(cls, ...) -> \"ServiceContext\":\n      ...\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: code\nHeader Path: ServiceContext/Usage Pattern/Configuring the service context\nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nLearn how to configure specific modules:\n- LLM\n- Embedding Model\n- Node Parser\n\n```\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Configuring the service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe also expose some common kwargs (of the above components) via the `ServiceContext.from_defaults` method\nfor convenience (so you don't have to manually construct them).\n \n**Kwargs for node parser**:\n- `chunk_size`: The size of the text chunk for a node . Is used for the node parser when they aren't provided.\n- `chunk_overlap`: The amount of overlap between nodes (i.e. text chunks).\n\n**Kwargs for prompt helper**:\n- `context_window`: The size of the context window of the LLM. Typically we set this \n  automatically with the model metadata. But we also allow explicit override via this parameter\n  for additional control (or in case the default is not available for certain latest\n  models)\n- `num_output`: The number of maximum output from the LLM. Typically we set this\n  automatically given the model metadata. This parameter does not actually limit the model\n  output, it affects the amount of \"space\" we save for the output, when computing \n  available context window size for packing text from retrieved Nodes.", "start_char_idx": 2461, "end_char_idx": 4507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37f4e6bc-d13d-494c-a5c3-8729a1bf22f9": {"__data__": {"id_": "37f4e6bc-d13d-494c-a5c3-8729a1bf22f9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "node_type": "1", "metadata": {}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "813844cc-9bd9-4731-8bdc-a67f5cbb84d6", "node_type": "1", "metadata": {}, "hash": "be08d262634a7d3ca8d5bad5395a91493fa35fd1ab0ab24ba2aec821bb9f3bd4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "node_type": "1", "metadata": {}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "class_name": "RelatedNodeInfo"}}, "hash": "69ba5059fe71bfe6d98f05cb867aee330860d1401a7781b2d3d8ea3e6dcc431c", "text": "Here's a complete example that sets up all objects using their default settings:\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Configuring the service context\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\nfrom llama_index.llms import OpenAI\nfrom llama_index.langchain_helpers.text_splitter import TokenTextSplitter\nfrom llama_index.node_parser import SimpleNodeParser\n\nllm = OpenAI(model='text-davinci-003', temperature=0, max_tokens=256)\nembed_model = OpenAIEmbedding()\nnode_parser = SimpleNodeParser(\n  text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n)\nprompt_helper = PromptHelper(\n  context_window=4096, \n  num_output=256, \n  chunk_overlap_ratio=0.1, \n  chunk_size_limit=None\n)\n\nservice_context = ServiceContext.from_defaults(\n  llm=llm,\n  embed_model=embed_model,\n  node_parser=node_parser,\n  prompt_helper=prompt_helper\n)\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Setting global configuration\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can set a service context as the global default that applies to the entire LlamaIndex pipeline:\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Setting global configuration\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.", "start_char_idx": 0, "end_char_idx": 1938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "813844cc-9bd9-4731-8bdc-a67f5cbb84d6": {"__data__": {"id_": "813844cc-9bd9-4731-8bdc-a67f5cbb84d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "node_type": "1", "metadata": {}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37f4e6bc-d13d-494c-a5c3-8729a1bf22f9", "node_type": "1", "metadata": {}, "hash": "69ba5059fe71bfe6d98f05cb867aee330860d1401a7781b2d3d8ea3e6dcc431c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c825415-8a8a-497c-9631-0480cd167818", "node_type": "1", "metadata": {}, "hash": "e22e476aa2849e803bc373c7c7d668e0d1536abdd8810685da4f372b7c6bc57d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "node_type": "1", "metadata": {}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "class_name": "RelatedNodeInfo"}}, "hash": "be08d262634a7d3ca8d5bad5395a91493fa35fd1ab0ab24ba2aec821bb9f3bd4", "text": "md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import set_global_service_context\nset_global_service_context(service_context)\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Setting local configuration\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can pass in a service context to specific part of the pipeline to override the default configuration:\n\nFile Name: Docs\\core_modules\\supporting_modules\\service_context.md\nContent Type: text\nHeader Path: ServiceContext/Usage Pattern/Setting local configuration\nLinks: \nfile_path: Docs\\core_modules\\supporting_modules\\service_context.md\nfile_name: service_context.md\nfile_type: None\nfile_size: 4024\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine(service_context=service_context)\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nFile Name: Docs\\development\\privacy.md\nContent Type: text\nHeader Path: Privacy and Security\nLinks: \nfile_path: Docs\\development\\privacy.md\nfile_name: privacy.md\nfile_type: None\nfile_size: 948\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, LLamaIndex sends your data to OpenAI for generating embeddings and natural language responses. However, it is important to note that this can be configured according to your preferences. LLamaIndex provides the flexibility to use your own embedding model or run a large language model locally if desired.", "start_char_idx": 1908, "end_char_idx": 3781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c825415-8a8a-497c-9631-0480cd167818": {"__data__": {"id_": "7c825415-8a8a-497c-9631-0480cd167818", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "node_type": "1", "metadata": {}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "813844cc-9bd9-4731-8bdc-a67f5cbb84d6", "node_type": "1", "metadata": {}, "hash": "be08d262634a7d3ca8d5bad5395a91493fa35fd1ab0ab24ba2aec821bb9f3bd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07fab882-9814-41a5-939b-40b9eae8261a", "node_type": "1", "metadata": {}, "hash": "42a75123789ea8d2445a009c21d908a7abceef9f9759442781d1bfa6d25826bc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "node_type": "1", "metadata": {}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "class_name": "RelatedNodeInfo"}}, "hash": "e22e476aa2849e803bc373c7c7d668e0d1536abdd8810685da4f372b7c6bc57d", "text": "File Name: Docs\\development\\privacy.md\nContent Type: text\nHeader Path: Privacy and Security/Data Privacy\nLinks: \nfile_path: Docs\\development\\privacy.md\nfile_name: privacy.md\nfile_type: None\nfile_size: 948\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRegarding data privacy, when using LLamaIndex with OpenAI, the privacy details and handling of your data are subject to OpenAI's policies. And each custom service other than OpenAI have their own policies as well.\n\nFile Name: Docs\\development\\privacy.md\nContent Type: text\nHeader Path: Privacy and Security/Vector stores\nfile_path: Docs\\development\\privacy.md\nfile_name: privacy.md\nfile_type: None\nfile_size: 948\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLLamaIndex offers modules to connect with other vector stores within indexes to store embeddings. It is worth noting that each vector store has its own privacy policies and practices, and LLamaIndex does not assume responsibility for how they handle or use your data. Also by default LLamaIndex have a default option to store your embeddings locally.\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Context\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn \"agent\" is an automated reasoning and decision engine. It takes in a user input/query and can make internal decisions for executing\nthat query in order to return the correct result. The key agent components can include, but are not limited to:\n- Breaking down a complex question into smaller ones\n- Choosing an external Tool to use + coming up with parameters for calling the Tool\n- Planning out a set of tasks\n- Storing previously completed tasks in a memory module\n\nResearch developments in LLMs (e.g.", "start_char_idx": 3783, "end_char_idx": 5728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07fab882-9814-41a5-939b-40b9eae8261a": {"__data__": {"id_": "07fab882-9814-41a5-939b-40b9eae8261a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "node_type": "1", "metadata": {}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c825415-8a8a-497c-9631-0480cd167818", "node_type": "1", "metadata": {}, "hash": "e22e476aa2849e803bc373c7c7d668e0d1536abdd8810685da4f372b7c6bc57d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "node_type": "1", "metadata": {}, "hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "class_name": "RelatedNodeInfo"}}, "hash": "42a75123789ea8d2445a009c21d908a7abceef9f9759442781d1bfa6d25826bc", "text": "ChatGPT Plugins), LLM research (ReAct, Toolformer) and LLM tooling (LangChain, Semantic Kernel) have popularized the concept of agents.", "start_char_idx": 5729, "end_char_idx": 5864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45ff6027-0e9a-468d-9b73-1858960cb1ca": {"__data__": {"id_": "45ff6027-0e9a-468d-9b73-1858960cb1ca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c16d2161-9918-4b03-96b7-ab1531cde427", "node_type": "1", "metadata": {}, "hash": "f6d16c8b8b4e92f06bcee35859f5af75a4f91abb7d7b9828cdc21912e9e47d1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6e663ab-d603-46b9-bc95-5ed81ab2ff82", "node_type": "1", "metadata": {}, "hash": "1c79c2e5d6ea3499579723425ac48637dd363e4c4e40b9c308b18dd97a94f437", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c16d2161-9918-4b03-96b7-ab1531cde427", "node_type": "1", "metadata": {}, "hash": "f6d16c8b8b4e92f06bcee35859f5af75a4f91abb7d7b9828cdc21912e9e47d1e", "class_name": "RelatedNodeInfo"}}, "hash": "7b98b83d80ee8fdf4d193843511508b67b0c3fd603787822e6662f279058f9e4", "text": "File Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides some amazing tools to manage and interact with your data within your LLM application. And it can be a core tool that you use while building an agent-based app.\n- On one hand, some components within LlamaIndex are \"agent-like\" - these make automated decisions to help a particular use case over your data.\n- On the other hand, LlamaIndex can be used as a core Tool within another agent framework.\n\nIn general, LlamaIndex components offer more explicit, constrained behavior for more specific use cases. Agent frameworks such as ReAct (implemented in LangChain) offer agents that are more unconstrained + \ncapable of general reasoning. \n\nThere are tradeoffs for using both - less-capable LLMs typically do better with more constraints. Take a look at our blog post on this for \na more information + a detailed analysis.\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/\"Agent-like\" Components within LlamaIndex\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex provides core modules capable of automated reasoning for different use cases over your data. Please check out our use cases doc for more details on high-level use cases that LlamaIndex can help fulfill.\n\nSome of these core modules are shown below along with example tutorials (not comprehensive, please click into the guides/how-tos for more details).\n\n**SubQuestionQueryEngine for Multi-Document Analysis**\n- Usage\n- Sub Question Query Engine (Intro)\n- 10Q Analysis (Uber)\n- 10K Analysis (Uber and Lyft)", "start_char_idx": 0, "end_char_idx": 2002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6e663ab-d603-46b9-bc95-5ed81ab2ff82": {"__data__": {"id_": "b6e663ab-d603-46b9-bc95-5ed81ab2ff82", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c16d2161-9918-4b03-96b7-ab1531cde427", "node_type": "1", "metadata": {}, "hash": "f6d16c8b8b4e92f06bcee35859f5af75a4f91abb7d7b9828cdc21912e9e47d1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45ff6027-0e9a-468d-9b73-1858960cb1ca", "node_type": "1", "metadata": {}, "hash": "7b98b83d80ee8fdf4d193843511508b67b0c3fd603787822e6662f279058f9e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d63fad17-aaf3-4edb-bb99-0714cce6906e", "node_type": "1", "metadata": {}, "hash": "1bf3937e3401880e14190381752becf6d85c82b3c853fed4432e1f5a7874b06a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c16d2161-9918-4b03-96b7-ab1531cde427", "node_type": "1", "metadata": {}, "hash": "f6d16c8b8b4e92f06bcee35859f5af75a4f91abb7d7b9828cdc21912e9e47d1e", "class_name": "RelatedNodeInfo"}}, "hash": "1c79c2e5d6ea3499579723425ac48637dd363e4c4e40b9c308b18dd97a94f437", "text": "**Query Transformations**\n- How-To\n- Multi-Step Query Decomposition (Notebook)\n\n**Routing**\n- Usage\n- Router Query Engine Guide (Notebook)\n\n**LLM Reranking**\n- Second Stage Processing How-To\n- LLM Reranking Guide (Great Gatsby)\n\n**Chat Engines**\n- Chat Engines How-To\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/Using LlamaIndex as as Tool within an Agent Framework\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can be used as as Tool within an agent framework - including LangChain, ChatGPT. These integrations are described below.\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/Using LlamaIndex as as Tool within an Agent Framework/LangChain\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe have deep integrations with LangChain. \nLlamaIndex query engines can be easily packaged as Tools to be used within a LangChain agent, and LlamaIndex can also be used as a memory module / retriever. Check out our guides/tutorials below!", "start_char_idx": 2005, "end_char_idx": 3369, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d63fad17-aaf3-4edb-bb99-0714cce6906e": {"__data__": {"id_": "d63fad17-aaf3-4edb-bb99-0714cce6906e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c16d2161-9918-4b03-96b7-ab1531cde427", "node_type": "1", "metadata": {}, "hash": "f6d16c8b8b4e92f06bcee35859f5af75a4f91abb7d7b9828cdc21912e9e47d1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6e663ab-d603-46b9-bc95-5ed81ab2ff82", "node_type": "1", "metadata": {}, "hash": "1c79c2e5d6ea3499579723425ac48637dd363e4c4e40b9c308b18dd97a94f437", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c16d2161-9918-4b03-96b7-ab1531cde427", "node_type": "1", "metadata": {}, "hash": "f6d16c8b8b4e92f06bcee35859f5af75a4f91abb7d7b9828cdc21912e9e47d1e", "class_name": "RelatedNodeInfo"}}, "hash": "1bf3937e3401880e14190381752becf6d85c82b3c853fed4432e1f5a7874b06a", "text": "Check out our guides/tutorials below!\n\n**Resources**\n- LangChain integration guide\n- Building a Chatbot Tutorial (LangChain + LlamaIndex)\n- OnDemandLoaderTool Tutorial\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/Using LlamaIndex as as Tool within an Agent Framework/ChatGPT\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can be used as a ChatGPT retrieval plugin (we have a TODO to develop a more general plugin as well).\n\n**Resources**\n- LlamaIndex ChatGPT Retrieval Plugin\n\nFile Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/Using LlamaIndex as as Tool within an Agent Framework/Native OpenAIAgent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWith the new OpenAI API that supports function calling, it\u2019s never been easier to build your own agent!\n\nLearn how to write your own OpenAI agent in **under 50 lines of code**, or directly use our super simple\n`OpenAIAgent` implementation.", "start_char_idx": 3332, "end_char_idx": 4647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "920566e4-b853-445f-b94f-1401e4c45891": {"__data__": {"id_": "920566e4-b853-445f-b94f-1401e4c45891", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25", "node_type": "1", "metadata": {}, "hash": "727857fa392684cfd622f209b870ddbbb15cb5cdbacace82d4896f7f863cdcf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2353a21-f9bd-4f00-95ba-52430bf65476", "node_type": "1", "metadata": {}, "hash": "3291a8795d102006855734236c69889116b8e6a9cb4d62ae4eebebbd576a048a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25", "node_type": "1", "metadata": {}, "hash": "727857fa392684cfd622f209b870ddbbb15cb5cdbacace82d4896f7f863cdcf1", "class_name": "RelatedNodeInfo"}}, "hash": "b71104176f2495a51a15aacc0cf8612afa3d79f5e52c4b69fa3b09f216183c9a", "text": "File Name: Docs\\end_to_end_tutorials\\agents.md\nContent Type: text\nHeader Path: Agents/Agents + LlamaIndex/Using LlamaIndex as as Tool within an Agent Framework/Native OpenAIAgent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\agents.md\nfile_name: agents.md\nfile_type: None\nfile_size: 5261\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n---\nmaxdepth: 1\n---\n/examples/agent/openai_agent.ipynb\n/examples/agent/openai_agent_with_query_engine.ipynb\n/examples/agent/openai_agent_retrieval.ipynb\n/examples/agent/openai_agent_query_cookbook.ipynb\n/examples/agent/openai_agent_query_plan.ipynb\n/examples/agent/openai_agent_context_retrieval.ipynb\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex is a python library, which means that integrating it with a full-stack web application will be a little different than what you might be used to.\n\nThis guide seeks to walk through the steps needed to create a basic API service written in python, and how this interacts with a TypeScript+React frontend.\n\nAll code examples here are available from the llama_index_starter_pack in the flask_react folder.", "start_char_idx": 0, "end_char_idx": 1474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2353a21-f9bd-4f00-95ba-52430bf65476": {"__data__": {"id_": "d2353a21-f9bd-4f00-95ba-52430bf65476", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25", "node_type": "1", "metadata": {}, "hash": "727857fa392684cfd622f209b870ddbbb15cb5cdbacace82d4896f7f863cdcf1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "920566e4-b853-445f-b94f-1401e4c45891", "node_type": "1", "metadata": {}, "hash": "b71104176f2495a51a15aacc0cf8612afa3d79f5e52c4b69fa3b09f216183c9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b1a051e-a7ff-4cea-9bc1-a872e4de5f09", "node_type": "1", "metadata": {}, "hash": "586bc07df23caaf05e44c7b71b924377dbe59b4ea607201a7f18077a22078a7a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25", "node_type": "1", "metadata": {}, "hash": "727857fa392684cfd622f209b870ddbbb15cb5cdbacace82d4896f7f863cdcf1", "class_name": "RelatedNodeInfo"}}, "hash": "3291a8795d102006855734236c69889116b8e6a9cb4d62ae4eebebbd576a048a", "text": "All code examples here are available from the llama_index_starter_pack in the flask_react folder.\n\nThe main technologies used in this guide are as follows:\n\n- python3.11\n- llama_index\n- flask\n- typescript\n- react\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor this guide, our backend will use a Flask API server to communicate with our frontend code. If you prefer, you can also easily translate this to a FastAPI server, or any other python server library of your choice.\n\nSetting up a server using Flask is easy. You import the package, create the app object, and then create your endpoints. Let's create a basic skeleton for the server first:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef home():\n    return \"Hello World!\"", "start_char_idx": 1377, "end_char_idx": 2877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b1a051e-a7ff-4cea-9bc1-a872e4de5f09": {"__data__": {"id_": "4b1a051e-a7ff-4cea-9bc1-a872e4de5f09", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25", "node_type": "1", "metadata": {}, "hash": "727857fa392684cfd622f209b870ddbbb15cb5cdbacace82d4896f7f863cdcf1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2353a21-f9bd-4f00-95ba-52430bf65476", "node_type": "1", "metadata": {}, "hash": "3291a8795d102006855734236c69889116b8e6a9cb4d62ae4eebebbd576a048a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25", "node_type": "1", "metadata": {}, "hash": "727857fa392684cfd622f209b870ddbbb15cb5cdbacace82d4896f7f863cdcf1", "class_name": "RelatedNodeInfo"}}, "hash": "586bc07df23caaf05e44c7b71b924377dbe59b4ea607201a7f18077a22078a7a", "text": "if __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5601)\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n_flask_demo.py_\n\nIf you run this file (`python flask_demo.py`), it will launch a server on port 5601. If you visit `http://localhost:5601/`, you will see the \"Hello World!\" text rendered in your browser. Nice!\n\nThe next step is deciding what functions we want to include in our server, and to start using LlamaIndex.\n\nTo keep things simple, the most basic operation we can provide is querying an existing index. Using the paul graham essay from LlamaIndex, create a documents folder and download+place the essay text file inside of it.", "start_char_idx": 2879, "end_char_idx": 3878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d1dac4c-03c6-4dd6-ae47-d58444ea7dc2": {"__data__": {"id_": "0d1dac4c-03c6-4dd6-ae47-d58444ea7dc2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "node_type": "1", "metadata": {}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7dbfa10c-386f-44aa-bf06-299b0b13eb72", "node_type": "1", "metadata": {}, "hash": "58faf1ed6f6b190d8c0b351059dbdafc95079adeb31921afd9002f61c6e42437", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "node_type": "1", "metadata": {}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "class_name": "RelatedNodeInfo"}}, "hash": "607532325b18a650ee3d9a0f5236e070ec14c9a1b52f6e2ebeffb6ae213dfd1d", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/Basic Flask - Handling User Index Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, let's write some code to initialize our index:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/Basic Flask - Handling User Index Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport os\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nos.environ['OPENAI_API_KEY'] = \"your key here\"\n\nindex = None\n\ndef initialize_index():\n    global index\n    storage_context = StorageContext.from_defaults()\n    if os.path.", "start_char_idx": 0, "end_char_idx": 1658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7dbfa10c-386f-44aa-bf06-299b0b13eb72": {"__data__": {"id_": "7dbfa10c-386f-44aa-bf06-299b0b13eb72", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "node_type": "1", "metadata": {}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d1dac4c-03c6-4dd6-ae47-d58444ea7dc2", "node_type": "1", "metadata": {}, "hash": "607532325b18a650ee3d9a0f5236e070ec14c9a1b52f6e2ebeffb6ae213dfd1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a72a30f-f122-48e7-b2b5-25813db6d3ae", "node_type": "1", "metadata": {}, "hash": "ff56a794c1d94d170fba5baf39848312ac621c148ae544efe278d1f72abfbc60", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "node_type": "1", "metadata": {}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "class_name": "RelatedNodeInfo"}}, "hash": "58faf1ed6f6b190d8c0b351059dbdafc95079adeb31921afd9002f61c6e42437", "text": "from_defaults()\n    if os.path.exists(index_dir):\n        index = load_index_from_storage(storage_context)\n    else:\n        documents = SimpleDirectoryReader(\"./documents\").load_data()\n        index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n        storage_context.persist(index_dir)\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis function will initialize our index. If we call this just before starting the flask server in the `main` function, then our index will be ready for user queries!\n\nOur query endpoint will accept `GET` requests with the query text as a parameter. Here's what the full endpoint function will look like:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom flask import request\n\n@app.route(\"/query\", methods=[\"GET\"])\ndef query_index():\n  global index\n  query_text = request.args.get(\"text\", None)\n  if query_text is None:\n    return \"No text found, please include a ?text=blah parameter in the URL\", 400\n  query_engine = index.as_query_engine()\n  response = query_engine.query(query_text)\n  return str(response),", "start_char_idx": 1627, "end_char_idx": 3544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a72a30f-f122-48e7-b2b5-25813db6d3ae": {"__data__": {"id_": "2a72a30f-f122-48e7-b2b5-25813db6d3ae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "node_type": "1", "metadata": {}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7dbfa10c-386f-44aa-bf06-299b0b13eb72", "node_type": "1", "metadata": {}, "hash": "58faf1ed6f6b190d8c0b351059dbdafc95079adeb31921afd9002f61c6e42437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cad717e1-fa4c-4cac-8db5-ab62967e12cb", "node_type": "1", "metadata": {}, "hash": "91d12b18b7150e8fdc52fc045728227dde02b2e6991a003abc282deeec1b586d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "node_type": "1", "metadata": {}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "class_name": "RelatedNodeInfo"}}, "hash": "ff56a794c1d94d170fba5baf39848312ac621c148ae544efe278d1f72abfbc60", "text": "as_query_engine()\n  response = query_engine.query(query_text)\n  return str(response), 200\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, we've introduced a few new concepts to our server:\n\n- a new `/query` endpoint, defined by the function decorator\n- a new import from flask, `request`, which is used to get parameters from the request\n- if the `text` parameter is missing, then we return an error message and an appropriate HTML response code\n- otherwise, we query the index, and return the response as a string\n\nA full query example that you can test in your browser might look something like this: `http://localhost:5601/query?text=what did the author do growing up` (once you press enter, the browser will convert the spaces into \"%20\" characters).\n\nThings are looking pretty good! We now have a functional API. Using your own documents, you can easily provide an interface for any application to call the flask API and get answers to queries.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/Advanced Flask - Handling User Document Uploads\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThings are looking pretty cool, but how can we take this a step further?", "start_char_idx": 3459, "end_char_idx": 5350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cad717e1-fa4c-4cac-8db5-ab62967e12cb": {"__data__": {"id_": "cad717e1-fa4c-4cac-8db5-ab62967e12cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "node_type": "1", "metadata": {}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a72a30f-f122-48e7-b2b5-25813db6d3ae", "node_type": "1", "metadata": {}, "hash": "ff56a794c1d94d170fba5baf39848312ac621c148ae544efe278d1f72abfbc60", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "node_type": "1", "metadata": {}, "hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "class_name": "RelatedNodeInfo"}}, "hash": "91d12b18b7150e8fdc52fc045728227dde02b2e6991a003abc282deeec1b586d", "text": "What if we want to allow users to build their own indexes by uploading their own documents? Have no fear, Flask can handle it all :muscle:.\n\nTo let users upload documents, we have to take some extra precautions. Instead of querying an existing index, the index will become **mutable**. If you have many users adding to the same index, we need to think about how to handle concurrency.", "start_char_idx": 5351, "end_char_idx": 5735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef51f9c8-e76c-4b1d-9fb9-da90bdf12754": {"__data__": {"id_": "ef51f9c8-e76c-4b1d-9fb9-da90bdf12754", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df036a43-245d-4c3e-b036-d33f56dabd94", "node_type": "1", "metadata": {}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef4efee0-7de1-49f9-84df-f7d840d2995a", "node_type": "1", "metadata": {}, "hash": "2f2d1b5a31c21478e278f568f96c62c7dff4c39282604a4029cafa083ae4df9c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "df036a43-245d-4c3e-b036-d33f56dabd94", "node_type": "1", "metadata": {}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "class_name": "RelatedNodeInfo"}}, "hash": "00835070339ed821bba6c348cc1e5c0ba1ac93f10839614653c9351b35495197", "text": "Our Flask server is threaded, which means multiple users can ping the server with requests which will be handled at the same time.\n\nOne option might be to create an index for each user or group, and store and fetch things from S3. But for this example, we will assume there is one locally stored index that users are interacting with.\n\nTo handle concurrent uploads and ensure sequential inserts into the index, we can use the `BaseManager` python package to provide sequential access to the index using a separate server and locks. This sounds scary, but it's not so bad! We will just move all our index operations (initializing, querying, inserting) into the `BaseManager` \"index_server\", which will be called from our Flask server.\n\nHere's a basic example of what our `index_server.py` will look like after we've moved our code:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/Advanced Flask - Handling User Document Uploads\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport os\nfrom multiprocessing import Lock\nfrom multiprocessing.managers import BaseManager\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, Document\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nos.", "start_char_idx": 0, "end_char_idx": 1912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef4efee0-7de1-49f9-84df-f7d840d2995a": {"__data__": {"id_": "ef4efee0-7de1-49f9-84df-f7d840d2995a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df036a43-245d-4c3e-b036-d33f56dabd94", "node_type": "1", "metadata": {}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef51f9c8-e76c-4b1d-9fb9-da90bdf12754", "node_type": "1", "metadata": {}, "hash": "00835070339ed821bba6c348cc1e5c0ba1ac93f10839614653c9351b35495197", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14ceac11-7853-42c0-a772-1d5f0cdafd76", "node_type": "1", "metadata": {}, "hash": "1feec6011aa6811f0b71888297617ace38092a336c567097a6e172b821115589", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "df036a43-245d-4c3e-b036-d33f56dabd94", "node_type": "1", "metadata": {}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "class_name": "RelatedNodeInfo"}}, "hash": "2f2d1b5a31c21478e278f568f96c62c7dff4c39282604a4029cafa083ae4df9c", "text": "environ['OPENAI_API_KEY'] = \"your key here\"\n\nindex = None\nlock = Lock()\n\ndef initialize_index():\n  global index\n\n  with lock:\n    # same as before .\n  .\n\ndef query_index(query_text):\n  global index\n  query_engine = index.as_query_engine()\n  response = query_engine.query(query_text)\n  return str(response)\n\nif __name__ == \"__main__\":\n    # init the global index\n    print(\"initializing index.\")\n    initialize_index()\n\n    # setup server\n    # NOTE: you might want to handle the password in a less hardcoded way\n    manager = BaseManager(('', 5602), b'password')\n    manager.register('query_index', query_index)\n    server = manager.get_server()\n\n    print(\"starting server.\")\n    server.serve_forever()\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n_index_server.py_\n\nSo, we've moved our functions, introduced the `Lock` object which ensures sequential access to the global index, registered our single function in the server, and started the server on port 5602 with the password `password`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14ceac11-7853-42c0-a772-1d5f0cdafd76": {"__data__": {"id_": "14ceac11-7853-42c0-a772-1d5f0cdafd76", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df036a43-245d-4c3e-b036-d33f56dabd94", "node_type": "1", "metadata": {}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef4efee0-7de1-49f9-84df-f7d840d2995a", "node_type": "1", "metadata": {}, "hash": "2f2d1b5a31c21478e278f568f96c62c7dff4c39282604a4029cafa083ae4df9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f077122e-bead-4974-9c39-e43fb995e396", "node_type": "1", "metadata": {}, "hash": "183ac8a82478974e856f9af92dfbfdb3ab5b759427c44ea87633c754359d56a4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "df036a43-245d-4c3e-b036-d33f56dabd94", "node_type": "1", "metadata": {}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "class_name": "RelatedNodeInfo"}}, "hash": "1feec6011aa6811f0b71888297617ace38092a336c567097a6e172b821115589", "text": "and started the server on port 5602 with the password `password`.\n\nThen, we can adjust our flask code as follows:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: for local testing only, do NOT deploy with your key hardcoded\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom multiprocessing.managers import BaseManager\nfrom flask import Flask, request\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmanager = BaseManager(('', 5602), b'password')\nmanager.register('query_index')\nmanager.connect()\n\n@app.route(\"/query\", methods=[\"GET\"])\ndef query_index():\n  global index\n  query_text = request.args.get(\"text\", None)\n  if query_text is None:\n    return \"No text found, please include a ?text=blah parameter in the URL\", 400\n  response = manager.query_index(query_text)._getvalue()\n  return str(response), 200\n\n@app.route(\"/\")\ndef home():\n    return \"Hello World!\"", "start_char_idx": 3268, "end_char_idx": 4858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f077122e-bead-4974-9c39-e43fb995e396": {"__data__": {"id_": "f077122e-bead-4974-9c39-e43fb995e396", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df036a43-245d-4c3e-b036-d33f56dabd94", "node_type": "1", "metadata": {}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14ceac11-7853-42c0-a772-1d5f0cdafd76", "node_type": "1", "metadata": {}, "hash": "1feec6011aa6811f0b71888297617ace38092a336c567097a6e172b821115589", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "df036a43-245d-4c3e-b036-d33f56dabd94", "node_type": "1", "metadata": {}, "hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "class_name": "RelatedNodeInfo"}}, "hash": "183ac8a82478974e856f9af92dfbfdb3ab5b759427c44ea87633c754359d56a4", "text": "if __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5601)\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n_flask_demo.py_\n\nThe two main changes are connecting to our existing `BaseManager` server and registering the functions, as well as calling the function through the manager in the `/query` endpoint.\n\nOne special thing to note is that `BaseManager` servers don't return objects quite as we expect. To resolve the return value into it's original object, we call the `_getvalue()` function.\n\nIf we allow users to upload their own documents, we should probably remove the Paul Graham essay from the documents folder, so let's do that first.", "start_char_idx": 4860, "end_char_idx": 5928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1523a33-fb8a-45e3-bda8-62185f5f82bd": {"__data__": {"id_": "b1523a33-fb8a-45e3-bda8-62185f5f82bd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce98334c-2023-441b-acea-e0729b9bee09", "node_type": "1", "metadata": {}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f74083dc-d459-4b03-b3a9-5f954eaa7326", "node_type": "1", "metadata": {}, "hash": "c851892fbf793ffa7b102d5036febe0a5f464d55422e61de535d6f538fe3d596", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ce98334c-2023-441b-acea-e0729b9bee09", "node_type": "1", "metadata": {}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "class_name": "RelatedNodeInfo"}}, "hash": "f250baf79e559546473875e6784bbdccdf6b4f3dfa354d0793863a8e1ca3d5a6", "text": "Then, let's add an endpoint to upload files! First, let's define our Flask endpoint function:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\nmanager.register('insert_into_index')\n...\n\n@app.route(\"/uploadFile\", methods=[\"POST\"])\ndef upload_file():\n    global manager\n    if 'file' not in request.files:\n        return \"Please send a POST request with a file\", 400\n\n    filepath = None\n    try:\n        uploaded_file = request.files[\"file\"]\n        filename = secure_filename(uploaded_file.filename)\n        filepath = os.path.join('documents', os.path.basename(filename))\n        uploaded_file.save(filepath)\n\n        if request.form.get(\"filename_as_doc_id\", None) is not None:\n            manager.insert_into_index(filepath, doc_id=filename)\n        else:\n            manager.insert_into_index(filepath)\n    except Exception as e:\n        # cleanup temp file\n        if filepath is not None and os.path.exists(filepath):\n            os.remove(filepath)\n        return \"Error: {}\".format(str(e)), 500\n\n    # cleanup temp file\n    if filepath is not None and os.path.exists(filepath):\n        os.remove(filepath)\n\n    return \"File inserted!", "start_char_idx": 0, "end_char_idx": 1562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f74083dc-d459-4b03-b3a9-5f954eaa7326": {"__data__": {"id_": "f74083dc-d459-4b03-b3a9-5f954eaa7326", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce98334c-2023-441b-acea-e0729b9bee09", "node_type": "1", "metadata": {}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1523a33-fb8a-45e3-bda8-62185f5f82bd", "node_type": "1", "metadata": {}, "hash": "f250baf79e559546473875e6784bbdccdf6b4f3dfa354d0793863a8e1ca3d5a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16219dff-01fe-46d5-91f1-c548caf59882", "node_type": "1", "metadata": {}, "hash": "b0748124b88a87c4f6ed752184c44924543e8d079cc6b41ec9d6f5a15b02ee70", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ce98334c-2023-441b-acea-e0729b9bee09", "node_type": "1", "metadata": {}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "class_name": "RelatedNodeInfo"}}, "hash": "c851892fbf793ffa7b102d5036febe0a5f464d55422e61de535d6f538fe3d596", "text": "\", 200\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNot too bad! You will notice that we write the file to disk. We could skip this if we only accept basic file formats like `txt` files, but written to disk we can take advantage of LlamaIndex's `SimpleDirectoryReader` to take care of a bunch of more complex file formats. Optionally, we also use a second `POST` argument to either use the filename as a doc_id or let LlamaIndex generate one for us. This will make more sense once we implement the frontend.\n\nWith these more complicated requests, I also suggest using a tool like Postman. Examples of using postman to test our endpoints are in the repository for this project.\n\nLastly, you'll notice we added a new function to the manager.", "start_char_idx": 1562, "end_char_idx": 2722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16219dff-01fe-46d5-91f1-c548caf59882": {"__data__": {"id_": "16219dff-01fe-46d5-91f1-c548caf59882", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce98334c-2023-441b-acea-e0729b9bee09", "node_type": "1", "metadata": {}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f74083dc-d459-4b03-b3a9-5f954eaa7326", "node_type": "1", "metadata": {}, "hash": "c851892fbf793ffa7b102d5036febe0a5f464d55422e61de535d6f538fe3d596", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9db7b730-8131-417b-b359-4a3669318a5e", "node_type": "1", "metadata": {}, "hash": "ae7d0f82b3a7b4ddfcc7c7d546cb3cbbbe986613579e054daf92354fd870445c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ce98334c-2023-441b-acea-e0729b9bee09", "node_type": "1", "metadata": {}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "class_name": "RelatedNodeInfo"}}, "hash": "b0748124b88a87c4f6ed752184c44924543e8d079cc6b41ec9d6f5a15b02ee70", "text": "Lastly, you'll notice we added a new function to the manager. Let's implement that inside `index_server.py`:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndef insert_into_index(doc_text, doc_id=None):\n    global index\n    document = SimpleDirectoryReader(input_files=[doc_text]).load_data()[0]\n    if doc_id is not None:\n        document.doc_id = doc_id\n\n    with lock:\n        index.insert(document)\n        index.storage_context.persist()\n\n...\nmanager.register('insert_into_index', insert_into_index)\n...\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/NOTE: you might want to handle the password in a less hardcoded way\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEasy! If we launch both the `index_server.py` and then the `flask_demo.py` python files, we have a Flask API server that can handle multiple requests to insert documents into a vector index and respond to user queries!\n\nTo support some functionality in the frontend, I've adjusted what some responses look like from the Flask API, as well as added some functionality to keep track of which documents are stored in the index (LlamaIndex doesn't currently support this in a user-friendly way, but we can augment it ourselves!).", "start_char_idx": 2661, "end_char_idx": 4579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9db7b730-8131-417b-b359-4a3669318a5e": {"__data__": {"id_": "9db7b730-8131-417b-b359-4a3669318a5e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce98334c-2023-441b-acea-e0729b9bee09", "node_type": "1", "metadata": {}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16219dff-01fe-46d5-91f1-c548caf59882", "node_type": "1", "metadata": {}, "hash": "b0748124b88a87c4f6ed752184c44924543e8d079cc6b41ec9d6f5a15b02ee70", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ce98334c-2023-441b-acea-e0729b9bee09", "node_type": "1", "metadata": {}, "hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "class_name": "RelatedNodeInfo"}}, "hash": "ae7d0f82b3a7b4ddfcc7c7d546cb3cbbbe986613579e054daf92354fd870445c", "text": "Lastly, I had to add CORS support to the server using the `Flask-cors` python package.\n\nCheck out the complete `flask_demo.py` and `index_server.py` scripts in the repository for the final minor changes, the`requirements.txt` file, and a sample `Dockerfile` to help with deployment.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/React Frontend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGenerally, React and Typescript are one of the most popular libraries and languages for writing webapps today. This guide will assume you are familiar with how these tools work, because otherwise this guide will triple in length :smile:.\n\nIn the repository, the frontend code is organized inside of the `react_frontend` folder.\n\nThe most relevant part of the frontend will be the `src/apis` folder.", "start_char_idx": 4580, "end_char_idx": 5674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e400ea9e-3d87-462c-b6e2-a5c578191c6c": {"__data__": {"id_": "e400ea9e-3d87-462c-b6e2-a5c578191c6c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6ee80ea-65f5-42b7-9644-5eedf61423be", "node_type": "1", "metadata": {}, "hash": "e5f23e8a83a7089dab7e70e13f6672cef099c89a76632bdb542a0069cc5853e8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}}, "hash": "31a5b41b420f10728ce298555ddf54f31fee278bf06a91e1cd41c57e5414bb4d", "text": "The most relevant part of the frontend will be the `src/apis` folder. This is where we make calls to the Flask server, supporting the following queries:\n\n- `/query` -- make a query to the existing index\n- `/uploadFile` -- upload a file to the flask server for insertion into the index\n- `/getDocuments` -- list the current document titles and a portion of their texts\n\nUsing these three queries, we can build a robust frontend that allows users to upload and keep track of their files, query the index, and view the query response and information about which text nodes were used to form the response.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/fetchDocuments.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis file contains the function to, you guessed it, fetch the list of current documents in the index.", "start_char_idx": 0, "end_char_idx": 1120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6ee80ea-65f5-42b7-9644-5eedf61423be": {"__data__": {"id_": "c6ee80ea-65f5-42b7-9644-5eedf61423be", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e400ea9e-3d87-462c-b6e2-a5c578191c6c", "node_type": "1", "metadata": {}, "hash": "31a5b41b420f10728ce298555ddf54f31fee278bf06a91e1cd41c57e5414bb4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a161bf29-9df1-4011-b4ef-153fd555897f", "node_type": "1", "metadata": {}, "hash": "cd28a9a8f0ff53576017fc30fdcc631baa255e33a32763432a4a8e02e3704d54", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}}, "hash": "e5f23e8a83a7089dab7e70e13f6672cef099c89a76632bdb542a0069cc5853e8", "text": "The code is as follows:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/fetchDocuments.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nexport type Document = {\n  id: string;\n  text: string;\n};\n\nconst fetchDocuments = async (): Promise<Document[]> => {\n  const response = await fetch(\"http://localhost:5601/getDocuments\", {\n    mode: \"cors\",\n  });\n\n  if (!response.ok) {\n    return [];\n  }\n\n  const documentList = (await response.json()) as Document[];\n  return documentList;\n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/fetchDocuments.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAs you can see, we make a query to the Flask server (here, it assumes running on localhost). Notice that we need to include the `mode: 'cors'` option, as we are making an external request.\n\nThen, we check if the response was ok, and if so, get the response json and return it. Here, the response json is a list of `Document` objects that are defined in the same file.", "start_char_idx": 1121, "end_char_idx": 2689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a161bf29-9df1-4011-b4ef-153fd555897f": {"__data__": {"id_": "a161bf29-9df1-4011-b4ef-153fd555897f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6ee80ea-65f5-42b7-9644-5eedf61423be", "node_type": "1", "metadata": {}, "hash": "e5f23e8a83a7089dab7e70e13f6672cef099c89a76632bdb542a0069cc5853e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4cfde42-c780-45e2-889d-ad88f7171da1", "node_type": "1", "metadata": {}, "hash": "7b589cb43e2ce37e62666a93ca188998d2b1f048cb2fbfcca3b323da652188c1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}}, "hash": "cd28a9a8f0ff53576017fc30fdcc631baa255e33a32763432a4a8e02e3704d54", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/queryIndex.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis file sends the user query to the flask server, and gets the response back, as well as details about which nodes in our index provided the response.", "start_char_idx": 2691, "end_char_idx": 3255, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4cfde42-c780-45e2-889d-ad88f7171da1": {"__data__": {"id_": "e4cfde42-c780-45e2-889d-ad88f7171da1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a161bf29-9df1-4011-b4ef-153fd555897f", "node_type": "1", "metadata": {}, "hash": "cd28a9a8f0ff53576017fc30fdcc631baa255e33a32763432a4a8e02e3704d54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "525b7a92-a6db-4210-b68b-2d2d4bfc0bdc", "node_type": "1", "metadata": {}, "hash": "1a9b226dfe85feb5e7e2310f39b578af5c239891aa82e84425cd17be3103d092", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}}, "hash": "7b589cb43e2ce37e62666a93ca188998d2b1f048cb2fbfcca3b323da652188c1", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/queryIndex.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nexport type ResponseSources = {\n  text: string;\n  doc_id: string;\n  start: number;\n  end: number;\n  similarity: number;\n};\n\nexport type QueryResponse = {\n  text: string;\n  sources: ResponseSources[];\n};\n\nconst queryIndex = async (query: string): Promise<QueryResponse> => {\n  const queryURL = new URL(\"http://localhost:5601/query?text=1\");\n  queryURL.searchParams.append(\"text\", query);\n\n  const response = await fetch(queryURL, { mode: \"cors\" });\n  if (!response.ok) {\n    return { text: \"Error in query\", sources: [] };\n  }\n\n  const queryResponse = (await response.json()) as QueryResponse;\n\n  return queryResponse;\n};\n\nexport default queryIndex;\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/queryIndex.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis is similar to the `fetchDocuments.tsx` file, with the main difference being we include the query text as a parameter in the URL. Then, we check if the response is ok and return it with the appropriate typescript type.", "start_char_idx": 3257, "end_char_idx": 4953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "525b7a92-a6db-4210-b68b-2d2d4bfc0bdc": {"__data__": {"id_": "525b7a92-a6db-4210-b68b-2d2d4bfc0bdc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4cfde42-c780-45e2-889d-ad88f7171da1", "node_type": "1", "metadata": {}, "hash": "7b589cb43e2ce37e62666a93ca188998d2b1f048cb2fbfcca3b323da652188c1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "node_type": "1", "metadata": {}, "hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "class_name": "RelatedNodeInfo"}}, "hash": "1a9b226dfe85feb5e7e2310f39b578af5c239891aa82e84425cd17be3103d092", "text": "Then, we check if the response is ok and return it with the appropriate typescript type.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/insertDocument.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nProbably the most complex API call is uploading a document. The function here accepts a file object and constructs a `POST` request using `FormData`.", "start_char_idx": 4865, "end_char_idx": 5520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89e8b848-6f96-432b-a008-633b169734ce": {"__data__": {"id_": "89e8b848-6f96-432b-a008-633b169734ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f546fdee-56fd-4002-9441-746ae6318f44", "node_type": "1", "metadata": {}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5d05ee3-c578-46b5-a3da-44e3c54c24f9", "node_type": "1", "metadata": {}, "hash": "5de44f6e70d8aa14b21d00c1a9f66226d613cd6ff031e4816522c641a9dd04e6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f546fdee-56fd-4002-9441-746ae6318f44", "node_type": "1", "metadata": {}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "class_name": "RelatedNodeInfo"}}, "hash": "a8e5549d313144caea015f7aba737bb0fcbd89061c1b126a348bb979793c8e61", "text": "The function here accepts a file object and constructs a `POST` request using `FormData`.\n\nThe actual response text is not used in the app but could be utilized to provide some user feedback on if the file failed to upload or not.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/insertDocument.tsx\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nconst insertDocument = async (file: File) => {\n  const formData = new FormData();\n  formData.append(\"file\", file);\n  formData.append(\"filename_as_doc_id\", \"true\");\n\n  const response = await fetch(\"http://localhost:5601/uploadFile\", {\n    mode: \"cors\",\n    method: \"POST\",\n    body: formData,\n  });\n\n  const responseText = response.text();\n  return responseText;\n};\n\nexport default insertDocument;\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/All the Other Frontend Good-ness\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAnd that pretty much wraps up the frontend portion! The rest of the react frontend code is some pretty basic react components, and my best attempt to make it look at least a little nice :smile:.\n\nI encourage to read the rest of the codebase and submit any PRs for improvements!", "start_char_idx": 0, "end_char_idx": 1753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5d05ee3-c578-46b5-a3da-44e3c54c24f9": {"__data__": {"id_": "d5d05ee3-c578-46b5-a3da-44e3c54c24f9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f546fdee-56fd-4002-9441-746ae6318f44", "node_type": "1", "metadata": {}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89e8b848-6f96-432b-a008-633b169734ce", "node_type": "1", "metadata": {}, "hash": "a8e5549d313144caea015f7aba737bb0fcbd89061c1b126a348bb979793c8e61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69f8fca8-4d0c-4eca-927c-83001797ad85", "node_type": "1", "metadata": {}, "hash": "a31566b03bc2eccac19a12cab94eec73883d18513fcb0b777dee3a3786e50b66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f546fdee-56fd-4002-9441-746ae6318f44", "node_type": "1", "metadata": {}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "class_name": "RelatedNodeInfo"}}, "hash": "5de44f6e70d8aa14b21d00c1a9f66226d613cd6ff031e4816522c641a9dd04e6", "text": "I encourage to read the rest of the codebase and submit any PRs for improvements!\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack Web App with LLamaIndex/Flask Backend/Conclusion\nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md\nfile_name: fullstack_app_guide.md\nfile_type: None\nfile_size: 16549\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis guide has covered a ton of information. We went from a basic \"Hello World\" Flask server written in python, to a fully functioning LlamaIndex powered backend and how to connect that to a frontend application.\n\nAs you can see, we can easily augment and wrap the services provided by LlamaIndex (like the little external document tracker) to help provide a good user experience on the frontend.\n\nYou could take this and add many features (multi-index/user support, saving objects into S3, adding a Pinecone vector server, etc.). And when you build an app after reading this, be sure to share the final result in the Discord! Good Luck! :muscle:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis guide seeks to walk you through using LlamaIndex with a production-ready web app starter template\ncalled Delphic.", "start_char_idx": 1672, "end_char_idx": 3321, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69f8fca8-4d0c-4eca-927c-83001797ad85": {"__data__": {"id_": "69f8fca8-4d0c-4eca-927c-83001797ad85", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f546fdee-56fd-4002-9441-746ae6318f44", "node_type": "1", "metadata": {}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5d05ee3-c578-46b5-a3da-44e3c54c24f9", "node_type": "1", "metadata": {}, "hash": "5de44f6e70d8aa14b21d00c1a9f66226d613cd6ff031e4816522c641a9dd04e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "066f7152-6f38-4f0f-a501-5600d66e757d", "node_type": "1", "metadata": {}, "hash": "1bdce6ac6fe5478262e02bd40e913c318a2c71cf3deb0ee028225ee31736659d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f546fdee-56fd-4002-9441-746ae6318f44", "node_type": "1", "metadata": {}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "class_name": "RelatedNodeInfo"}}, "hash": "a31566b03bc2eccac19a12cab94eec73883d18513fcb0b777dee3a3786e50b66", "text": "All code examples here are available from\nthe Delphic repo\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/What We're Building\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere's a quick demo of the out-of-the-box functionality of Delphic:\n\nhttps://user-images.githubusercontent.com/5049984/233236432-aa4980b6-a510-42f3-887a-81485c9644e6.mp4\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Architectural Overview\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDelphic leverages the LlamaIndex python library to let users to create their own document collections they can then\nquery in a responsive frontend.\n\nWe chose a stack that provides a responsive, robust mix of technologies that can (1) orchestrate complex python\nprocessing tasks while providing (2) a modern, responsive frontend and (3) a secure backend to build additional\nfunctionality upon.\n\nThe core libraries are:\n\n1. Django\n2. Django Channels\n3. Django Ninja\n4. Redis\n5. Celery\n6. LlamaIndex\n7. Langchain\n8. React\n9.", "start_char_idx": 3322, "end_char_idx": 4917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "066f7152-6f38-4f0f-a501-5600d66e757d": {"__data__": {"id_": "066f7152-6f38-4f0f-a501-5600d66e757d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f546fdee-56fd-4002-9441-746ae6318f44", "node_type": "1", "metadata": {}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69f8fca8-4d0c-4eca-927c-83001797ad85", "node_type": "1", "metadata": {}, "hash": "a31566b03bc2eccac19a12cab94eec73883d18513fcb0b777dee3a3786e50b66", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f546fdee-56fd-4002-9441-746ae6318f44", "node_type": "1", "metadata": {}, "hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "class_name": "RelatedNodeInfo"}}, "hash": "1bdce6ac6fe5478262e02bd40e913c318a2c71cf3deb0ee028225ee31736659d", "text": "LlamaIndex\n7. Langchain\n8. React\n9. Docker & Docker Compose\n\nThanks to this modern stack built on the super stable Django web framework, the starter Delphic app boasts a streamlined\ndeveloper experience, built-in authentication and user management, asynchronous vector store processing, and\nweb-socket-based query connections for a responsive UI. In addition, our frontend is built with TypeScript and is based\non MUI React for a responsive and modern user interface.", "start_char_idx": 4882, "end_char_idx": 5349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b0061c9-30b8-4557-8a86-ddda10331ccc": {"__data__": {"id_": "8b0061c9-30b8-4557-8a86-ddda10331ccc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5", "node_type": "1", "metadata": {}, "hash": "566d0db4152fa794e55cb242d1a9fa3d486cbecf1c321e21d85f17730e3294bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "739efa70-5c4d-4903-9b7d-7ae0e5b434bf", "node_type": "1", "metadata": {}, "hash": "5b25258bc2d443d17f40d011f135b4ce49e8fe5a3be21b7b8dc1b8ae3067d612", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5", "node_type": "1", "metadata": {}, "hash": "566d0db4152fa794e55cb242d1a9fa3d486cbecf1c321e21d85f17730e3294bb", "class_name": "RelatedNodeInfo"}}, "hash": "9b608f07f5722b5d23c1706ac7a2d3b5b4256d7444d0c0c70acbe296cdfea301", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/System Requirements\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCelery doesn't work on Windows. It may be deployable with Windows Subsystem for Linux, but configuring that is beyond\nthe scope of this tutorial. For this reason, we recommend you only follow this tutorial if you're running Linux or OSX.\nYou will need Docker and Docker Compose installed to deploy the application. Local development will require node version\nmanager (nvm).\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Project Directory Overview\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe Delphic application has a structured backend directory organization that follows common Django project conventions.\nFrom the repo root, in the `./delphic` subfolder, the main folders are:\n\n1. `contrib`: This directory contains custom modifications or additions to Django's built-in `contrib` apps.\n2. `indexes`: This directory contains the core functionality related to document indexing and LLM integration. It\n   includes:\n\n- `admin.py`: Django admin configuration for the app\n- `apps.py`: Application configuration\n- `models.py`: Contains the app's database models\n- `migrations`: Directory containing database schema migrations for the app\n- `signals.py`: Defines any signals for the app\n- `tests.py`: Unit tests for the app\n\n3.", "start_char_idx": 0, "end_char_idx": 1973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "739efa70-5c4d-4903-9b7d-7ae0e5b434bf": {"__data__": {"id_": "739efa70-5c4d-4903-9b7d-7ae0e5b434bf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5", "node_type": "1", "metadata": {}, "hash": "566d0db4152fa794e55cb242d1a9fa3d486cbecf1c321e21d85f17730e3294bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b0061c9-30b8-4557-8a86-ddda10331ccc", "node_type": "1", "metadata": {}, "hash": "9b608f07f5722b5d23c1706ac7a2d3b5b4256d7444d0c0c70acbe296cdfea301", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "620307eb-ce1e-4985-98aa-594dc9bdc5c7", "node_type": "1", "metadata": {}, "hash": "1cbf1b394237fa64efdd496653203761da05a656bf9df125d9196d533b44dc55", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5", "node_type": "1", "metadata": {}, "hash": "566d0db4152fa794e55cb242d1a9fa3d486cbecf1c321e21d85f17730e3294bb", "class_name": "RelatedNodeInfo"}}, "hash": "5b25258bc2d443d17f40d011f135b4ce49e8fe5a3be21b7b8dc1b8ae3067d612", "text": "`tasks`: This directory contains tasks for asynchronous processing using Celery. The `index_tasks.py` file includes\n   the tasks for creating vector indexes.\n4. `users`: This directory is dedicated to user management, including:\n5. `utils`: This directory contains utility modules and functions that are used across the application, such as custom\n   storage backends, path helpers, and collection-related utilities.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Database Models\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe Delphic application has two core models: `Document` and `Collection`. These models represent the central entities\nthe application deals with when indexing and querying documents using LLMs. They're defined in\n`./delphic/indexes/models.py`.\n\n1. `Collection`:\n\n- `api_key`: A foreign key that links a collection to an API key. This helps associate jobs with the source API key.\n- `title`: A character field that provides a title for the collection.\n- `description`: A text field that provides a description of the collection.\n- `status`: A character field that stores the processing status of the collection, utilizing the `CollectionStatus`\n  enumeration.\n- `created`: A datetime field that records when the collection was created.\n- `modified`: A datetime field that records the last modification time of the collection.\n- `model`: A file field that stores the model associated with the collection.\n- `processing`: A boolean field that indicates if the collection is currently being processed.\n\n2. `Document`:\n\n- `collection`: A foreign key that links a document to a collection. This represents the relationship between documents\n  and collections.\n- `file`: A file field that stores the uploaded document file.\n- `description`: A text field that provides a description of the document.\n- `created`: A datetime field that records when the document was created.", "start_char_idx": 1974, "end_char_idx": 4188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "620307eb-ce1e-4985-98aa-594dc9bdc5c7": {"__data__": {"id_": "620307eb-ce1e-4985-98aa-594dc9bdc5c7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5", "node_type": "1", "metadata": {}, "hash": "566d0db4152fa794e55cb242d1a9fa3d486cbecf1c321e21d85f17730e3294bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "739efa70-5c4d-4903-9b7d-7ae0e5b434bf", "node_type": "1", "metadata": {}, "hash": "5b25258bc2d443d17f40d011f135b4ce49e8fe5a3be21b7b8dc1b8ae3067d612", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5", "node_type": "1", "metadata": {}, "hash": "566d0db4152fa794e55cb242d1a9fa3d486cbecf1c321e21d85f17730e3294bb", "class_name": "RelatedNodeInfo"}}, "hash": "1cbf1b394237fa64efdd496653203761da05a656bf9df125d9196d533b44dc55", "text": "- `created`: A datetime field that records when the document was created.\n- `modified`: A datetime field that records the last modification time of the document.\n\nThese models provide a solid foundation for collections of documents and the indexes created from them with LlamaIndex.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDjango Ninja is a web framework for building APIs with Django and Python 3.7+ type hints. It provides a simple,\nintuitive, and expressive way of defining API endpoints, leveraging Python\u2019s type hints to automatically generate input\nvalidation, serialization, and documentation.\n\nIn the Delphic repo,\nthe `./config/api/endpoints.py`\nfile contains the API routes and logic for the API endpoints. Now, let\u2019s briefly address the purpose of each endpoint\nin the `endpoints.py` file:\n\n1. `/heartbeat`: A simple GET endpoint to check if the API is up and running. Returns `True` if the API is accessible.\n   This is helpful for Kubernetes setups that expect to be able to query your container to ensure it's up and running.\n\n2. `/collections/create`: A POST endpoint to create a new `Collection`. Accepts form parameters such\n   as `title`, `description`, and a list of `files`. Creates a new `Collection` and `Document` instances for each file,\n   and schedules a Celery task to create an index.", "start_char_idx": 4115, "end_char_idx": 5820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ae0fcd0-1f6f-42f9-94ee-8382150cea36": {"__data__": {"id_": "1ae0fcd0-1f6f-42f9-94ee-8382150cea36", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822", "node_type": "1", "metadata": {}, "hash": "48af84f1c7b675ada7b29bd678427298414d187f71728887fa2e7cfb36d3df19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52c768ef-e147-4809-84f2-b35c6c9b4a1b", "node_type": "1", "metadata": {}, "hash": "22c3440071b256314eeba7c95c27fd6c123f4cd7131b4c23b6e0963727fc67b4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822", "node_type": "1", "metadata": {}, "hash": "48af84f1c7b675ada7b29bd678427298414d187f71728887fa2e7cfb36d3df19", "class_name": "RelatedNodeInfo"}}, "hash": "85c9ff20b7e3d35acac5ca4f6338407ba9b491a7c25550faeb258796d078ca72", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@collections_router.post(\"/create\")\nasync def create_collection(request,\n                            title: str = Form(...),\n                            description: str = Form(...),\n                            files: list[UploadedFile] = File(...), ):\n    key = None if getattr(request, \"auth\", None) is None else request.auth\n    if key is not None:\n        key = await key\n\n    collection_instance = Collection(\n        api_key=key,\n        title=title,\n        description=description,\n        status=CollectionStatusEnum.QUEUED,\n    )\n\n    await sync_to_async(collection_instance.save)()\n\n    for uploaded_file in files:\n        doc_data = uploaded_file.file.read()\n        doc_file = ContentFile(doc_data, uploaded_file.name)\n        document = Document(collection=collection_instance, file=doc_file)\n        await sync_to_async(document.save)()\n\n    create_index.si(collection_instance.id).apply_async()\n\n    return await sync_to_async(CollectionModelSchema)(\n        ...\n    )\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n3. `/collections/query` \u2014 a POST endpoint to query a document collection using the LLM.", "start_char_idx": 0, "end_char_idx": 1937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52c768ef-e147-4809-84f2-b35c6c9b4a1b": {"__data__": {"id_": "52c768ef-e147-4809-84f2-b35c6c9b4a1b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822", "node_type": "1", "metadata": {}, "hash": "48af84f1c7b675ada7b29bd678427298414d187f71728887fa2e7cfb36d3df19", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ae0fcd0-1f6f-42f9-94ee-8382150cea36", "node_type": "1", "metadata": {}, "hash": "85c9ff20b7e3d35acac5ca4f6338407ba9b491a7c25550faeb258796d078ca72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99e103ce-f5fa-4bb6-afce-e0d061dfd115", "node_type": "1", "metadata": {}, "hash": "7290c4330d22865ad724f6a438ad024d8e6a3fc91f59c433c4db28d603987e97", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822", "node_type": "1", "metadata": {}, "hash": "48af84f1c7b675ada7b29bd678427298414d187f71728887fa2e7cfb36d3df19", "class_name": "RelatedNodeInfo"}}, "hash": "22c3440071b256314eeba7c95c27fd6c123f4cd7131b4c23b6e0963727fc67b4", "text": "`/collections/query` \u2014 a POST endpoint to query a document collection using the LLM. Accepts a JSON payload\n   containing `collection_id` and `query_str`, and returns a response generated by querying the collection. We don't\n   actually use this endpoint in our chat GUI (We use a websocket - see below), but you could build an app to integrate\n   to this REST endpoint to query a specific collection.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@collections_router.post(\"/query\",\n                         response=CollectionQueryOutput,\n                         summary=\"Ask a question of a document collection\", )\ndef query_collection_view(request: HttpRequest, query_input: CollectionQueryInput):\n    collection_id = query_input.collection_id\n    query_str = query_input.query_str\n    response = query_collection(collection_id, query_str)\n    return {\"response\": response}\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n4. `/collections/available`: A GET endpoint that returns a list of all collections created with the user's API key. The\n   output is serialized using the `CollectionModelSchema`.", "start_char_idx": 1853, "end_char_idx": 3729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99e103ce-f5fa-4bb6-afce-e0d061dfd115": {"__data__": {"id_": "99e103ce-f5fa-4bb6-afce-e0d061dfd115", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822", "node_type": "1", "metadata": {}, "hash": "48af84f1c7b675ada7b29bd678427298414d187f71728887fa2e7cfb36d3df19", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52c768ef-e147-4809-84f2-b35c6c9b4a1b", "node_type": "1", "metadata": {}, "hash": "22c3440071b256314eeba7c95c27fd6c123f4cd7131b4c23b6e0963727fc67b4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822", "node_type": "1", "metadata": {}, "hash": "48af84f1c7b675ada7b29bd678427298414d187f71728887fa2e7cfb36d3df19", "class_name": "RelatedNodeInfo"}}, "hash": "7290c4330d22865ad724f6a438ad024d8e6a3fc91f59c433c4db28d603987e97", "text": "The\n   output is serialized using the `CollectionModelSchema`.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@collections_router.get(\"/available\",\n                        response=list[CollectionModelSchema],\n                        summary=\"Get a list of all of the collections created with my api_key\", )\nasync def get_my_collections_view(request: HttpRequest):\n    key = None if getattr(request, \"auth\", None) is None else request.auth\n    if key is not None:\n        key = await key\n\n    collections = Collection.objects.filter(api_key=key)\n\n    return [\n        {\n            ...\n        }\n        async for collection in collections\n    ]\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n5. `/collections/{collection_id}/add_file`: A POST endpoint to add a file to an existing collection. Accepts\n   a `collection_id` path parameter, and form parameters such as `file` and `description`. Adds the file as a `Document`\n   instance associated with the specified collection.", "start_char_idx": 3667, "end_char_idx": 5415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfc542d7-8752-42ab-a833-8d9277ad678c": {"__data__": {"id_": "cfc542d7-8752-42ab-a833-8d9277ad678c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6184d959-1551-4667-8767-3dd54a1351c3", "node_type": "1", "metadata": {}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f66f044e-3eee-4483-8344-ece21a635523", "node_type": "1", "metadata": {}, "hash": "38e5f7b893a2df1d1454fdbe3a1d7652a1d68e6c962ee2d93fa517c997fbb48e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6184d959-1551-4667-8767-3dd54a1351c3", "node_type": "1", "metadata": {}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "class_name": "RelatedNodeInfo"}}, "hash": "e6413f2553a06884361590a1ee5bf4cc70ff9b0d35d37886e9284d52958559fb", "text": "Adds the file as a `Document`\n   instance associated with the specified collection.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Django Ninja API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@collections_router.post(\"/{collection_id}/add_file\", summary=\"Add a file to a collection\")\nasync def add_file_to_collection(request,\n                                 collection_id: int,\n                                 file: UploadedFile = File(...),\n                                 description: str = Form(...), ):\n    collection = await sync_to_async(Collection.objects.get)(id=collection_id\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Intro to Websockets\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWebSockets are a communication protocol that enables bidirectional and full-duplex communication between a client and a\nserver over a single, long-lived connection. The WebSocket protocol is designed to work over the same ports as HTTP and\nHTTPS (ports 80 and 443, respectively) and uses a similar handshake process to establish a connection. Once the\nconnection is established, data can be sent in both directions as \u201cframes\u201d without the need to reestablish the\nconnection each time, unlike traditional HTTP requests.\n\nThere are several reasons to use WebSockets, particularly when working with code that takes a long time to load into\nmemory but is quick to run once loaded:\n\n1.", "start_char_idx": 0, "end_char_idx": 2029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f66f044e-3eee-4483-8344-ece21a635523": {"__data__": {"id_": "f66f044e-3eee-4483-8344-ece21a635523", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6184d959-1551-4667-8767-3dd54a1351c3", "node_type": "1", "metadata": {}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfc542d7-8752-42ab-a833-8d9277ad678c", "node_type": "1", "metadata": {}, "hash": "e6413f2553a06884361590a1ee5bf4cc70ff9b0d35d37886e9284d52958559fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e35aa78-97d4-4392-9225-2fac96eab48d", "node_type": "1", "metadata": {}, "hash": "88b699596ee3beb45dff4a98770bd836f988cfa1cb1e65a04035f501d7c5bb28", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6184d959-1551-4667-8767-3dd54a1351c3", "node_type": "1", "metadata": {}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "class_name": "RelatedNodeInfo"}}, "hash": "38e5f7b893a2df1d1454fdbe3a1d7652a1d68e6c962ee2d93fa517c997fbb48e", "text": "**Performance**: WebSockets eliminate the overhead associated with opening and closing multiple connections for each\n   request, reducing latency.\n2. **Efficiency**: WebSockets allow for real-time communication without the need for polling, resulting in more\n   efficient use of resources and better responsiveness.\n3. **Scalability**: WebSockets can handle a large number of simultaneous connections, making it ideal for applications\n   that require high concurrency.\n\nIn the case of the Delphic application, using WebSockets makes sense as the LLMs can be expensive to load into memory.\nBy establishing a WebSocket connection, the LLM can remain loaded in memory, allowing subsequent requests to be\nprocessed quickly without the need to reload the model each time.\n\nThe ASGI configuration file `./config/asgi.py` defines how\nthe application should handle incoming connections, using the Django Channels `ProtocolTypeRouter` to route connections\nbased on their protocol type. In this case, we have two protocol types: \"http\" and \"websocket\".\n\nThe \u201chttp\u201d protocol type uses the standard Django ASGI application to handle HTTP requests, while the \u201cwebsocket\u201d\nprotocol type uses a custom `TokenAuthMiddleware` to authenticate WebSocket connections. The `URLRouter` within\nthe `TokenAuthMiddleware` defines a URL pattern for the `CollectionQueryConsumer`, which is responsible for handling\nWebSocket connections related to querying document collections.", "start_char_idx": 2030, "end_char_idx": 3480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e35aa78-97d4-4392-9225-2fac96eab48d": {"__data__": {"id_": "0e35aa78-97d4-4392-9225-2fac96eab48d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6184d959-1551-4667-8767-3dd54a1351c3", "node_type": "1", "metadata": {}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f66f044e-3eee-4483-8344-ece21a635523", "node_type": "1", "metadata": {}, "hash": "38e5f7b893a2df1d1454fdbe3a1d7652a1d68e6c962ee2d93fa517c997fbb48e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76b3aeb2-6333-4b90-b673-a51c9a7427f6", "node_type": "1", "metadata": {}, "hash": "d2620f0e490e8aba1f72e181f072dd8ff52a5c73b9ed7291d5028c465a91e4bd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6184d959-1551-4667-8767-3dd54a1351c3", "node_type": "1", "metadata": {}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "class_name": "RelatedNodeInfo"}}, "hash": "88b699596ee3beb45dff4a98770bd836f988cfa1cb1e65a04035f501d7c5bb28", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Intro to Websockets\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\napplication = ProtocolTypeRouter(\n    {\n        \"http\": get_asgi_application(),\n        \"websocket\": TokenAuthMiddleware(\n            URLRouter(\n                [\n                    re_path(\n                        r\"ws/collections/(?P<collection_id>\\w+)/query/$\",\n                        CollectionQueryConsumer.as_asgi(),\n                    ),\n                ]\n            )\n        ),\n    }\n)\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Intro to Websockets\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis configuration allows clients to establish WebSocket connections with the Delphic application to efficiently query\ndocument collections using the LLMs, without the need to reload the models for each request.", "start_char_idx": 3482, "end_char_idx": 4963, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76b3aeb2-6333-4b90-b673-a51c9a7427f6": {"__data__": {"id_": "76b3aeb2-6333-4b90-b673-a51c9a7427f6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6184d959-1551-4667-8767-3dd54a1351c3", "node_type": "1", "metadata": {}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e35aa78-97d4-4392-9225-2fac96eab48d", "node_type": "1", "metadata": {}, "hash": "88b699596ee3beb45dff4a98770bd836f988cfa1cb1e65a04035f501d7c5bb28", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6184d959-1551-4667-8767-3dd54a1351c3", "node_type": "1", "metadata": {}, "hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "class_name": "RelatedNodeInfo"}}, "hash": "d2620f0e490e8aba1f72e181f072dd8ff52a5c73b9ed7291d5028c465a91e4bd", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `CollectionQueryConsumer` class\nin `config/api/websockets/queries.py` is\nresponsible for handling WebSocket connections related to querying document collections. It inherits from\nthe `AsyncWebsocketConsumer` class provided by Django Channels.\n\nThe `CollectionQueryConsumer` class has three main methods:\n\n1. `connect`: Called when a WebSocket is handshaking as part of the connection process.\n2. `disconnect`: Called when a WebSocket closes for any reason.\n3. `receive`: Called when the server receives a message from the WebSocket.", "start_char_idx": 4965, "end_char_idx": 5934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e12cb0b4-bbc6-4d53-b25d-b522cc0a075d": {"__data__": {"id_": "e12cb0b4-bbc6-4d53-b25d-b522cc0a075d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93f420f1-19c2-4478-9b9d-e12cb1d27eb1", "node_type": "1", "metadata": {}, "hash": "2a03923d06ae800c8f9fac91c2d9bb1422a9b469fd6fc24aa44a1bd1d72427ff", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}}, "hash": "e4bceb5b413b0d138e6282e24dcdb6f70b2e51a4d341fe47a01829f3e5f4954a", "text": "3. `receive`: Called when the server receives a message from the WebSocket.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket connect listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `connect` method is responsible for establishing the connection, extracting the collection ID from the connection\npath, loading the collection model, and accepting the connection.", "start_char_idx": 0, "end_char_idx": 720, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93f420f1-19c2-4478-9b9d-e12cb1d27eb1": {"__data__": {"id_": "93f420f1-19c2-4478-9b9d-e12cb1d27eb1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e12cb0b4-bbc6-4d53-b25d-b522cc0a075d", "node_type": "1", "metadata": {}, "hash": "e4bceb5b413b0d138e6282e24dcdb6f70b2e51a4d341fe47a01829f3e5f4954a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e21a4a8-1405-4c27-8d5f-0de975f26508", "node_type": "1", "metadata": {}, "hash": "759923acb33b279a82ba76ac10e4a5cf4a8bc943a8808aa6745eddb3ae311e79", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}}, "hash": "2a03923d06ae800c8f9fac91c2d9bb1422a9b469fd6fc24aa44a1bd1d72427ff", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket connect listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nasync def connect(self):\n    try:\n        self.collection_id = extract_connection_id(self.scope[\"path\"])\n        self.index = await load_collection_model(self.collection_id)\n        await self.accept()\n\nexcept ValueError as e:\nawait self.accept()\nawait self.close(code=4000)\nexcept Exception as e:\npass\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket disconnect listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `disconnect` method is empty in this case, as there are no additional actions to be taken when the WebSocket is\nclosed.", "start_char_idx": 722, "end_char_idx": 2072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e21a4a8-1405-4c27-8d5f-0de975f26508": {"__data__": {"id_": "3e21a4a8-1405-4c27-8d5f-0de975f26508", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93f420f1-19c2-4478-9b9d-e12cb1d27eb1", "node_type": "1", "metadata": {}, "hash": "2a03923d06ae800c8f9fac91c2d9bb1422a9b469fd6fc24aa44a1bd1d72427ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e00d7077-fbdc-410e-88a2-dbfd059192e8", "node_type": "1", "metadata": {}, "hash": "92fd6c66bb50454d98646ed7fa849b194914a14c4ac173910fc7ed3d84160dc9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}}, "hash": "759923acb33b279a82ba76ac10e4a5cf4a8bc943a8808aa6745eddb3ae311e79", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket receive listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `receive` method is responsible for processing incoming messages from the WebSocket. It takes the incoming message,\ndecodes it, and then queries the loaded collection model using the provided query. The response is then formatted as a\nmarkdown string and sent back to the client over the WebSocket connection.", "start_char_idx": 2074, "end_char_idx": 2847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e00d7077-fbdc-410e-88a2-dbfd059192e8": {"__data__": {"id_": "e00d7077-fbdc-410e-88a2-dbfd059192e8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e21a4a8-1405-4c27-8d5f-0de975f26508", "node_type": "1", "metadata": {}, "hash": "759923acb33b279a82ba76ac10e4a5cf4a8bc943a8808aa6745eddb3ae311e79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09770b69-569d-422e-89e6-b71b288c5ae1", "node_type": "1", "metadata": {}, "hash": "9ea998e60bc700d38817a711ca13064a0b448dc8298c8617348a1ae30ebaf99a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}}, "hash": "92fd6c66bb50454d98646ed7fa849b194914a14c4ac173910fc7ed3d84160dc9", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket receive listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nasync def receive(self, text_data):\n    text_data_json = json.loads(text_data)\n\n    if self.index is not None:\n        query_str = text_data_json[\"query\"]\n        modified_query_str = f\"Please return a nicely formatted markdown string to this request:\\n\\n{query_str}\"\n        query_engine = self.index.as_query_engine()\n        response = query_engine.query(modified_query_str)\n\n        markdown_response = f\"## Response\\n\\n{response}\\n\\n\"\n        if response.source_nodes:\n            markdown_sources = f\"## Sources\\n\\n{response.get_formatted_sources()}\"\n        else:\n            markdown_sources = \"\"\n\n        formatted_response = f\"{markdown_response}{markdown_sources}\"\n\n        await self.send(json.dumps({\"response\": formatted_response}, indent=4))\n    else:\n        await self.send(json.dumps({\"error\": \"No index loaded for this connection.\"}, indent=4))\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket receive listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo load the collection model, the `load_collection_model` function is used, which can be found\nin `delphic/utils/collections.py`.", "start_char_idx": 2849, "end_char_idx": 4763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09770b69-569d-422e-89e6-b71b288c5ae1": {"__data__": {"id_": "09770b69-569d-422e-89e6-b71b288c5ae1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e00d7077-fbdc-410e-88a2-dbfd059192e8", "node_type": "1", "metadata": {}, "hash": "92fd6c66bb50454d98646ed7fa849b194914a14c4ac173910fc7ed3d84160dc9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "522de47a-97a6-4d73-948f-ad40577a9d06", "node_type": "1", "metadata": {}, "hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "class_name": "RelatedNodeInfo"}}, "hash": "9ea998e60bc700d38817a711ca13064a0b448dc8298c8617348a1ae30ebaf99a", "text": "This\nfunction retrieves the collection object with the given collection ID, checks if a JSON file for the collection model\nexists, and if not, creates one. Then, it sets up the `LLMPredictor` and `ServiceContext` before loading\nthe `VectorStoreIndex` using the cache file.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Websocket receive listener\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nasync def load_collection_model(collection_id: str | int) -> VectorStoreIndex:\n    \"\"\"\n    Load the Collection model from cache or the database, and return the index.\n\n    Args:\n        collection_id (Union[str, int]): The ID of the Collection model instance.\n\n    Returns:\n        VectorStoreIndex: The loaded index.\n\n    This function performs the following steps:\n    1.", "start_char_idx": 4764, "end_char_idx": 5871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "710f2056-4774-4562-9322-e9bd4e56e47f": {"__data__": {"id_": "710f2056-4774-4562-9322-e9bd4e56e47f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "716fffcb-e467-4b3f-b39b-40b128c950fa", "node_type": "1", "metadata": {}, "hash": "bc1f11fe85f292099878a8af572638629b18b9280808e41971daef1e47e7bd29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e46d80c-1192-4e95-9212-1c040333c957", "node_type": "1", "metadata": {}, "hash": "25049273d985ca30366bd2b3f03812fe6d910677343bd69235c8e6441834a4e9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "716fffcb-e467-4b3f-b39b-40b128c950fa", "node_type": "1", "metadata": {}, "hash": "bc1f11fe85f292099878a8af572638629b18b9280808e41971daef1e47e7bd29", "class_name": "RelatedNodeInfo"}}, "hash": "27e568ed32d6da847614a6fffe329845f6d707f33e8ea4aaa31094133907d0c4", "text": "This function performs the following steps:\n    1. Retrieve the Collection object with the given collection_id.\n    2. Check if a JSON file with the name '/cache/model_{collection_id}.json' exists.\n    3. If the JSON file doesn't exist, load the JSON from the Collection.model FileField and save it to\n       '/cache/model_{collection_id}.json'.\n    4. Call VectorStoreIndex.load_from_disk with the cache_file_path.\n    \"\"\"\n    # Retrieve the Collection object\n    collection = await Collection.objects.aget(id=collection_id)\n    logger.info(f\"load_collection_model() - loaded collection {collection_id}\")\n\n    # Make sure there's a model\n    if collection.model.name:\n        logger.info(\"load_collection_model() - Setup local json index file\")\n\n        # Check if the JSON file exists\n        cache_dir = Path(settings.BASE_DIR) / \"cache\"\n        cache_file_path = cache_dir / f\"model_{collection_id}.json\"\n        if not cache_file_path.exists():\n            cache_dir.mkdir(parents=True, exist_ok=True)\n            with collection.model.open(\"rb\") as model_file:\n                with cache_file_path.open(\"w+\", encoding=\"utf-8\") as cache_file:\n                    cache_file.write(model_file.read().decode(\"utf-8\"))\n\n        # define LLM\n        logger.info(\n            f\"load_collection_model() - Setup service context with tokens {settings.MAX_TOKENS} and \"\n            f\"model {settings.MODEL_NAME}\"\n        )\n        llm_predictor = LLMPredictor(\n            llm=OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=512)\n        )\n        service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n        # Call VectorStoreIndex.load_from_disk\n        logger.info(\"load_collection_model() - Load llama index\")\n        index = VectorStoreIndex.load_from_disk(\n            cache_file_path, service_context=service_context\n        )\n        logger.info(\n            \"load_collection_model() - Llamaindex loaded and ready for query...\"\n        )\n\n    else:\n        logger.error(\n            f\"load_collection_model() - collection {collection_id} has no model!\"\n        )\n        raise ValueError(\"No model exists for this collection!\")", "start_char_idx": 0, "end_char_idx": 2170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e46d80c-1192-4e95-9212-1c040333c957": {"__data__": {"id_": "7e46d80c-1192-4e95-9212-1c040333c957", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "716fffcb-e467-4b3f-b39b-40b128c950fa", "node_type": "1", "metadata": {}, "hash": "bc1f11fe85f292099878a8af572638629b18b9280808e41971daef1e47e7bd29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "710f2056-4774-4562-9322-e9bd4e56e47f", "node_type": "1", "metadata": {}, "hash": "27e568ed32d6da847614a6fffe329845f6d707f33e8ea4aaa31094133907d0c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9cf4a007-4214-431f-acab-37e91d82c897", "node_type": "1", "metadata": {}, "hash": "73b09ad8eb672f367f4688a9fd60f191c20c3619a31f7dd53f6b857df4dcc7a1", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "716fffcb-e467-4b3f-b39b-40b128c950fa", "node_type": "1", "metadata": {}, "hash": "bc1f11fe85f292099878a8af572638629b18b9280808e41971daef1e47e7bd29", "class_name": "RelatedNodeInfo"}}, "hash": "25049273d985ca30366bd2b3f03812fe6d910677343bd69235c8e6441834a4e9", "text": ")\n        raise ValueError(\"No model exists for this collection!\")\n\n    return index\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Overview\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe chose to use TypeScript, React and Material-UI (MUI) for the Delphic project\u2019s frontend for a couple reasons. First,\nas the most popular component library (MUI) for the most popular frontend framework (React), this choice makes this\nproject accessible to a huge community of developers. Second, React is, at this point, a stable and generally well-liked\nframework that delivers valuable abstractions in the form of its virtual DOM while still being relatively stable and, in\nour opinion, pretty easy to learn, again making it accessible.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Frontend Project Structure\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe frontend can be found in the `/frontend` directory of the\nrepo, with the React-related components being in `/frontend/src` . You\u2019ll notice there is a DockerFile in the `frontend`\ndirectory and several folders and files related to configuring our frontend web\nserver \u2014 nginx.\n\nThe `/frontend/src/App.tsx` file serves as the entry point of the application.", "start_char_idx": 2104, "end_char_idx": 3992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cf4a007-4214-431f-acab-37e91d82c897": {"__data__": {"id_": "9cf4a007-4214-431f-acab-37e91d82c897", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "716fffcb-e467-4b3f-b39b-40b128c950fa", "node_type": "1", "metadata": {}, "hash": "bc1f11fe85f292099878a8af572638629b18b9280808e41971daef1e47e7bd29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e46d80c-1192-4e95-9212-1c040333c957", "node_type": "1", "metadata": {}, "hash": "25049273d985ca30366bd2b3f03812fe6d910677343bd69235c8e6441834a4e9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "716fffcb-e467-4b3f-b39b-40b128c950fa", "node_type": "1", "metadata": {}, "hash": "bc1f11fe85f292099878a8af572638629b18b9280808e41971daef1e47e7bd29", "class_name": "RelatedNodeInfo"}}, "hash": "73b09ad8eb672f367f4688a9fd60f191c20c3619a31f7dd53f6b857df4dcc7a1", "text": "The `/frontend/src/App.tsx` file serves as the entry point of the application. It defines the main components, such as\nthe login form, the drawer layout, and the collection create modal. The main components are conditionally rendered based\non whether the user is logged in and has an authentication token.\n\nThe DrawerLayout2 component is defined in the`DrawerLayour2.tsx` file. This component manages the layout of the\napplication and provides the navigation and main content areas.\n\nSince the application is relatively simple, we can get away with not using a complex state management solution like\nRedux and just use React\u2019s useState hooks.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Grabbing Collections from the Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe collections available to the logged-in user are retrieved and displayed in the DrawerLayout2 component. The process\ncan be broken down into the following steps:\n\n1.", "start_char_idx": 3914, "end_char_idx": 5197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92b82f76-caaa-4ca6-811c-8c208c13734c": {"__data__": {"id_": "92b82f76-caaa-4ca6-811c-8c208c13734c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dad26612-babc-4a44-84ef-3f98b427ac0c", "node_type": "1", "metadata": {}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cd5c2bd-963a-4ba5-9615-7c0016d9970c", "node_type": "1", "metadata": {}, "hash": "3e70abe3b1cb13dca53e3718aaa420440e67dedb2424ffde92a73e759a93b921", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dad26612-babc-4a44-84ef-3f98b427ac0c", "node_type": "1", "metadata": {}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "class_name": "RelatedNodeInfo"}}, "hash": "639bfe734f2a88b0110be2ef9013ef30dcf1ffbbed92abd588103006ed56ee2f", "text": "The process\ncan be broken down into the following steps:\n\n1. Initializing state variables:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Grabbing Collections from the Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nconst[collections, setCollections] = useState < CollectionModelSchema[] > ([]);\nconst[loading, setLoading] = useState(true);\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Grabbing Collections from the Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere, we initialize two state variables: `collections` to store the list of collections and `loading` to track whether\nthe collections are being fetched.\n\n2.", "start_char_idx": 0, "end_char_idx": 1317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cd5c2bd-963a-4ba5-9615-7c0016d9970c": {"__data__": {"id_": "2cd5c2bd-963a-4ba5-9615-7c0016d9970c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dad26612-babc-4a44-84ef-3f98b427ac0c", "node_type": "1", "metadata": {}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92b82f76-caaa-4ca6-811c-8c208c13734c", "node_type": "1", "metadata": {}, "hash": "639bfe734f2a88b0110be2ef9013ef30dcf1ffbbed92abd588103006ed56ee2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "915f2f95-9027-49f7-a8e8-9910c2c23fe6", "node_type": "1", "metadata": {}, "hash": "d2a6e992b350c20d4b35cc304584cf15a2019c72427a44dbb788ab2d830720a7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dad26612-babc-4a44-84ef-3f98b427ac0c", "node_type": "1", "metadata": {}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "class_name": "RelatedNodeInfo"}}, "hash": "3e70abe3b1cb13dca53e3718aaa420440e67dedb2424ffde92a73e759a93b921", "text": "2. Collections are fetched for the logged-in user with the `fetchCollections()` function:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Grabbing Collections from the Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nconst\nfetchCollections = async () = > {\ntry {\nconst accessToken = localStorage.getItem(\"accessToken\");\nif (accessToken) {\nconst response = await getMyCollections(accessToken);\nsetCollections(response.data);\n}\n} catch (error) {\nconsole.error(error);\n} finally {\nsetLoading(false);\n}\n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Grabbing Collections from the Backend\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `fetchCollections` function retrieves the collections for the logged-in user by calling the `getMyCollections` API\nfunction with the user's access token. It then updates the `collections` state with the retrieved data and sets\nthe `loading` state to `false` to indicate that fetching is complete.", "start_char_idx": 1315, "end_char_idx": 2934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "915f2f95-9027-49f7-a8e8-9910c2c23fe6": {"__data__": {"id_": "915f2f95-9027-49f7-a8e8-9910c2c23fe6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dad26612-babc-4a44-84ef-3f98b427ac0c", "node_type": "1", "metadata": {}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cd5c2bd-963a-4ba5-9615-7c0016d9970c", "node_type": "1", "metadata": {}, "hash": "3e70abe3b1cb13dca53e3718aaa420440e67dedb2424ffde92a73e759a93b921", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4e4aca4-1228-412c-a869-47566bb7958b", "node_type": "1", "metadata": {}, "hash": "75c417795abfac11f22a6edcc2048221572b458ca0d39da45abd4452ed123214", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dad26612-babc-4a44-84ef-3f98b427ac0c", "node_type": "1", "metadata": {}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "class_name": "RelatedNodeInfo"}}, "hash": "d2a6e992b350c20d4b35cc304584cf15a2019c72427a44dbb788ab2d830720a7", "text": "File Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Displaying Collections\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe latest collectios are displayed in the drawer like this:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Displaying Collections\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n< List >\n{collections.map((collection) = > (\n    < div key={collection.id} >\n    < ListItem disablePadding >\n    < ListItemButton\n    disabled={\n    collection.status != = CollectionStatus.COMPLETE | |\n    !collection.has_model\n    }\n    onClick={() = > handleCollectionClick(collection)}\nselected = {\n    selectedCollection & &\n    selectedCollection.id == = collection.id\n}\n>\n< ListItemText\nprimary = {collection.title} / >\n          {collection.status == = CollectionStatus.RUNNING ?", "start_char_idx": 2936, "end_char_idx": 4396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4e4aca4-1228-412c-a869-47566bb7958b": {"__data__": {"id_": "d4e4aca4-1228-412c-a869-47566bb7958b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dad26612-babc-4a44-84ef-3f98b427ac0c", "node_type": "1", "metadata": {}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "915f2f95-9027-49f7-a8e8-9910c2c23fe6", "node_type": "1", "metadata": {}, "hash": "d2a6e992b350c20d4b35cc304584cf15a2019c72427a44dbb788ab2d830720a7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "dad26612-babc-4a44-84ef-3f98b427ac0c", "node_type": "1", "metadata": {}, "hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "class_name": "RelatedNodeInfo"}}, "hash": "75c417795abfac11f22a6edcc2048221572b458ca0d39da45abd4452ed123214", "text": "(\n    < CircularProgress\n    size={24}\n    style={{position: \"absolute\", right: 16}}\n    / >\n): null}\n< / ListItemButton >\n    < / ListItem >\n        < / div >\n))}\n< / List >\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Displaying Collections\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou\u2019ll notice that the `disabled` property of a collection\u2019s `ListItemButton` is set based on whether the collection's\nstatus is not `CollectionStatus.COMPLETE` or the collection does not have a model (`!collection.has_model`). If either\nof these conditions is true, the button is disabled, preventing users from selecting an incomplete or model-less\ncollection. Where the CollectionStatus is RUNNING, we also show a loading wheel over the button.\n\nIn a separate `useEffect` hook, we check if any collection in the `collections` state has a status\nof `CollectionStatus.RUNNING` or `CollectionStatus.QUEUED`.", "start_char_idx": 4397, "end_char_idx": 5636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e69cfef-1cb9-4673-aadb-5718751b3c8a": {"__data__": {"id_": "9e69cfef-1cb9-4673-aadb-5718751b3c8a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400", "node_type": "1", "metadata": {}, "hash": "dbb8412828fb2951465dbfd852768f350087a020e6e9e05b443ad1d53a4a8442", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70fc7448-eb16-4b30-bf32-459082292708", "node_type": "1", "metadata": {}, "hash": "0414475b61aacf66f7b41cad8a2b1acb68d631c807b45ce18aa162187f5a264b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400", "node_type": "1", "metadata": {}, "hash": "dbb8412828fb2951465dbfd852768f350087a020e6e9e05b443ad1d53a4a8442", "class_name": "RelatedNodeInfo"}}, "hash": "3fcd5abd19cb5b7d3a6574c10bbccd39d08a445aa69178a8e1cc2ae042a2dab5", "text": "If so, we set up an interval to repeatedly call\nthe `fetchCollections` function every 15 seconds (15,000 milliseconds) to update the collection statuses. This way, the\napplication periodically checks for completed collections, and the UI is updated accordingly when the processing is\ndone.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Displaying Collections\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nuseEffect(() = > {\n    let\ninterval: NodeJS.Timeout;\nif (\n    collections.some(\n        (collection) = >\ncollection.status == = CollectionStatus.RUNNING | |\ncollection.status == = CollectionStatus.QUEUED\n)\n) {\n    interval = setInterval(() = > {\n    fetchCollections();\n}, 15000);\n}\nreturn () = > clearInterval(interval);\n}, [collections]);\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat View Component\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `ChatView` component in `frontend/src/chat/ChatView.tsx` is responsible for handling and displaying a chat interface\nfor a user to interact with a collection. The component establishes a WebSocket connection to communicate in real-time\nwith the server, sending and receiving messages.\n\nKey features of the `ChatView` component include:\n\n1. Establishing and managing the WebSocket connection with the server.\n2.", "start_char_idx": 0, "end_char_idx": 1956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70fc7448-eb16-4b30-bf32-459082292708": {"__data__": {"id_": "70fc7448-eb16-4b30-bf32-459082292708", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400", "node_type": "1", "metadata": {}, "hash": "dbb8412828fb2951465dbfd852768f350087a020e6e9e05b443ad1d53a4a8442", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e69cfef-1cb9-4673-aadb-5718751b3c8a", "node_type": "1", "metadata": {}, "hash": "3fcd5abd19cb5b7d3a6574c10bbccd39d08a445aa69178a8e1cc2ae042a2dab5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81e9c672-8e58-419e-a558-49379373a75d", "node_type": "1", "metadata": {}, "hash": "a6e97de4e9fca0100f3c9f3e7df404a61f99b3e107266f0bc4bcccfc41cf1aae", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400", "node_type": "1", "metadata": {}, "hash": "dbb8412828fb2951465dbfd852768f350087a020e6e9e05b443ad1d53a4a8442", "class_name": "RelatedNodeInfo"}}, "hash": "0414475b61aacf66f7b41cad8a2b1acb68d631c807b45ce18aa162187f5a264b", "text": "Establishing and managing the WebSocket connection with the server.\n2. Displaying messages from the user and the server in a chat-like format.\n3. Handling user input to send messages to the server.\n4. Updating the messages state and UI based on received messages from the server.\n5. Displaying connection status and errors, such as loading messages, connecting to the server, or encountering errors\n   while loading a collection.\n\nTogether, all of this allows users to interact with their selected collection with a very smooth, low-latency\nexperience.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe WebSocket connection in the `ChatView` component is used to establish real-time communication between the client and\nthe server. The WebSocket connection is set up and managed in the `ChatView` component as follows:\n\nFirst, we want to initialize the the WebSocket reference:\n\nconst websocket = useRef<WebSocket | null>(null);\n\nA `websocket` reference is created using `useRef`, which holds the WebSocket object that will be used for\ncommunication. `useRef` is a hook in React that allows you to create a mutable reference object that persists across\nrenders. It is particularly useful when you need to hold a reference to a mutable object, such as a WebSocket\nconnection, without causing unnecessary re-renders.\n\nIn the `ChatView` component, the WebSocket connection needs to be established and maintained throughout the lifetime of\nthe component, and it should not trigger a re-render when the connection state changes. By using `useRef`, you ensure\nthat the WebSocket connection is kept as a reference, and the component only re-renders when there are actual state\nchanges, such as updating messages or displaying errors.\n\nThe `setupWebsocket` function is responsible for establishing the WebSocket connection and setting up event handlers to\nhandle different WebSocket events.", "start_char_idx": 1886, "end_char_idx": 4178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81e9c672-8e58-419e-a558-49379373a75d": {"__data__": {"id_": "81e9c672-8e58-419e-a558-49379373a75d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400", "node_type": "1", "metadata": {}, "hash": "dbb8412828fb2951465dbfd852768f350087a020e6e9e05b443ad1d53a4a8442", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70fc7448-eb16-4b30-bf32-459082292708", "node_type": "1", "metadata": {}, "hash": "0414475b61aacf66f7b41cad8a2b1acb68d631c807b45ce18aa162187f5a264b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400", "node_type": "1", "metadata": {}, "hash": "dbb8412828fb2951465dbfd852768f350087a020e6e9e05b443ad1d53a4a8442", "class_name": "RelatedNodeInfo"}}, "hash": "a6e97de4e9fca0100f3c9f3e7df404a61f99b3e107266f0bc4bcccfc41cf1aae", "text": "Overall, the setupWebsocket function looks like this:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nconst setupWebsocket = () => {  \n  setConnecting(true);  \n  // Here, a new WebSocket object is created using the specified URL, which includes the   \n  // selected collection's ID and the user's authentication token.  \n    \n  websocket.current = new WebSocket(  \n    `ws://localhost:8000/ws/collections/${selectedCollection.id}/query/?token=${authToken}`  \n  );  \n  \n  websocket.current.onopen = (event) => {  \n    //...  \n  };  \n  \n  websocket.current.onmessage = (event) => {  \n    //...  \n  };  \n  \n  websocket.current.onclose = (event) => {  \n    //...  \n  };  \n  \n  websocket.current.onerror = (event) => {  \n    //...  \n  };  \n  \n  return () => {  \n    websocket.current?.close();  \n  };  \n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNotice in a bunch of places we trigger updates to the GUI based on the information from the web socket client.\n\nWhen the component first opens and we try to establish a connection, the `onopen` listener is triggered.", "start_char_idx": 4180, "end_char_idx": 6061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fa243cd-4c13-470e-85e8-6cc591c573e0": {"__data__": {"id_": "5fa243cd-4c13-470e-85e8-6cc591c573e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14c27008-5937-4330-9775-61c1868104c5", "node_type": "1", "metadata": {}, "hash": "43ed4273bd08cd6ce2d235818fa84cd3b440ff95d3eb937e7239f491abcf7a06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81f733f2-318d-48cd-b64c-6dd5478fbd63", "node_type": "1", "metadata": {}, "hash": "4f1a70ed93695c911ac863d09b929d88865d8a884df57ce1b02e813c67530bd9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "14c27008-5937-4330-9775-61c1868104c5", "node_type": "1", "metadata": {}, "hash": "43ed4273bd08cd6ce2d235818fa84cd3b440ff95d3eb937e7239f491abcf7a06", "class_name": "RelatedNodeInfo"}}, "hash": "f5d73266a3264fa5befc9df822af751b8aa111e30e5002cc4d06cc35663f5306", "text": "In the\ncallback, the component updates the states to reflect that the connection is established, any previous errors are\ncleared, and no messages are awaiting responses:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwebsocket.current.onopen = (event) => {  \n  setError(false);  \n  setConnecting(false);  \n  setAwaitingMessage(false);  \n  \n  console.log(\"WebSocket connected:\", event);  \n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n`onmessage`is triggered when a new message is received from the server through the WebSocket connection.", "start_char_idx": 0, "end_char_idx": 1360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81f733f2-318d-48cd-b64c-6dd5478fbd63": {"__data__": {"id_": "81f733f2-318d-48cd-b64c-6dd5478fbd63", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14c27008-5937-4330-9775-61c1868104c5", "node_type": "1", "metadata": {}, "hash": "43ed4273bd08cd6ce2d235818fa84cd3b440ff95d3eb937e7239f491abcf7a06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fa243cd-4c13-470e-85e8-6cc591c573e0", "node_type": "1", "metadata": {}, "hash": "f5d73266a3264fa5befc9df822af751b8aa111e30e5002cc4d06cc35663f5306", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc39d683-788b-4286-8c03-a2237a41817c", "node_type": "1", "metadata": {}, "hash": "761fe765126de7e161e0f951e8e21a8a016a49a38b74fff2ade5518ad3d3683d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "14c27008-5937-4330-9775-61c1868104c5", "node_type": "1", "metadata": {}, "hash": "43ed4273bd08cd6ce2d235818fa84cd3b440ff95d3eb937e7239f491abcf7a06", "class_name": "RelatedNodeInfo"}}, "hash": "4f1a70ed93695c911ac863d09b929d88865d8a884df57ce1b02e813c67530bd9", "text": "In the\ncallback, the received data is parsed and the `messages` state is updated with the new message from the server:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwebsocket.current.onmessage = (event) => {  \n  const data = JSON.parse(event.data);  \n  console.log(\"WebSocket message received:\", data);  \n  setAwaitingMessage(false);  \n  \n  if (data.response) {  \n    // Update the messages state with the new message from the server  \n    setMessages((prevMessages) => [  \n      ...prevMessages,  \n      {  \n        sender_id: \"server\",  \n        message: data.response,  \n        timestamp: new Date().toLocaleTimeString(),  \n      },  \n    ]);  \n  }  \n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n`onclose`is triggered when the WebSocket connection is closed. In the callback, the component checks for a specific\nclose code (`4000`) to display a warning toast and update the component states accordingly.", "start_char_idx": 1361, "end_char_idx": 3092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc39d683-788b-4286-8c03-a2237a41817c": {"__data__": {"id_": "bc39d683-788b-4286-8c03-a2237a41817c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14c27008-5937-4330-9775-61c1868104c5", "node_type": "1", "metadata": {}, "hash": "43ed4273bd08cd6ce2d235818fa84cd3b440ff95d3eb937e7239f491abcf7a06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81f733f2-318d-48cd-b64c-6dd5478fbd63", "node_type": "1", "metadata": {}, "hash": "4f1a70ed93695c911ac863d09b929d88865d8a884df57ce1b02e813c67530bd9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "14c27008-5937-4330-9775-61c1868104c5", "node_type": "1", "metadata": {}, "hash": "43ed4273bd08cd6ce2d235818fa84cd3b440ff95d3eb937e7239f491abcf7a06", "class_name": "RelatedNodeInfo"}}, "hash": "761fe765126de7e161e0f951e8e21a8a016a49a38b74fff2ade5518ad3d3683d", "text": "It also logs the close\nevent:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwebsocket.current.onclose = (event) => {  \n  if (event.code === 4000) {  \n    toast.warning(  \n      \"Selected collection's model is unavailable. Was it created properly?\"  \n    );  \n    setError(true);  \n    setConnecting(false);  \n    setAwaitingMessage(false);  \n  }  \n  console.log(\"WebSocket closed:\", event);  \n};\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFinally, `onerror` is triggered when an error occurs with the WebSocket connection.", "start_char_idx": 3093, "end_char_idx": 4438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "984ee8d9-3183-4a7e-9648-0c93d1a97c3a": {"__data__": {"id_": "984ee8d9-3183-4a7e-9648-0c93d1a97c3a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94", "node_type": "1", "metadata": {}, "hash": "596a282cd8020eed7a0e3e979a696cdcc65fe1221906cd27acd8a7902b1d2f34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "806b115b-52e6-46b9-a17c-8d3f98522927", "node_type": "1", "metadata": {}, "hash": "8e25a79220d9f5175f6a4bd491daa781eeb65ecf702713f8b71f575ab95b61d2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94", "node_type": "1", "metadata": {}, "hash": "596a282cd8020eed7a0e3e979a696cdcc65fe1221906cd27acd8a7902b1d2f34", "class_name": "RelatedNodeInfo"}}, "hash": "c0696ecb6e7a7e99ffa17aecc83a3b9261e882be14eeb9900ead618b581fad05", "text": "In the callback, the component\nupdates the states to reflect the error and logs the error event:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Chat Websocket Client\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwebsocket.current.onerror = (event) => {\n      setError(true);\n      setConnecting(false);\n      setAwaitingMessage(false);\n\n      console.error(\"WebSocket error:\", event);\n    };\n  ```\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Rendering our Chat Messages\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn the `ChatView` component, the layout is determined using CSS styling and Material-UI components. The main layout\nconsists of a container with a `flex` display and a column-oriented `flexDirection`. This ensures that the content\nwithin the container is arranged vertically.\n\nThere are three primary sections within the layout:\n\n1. The chat messages area: This section takes up most of the available space and displays a list of messages exchanged\n   between the user and the server. It has an overflow-y set to \u2018auto\u2019, which allows scrolling when the content\n   overflows the available space. The messages are rendered using the `ChatMessage` component for each message and\n   a `ChatMessageLoading` component to show the loading state while waiting for a server response.\n2.", "start_char_idx": 0, "end_char_idx": 1978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "806b115b-52e6-46b9-a17c-8d3f98522927": {"__data__": {"id_": "806b115b-52e6-46b9-a17c-8d3f98522927", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94", "node_type": "1", "metadata": {}, "hash": "596a282cd8020eed7a0e3e979a696cdcc65fe1221906cd27acd8a7902b1d2f34", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "984ee8d9-3183-4a7e-9648-0c93d1a97c3a", "node_type": "1", "metadata": {}, "hash": "c0696ecb6e7a7e99ffa17aecc83a3b9261e882be14eeb9900ead618b581fad05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "589174b0-b27e-46e7-acfb-3c0f5105cfbc", "node_type": "1", "metadata": {}, "hash": "befd931d431fa7fa99755595701baf4092967b74a8af07a93aae8b602ef2aa0a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94", "node_type": "1", "metadata": {}, "hash": "596a282cd8020eed7a0e3e979a696cdcc65fe1221906cd27acd8a7902b1d2f34", "class_name": "RelatedNodeInfo"}}, "hash": "8e25a79220d9f5175f6a4bd491daa781eeb65ecf702713f8b71f575ab95b61d2", "text": "2. The divider: A Material-UI `Divider` component is used to separate the chat messages area from the input area,\n   creating a clear visual distinction between the two sections.\n3. The input area: This section is located at the bottom and allows the user to type and send messages. It contains\n   a `TextField` component from Material-UI, which is set to accept multiline input with a maximum of 2 rows. The input\n   area also includes a `Button` component to send the message. The user can either click the \"Send\" button or press \"\n   Enter\" on their keyboard to send the message.\n\nThe user inputs accepted in the `ChatView` component are text messages that the user types in the `TextField`. The\ncomponent processes these text inputs and sends them to the server through the WebSocket connection.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Prerequisites\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo deploy the app, you're going to need Docker and Docker Compose installed. If you're on Ubuntu or another, common\nLinux distribution, DigitalOcean has\na great Docker tutorial and\nanother great tutorial\nfor Docker Compose\nyou can follow. If those don't work for you, try\nthe official docker documentation.", "start_char_idx": 1976, "end_char_idx": 3530, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "589174b0-b27e-46e7-acfb-3c0f5105cfbc": {"__data__": {"id_": "589174b0-b27e-46e7-acfb-3c0f5105cfbc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94", "node_type": "1", "metadata": {}, "hash": "596a282cd8020eed7a0e3e979a696cdcc65fe1221906cd27acd8a7902b1d2f34", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "806b115b-52e6-46b9-a17c-8d3f98522927", "node_type": "1", "metadata": {}, "hash": "8e25a79220d9f5175f6a4bd491daa781eeb65ecf702713f8b71f575ab95b61d2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94", "node_type": "1", "metadata": {}, "hash": "596a282cd8020eed7a0e3e979a696cdcc65fe1221906cd27acd8a7902b1d2f34", "class_name": "RelatedNodeInfo"}}, "hash": "befd931d431fa7fa99755595701baf4092967b74a8af07a93aae8b602ef2aa0a", "text": "If those don't work for you, try\nthe official docker documentation.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe project is based on django-cookiecutter, and it\u2019s pretty easy to get it deployed on a VM and configured to serve\nHTTPs traffic for a specific domain. The configuration is somewhat involved, however \u2014 not because of this project, but\nit\u2019s just a fairly involved topic to configure your certificates, DNS, etc.\n\nFor the purposes of this guide, let\u2019s just get running locally. Perhaps we\u2019ll release a guide on production deployment.\nIn the meantime, check out\nthe Django Cookiecutter project docs\nfor starters.\n\nThis guide assumes your goal is to get the application up and running for use. If you want to develop, most likely you\nwon\u2019t want to launch the compose stack with the \u2014 profiles fullstack flag and will instead want to launch the react\nfrontend using the node development server.", "start_char_idx": 3463, "end_char_idx": 4773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb1d53a1-ab5e-4e33-913a-7cf24c76b83f": {"__data__": {"id_": "eb1d53a1-ab5e-4e33-913a-7cf24c76b83f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c", "node_type": "1", "metadata": {}, "hash": "03de927b2bbd47b32970de07588b746f6cf27b4800b8a086bf7ea480effcda95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0990ce95-ea2e-463b-a0eb-aeb5aaa435ae", "node_type": "1", "metadata": {}, "hash": "33fc09321bf9423eb9270691720880d3bc0d92d6daea32c0085439989d78d266", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c", "node_type": "1", "metadata": {}, "hash": "03de927b2bbd47b32970de07588b746f6cf27b4800b8a086bf7ea480effcda95", "class_name": "RelatedNodeInfo"}}, "hash": "10857ada186dc32f49f4b563908b062023ba849d2f0c12bb9374f7ae6a1aeb76", "text": "To deploy, first clone the repo:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngit clone https://github.com/yourusername/delphic.git\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nChange into the project directory:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncd delphic\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.", "start_char_idx": 0, "end_char_idx": 1552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0990ce95-ea2e-463b-a0eb-aeb5aaa435ae": {"__data__": {"id_": "0990ce95-ea2e-463b-a0eb-aeb5aaa435ae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c", "node_type": "1", "metadata": {}, "hash": "03de927b2bbd47b32970de07588b746f6cf27b4800b8a086bf7ea480effcda95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb1d53a1-ab5e-4e33-913a-7cf24c76b83f", "node_type": "1", "metadata": {}, "hash": "10857ada186dc32f49f4b563908b062023ba849d2f0c12bb9374f7ae6a1aeb76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1efa5e14-1c8a-4255-940c-32d59f1c2d60", "node_type": "1", "metadata": {}, "hash": "979482d022fd53b1631eda14492e83ebb25e6c756640d7cd7a0ad73fd9bcc253", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c", "node_type": "1", "metadata": {}, "hash": "03de927b2bbd47b32970de07588b746f6cf27b4800b8a086bf7ea480effcda95", "class_name": "RelatedNodeInfo"}}, "hash": "33fc09321bf9423eb9270691720880d3bc0d92d6daea32c0085439989d78d266", "text": "md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCopy the sample environment files:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmkdir -p ./.envs/.local/  \ncp -a ./docs/sample_envs/local/.frontend ./frontend  \ncp -a ./docs/sample_envs/local/.django ./.envs/.local  \ncp -a ./docs/sample_envs/local/.postgres ./.envs/.local\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nEdit the `.django` and `.postgres` configuration files to include your OpenAI API key and set a unique password for your\ndatabase user.", "start_char_idx": 1552, "end_char_idx": 3202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1efa5e14-1c8a-4255-940c-32d59f1c2d60": {"__data__": {"id_": "1efa5e14-1c8a-4255-940c-32d59f1c2d60", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c", "node_type": "1", "metadata": {}, "hash": "03de927b2bbd47b32970de07588b746f6cf27b4800b8a086bf7ea480effcda95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0990ce95-ea2e-463b-a0eb-aeb5aaa435ae", "node_type": "1", "metadata": {}, "hash": "33fc09321bf9423eb9270691720880d3bc0d92d6daea32c0085439989d78d266", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c", "node_type": "1", "metadata": {}, "hash": "03de927b2bbd47b32970de07588b746f6cf27b4800b8a086bf7ea480effcda95", "class_name": "RelatedNodeInfo"}}, "hash": "979482d022fd53b1631eda14492e83ebb25e6c756640d7cd7a0ad73fd9bcc253", "text": "You can also set the response token limit in the .django file or switch which OpenAI model you want to\nuse. GPT4 is supported, assuming you\u2019re authorized to access it.\n\nBuild the docker compose stack with the `--profiles fullstack` flag:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nsudo docker-compose --profiles fullstack -f local.yml build\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe fullstack flag instructs compose to build a docker container from the frontend folder and this will be launched\nalong with all of the needed, backend containers. It takes a long time to build a production React container, however,\nso we don\u2019t recommend you develop this way. Follow\nthe instructions in the project readme.md for development environment\nsetup instructions.", "start_char_idx": 3203, "end_char_idx": 4778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a56c91e-5fd2-4fbb-908d-1ec63cdb2351": {"__data__": {"id_": "2a56c91e-5fd2-4fbb-908d-1ec63cdb2351", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "936e4894-59f4-4fae-812f-9cedd308bd4a", "node_type": "1", "metadata": {}, "hash": "67a772a64c948b00adc97c3bc5203ff012636751548eecb5a11ad1bb177a5675", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ef5868c-09ff-4247-919d-ec84260604a5", "node_type": "1", "metadata": {}, "hash": "e50037f5b7e3ed4fa4d02e6c478a2737f3ffe4009e74693b43b1f5902870afd6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "936e4894-59f4-4fae-812f-9cedd308bd4a", "node_type": "1", "metadata": {}, "hash": "67a772a64c948b00adc97c3bc5203ff012636751548eecb5a11ad1bb177a5675", "class_name": "RelatedNodeInfo"}}, "hash": "52271a0a7b0c5eaaf4d7b71b5f9c33acdaf6de00922ce4fb3ad0a971cf1b20c0", "text": "Follow\nthe instructions in the project readme.md for development environment\nsetup instructions.\n\nFinally, bring up the application:\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nsudo docker-compose -f local.yml up\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Build and Deploy\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, visit `localhost:3000` in your browser to see the frontend, and use the Delphic application locally.\n\nFile Name: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nContent Type: text\nHeader Path: A Guide to Building a Full-Stack LlamaIndex Web App with Delphic/Django Backend/Websocket Handler/Setup Users\nfile_path: Docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md\nfile_name: fullstack_with_delphic.md\nfile_type: None\nfile_size: 35149\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn order to actually use the application (at the moment, we intend to make it possible to share certain models with\nunauthenticated users), you need a login.", "start_char_idx": 0, "end_char_idx": 1772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ef5868c-09ff-4247-919d-ec84260604a5": {"__data__": {"id_": "3ef5868c-09ff-4247-919d-ec84260604a5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "936e4894-59f4-4fae-812f-9cedd308bd4a", "node_type": "1", "metadata": {}, "hash": "67a772a64c948b00adc97c3bc5203ff012636751548eecb5a11ad1bb177a5675", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a56c91e-5fd2-4fbb-908d-1ec63cdb2351", "node_type": "1", "metadata": {}, "hash": "52271a0a7b0c5eaaf4d7b71b5f9c33acdaf6de00922ce4fb3ad0a971cf1b20c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "748bfeef-d12b-42d3-874e-8737ab6b65ab", "node_type": "1", "metadata": {}, "hash": "938048aaa13e2086dfc23f59935b0b0c3816ca9cd73c3aca9ed0294f7d452da0", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "936e4894-59f4-4fae-812f-9cedd308bd4a", "node_type": "1", "metadata": {}, "hash": "67a772a64c948b00adc97c3bc5203ff012636751548eecb5a11ad1bb177a5675", "class_name": "RelatedNodeInfo"}}, "hash": "e50037f5b7e3ed4fa4d02e6c478a2737f3ffe4009e74693b43b1f5902870afd6", "text": "You can use either a superuser or non-superuser. In either case, someone needs\nto first create a superuser using the console:\n\n**Why set up a Django superuser?** A Django superuser has all the permissions in the application and can manage all\naspects of the system, including creating, modifying, and deleting users, collections, and other data. Setting up a\nsuperuser allows you to fully control and manage the application.\n\n**How to create a Django superuser:**\n\n1 Run the following command to create a superuser:\n\nsudo docker-compose -f local.yml run django python manage.py createsuperuser\n\n2 You will be prompted to provide a username, email address, and password for the superuser. Enter the required\ninformation.\n\n**How to create additional users using Django admin:**\n\n1. Start your Delphic application locally following the deployment instructions.\n2. Visit the Django admin interface by navigating to `http://localhost:8000/admin` in your browser.\n3. Log in with the superuser credentials you created earlier.\n4. Click on \u201cUsers\u201d under the \u201cAuthentication and Authorization\u201d section.\n5. Click on the \u201cAdd user +\u201d button in the top right corner.\n6. Enter the required information for the new user, such as username and password. Click \u201cSave\u201d to create the user.\n7. To grant the new user additional permissions or make them a superuser, click on their username in the user list,\n   scroll down to the \u201cPermissions\u201d section, and configure their permissions accordingly. Save your changes.\n\nFile Name: Docs\\end_to_end_tutorials\\apps.md\nContent Type: text\nHeader Path: Full-Stack Web Application\nfile_path: Docs\\end_to_end_tutorials\\apps.md\nfile_name: apps.md\nfile_type: None\nfile_size: 742\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit.\n\nWe provide tutorials and resources to help you get started in this area.", "start_char_idx": 1773, "end_char_idx": 3847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "748bfeef-d12b-42d3-874e-8737ab6b65ab": {"__data__": {"id_": "748bfeef-d12b-42d3-874e-8737ab6b65ab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "936e4894-59f4-4fae-812f-9cedd308bd4a", "node_type": "1", "metadata": {}, "hash": "67a772a64c948b00adc97c3bc5203ff012636751548eecb5a11ad1bb177a5675", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ef5868c-09ff-4247-919d-ec84260604a5", "node_type": "1", "metadata": {}, "hash": "e50037f5b7e3ed4fa4d02e6c478a2737f3ffe4009e74693b43b1f5902870afd6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "936e4894-59f4-4fae-812f-9cedd308bd4a", "node_type": "1", "metadata": {}, "hash": "67a772a64c948b00adc97c3bc5203ff012636751548eecb5a11ad1bb177a5675", "class_name": "RelatedNodeInfo"}}, "hash": "938048aaa13e2086dfc23f59935b0b0c3816ca9cd73c3aca9ed0294f7d452da0", "text": "We provide tutorials and resources to help you get started in this area.\n\nRelevant Resources:\n- Fullstack Application Guide\n- Fullstack Application with Delphic\n- A Guide to Extracting Terms and Definitions\n- LlamaIndex Starter Pack\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex is an interface between your data and LLM's; it offers the toolkit for you to setup a query interface around your data for any downstream task, whether it's question-answering, summarization, or more.\n\nIn this tutorial, we show you how to build a context augmented chatbot. We use Langchain for the underlying Agent/Chatbot abstractions, and we use LlamaIndex for the data retrieval/lookup/querying! The result is a chatbot agent that has access to a rich set of \"data interface\" Tools that LlamaIndex provides to answer queries over your data.\n\n**Note**: This is a continuation of some initial work building a query interface over SEC 10-K filings - check it out here.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Context\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn this tutorial, we build an \"10-K Chatbot\" by downloading the raw UBER 10-K HTML filings from Dropbox. The user can choose to ask questions regarding the 10-K filings.", "start_char_idx": 3775, "end_char_idx": 5582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8066023d-e814-45de-bdc2-b331be7e8df4": {"__data__": {"id_": "8066023d-e814-45de-bdc2-b331be7e8df4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57d98462-a7ab-4c26-8cfa-c07b48f6f17d", "node_type": "1", "metadata": {}, "hash": "23e160dfe08668d48eae9b90ee4f6b4e2a157446b9f8d5fb2298082058dec76d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a716a35-a27d-46ef-bd71-7c33323126a4", "node_type": "1", "metadata": {}, "hash": "5edfc4a059dbe561c3cec3af0edf6b867e7366149b84e3eb406b84d60c1ca9b4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "57d98462-a7ab-4c26-8cfa-c07b48f6f17d", "node_type": "1", "metadata": {}, "hash": "23e160dfe08668d48eae9b90ee4f6b4e2a157446b9f8d5fb2298082058dec76d", "class_name": "RelatedNodeInfo"}}, "hash": "3fed6bcfd8ed393fadc8380d82f135cfd32d6b7d3437de21136d4030ffcc3537", "text": "The user can choose to ask questions regarding the 10-K filings.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Ingest Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLet's first download the raw 10-k files, from 2019-2022.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/download files\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n!mkdir data\n!wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip\n!unzip data/UBER.zip -d data\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/download files\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe use the Unstructured library to parse the HTML files into formatted text.\nWe have a direct integration with Unstructured through LlamaHub - this allows us to convert any text into a Document format that LlamaIndex can ingest.", "start_char_idx": 0, "end_char_idx": 1588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a716a35-a27d-46ef-bd71-7c33323126a4": {"__data__": {"id_": "1a716a35-a27d-46ef-bd71-7c33323126a4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57d98462-a7ab-4c26-8cfa-c07b48f6f17d", "node_type": "1", "metadata": {}, "hash": "23e160dfe08668d48eae9b90ee4f6b4e2a157446b9f8d5fb2298082058dec76d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8066023d-e814-45de-bdc2-b331be7e8df4", "node_type": "1", "metadata": {}, "hash": "3fed6bcfd8ed393fadc8380d82f135cfd32d6b7d3437de21136d4030ffcc3537", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "57d98462-a7ab-4c26-8cfa-c07b48f6f17d", "node_type": "1", "metadata": {}, "hash": "23e160dfe08668d48eae9b90ee4f6b4e2a157446b9f8d5fb2298082058dec76d", "class_name": "RelatedNodeInfo"}}, "hash": "5edfc4a059dbe561c3cec3af0edf6b867e7366149b84e3eb406b84d60c1ca9b4", "text": "File Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/download files\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import download_loader, VectorStoreIndex, ServiceContext, StorageContext, load_index_from_storage\nfrom pathlib import Path\n\nyears = [2022, 2021, 2020, 2019]\nUnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n\nloader = UnstructuredReader()\ndoc_set = {}\nall_docs = []\nfor year in years:\n    year_docs = loader.load_data(file=Path(f'./data/UBER/UBER_{year}.html'), split_documents=False)\n    # insert year metadata into each year\n    for d in year_docs:\n        d.metadata = {\"year\": year}\n    doc_set[year] = year_docs\n    all_docs.extend(year_docs)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up Vector Indices for each year\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe first setup a vector index for each year. Each vector index allows us \nto ask questions about the 10-K filing of a given year.\n\nWe build each index and save it to disk.", "start_char_idx": 1590, "end_char_idx": 3124, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "437e4f47-ffdd-40dd-a21f-5d074f720231": {"__data__": {"id_": "437e4f47-ffdd-40dd-a21f-5d074f720231", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff0056b9-5f40-4b65-b337-973ab0014686", "node_type": "1", "metadata": {}, "hash": "acade153dff820885cbce8848a7524b9ddbe32f3022cf27fbc0b889763845ebc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8619cde6-3639-40f4-b458-81a52505ab74", "node_type": "1", "metadata": {}, "hash": "8b97e9091dbf7f201d7ebe07a1be3b507ebccbe157c759a5c9d8393ceee932d7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ff0056b9-5f40-4b65-b337-973ab0014686", "node_type": "1", "metadata": {}, "hash": "acade153dff820885cbce8848a7524b9ddbe32f3022cf27fbc0b889763845ebc", "class_name": "RelatedNodeInfo"}}, "hash": "c83ae1fc5b6e3e4310bc9cf4ce8b72de5b6c286e71a5aa635fdd60e7dc252ef5", "text": "We build each index and save it to disk.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/initialize simple vector indices + global vector index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nservice_context = ServiceContext.from_defaults(chunk_size=512)\nindex_set = {}\nfor year in years:\n    storage_context = StorageContext.from_defaults()\n    cur_index = VectorStoreIndex.from_documents(\n        doc_set[year], \n        service_context=service_context,\n        storage_context=storage_context,\n    )\n    index_set[year] = cur_index\n    storage_context.persist(persist_dir=f'./storage/{year}')\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/initialize simple vector indices + global vector index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo load an index from disk, do the following\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Load indices from disk\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.", "start_char_idx": 0, "end_char_idx": 1572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8619cde6-3639-40f4-b458-81a52505ab74": {"__data__": {"id_": "8619cde6-3639-40f4-b458-81a52505ab74", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff0056b9-5f40-4b65-b337-973ab0014686", "node_type": "1", "metadata": {}, "hash": "acade153dff820885cbce8848a7524b9ddbe32f3022cf27fbc0b889763845ebc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "437e4f47-ffdd-40dd-a21f-5d074f720231", "node_type": "1", "metadata": {}, "hash": "c83ae1fc5b6e3e4310bc9cf4ce8b72de5b6c286e71a5aa635fdd60e7dc252ef5", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ff0056b9-5f40-4b65-b337-973ab0014686", "node_type": "1", "metadata": {}, "hash": "acade153dff820885cbce8848a7524b9ddbe32f3022cf27fbc0b889763845ebc", "class_name": "RelatedNodeInfo"}}, "hash": "8b97e9091dbf7f201d7ebe07a1be3b507ebccbe157c759a5c9d8393ceee932d7", "text": "md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_set = {}\nfor year in years:\n    storage_context = StorageContext.from_defaults(persist_dir=f'./storage/{year}')\n    cur_index = load_index_from_storage(storage_context=storage_context)\n    index_set[year] = cur_index\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Composing a Graph to Synthesize Answers Across 10-K Filings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSince we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings. \n\nTo address this, we compose a \"graph\" which consists of a list index defined over the 4 vector indices. Querying this graph would first retrieve information from each vector index, and combine information together via the list index.", "start_char_idx": 1539, "end_char_idx": 2765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e7db9ab-0518-496c-bde8-9b1e0f2955d5": {"__data__": {"id_": "3e7db9ab-0518-496c-bde8-9b1e0f2955d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c51945eb-302c-4d86-90c2-dd965b7f1868", "node_type": "1", "metadata": {}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72077ad7-27c8-4e4d-a54c-097473a95004", "node_type": "1", "metadata": {}, "hash": "0cbcc028a378662bc86a357c042405b1d16feeb240bb91739bf220d84b36548c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c51945eb-302c-4d86-90c2-dd965b7f1868", "node_type": "1", "metadata": {}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "class_name": "RelatedNodeInfo"}}, "hash": "27b20d12ad0764a920f13817ce057194500e5325883a110c7218e49f7716b53a", "text": "File Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Composing a Graph to Synthesize Answers Across 10-K Filings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import ListIndex, LLMPredictor, ServiceContext, load_graph_from_storage\nfrom langchain import OpenAI\nfrom llama_index.indices.composability import ComposableGraph\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/describe each index to help traversal of composed graph\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_summaries = [f\"UBER 10-k Filing for {year} fiscal year\" for year in years]\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define an LLMPredictor set number of output tokens\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, max_tokens=512))\nservice_context = ServiceContext.", "start_char_idx": 0, "end_char_idx": 1606, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72077ad7-27c8-4e4d-a54c-097473a95004": {"__data__": {"id_": "72077ad7-27c8-4e4d-a54c-097473a95004", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c51945eb-302c-4d86-90c2-dd965b7f1868", "node_type": "1", "metadata": {}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e7db9ab-0518-496c-bde8-9b1e0f2955d5", "node_type": "1", "metadata": {}, "hash": "27b20d12ad0764a920f13817ce057194500e5325883a110c7218e49f7716b53a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d23269e7-9c04-4e98-a766-dee3f17a884a", "node_type": "1", "metadata": {}, "hash": "0b33792f6c885e0c443d8ed7057621cbc9ecb5ad0084b43368477b3f89204a7f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c51945eb-302c-4d86-90c2-dd965b7f1868", "node_type": "1", "metadata": {}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "class_name": "RelatedNodeInfo"}}, "hash": "0cbcc028a378662bc86a357c042405b1d16feeb240bb91739bf220d84b36548c", "text": "max_tokens=512))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\nstorage_context = StorageContext.from_defaults()\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/allows us to synthesize information across each index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph = ComposableGraph.from_indices(\n    ListIndex,\n    [index_set[y] for y in years], \n    index_summaries=index_summaries,\n    service_context=service_context,\n    storage_context = storage_context,\n)\nroot_id = graph.root_id\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/[optional] save to disk\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context.persist(persist_dir=f'./storage/root')\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/[optional] load from disk, so you don't need to build graph from scratch\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.", "start_char_idx": 1556, "end_char_idx": 3081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d23269e7-9c04-4e98-a766-dee3f17a884a": {"__data__": {"id_": "d23269e7-9c04-4e98-a766-dee3f17a884a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c51945eb-302c-4d86-90c2-dd965b7f1868", "node_type": "1", "metadata": {}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72077ad7-27c8-4e4d-a54c-097473a95004", "node_type": "1", "metadata": {}, "hash": "0cbcc028a378662bc86a357c042405b1d16feeb240bb91739bf220d84b36548c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58719532-4381-4a45-9db6-9944aa10f590", "node_type": "1", "metadata": {}, "hash": "adef584f01e6af59e2d7e76c2109a1f1a881a50ececec3e3b29da13a99ab432a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c51945eb-302c-4d86-90c2-dd965b7f1868", "node_type": "1", "metadata": {}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "class_name": "RelatedNodeInfo"}}, "hash": "0b33792f6c885e0c443d8ed7057621cbc9ecb5ad0084b43368477b3f89204a7f", "text": "md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph = load_graph_from_storage(\n    root_id=root_id, \n    service_context=service_context,\n    storage_context=storage_context,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up the Tools + Langchain Chatbot Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe use Langchain to setup the outer chatbot agent, which has access to a set of Tools.\nLlamaIndex provides some wrappers around indices and graphs so that they can be easily used within a Tool interface.", "start_char_idx": 3048, "end_char_idx": 3945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58719532-4381-4a45-9db6-9944aa10f590": {"__data__": {"id_": "58719532-4381-4a45-9db6-9944aa10f590", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c51945eb-302c-4d86-90c2-dd965b7f1868", "node_type": "1", "metadata": {}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d23269e7-9c04-4e98-a766-dee3f17a884a", "node_type": "1", "metadata": {}, "hash": "0b33792f6c885e0c443d8ed7057621cbc9ecb5ad0084b43368477b3f89204a7f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c51945eb-302c-4d86-90c2-dd965b7f1868", "node_type": "1", "metadata": {}, "hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "class_name": "RelatedNodeInfo"}}, "hash": "adef584f01e6af59e2d7e76c2109a1f1a881a50ececec3e3b29da13a99ab432a", "text": "File Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/do imports\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\nfrom langchain.agents import initialize_agent\n\nfrom llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/do imports\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe want to define a separate Tool for each index (corresponding to a given year), as well \nas the graph. We can define all tools under a central `LlamaToolkit` interface.\n\nBelow, we define a `IndexToolConfig` for our graph.", "start_char_idx": 3947, "end_char_idx": 5132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edd8b01c-c34b-4ca6-b23a-aeb0a80d8e26": {"__data__": {"id_": "edd8b01c-c34b-4ca6-b23a-aeb0a80d8e26", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c9c6365-4eb9-4dce-bdbc-ece01038550d", "node_type": "1", "metadata": {}, "hash": "b30bec469e11d89dc619bc0ee351932910bd8346b0fee593b11f6d0701193dd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66dafd3b-417d-44f2-9cb8-e17eff5347be", "node_type": "1", "metadata": {}, "hash": "f99156030e53ba4d94d4e838af4b177ab3d28d624b24af4d2f31bec7de363984", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6c9c6365-4eb9-4dce-bdbc-ece01038550d", "node_type": "1", "metadata": {}, "hash": "b30bec469e11d89dc619bc0ee351932910bd8346b0fee593b11f6d0701193dd5", "class_name": "RelatedNodeInfo"}}, "hash": "58667287208b1a9e0508abf65ba4b93b36fe5d3f5719576e0bcec8660a34dc3d", "text": "Below, we define a `IndexToolConfig` for our graph. Note that we also import a `DecomposeQueryTransform` module for use within each vector index within the graph - this allows us to \"decompose\" the overall query into a query that can be answered from each subindex. (see example below).\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define a decompose transform\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor, verbose=True\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define custom retrievers\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\n\ncustom_query_engines = {}\nfor index in index_set.values():\n    query_engine = index.as_query_engine()\n    query_engine = TransformQueryEngine(\n        query_engine,\n        query_transform=decompose_transform,\n        transform_extra_info={'index_summary': index.index_struct.summary},\n    )\n    custom_query_engines[index.index_id] = query_engine\ncustom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n    response_mode='tree_summarize',\n    verbose=True,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.", "start_char_idx": 0, "end_char_idx": 1845, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66dafd3b-417d-44f2-9cb8-e17eff5347be": {"__data__": {"id_": "66dafd3b-417d-44f2-9cb8-e17eff5347be", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c9c6365-4eb9-4dce-bdbc-ece01038550d", "node_type": "1", "metadata": {}, "hash": "b30bec469e11d89dc619bc0ee351932910bd8346b0fee593b11f6d0701193dd5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edd8b01c-c34b-4ca6-b23a-aeb0a80d8e26", "node_type": "1", "metadata": {}, "hash": "58667287208b1a9e0508abf65ba4b93b36fe5d3f5719576e0bcec8660a34dc3d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6c9c6365-4eb9-4dce-bdbc-ece01038550d", "node_type": "1", "metadata": {}, "hash": "b30bec469e11d89dc619bc0ee351932910bd8346b0fee593b11f6d0701193dd5", "class_name": "RelatedNodeInfo"}}, "hash": "f99156030e53ba4d94d4e838af4b177ab3d28d624b24af4d2f31bec7de363984", "text": "md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/construct query engine\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph_query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/tool config\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph_config = IndexToolConfig(\n    query_engine=graph_query_engine,\n    name=f\"Graph Index\",\n    description=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber.", "start_char_idx": 1845, "end_char_idx": 2830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b41fc5f-6cd9-4ffc-aa78-c6d8e54bbe39": {"__data__": {"id_": "4b41fc5f-6cd9-4ffc-aa78-c6d8e54bbe39", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56119b9c-ea21-4227-b3e6-1493babf84d6", "node_type": "1", "metadata": {}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e47dfe0e-495d-440f-9241-cc0c14b80566", "node_type": "1", "metadata": {}, "hash": "51a7d6cf9fa9dea0837c1962826599c21aa1ccfefa887b936858134edb87a003", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56119b9c-ea21-4227-b3e6-1493babf84d6", "node_type": "1", "metadata": {}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "class_name": "RelatedNodeInfo"}}, "hash": "a6c60eee815c7989c80ab8b432e9e1dd7158603deb7f416624e7ae8d05272c30", "text": "\",\n    tool_kwargs={\"return_direct\": True}\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/tool config\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBesides the `IndexToolConfig` object for the graph, we also define an `IndexToolConfig` corresponding to each index:\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define toolkit\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_configs = []\nfor y in range(2019, 2023):\n    query_engine = index_set[y].as_query_engine(\n        similarity_top_k=3,\n    )\n    tool_config = IndexToolConfig(\n        query_engine=query_engine, \n        name=f\"Vector Index {y}\",\n        description=f\"useful for when you want to answer queries about the {y} SEC 10-K for Uber\",\n        tool_kwargs={\"return_direct\": True}\n    )\n    index_configs.append(tool_config)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define toolkit\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.", "start_char_idx": 0, "end_char_idx": 1575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e47dfe0e-495d-440f-9241-cc0c14b80566": {"__data__": {"id_": "e47dfe0e-495d-440f-9241-cc0c14b80566", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56119b9c-ea21-4227-b3e6-1493babf84d6", "node_type": "1", "metadata": {}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b41fc5f-6cd9-4ffc-aa78-c6d8e54bbe39", "node_type": "1", "metadata": {}, "hash": "a6c60eee815c7989c80ab8b432e9e1dd7158603deb7f416624e7ae8d05272c30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7759f0d7-7e25-4712-a447-3adbdb4dafbb", "node_type": "1", "metadata": {}, "hash": "d745e21ac19485d2ba920f486a5a5565994d8965e71f436a9ca63d284b09a7bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56119b9c-ea21-4227-b3e6-1493babf84d6", "node_type": "1", "metadata": {}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "class_name": "RelatedNodeInfo"}}, "hash": "51a7d6cf9fa9dea0837c1962826599c21aa1ccfefa887b936858134edb87a003", "text": "md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFinally, we combine these configs with our `LlamaToolkit`:\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define toolkit\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntoolkit = LlamaToolkit(\n    index_configs=index_configs + [graph_config],\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define toolkit\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFinally, we call `create_llama_chat_agent` to create our Langchain chatbot agent, which\nhas access to the 5 Tools we defined above:\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/define toolkit\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.", "start_char_idx": 1542, "end_char_idx": 2961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7759f0d7-7e25-4712-a447-3adbdb4dafbb": {"__data__": {"id_": "7759f0d7-7e25-4712-a447-3adbdb4dafbb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56119b9c-ea21-4227-b3e6-1493babf84d6", "node_type": "1", "metadata": {}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e47dfe0e-495d-440f-9241-cc0c14b80566", "node_type": "1", "metadata": {}, "hash": "51a7d6cf9fa9dea0837c1962826599c21aa1ccfefa887b936858134edb87a003", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ffa6103-38a6-42ec-afd2-efd386dd31dc", "node_type": "1", "metadata": {}, "hash": "f106a22ad4c509c919246c575588045de51b35395d598c73d384619867dd0f19", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56119b9c-ea21-4227-b3e6-1493babf84d6", "node_type": "1", "metadata": {}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "class_name": "RelatedNodeInfo"}}, "hash": "d745e21ac19485d2ba920f486a5a5565994d8965e71f436a9ca63d284b09a7bb", "text": "md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm=OpenAI(temperature=0)\nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n    verbose=True\n)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can now test the agent with various queries.\n\nIf we test it with a simple \"hello\" query, the agent does not use any Tools.", "start_char_idx": 2928, "end_char_idx": 3773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ffa6103-38a6-42ec-afd2-efd386dd31dc": {"__data__": {"id_": "1ffa6103-38a6-42ec-afd2-efd386dd31dc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56119b9c-ea21-4227-b3e6-1493babf84d6", "node_type": "1", "metadata": {}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7759f0d7-7e25-4712-a447-3adbdb4dafbb", "node_type": "1", "metadata": {}, "hash": "d745e21ac19485d2ba920f486a5a5565994d8965e71f436a9ca63d284b09a7bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56119b9c-ea21-4227-b3e6-1493babf84d6", "node_type": "1", "metadata": {}, "hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "class_name": "RelatedNodeInfo"}}, "hash": "f106a22ad4c509c919246c575588045de51b35395d598c73d384619867dd0f19", "text": "If we test it with a simple \"hello\" query, the agent does not use any Tools.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent_chain.run(input=\"hi, i am bob\")\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: code\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```\n> Entering new AgentExecutor chain...\n\nThought: Do I need to use a tool? No\nAI: Hi Bob, nice to meet you! How can I help you today?\n\n> Finished chain.\n'Hi Bob, nice to meet you! How can I help you today?'", "start_char_idx": 3697, "end_char_idx": 4764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "394f6166-dfb9-459b-94e6-3efd3836717b": {"__data__": {"id_": "394f6166-dfb9-459b-94e6-3efd3836717b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "node_type": "1", "metadata": {}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2dc63329-0589-4bdc-98f1-81d61a1c2796", "node_type": "1", "metadata": {}, "hash": "d396abc0d2a7954867b490612cf643782d2d6ad10e9c34e463f4da59641514f2", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "node_type": "1", "metadata": {}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "class_name": "RelatedNodeInfo"}}, "hash": "5998e617e287e74a5c98d2a8beea209c4a1ae01cab0175188675b42bb9a910d1", "text": "'Hi Bob, nice to meet you! How can I help you today?'\n```\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf we test it with a query regarding the 10-k of a given year, the agent will use\nthe relevant vector index Tool.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nagent_chain.run(input=\"What were some of the biggest risk factors in 2020 for Uber?\")\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: code\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```\n> Entering new AgentExecutor chain...\n\nThought: Do I need to use a tool?", "start_char_idx": 0, "end_char_idx": 1454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2dc63329-0589-4bdc-98f1-81d61a1c2796": {"__data__": {"id_": "2dc63329-0589-4bdc-98f1-81d61a1c2796", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "node_type": "1", "metadata": {}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "394f6166-dfb9-459b-94e6-3efd3836717b", "node_type": "1", "metadata": {}, "hash": "5998e617e287e74a5c98d2a8beea209c4a1ae01cab0175188675b42bb9a910d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a03e044-a964-482d-b48e-4eef196677a9", "node_type": "1", "metadata": {}, "hash": "5600f256804a7386823f9602e7e982c154a9c6e93bf0fb1f756df26724b39f03", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "node_type": "1", "metadata": {}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "class_name": "RelatedNodeInfo"}}, "hash": "d396abc0d2a7954867b490612cf643782d2d6ad10e9c34e463f4da59641514f2", "text": "Yes\nAction: Vector Index 2020\nAction Input: Risk Factors\n...\n\nObservation: \n\nRisk Factors\n\nThe COVID-19 pandemic and the impact of actions to mitigate the pandemic has adversely affected and continues to adversely affect our business, financial condition, and results of operations.\n\n...\n'\\n\\nRisk Factors\\n\\nThe COVID-19 pandemic and the impact of actions to mitigate the pandemic has adversely affected and continues to adversely affect our business,\n\n```\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFinally, if we test it with a query to compare/contrast risk factors across years,\nthe agent will use the graph index Tool.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncross_query_str = (\n    \"Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\"", "start_char_idx": 1455, "end_char_idx": 2919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a03e044-a964-482d-b48e-4eef196677a9": {"__data__": {"id_": "8a03e044-a964-482d-b48e-4eef196677a9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "node_type": "1", "metadata": {}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2dc63329-0589-4bdc-98f1-81d61a1c2796", "node_type": "1", "metadata": {}, "hash": "d396abc0d2a7954867b490612cf643782d2d6ad10e9c34e463f4da59641514f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "faf180dc-3423-4bfe-a347-42687a9736a1", "node_type": "1", "metadata": {}, "hash": "7bd5ddb2adc0265c8adf9526e336f5098463ddbd67a46e80c3ed51cace8cb9ec", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "node_type": "1", "metadata": {}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "class_name": "RelatedNodeInfo"}}, "hash": "5600f256804a7386823f9602e7e982c154a9c6e93bf0fb1f756df26724b39f03", "text": "Give answer in bullet points.\"\n)\nagent_chain.run(input=cross_query_str)\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: code\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Testing the Agent\nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```\n> Entering new AgentExecutor chain...\n\nThought: Do I need to use a tool? Yes\nAction: Graph Index\nAction Input: Compare/contrast the risk factors described in the Uber 10-K across years.> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2022 fiscal year?\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2022 fiscal year?\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 964 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n> Got response: \nThe risk factors described in the Uber 10-K for the 2022 fiscal year include: the potential for changes in the classification of Drivers, the potential for increased competition, the potential for...\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2021 fiscal year?\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2021 fiscal year?\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 590 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n> Got response: \n1.", "start_char_idx": 2889, "end_char_idx": 4860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "faf180dc-3423-4bfe-a347-42687a9736a1": {"__data__": {"id_": "faf180dc-3423-4bfe-a347-42687a9736a1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "node_type": "1", "metadata": {}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a03e044-a964-482d-b48e-4eef196677a9", "node_type": "1", "metadata": {}, "hash": "5600f256804a7386823f9602e7e982c154a9c6e93bf0fb1f756df26724b39f03", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e", "node_type": "1", "metadata": {}, "hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "class_name": "RelatedNodeInfo"}}, "hash": "7bd5ddb2adc0265c8adf9526e336f5098463ddbd67a46e80c3ed51cace8cb9ec", "text": "The COVID-19 pandemic and the impact of actions to mitigate the pandemic have adversely affected and may continue to adversely affect parts of our business.\n\n2. Our business would be adversely ...\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2020 fiscal year?\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2020 fiscal year?", "start_char_idx": 4861, "end_char_idx": 5427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7755ee58-4020-43b4-93de-6884e6fb133b": {"__data__": {"id_": "7755ee58-4020-43b4-93de-6884e6fb133b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4044733-7eda-41ed-ab44-516f207a5536", "node_type": "1", "metadata": {}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4bdb967-6c11-42dd-89b1-fd0eb2b8fb1b", "node_type": "1", "metadata": {}, "hash": "9a7c2a389cbc6e995f4f7cb621e09194f4528bcafc478473fb2af7400c6c560a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e4044733-7eda-41ed-ab44-516f207a5536", "node_type": "1", "metadata": {}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "class_name": "RelatedNodeInfo"}}, "hash": "f4a3c8297b27aebdbfb082b63d0e114aec3d6379d33532a12adf9889bdfba7ef", "text": "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 516 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n> Got response: \nThe risk factors described in the Uber 10-K for the 2020 fiscal year include: the timing of widespread adoption of vaccines against the virus, additional actions that may be taken by governmental ...\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2019 fiscal year?\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.\n> New query:  What are the risk factors described in the Uber 10-K for the 2019 fiscal year?", "start_char_idx": 0, "end_char_idx": 769, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4bdb967-6c11-42dd-89b1-fd0eb2b8fb1b": {"__data__": {"id_": "a4bdb967-6c11-42dd-89b1-fd0eb2b8fb1b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4044733-7eda-41ed-ab44-516f207a5536", "node_type": "1", "metadata": {}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7755ee58-4020-43b4-93de-6884e6fb133b", "node_type": "1", "metadata": {}, "hash": "f4a3c8297b27aebdbfb082b63d0e114aec3d6379d33532a12adf9889bdfba7ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30ac8e2f-551e-4d9d-bd5e-8766930322fb", "node_type": "1", "metadata": {}, "hash": "9a07e902a59b7002fe4bb60a018ecea2629688e0cd434d8ae9f4074e32258b98", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e4044733-7eda-41ed-ab44-516f207a5536", "node_type": "1", "metadata": {}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "class_name": "RelatedNodeInfo"}}, "hash": "9a7c2a389cbc6e995f4f7cb621e09194f4528bcafc478473fb2af7400c6c560a", "text": "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1020 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\nINFO:llama_index.indices.common.tree.base:> Building index from nodes: 0 chunks\n> Got response: \nRisk factors described in the Uber 10-K for the 2019 fiscal year include: competition from other transportation providers; the impact of government regulations; the impact of litigation; the impac...\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 7039 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 72 tokens\n\nObservation: \nIn 2020, the risk factors included the timing of widespread adoption of vaccines against the virus, additional actions that may be taken by governmental authorities, the further impact on the business of Drivers\n\n...\n\n```\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up the Chatbot Loop\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to converse with our SEC-augmented chatbot!", "start_char_idx": 770, "end_char_idx": 2202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30ac8e2f-551e-4d9d-bd5e-8766930322fb": {"__data__": {"id_": "30ac8e2f-551e-4d9d-bd5e-8766930322fb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4044733-7eda-41ed-ab44-516f207a5536", "node_type": "1", "metadata": {}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4bdb967-6c11-42dd-89b1-fd0eb2b8fb1b", "node_type": "1", "metadata": {}, "hash": "9a7c2a389cbc6e995f4f7cb621e09194f4528bcafc478473fb2af7400c6c560a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01d84a18-fafb-4941-967f-3112652eb5a6", "node_type": "1", "metadata": {}, "hash": "318cac52d3ad2c95d507a25c382c26474204b7670893bcc48418b031550ae6fe", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e4044733-7eda-41ed-ab44-516f207a5536", "node_type": "1", "metadata": {}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "class_name": "RelatedNodeInfo"}}, "hash": "9a07e902a59b7002fe4bb60a018ecea2629688e0cd434d8ae9f4074e32258b98", "text": "File Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up the Chatbot Loop\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nwhile True:\n    text_input = input(\"User: \")\n    response = agent_chain.run(input=text_input)\n    print(f'Agent: {response}')\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up the Chatbot Loop\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere's an example of the loop in action:\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Setting up the Chatbot Loop\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nUser:  What were some of the legal proceedings against Uber in 2022?", "start_char_idx": 2204, "end_char_idx": 3596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01d84a18-fafb-4941-967f-3112652eb5a6": {"__data__": {"id_": "01d84a18-fafb-4941-967f-3112652eb5a6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4044733-7eda-41ed-ab44-516f207a5536", "node_type": "1", "metadata": {}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30ac8e2f-551e-4d9d-bd5e-8766930322fb", "node_type": "1", "metadata": {}, "hash": "9a07e902a59b7002fe4bb60a018ecea2629688e0cd434d8ae9f4074e32258b98", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e4044733-7eda-41ed-ab44-516f207a5536", "node_type": "1", "metadata": {}, "hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "class_name": "RelatedNodeInfo"}}, "hash": "318cac52d3ad2c95d507a25c382c26474204b7670893bcc48418b031550ae6fe", "text": "Agent: \n\nIn 2022, legal proceedings against Uber include a motion to compel arbitration, an appeal of a ruling that Proposition 22 is unconstitutional, a complaint alleging that drivers are employees and entitled to protections under the wage and labor laws, a summary judgment motion, allegations of misclassification of drivers and related employment violations in New York, fraud related to certain deductions, class actions in Australia alleging that Uber entities conspired to injure the group members during the period 2014 to 2017 by either directly breaching transport legislation or commissioning offenses against transport legislation by UberX Drivers in Australia, and claims of lost income and decreased value of certain taxi. Additionally, Uber is facing a challenge in California Superior Court alleging that Proposition 22 is unconstitutional, and a preliminary injunction order prohibiting Uber from classifying Drivers as independent contractors and from violating various wage and hour laws.\n\nUser:\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nContent Type: text\nHeader Path: \ud83d\udcac\ud83e\udd16 How to Build a Chatbot/Notebook\nfile_path: Docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md\nfile_name: building_a_chatbot.md\nfile_type: None\nfile_size: 15385\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTake a look at our corresponding notebook.\n\nFile Name: Docs\\end_to_end_tutorials\\chatbots.md\nContent Type: text\nHeader Path: Chatbots\nfile_path: Docs\\end_to_end_tutorials\\chatbots.md\nfile_name: chatbots.md\nfile_type: None\nfile_size: 333\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nChatbots are an incredibly popular use case for LLM's. LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents.", "start_char_idx": 3597, "end_char_idx": 5434, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f09fe913-223f-431a-a64c-75f594606793": {"__data__": {"id_": "f09fe913-223f-431a-a64c-75f594606793", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8", "node_type": "1", "metadata": {}, "hash": "4f258d579fc345c1ae314aa72d27d5a525a23370b1ab6a23f1fe7273d096d1b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9912ac21-cfcb-4e72-8552-12e6c54aa10a", "node_type": "1", "metadata": {}, "hash": "abc91f0c220d89ea534e595591d637b829c08944209c395449596a64fd8c910d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8", "node_type": "1", "metadata": {}, "hash": "4f258d579fc345c1ae314aa72d27d5a525a23370b1ab6a23f1fe7273d096d1b2", "class_name": "RelatedNodeInfo"}}, "hash": "e962278af8fe51e7970f6da1b748c971aa7d7b419d652bf7fe25c173d7780b16", "text": "LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents.\n\nRelevant Resources:\n- Building a Chatbot\n- Using with a LangChain Agent\n\nFile Name: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nContent Type: text\nHeader Path: Discover LlamaIndex Video Series\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nfile_name: discover_llamaindex.md\nfile_type: None\nfile_size: 1275\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis page contains links to videos + associated notebooks for our ongoing video tutorial series \"Discover LlamaIndex\".\n\nFile Name: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nContent Type: text\nHeader Path: Discover LlamaIndex Video Series/SubQuestionQueryEngine + 10K Analysis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nfile_name: discover_llamaindex.md\nfile_type: None\nfile_size: 1275\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis video covers the `SubQuestionQueryEngine` and how it can be applied to financial documents to help decompose complex queries into multiple sub-questions.\n\nYoutube\n\nNotebook\n\nFile Name: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nContent Type: text\nHeader Path: Discover LlamaIndex Video Series/Discord Document Management\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nfile_name: discover_llamaindex.md\nfile_type: None\nfile_size: 1275\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis video covers managing documents from a source that is consantly updating (i.e Discord) and how you can avoid document duplication and save embedding tokens.", "start_char_idx": 0, "end_char_idx": 1725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9912ac21-cfcb-4e72-8552-12e6c54aa10a": {"__data__": {"id_": "9912ac21-cfcb-4e72-8552-12e6c54aa10a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8", "node_type": "1", "metadata": {}, "hash": "4f258d579fc345c1ae314aa72d27d5a525a23370b1ab6a23f1fe7273d096d1b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f09fe913-223f-431a-a64c-75f594606793", "node_type": "1", "metadata": {}, "hash": "e962278af8fe51e7970f6da1b748c971aa7d7b419d652bf7fe25c173d7780b16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1153b072-6b34-4917-b560-7868b64009ff", "node_type": "1", "metadata": {}, "hash": "9cfd71943c0bed86681cd5d18c258a378b6993839810a7117958fa5b0b343a0f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8", "node_type": "1", "metadata": {}, "hash": "4f258d579fc345c1ae314aa72d27d5a525a23370b1ab6a23f1fe7273d096d1b2", "class_name": "RelatedNodeInfo"}}, "hash": "abc91f0c220d89ea534e595591d637b829c08944209c395449596a64fd8c910d", "text": "Youtube\n\nNotebook + Supplimentary Material\n\nReference Docs\n\nFile Name: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nContent Type: text\nHeader Path: Discover LlamaIndex Video Series/Joint Text to SQL and Semantic Search\nfile_path: Docs\\end_to_end_tutorials\\discover_llamaindex.md\nfile_name: discover_llamaindex.md\nfile_type: None\nfile_size: 1275\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis video covers the tools built into LlamaIndex for combining SQL and semantic search into a single unified query interface.\n\nYoutube\n\nNotebook\n\nFile Name: Docs\\end_to_end_tutorials\\privacy.md\nContent Type: text\nHeader Path: Private Setup\nfile_path: Docs\\end_to_end_tutorials\\privacy.md\nfile_name: privacy.md\nfile_type: None\nfile_size: 169\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRelevant Resources:\n- Using LlamaIndex with Local Models\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlama Index has many use cases (semantic search, summarization, etc.) that are well documented. However, this doesn't mean we can't apply Llama Index to very specific use cases!\n\nIn this tutorial, we will go through the design process of using Llama Index to extract terms and definitions from text, while allowing users to query those terms later. Using Streamlit, we can provide an easy way to build frontend for running and testing all of this, and quickly iterate with our design.", "start_char_idx": 1727, "end_char_idx": 3559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1153b072-6b34-4917-b560-7868b64009ff": {"__data__": {"id_": "1153b072-6b34-4917-b560-7868b64009ff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8", "node_type": "1", "metadata": {}, "hash": "4f258d579fc345c1ae314aa72d27d5a525a23370b1ab6a23f1fe7273d096d1b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9912ac21-cfcb-4e72-8552-12e6c54aa10a", "node_type": "1", "metadata": {}, "hash": "abc91f0c220d89ea534e595591d637b829c08944209c395449596a64fd8c910d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8", "node_type": "1", "metadata": {}, "hash": "4f258d579fc345c1ae314aa72d27d5a525a23370b1ab6a23f1fe7273d096d1b2", "class_name": "RelatedNodeInfo"}}, "hash": "9cfd71943c0bed86681cd5d18c258a378b6993839810a7117958fa5b0b343a0f", "text": "This tutorial assumes you have Python3.9+ and the following packages installed:\n\n- llama-index\n- streamlit\n\nAt the base level, our objective is to take text from a document, extract terms and definitions, and then provide a way for users to query that knowledge base of terms and definitions. The tutorial will go over features from both Llama Index and Streamlit, and hopefully provide some interesting solutions for common problems that come up.\n\nThe final version of this tutorial can be found here and a live hosted demo is available on Huggingface Spaces.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Uploading Text\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nStep one is giving users a way to upload documents. Let\u2019s write some code using Streamlit to provide the interface for this! Use the following code and launch the app with `streamlit run app.py`.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Uploading Text\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport streamlit as st\n\nst.title(\"\ud83e\udd99 Llama Index Term Extractor \ud83e\udd99\")\n\ndocument_text = st.text_area(\"Or enter raw text\")\nif st.button(\"Extract Terms and Definitions\") and document_text:\n    with st.spinner(\"Extracting...\"):\n        extracted_terms = document text  # this is a placeholder!", "start_char_idx": 3561, "end_char_idx": 5478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6096b9e3-fcbc-4aad-bbc1-ba2ba633f284": {"__data__": {"id_": "6096b9e3-fcbc-4aad-bbc1-ba2ba633f284", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441", "node_type": "1", "metadata": {}, "hash": "8ba80fd34c4d6077da47a0dd44f97bb9e35b7441e2b96dda4986c048db106e01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bf3ee39-9515-4002-a9f6-049f1d3b26ec", "node_type": "1", "metadata": {}, "hash": "75e19492e328af0e653d6217a1960ac0a3a21cd94e7a8d93a5556e941d3b5abf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441", "node_type": "1", "metadata": {}, "hash": "8ba80fd34c4d6077da47a0dd44f97bb9e35b7441e2b96dda4986c048db106e01", "class_name": "RelatedNodeInfo"}}, "hash": "60e558ee03e6f032bf7befad319be4157403bf56b6f6f74079fd67ba584deb0b", "text": "st.write(extracted_terms)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Uploading Text\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSuper simple right! But you'll notice that the app doesn't do anything useful yet. To use llama_index, we also need to setup our OpenAI LLM. There are a bunch of possible settings for the LLM, so we can let the user figure out what's best. We should also let the user set the prompt that will extract the terms (which will also help us debug what works best).\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/LLM Settings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis next step introduces some tabs to our app, to separate it into different panes that provide different features.", "start_char_idx": 0, "end_char_idx": 1374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bf3ee39-9515-4002-a9f6-049f1d3b26ec": {"__data__": {"id_": "6bf3ee39-9515-4002-a9f6-049f1d3b26ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441", "node_type": "1", "metadata": {}, "hash": "8ba80fd34c4d6077da47a0dd44f97bb9e35b7441e2b96dda4986c048db106e01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6096b9e3-fcbc-4aad-bbc1-ba2ba633f284", "node_type": "1", "metadata": {}, "hash": "60e558ee03e6f032bf7befad319be4157403bf56b6f6f74079fd67ba584deb0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca4d5a00-f5ab-47a8-97c9-00f8f7660b47", "node_type": "1", "metadata": {}, "hash": "46c9dbb47802daf7343e30af7872af0c00e8c5b1d504afb0a6ebf84bad034688", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441", "node_type": "1", "metadata": {}, "hash": "8ba80fd34c4d6077da47a0dd44f97bb9e35b7441e2b96dda4986c048db106e01", "class_name": "RelatedNodeInfo"}}, "hash": "75e19492e328af0e653d6217a1960ac0a3a21cd94e7a8d93a5556e941d3b5abf", "text": "Let's create a tab for LLM settings and for uploading text:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/LLM Settings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport os\nimport streamlit as st\n\nDEFAULT_TERM_STR = (\n    \"Make a list of terms and definitions that are defined in the context, \"\n    \"with one pair on each line. \"\n    \"If a term is missing it's definition, use your best judgment. \"\n    \"Write each line as as follows:\\nTerm: <term> Definition: <definition>\"\n)\n\nst.title(\"\ud83e\udd99 Llama Index Term Extractor \ud83e\udd99\")\n\nsetup_tab, upload_tab = st.tabs([\"Setup\", \"Upload/Extract Terms\"])\n\nwith setup_tab:\n    st.subheader(\"LLM Setup\")\n    api_key = st.text_input(\"Enter your OpenAI API key here\", type=\"password\")\n    llm_name = st.selectbox('Which LLM?', [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-4\"])\n    model_temperature = st.slider(\"LLM Temperature\", min_value=0.0, max_value=1.0, step=0.1)\n    term_extract_str = st.text_area(\"The query to extract terms and definitions with.\", value=DEFAULT_TERM_STR)\n\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    document_text = st.text_area(\"Or enter raw text\")\n    if st.button(\"Extract Terms and Definitions\") and document_text:\n        with st.spinner(\"Extracting...\"):\n            extracted_terms = document text  # this is a placeholder!", "start_char_idx": 1375, "end_char_idx": 3019, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca4d5a00-f5ab-47a8-97c9-00f8f7660b47": {"__data__": {"id_": "ca4d5a00-f5ab-47a8-97c9-00f8f7660b47", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441", "node_type": "1", "metadata": {}, "hash": "8ba80fd34c4d6077da47a0dd44f97bb9e35b7441e2b96dda4986c048db106e01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bf3ee39-9515-4002-a9f6-049f1d3b26ec", "node_type": "1", "metadata": {}, "hash": "75e19492e328af0e653d6217a1960ac0a3a21cd94e7a8d93a5556e941d3b5abf", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441", "node_type": "1", "metadata": {}, "hash": "8ba80fd34c4d6077da47a0dd44f97bb9e35b7441e2b96dda4986c048db106e01", "class_name": "RelatedNodeInfo"}}, "hash": "46c9dbb47802daf7343e30af7872af0c00e8c5b1d504afb0a6ebf84bad034688", "text": "st.write(extracted_terms)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/LLM Settings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow our app has two tabs, which really helps with the organization. You'll also noticed I added a default prompt to extract terms -- you can change this later once you try extracting some terms, it's just the prompt I arrived at after experimenting a bit.\n\nSpeaking of extracting terms, it's time to add some functions to do just that!\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Extracting and Storing Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow that we are able to define LLM settings and upload text, we can try using Llama Index to extract the terms from text for us!\n\nWe can add the following functions to both initialize our LLM, as well as use it to extract terms from the input text.", "start_char_idx": 3028, "end_char_idx": 4524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55916261-7870-443d-8cbd-4b51467a77d6": {"__data__": {"id_": "55916261-7870-443d-8cbd-4b51467a77d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "node_type": "1", "metadata": {}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76ead195-2cbb-415d-8494-0bcb751662f0", "node_type": "1", "metadata": {}, "hash": "0b8340ad54712d5501d2b8c96bf935cd56aa21baddf7fd1dd33b5efacf7e9fa6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "node_type": "1", "metadata": {}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "class_name": "RelatedNodeInfo"}}, "hash": "832af99c6255ffca1f7494dfe1a03194b4c301546a9dcedc3a0ced00d79db5fe", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Extracting and Storing Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Document, ListIndex, LLMPredictor, ServiceContext, load_index_from_storage\n\ndef get_llm(llm_name, model_temperature, api_key, max_tokens=256):\n    os.environ['OPENAI_API_KEY'] = api_key\n    if llm_name == \"text-davinci-003\":\n        return OpenAI(temperature=model_temperature, model_name=llm_name, max_tokens=max_tokens)\n    else:\n        return ChatOpenAI(temperature=model_temperature, model_name=llm_name, max_tokens=max_tokens)\n\ndef extract_terms(documents, term_extract_str, llm_name, model_temperature, api_key):\n    llm = get_llm(llm_name, model_temperature, api_key, max_tokens=1024)\n\n    service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm),\n                                                   chunk_size=1024)\n\n    temp_index = ListIndex.from_documents(documents, service_context=service_context)\n    query_engine = temp_index.as_query_engine(response_mode=\"tree_summarize\")\n    terms_definitions = str(query_engine.query(term_extract_str))\n    terms_definitions = [x for x in terms_definitions.split(\"\\n\") if x and 'Term:' in x and 'Definition:' in x]\n    # parse the text into a dict\n    terms_to_definition = {x.split(\"Definition:\")[0].split(\"Term:\")[-1].strip(): x.split(\"Definition:\")[-1].strip() for x in terms_definitions}\n    return terms_to_definition\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.", "start_char_idx": 0, "end_char_idx": 1860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76ead195-2cbb-415d-8494-0bcb751662f0": {"__data__": {"id_": "76ead195-2cbb-415d-8494-0bcb751662f0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "node_type": "1", "metadata": {}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55916261-7870-443d-8cbd-4b51467a77d6", "node_type": "1", "metadata": {}, "hash": "832af99c6255ffca1f7494dfe1a03194b4c301546a9dcedc3a0ced00d79db5fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ac06fb4-4158-4fa2-b621-af3966e83d18", "node_type": "1", "metadata": {}, "hash": "ffbafe46a741ae995be77c63257db2865862a47653043c84e4a38b37e9f4e92b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "node_type": "1", "metadata": {}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "class_name": "RelatedNodeInfo"}}, "hash": "0b8340ad54712d5501d2b8c96bf935cd56aa21baddf7fd1dd33b5efacf7e9fa6", "text": "md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Extracting and Storing Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, using the new functions, we can finally extract our terms!\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Extracting and Storing Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    document_text = st.text_area(\"Or enter raw text\")\n    if st.button(\"Extract Terms and Definitions\") and document_text:\n        with st.spinner(\"Extracting...\"):\n            extracted_terms = extract_terms([Document(text=document_text)],\n                                            term_extract_str, llm_name,\n                                            model_temperature, api_key)\n        st.write(extracted_terms)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Extracting and Storing Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThere's a lot going on now, let's take a moment to go over what is happening.\n\n`get_llm()` is instantiating the LLM based on the user configuration from the setup tab.", "start_char_idx": 1860, "end_char_idx": 3849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ac06fb4-4158-4fa2-b621-af3966e83d18": {"__data__": {"id_": "6ac06fb4-4158-4fa2-b621-af3966e83d18", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "node_type": "1", "metadata": {}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76ead195-2cbb-415d-8494-0bcb751662f0", "node_type": "1", "metadata": {}, "hash": "0b8340ad54712d5501d2b8c96bf935cd56aa21baddf7fd1dd33b5efacf7e9fa6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9f6c430-aa2b-43f7-a063-8888fe5435b3", "node_type": "1", "metadata": {}, "hash": "40a91e2f74e267146128ca32c39300d6dfc3e30f4c177854347d1b99b1338f45", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "node_type": "1", "metadata": {}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "class_name": "RelatedNodeInfo"}}, "hash": "ffbafe46a741ae995be77c63257db2865862a47653043c84e4a38b37e9f4e92b", "text": "Based on the model name, we need to use the appropriate class (`OpenAI` vs. `ChatOpenAI`).\n\n`extract_terms()` is where all the good stuff happens. First, we call `get_llm()` with `max_tokens=1024`, since we don't want to limit the model too much when it is extracting our terms and definitions (the default is 256 if not set). Then, we define our `ServiceContext` object, aligning `num_output` with our `max_tokens` value, as well as setting the chunk size to be no larger than the output. When documents are indexed by Llama Index, they are broken into chunks (also called nodes) if they are large, and `chunk_size` sets the size for these chunks.\n\nNext, we create a temporary list index and pass in our service context. A list index will read every single piece of text in our index, which is perfect for extracting terms. Finally, we use our pre-defined query text to extract terms, using `response_mode=\"tree_summarize`. This response mode will generate a tree of summaries from the bottom up, where each parent summarizes its children. Finally, the top of the tree is returned, which will contain all our extracted terms and definitions.\n\nLastly, we do some minor post processing. We assume the model followed instructions and put a term/definition pair on each line. If a line is missing the `Term:` or `Definition:` labels, we skip it. Then, we convert this to a dictionary for easy storage!\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Saving Extracted Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow that we can extract terms, we need to put them somewhere so that we can query for them later. A `VectorStoreIndex` should be a perfect choice for now!", "start_char_idx": 3850, "end_char_idx": 5848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9f6c430-aa2b-43f7-a063-8888fe5435b3": {"__data__": {"id_": "c9f6c430-aa2b-43f7-a063-8888fe5435b3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "node_type": "1", "metadata": {}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ac06fb4-4158-4fa2-b621-af3966e83d18", "node_type": "1", "metadata": {}, "hash": "ffbafe46a741ae995be77c63257db2865862a47653043c84e4a38b37e9f4e92b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "node_type": "1", "metadata": {}, "hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "class_name": "RelatedNodeInfo"}}, "hash": "40a91e2f74e267146128ca32c39300d6dfc3e30f4c177854347d1b99b1338f45", "text": "A `VectorStoreIndex` should be a perfect choice for now! But in addition, our app should also keep track of which terms are inserted into the index so that we can inspect them later. Using `st.session_state`, we can store the current list of terms in a session dict, unique to each user!\n\nFirst things first though, let's add a feature to initialize a global vector index and another function to insert the extracted terms.", "start_char_idx": 5792, "end_char_idx": 6215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26917647-8356-409e-b1d2-f587e7ad7b88": {"__data__": {"id_": "26917647-8356-409e-b1d2-f587e7ad7b88", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47", "node_type": "1", "metadata": {}, "hash": "49defd404fba505f60b24efbb9033e99a730a0c06bf60ce5ff71441855e30fc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d135c7d-8eaa-478b-bdd1-ad2578ab3094", "node_type": "1", "metadata": {}, "hash": "f68f1ccfc523b8bac200e4e155065f005e11a23b1c6424910bb6c67198547cc7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47", "node_type": "1", "metadata": {}, "hash": "49defd404fba505f60b24efbb9033e99a730a0c06bf60ce5ff71441855e30fc1", "class_name": "RelatedNodeInfo"}}, "hash": "80c017d2c18b86e0d25937a58ca35edad55185351eb0f7a21dd4a37912e05258", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Saving Extracted Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\nif 'all_terms' not in st.session_state:\n    st.session_state['all_terms'] = DEFAULT_TERMS\n...\n\ndef insert_terms(terms_to_definition):\n    for term, definition in terms_to_definition.items():\n        doc = Document(text=f\"Term: {term}\\nDefinition: {definition}\")\n        st.session_state['llama_index'].insert(doc)\n\n@st.cache_resource\ndef initialize_index(llm_name, model_temperature, api_key):\n    \"\"\"Create the VectorStoreIndex object.\"\"\"\n    llm = get_llm(llm_name, model_temperature, api_key)\n\n    service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n\n    index = VectorStoreIndex([], service_context=service_context)\n\n    return index\n\n...\n\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    if st.button(\"Initialize Index and Reset Terms\"):\n        st.session_state['llama_index'] = initialize_index(llm_name, model_temperature, api_key)\n        st.session_state['all_terms'] = {}\n\n    if \"llama_index\" in st.session_state:\n        st.markdown(\"Either upload an image/screenshot of a document, or enter the text manually.\")\n        document_text = st.text_area(\"Or enter raw text\")\n        if st.button(\"Extract Terms and Definitions\") and (uploaded_file or document_text):\n            st.session_state['terms'] = {}\n            terms_docs = {}\n            with st.spinner(\"Extracting...\"):\n                terms_docs.update(extract_terms([Document(text=document_text)], term_extract_str, llm_name, model_temperature, api_key))\n            st.session_state['terms'].update(terms_docs)\n\n        if \"terms\" in st.session_state and st.session_state[\"terms\"]::\n            st.markdown(\"Extracted terms\")\n            st.json(st.session_state['terms'])\n\n            if st.button(\"Insert terms?", "start_char_idx": 0, "end_char_idx": 2191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d135c7d-8eaa-478b-bdd1-ad2578ab3094": {"__data__": {"id_": "2d135c7d-8eaa-478b-bdd1-ad2578ab3094", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47", "node_type": "1", "metadata": {}, "hash": "49defd404fba505f60b24efbb9033e99a730a0c06bf60ce5ff71441855e30fc1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26917647-8356-409e-b1d2-f587e7ad7b88", "node_type": "1", "metadata": {}, "hash": "80c017d2c18b86e0d25937a58ca35edad55185351eb0f7a21dd4a37912e05258", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e5af7bb-9eff-44b7-a85a-279d750c3dc2", "node_type": "1", "metadata": {}, "hash": "d5395ade6f5a94df0aeadd66a22b1cebc4396c4cacd3a85ec4bc08bf2841eed6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47", "node_type": "1", "metadata": {}, "hash": "49defd404fba505f60b24efbb9033e99a730a0c06bf60ce5ff71441855e30fc1", "class_name": "RelatedNodeInfo"}}, "hash": "f68f1ccfc523b8bac200e4e155065f005e11a23b1c6424910bb6c67198547cc7", "text": "\"):\n                with st.spinner(\"Inserting terms\"):\n                    insert_terms(st.session_state['terms'])\n                st.session_state['all_terms'].update(st.session_state['terms'])\n                st.session_state['terms'] = {}\n                st.experimental_rerun()\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Saving Extracted Terms\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow you are really starting to leverage the power of streamlit! Let's start with the code under the upload tab. We added a button to initialize the vector index, and we store it in the global streamlit state dictionary, as well as resetting the currently extracted terms. Then, after extracting terms from the input text, we store it the extracted terms in the global state again and give the user a chance to review them before inserting. If the insert button is pressed, then we call our insert terms function, update our global tracking of inserted terms, and remove the most recently extracted terms from the session state.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Querying for Extracted Terms/Definitions\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWith the terms and definitions extracted and saved, how can we use them? And how will the user even remember what's previously been saved?? We can simply add some more tabs to the app to handle these features.", "start_char_idx": 2191, "end_char_idx": 4219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e5af7bb-9eff-44b7-a85a-279d750c3dc2": {"__data__": {"id_": "6e5af7bb-9eff-44b7-a85a-279d750c3dc2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47", "node_type": "1", "metadata": {}, "hash": "49defd404fba505f60b24efbb9033e99a730a0c06bf60ce5ff71441855e30fc1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d135c7d-8eaa-478b-bdd1-ad2578ab3094", "node_type": "1", "metadata": {}, "hash": "f68f1ccfc523b8bac200e4e155065f005e11a23b1c6424910bb6c67198547cc7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47", "node_type": "1", "metadata": {}, "hash": "49defd404fba505f60b24efbb9033e99a730a0c06bf60ce5ff71441855e30fc1", "class_name": "RelatedNodeInfo"}}, "hash": "d5395ade6f5a94df0aeadd66a22b1cebc4396c4cacd3a85ec4bc08bf2841eed6", "text": "We can simply add some more tabs to the app to handle these features.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Querying for Extracted Terms/Definitions\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\nsetup_tab, terms_tab, upload_tab, query_tab = st.tabs(\n    [\"Setup\", \"All Terms\", \"Upload/Extract Terms\", \"Query Terms\"]\n)\n...\nwith terms_tab:\n    with terms_tab:\n    st.subheader(\"Current Extracted Terms and Definitions\")\n    st.json(st.session_state[\"all_terms\"])\n...\nwith query_tab:\n    st.subheader(\"Query for Terms/Definitions!\")\n    st.markdown(\n        (\n            \"The LLM will attempt to answer your query, and augment it's answers using the terms/definitions you've inserted. \"\n            \"If a term is not in the index, it will answer using it's internal knowledge.\"\n        )\n    )\n    if st.button(\"Initialize Index and Reset Terms\", key=\"init_index_2\"):\n        st.session_state[\"llama_index\"] = initialize_index(\n            llm_name, model_temperature, api_key\n        )\n        st.session_state[\"all_terms\"] = {}\n\n    if \"llama_index\" in st.session_state:\n        query_text = st.text_input(\"Ask about a term or definition:\")\n        if query_text:\n            query_text = query_text + \"\\nIf you can't find the answer, answer the query with the best of your knowledge.\"", "start_char_idx": 4150, "end_char_idx": 5777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1b98ca6-819a-4536-81f2-b981441c17c1": {"__data__": {"id_": "a1b98ca6-819a-4536-81f2-b981441c17c1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201", "node_type": "1", "metadata": {}, "hash": "fd598698587a580a0391bf9cc9c05ce5d28d41129bf3d7ad454c42b7a8a9cf11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24995949-e50d-41b4-80ed-8a7b44de0c9d", "node_type": "1", "metadata": {}, "hash": "e5253e4e6c6bcae38b8d49d777700a90eb75e26b7db0561a73f6b11861690b12", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201", "node_type": "1", "metadata": {}, "hash": "fd598698587a580a0391bf9cc9c05ce5d28d41129bf3d7ad454c42b7a8a9cf11", "class_name": "RelatedNodeInfo"}}, "hash": "777c60a53b55932834eb4efdfb02f1371a49893f9de0fd3c8239bcd430c3ba95", "text": "with st.spinner(\"Generating answer...\"):\n                response = st.session_state[\"llama_index\"].query(\n                    query_text, similarity_top_k=5, response_mode=\"compact\"\n                )\n            st.markdown(str(response))\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Querying for Extracted Terms/Definitions\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWhile this is mostly basic, some important things to note:\n\n- Our initialize button has the same text as our other button. Streamlit will complain about this, so we provide a unique key instead.\n- Some additional text has been added to the query! This is to try and compensate for times when the index does not have the answer.\n- In our index query, we've specified two options:\n  - `similarity_top_k=5` means the index will fetch the top 5 closest matching terms/definitions to the query.\n  - `response_mode=\"compact\"` means as much text as possible from the 5 matching terms/definitions will be used in each LLM call. Without this, the index would make at least 5 calls to the LLM, which can slow things down for the user.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWell, actually I hope you've been testing as we went. But now, let's try one complete test.\n\n1. Refresh the app\n2. Enter your LLM settings\n3.", "start_char_idx": 0, "end_char_idx": 2004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24995949-e50d-41b4-80ed-8a7b44de0c9d": {"__data__": {"id_": "24995949-e50d-41b4-80ed-8a7b44de0c9d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201", "node_type": "1", "metadata": {}, "hash": "fd598698587a580a0391bf9cc9c05ce5d28d41129bf3d7ad454c42b7a8a9cf11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1b98ca6-819a-4536-81f2-b981441c17c1", "node_type": "1", "metadata": {}, "hash": "777c60a53b55932834eb4efdfb02f1371a49893f9de0fd3c8239bcd430c3ba95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8341126-de12-4e80-9eb5-b118eca3e5a4", "node_type": "1", "metadata": {}, "hash": "c3521d5953a0a69dae1745e32ab578d92da60e78e0eb773330e7bc9abf63dd38", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201", "node_type": "1", "metadata": {}, "hash": "fd598698587a580a0391bf9cc9c05ce5d28d41129bf3d7ad454c42b7a8a9cf11", "class_name": "RelatedNodeInfo"}}, "hash": "e5253e4e6c6bcae38b8d49d777700a90eb75e26b7db0561a73f6b11861690b12", "text": "1. Refresh the app\n2. Enter your LLM settings\n3. Head over to the query tab\n4. Ask the following: `What is a bunnyhug?`\n5. The app should give some nonsense response. If you didn't know, a bunnyhug is another word for a hoodie, used by people from the Canadian Prairies!\n6. Let's add this definition to the app. Open the upload tab and enter the following text: `A bunnyhug is a common term used to describe a hoodie. This term is used by people from the Canadian Prairies.`\n7. Click the extract button. After a few moments, the app should display the correctly extracted term/definition. Click the insert term button to save it!\n8. If we open the terms tab, the term and definition we just extracted should be displayed\n9. Go back to the query tab and try asking what a bunnyhug is. Now, the answer should be correct!\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWith our base app working, it might feel like a lot of work to build up a useful index. What if we gave the user some kind of starting point to show off the app's query capabilities? We can do just that!", "start_char_idx": 1956, "end_char_idx": 3453, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8341126-de12-4e80-9eb5-b118eca3e5a4": {"__data__": {"id_": "c8341126-de12-4e80-9eb5-b118eca3e5a4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201", "node_type": "1", "metadata": {}, "hash": "fd598698587a580a0391bf9cc9c05ce5d28d41129bf3d7ad454c42b7a8a9cf11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24995949-e50d-41b4-80ed-8a7b44de0c9d", "node_type": "1", "metadata": {}, "hash": "e5253e4e6c6bcae38b8d49d777700a90eb75e26b7db0561a73f6b11861690b12", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201", "node_type": "1", "metadata": {}, "hash": "fd598698587a580a0391bf9cc9c05ce5d28d41129bf3d7ad454c42b7a8a9cf11", "class_name": "RelatedNodeInfo"}}, "hash": "c3521d5953a0a69dae1745e32ab578d92da60e78e0eb773330e7bc9abf63dd38", "text": "We can do just that! First, let's make a small change to our app so that we save the index to disk after every upload:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndef insert_terms(terms_to_definition):\n    for term, definition in terms_to_definition.items():\n        doc = Document(text=f\"Term: {term}\\nDefinition: {definition}\")\n        st.session_state['llama_index'].insert(doc)\n    # TEMPORARY - save to disk\n    st.session_state['llama_index'].storage_context.persist()\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, we need some document to extract from! The repository for this project used the wikipedia page on New York City, and you can find the text here.\n\nIf you paste the text into the upload tab and run it (it may take some time), we can insert the extracted terms. Make sure to also copy the text for the extracted terms into a notepad or similar before inserting into the index! We will need them in a second.\n\nAfter inserting, remove the line of code we used to save the index to disk.", "start_char_idx": 3433, "end_char_idx": 5300, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0b19d2d-3f10-4e64-9d84-79b1e7539cf3": {"__data__": {"id_": "a0b19d2d-3f10-4e64-9d84-79b1e7539cf3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "node_type": "1", "metadata": {}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d530c033-2e49-4ed0-8be0-10fb13ac6abf", "node_type": "1", "metadata": {}, "hash": "82a5c348eac6fa43988a07e26cbabc7a6d6af015a37cfd859baf4fe9a0c18854", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "node_type": "1", "metadata": {}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "class_name": "RelatedNodeInfo"}}, "hash": "7895101163952bb86ec5b2f5602e5c1c63a6c1955174767d7d54f98a836a307e", "text": "After inserting, remove the line of code we used to save the index to disk. With a starting index now saved, we can modify our `initialize_index` function to look like this:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n@st.cache_resource\ndef initialize_index(llm_name, model_temperature, api_key):\n    \"\"\"Load the Index object.\"\"\"\n    llm = get_llm(llm_name, model_temperature, api_key)\n\n    service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n\n    index = load_index_from_storage(service_context=service_context)\n\n    return index\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDid you remember to save that giant list of extracted terms in a notepad?", "start_char_idx": 0, "end_char_idx": 1542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d530c033-2e49-4ed0-8be0-10fb13ac6abf": {"__data__": {"id_": "d530c033-2e49-4ed0-8be0-10fb13ac6abf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "node_type": "1", "metadata": {}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0b19d2d-3f10-4e64-9d84-79b1e7539cf3", "node_type": "1", "metadata": {}, "hash": "7895101163952bb86ec5b2f5602e5c1c63a6c1955174767d7d54f98a836a307e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fd96e80-2333-4431-9fd6-c05da53d4fd6", "node_type": "1", "metadata": {}, "hash": "299fdc802caabb2a8ca2829e4aa13df1c2939deb7fdc4cbc6574b952bbfe45e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "node_type": "1", "metadata": {}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "class_name": "RelatedNodeInfo"}}, "hash": "82a5c348eac6fa43988a07e26cbabc7a6d6af015a37cfd859baf4fe9a0c18854", "text": "Now when our app initializes, we want to pass in the default terms that are in the index to our global terms state:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n...\nif \"all_terms\" not in st.session_state:\n    st.session_state[\"all_terms\"] = DEFAULT_TERMS\n...\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 1 - Create a Starting Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRepeat the above anywhere where we were previously resetting the `all_terms` values.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 2 - (Refining) Better Prompts\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you play around with the app a bit now, you might notice that it stopped following our prompt!", "start_char_idx": 1543, "end_char_idx": 3366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fd96e80-2333-4431-9fd6-c05da53d4fd6": {"__data__": {"id_": "1fd96e80-2333-4431-9fd6-c05da53d4fd6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "node_type": "1", "metadata": {}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d530c033-2e49-4ed0-8be0-10fb13ac6abf", "node_type": "1", "metadata": {}, "hash": "82a5c348eac6fa43988a07e26cbabc7a6d6af015a37cfd859baf4fe9a0c18854", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0764f8c-3df3-4eb5-b37e-a0ac7708814c", "node_type": "1", "metadata": {}, "hash": "33b26bacf1a6879e0b9713b4cf56f9c88b67d0410246fdaa0c934f5c8594b6cc", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "node_type": "1", "metadata": {}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "class_name": "RelatedNodeInfo"}}, "hash": "299fdc802caabb2a8ca2829e4aa13df1c2939deb7fdc4cbc6574b952bbfe45e7", "text": "Remember, we added to our `query_str` variable that if the term/definition could not be found, answer to the best of its knowledge. But now if you try asking about random terms (like bunnyhug!), it may or may not follow those instructions.\n\nThis is due to the concept of \"refining\" answers in Llama Index. Since we are querying across the top 5 matching results, sometimes all the results do not fit in a single prompt! OpenAI models typically have a max input size of 4097 tokens. So, Llama Index accounts for this by breaking up the matching results into chunks that will fit into the prompt. After Llama Index gets an initial answer from the first API call, it sends the next chunk to the API, along with the previous answer, and asks the model to refine that answer.\n\nSo, the refine process seems to be messing with our results! Rather than appending extra instructions to the `query_str`, remove that, and Llama Index will let us provide our own custom prompts! Let's create those now, using the default prompts and chat specific prompts as a guide.", "start_char_idx": 3367, "end_char_idx": 4421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0764f8c-3df3-4eb5-b37e-a0ac7708814c": {"__data__": {"id_": "c0764f8c-3df3-4eb5-b37e-a0ac7708814c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "node_type": "1", "metadata": {}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fd96e80-2333-4431-9fd6-c05da53d4fd6", "node_type": "1", "metadata": {}, "hash": "299fdc802caabb2a8ca2829e4aa13df1c2939deb7fdc4cbc6574b952bbfe45e7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb", "node_type": "1", "metadata": {}, "hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "class_name": "RelatedNodeInfo"}}, "hash": "33b26bacf1a6879e0b9713b4cf56f9c88b67d0410246fdaa0c934f5c8594b6cc", "text": "Let's create those now, using the default prompts and chat specific prompts as a guide. Using a new file `constants.py`, let's create some new query templates:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 2 - (Refining) Better Prompts\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\nfrom langchain.prompts.chat import (\n    AIMessagePromptTemplate,\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Text QA templates\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDEFAULT_TEXT_QA_PROMPT_TMPL = (\n    \"Context information is below.", "start_char_idx": 4334, "end_char_idx": 5776, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "862af2ce-1189-47b6-98a3-ddb8ae2e94d8": {"__data__": {"id_": "862af2ce-1189-47b6-98a3-ddb8ae2e94d8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7", "node_type": "1", "metadata": {}, "hash": "18ebaf1f87fd810a5d9741c3d51fd342383fc58d0e6e1108bffba8205383627e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe5fcf31-cc29-4408-8f3f-6cdf1fabac33", "node_type": "1", "metadata": {}, "hash": "72d060dc8fe3fcd2c2bfc9877b86b07d5805f4d7a3c4386e5f5e618cfa908ece", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7", "node_type": "1", "metadata": {}, "hash": "18ebaf1f87fd810a5d9741c3d51fd342383fc58d0e6e1108bffba8205383627e", "class_name": "RelatedNodeInfo"}}, "hash": "18d47bb2cebcf2de83e7eeddc501521a04173a4286dadac27d348e93246c0111", "text": "\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the context information answer the following question \"\n    \"(if you don't know the answer, use the best of your knowledge): {query_str}\\n\"\n)\nTEXT_QA_TEMPLATE = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Refine templates\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDEFAULT_REFINE_PROMPT_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context and using the best of your knowledge, improve the existing answer. \"\n    \"If you can't improve the existing answer, just repeat it again.\"\n)\nDEFAULT_REFINE_PROMPT = RefinePrompt(DEFAULT_REFINE_PROMPT_TMPL)\n\nCHAT_REFINE_PROMPT_TMPL_MSGS = [\n    HumanMessagePromptTemplate.from_template(\"{query_str}\"),\n    AIMessagePromptTemplate.from_template(\"{existing_answer}\"),\n    HumanMessagePromptTemplate.from_template(\n        \"We have the opportunity to refine the above answer \"\n        \"(only if needed) with some more context below.\\n\"\n        \"------------\\n\"\n        \"{context_msg}\\n\"\n        \"------------\\n\"\n        \"Given the new context and using the best of your knowledge, improve the existing answer. \"\n    \"If you can't improve the existing answer, just repeat it again.\"", "start_char_idx": 0, "end_char_idx": 1895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe5fcf31-cc29-4408-8f3f-6cdf1fabac33": {"__data__": {"id_": "fe5fcf31-cc29-4408-8f3f-6cdf1fabac33", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7", "node_type": "1", "metadata": {}, "hash": "18ebaf1f87fd810a5d9741c3d51fd342383fc58d0e6e1108bffba8205383627e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "862af2ce-1189-47b6-98a3-ddb8ae2e94d8", "node_type": "1", "metadata": {}, "hash": "18d47bb2cebcf2de83e7eeddc501521a04173a4286dadac27d348e93246c0111", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "294dca04-e918-4d16-81ce-784ed3a08e3f", "node_type": "1", "metadata": {}, "hash": "6c34e4e2e7874f9199159ab27c7ce5554c884a420b716fea7b983e32668217d8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7", "node_type": "1", "metadata": {}, "hash": "18ebaf1f87fd810a5d9741c3d51fd342383fc58d0e6e1108bffba8205383627e", "class_name": "RelatedNodeInfo"}}, "hash": "72d060dc8fe3fcd2c2bfc9877b86b07d5805f4d7a3c4386e5f5e618cfa908ece", "text": "\"If you can't improve the existing answer, just repeat it again.\"\n    ),\n]\n\nCHAT_REFINE_PROMPT_LC = ChatPromptTemplate.from_messages(CHAT_REFINE_PROMPT_TMPL_MSGS)\nCHAT_REFINE_PROMPT = RefinePrompt.from_langchain_prompt(CHAT_REFINE_PROMPT_LC)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/refine prompt selector\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDEFAULT_REFINE_PROMPT_SEL_LC = ConditionalPromptSelector(\n    default_prompt=DEFAULT_REFINE_PROMPT.get_langchain_prompt(),\n    conditionals=[(is_chat_model, CHAT_REFINE_PROMPT.get_langchain_prompt())],\n)\nREFINE_TEMPLATE = RefinePrompt(\n    langchain_prompt_selector=DEFAULT_REFINE_PROMPT_SEL_LC\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/refine prompt selector\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThat seems like a lot of code, but it's not too bad! If you looked at the default prompts, you might have noticed that there are default prompts, and prompts specific to chat models. Continuing that trend, we do the same for our custom prompts. Then, using a prompt selector, we can combine both prompts into a single object. If the LLM being used is a chat model (ChatGPT, GPT-4), then the chat prompts are used. Otherwise, use the normal prompt templates.", "start_char_idx": 1830, "end_char_idx": 3742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "294dca04-e918-4d16-81ce-784ed3a08e3f": {"__data__": {"id_": "294dca04-e918-4d16-81ce-784ed3a08e3f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7", "node_type": "1", "metadata": {}, "hash": "18ebaf1f87fd810a5d9741c3d51fd342383fc58d0e6e1108bffba8205383627e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe5fcf31-cc29-4408-8f3f-6cdf1fabac33", "node_type": "1", "metadata": {}, "hash": "72d060dc8fe3fcd2c2bfc9877b86b07d5805f4d7a3c4386e5f5e618cfa908ece", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7", "node_type": "1", "metadata": {}, "hash": "18ebaf1f87fd810a5d9741c3d51fd342383fc58d0e6e1108bffba8205383627e", "class_name": "RelatedNodeInfo"}}, "hash": "6c34e4e2e7874f9199159ab27c7ce5554c884a420b716fea7b983e32668217d8", "text": "Otherwise, use the normal prompt templates.\n\nAnother thing to note is that we only defined one QA template. In a chat model, this will be converted to a single \"human\" message.\n\nSo, now we can import these prompts into our app and use them during the query.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/refine prompt selector\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom constants import REFINE_TEMPLATE, TEXT_QA_TEMPLATE\n...\n    if \"llama_index\" in st.session_state:\n        query_text = st.text_input(\"Ask about a term or definition:\")\n        if query_text:\n            query_text = query_text  # Notice we removed the old instructions\n            with st.spinner(\"Generating answer...\"):\n                response = st.session_state[\"llama_index\"].query(\n                    query_text, similarity_top_k=5, response_mode=\"compact\",\n                    text_qa_template=TEXT_QA_TEMPLATE, refine_template=REFINE_TEMPLATE\n                )\n            st.markdown(str(response))\n...\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/refine prompt selector\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you experiment a bit more with queries, hopefully you notice that the responses follow our instructions a little better now!", "start_char_idx": 3699, "end_char_idx": 5617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b7e4e7c-911c-4c48-a02f-cb33dac04468": {"__data__": {"id_": "9b7e4e7c-911c-4c48-a02f-cb33dac04468", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8307386e-c419-4780-bf97-9408b4967a9e", "node_type": "1", "metadata": {}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddcf50e2-c728-4522-bbb6-b18e29fb2a26", "node_type": "1", "metadata": {}, "hash": "41651a9a2abe4c2083069fca5c5c9c14df67e86b298f6c27b11d3c183445c092", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8307386e-c419-4780-bf97-9408b4967a9e", "node_type": "1", "metadata": {}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "class_name": "RelatedNodeInfo"}}, "hash": "5899c104c946157d35d1ffb145da3f0c32f21a1af536feb179e5f2aff05b915b", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 3 - Image Support\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlama index also supports images! Using Llama Index, we can upload images of documents (papers, letters, etc.), and Llama Index handles extracting the text. We can leverage this to also allow users to upload images of their documents and extract terms and definitions from them.\n\nIf you get an import error about PIL, install it using `pip install Pillow` first.", "start_char_idx": 0, "end_char_idx": 826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddcf50e2-c728-4522-bbb6-b18e29fb2a26": {"__data__": {"id_": "ddcf50e2-c728-4522-bbb6-b18e29fb2a26", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8307386e-c419-4780-bf97-9408b4967a9e", "node_type": "1", "metadata": {}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b7e4e7c-911c-4c48-a02f-cb33dac04468", "node_type": "1", "metadata": {}, "hash": "5899c104c946157d35d1ffb145da3f0c32f21a1af536feb179e5f2aff05b915b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50fd7008-631d-4845-b3da-5825d697de42", "node_type": "1", "metadata": {}, "hash": "f83d66e98296c6a2beeecc1d49f56dd429697892ec3df1321d120520bf926546", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8307386e-c419-4780-bf97-9408b4967a9e", "node_type": "1", "metadata": {}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "class_name": "RelatedNodeInfo"}}, "hash": "41651a9a2abe4c2083069fca5c5c9c14df67e86b298f6c27b11d3c183445c092", "text": "If you get an import error about PIL, install it using `pip install Pillow` first.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 3 - Image Support\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom PIL import Image\nfrom llama_index.readers.file.base import DEFAULT_FILE_EXTRACTOR, ImageParser\n\n@st.cache_resource\ndef get_file_extractor():\n    image_parser = ImageParser(keep_image=True, parse_text=True)\n    file_extractor = DEFAULT_FILE_EXTRACTOR\n    file_extractor.update(\n        {\n            \".jpg\": image_parser,\n            \".png\": image_parser,\n            \".jpeg\": image_parser,\n        }\n    )\n\n    return file_extractor\n\nfile_extractor = get_file_extractor()\n...\nwith upload_tab:\n    st.subheader(\"Extract and Query Definitions\")\n    if st.button(\"Initialize Index and Reset Terms\", key=\"init_index_1\"):\n        st.session_state[\"llama_index\"] = initialize_index(\n            llm_name, model_temperature, api_key\n        )\n        st.session_state[\"all_terms\"] = DEFAULT_TERMS\n\n    if \"llama_index\" in st.session_state:\n        st.markdown(\n            \"Either upload an image/screenshot of a document, or enter the text manually.\"", "start_char_idx": 744, "end_char_idx": 2241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50fd7008-631d-4845-b3da-5825d697de42": {"__data__": {"id_": "50fd7008-631d-4845-b3da-5825d697de42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8307386e-c419-4780-bf97-9408b4967a9e", "node_type": "1", "metadata": {}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddcf50e2-c728-4522-bbb6-b18e29fb2a26", "node_type": "1", "metadata": {}, "hash": "41651a9a2abe4c2083069fca5c5c9c14df67e86b298f6c27b11d3c183445c092", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68ae0c0f-f71c-44eb-bbf1-a2795f3db639", "node_type": "1", "metadata": {}, "hash": "76b95f285c68c9df4f0a51ae33782fbeedcf320a689ee58beaf285d7d3fc92a3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8307386e-c419-4780-bf97-9408b4967a9e", "node_type": "1", "metadata": {}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "class_name": "RelatedNodeInfo"}}, "hash": "f83d66e98296c6a2beeecc1d49f56dd429697892ec3df1321d120520bf926546", "text": ")\n        uploaded_file = st.file_uploader(\n            \"Upload an image/screenshot of a document:\", type=[\"png\", \"jpg\", \"jpeg\"]\n        )\n        document_text = st.text_area(\"Or enter raw text\")\n        if st.button(\"Extract Terms and Definitions\") and (\n            uploaded_file or document_text\n        ):\n            st.session_state[\"terms\"] = {}\n            terms_docs = {}\n            with st.spinner(\"Extracting (images may be slow)...\"):\n                if document_text:\n                    terms_docs.update(\n                        extract_terms(\n                            [Document(text=document_text)],\n                            term_extract_str,\n                            llm_name,\n                            model_temperature,\n                            api_key,\n                        )\n                    )\n                if uploaded_file:\n                    Image.open(uploaded_file).convert(\"RGB\").save(\"temp.png\")\n                    img_reader = SimpleDirectoryReader(\n                        input_files=[\"temp.png\"], file_extractor=file_extractor\n                    )\n                    img_docs = img_reader.load_data()\n                    os.remove(\"temp.png\")\n                    terms_docs.update(\n                        extract_terms(\n                            img_docs,\n                            term_extract_str,\n                            llm_name,\n                            model_temperature,\n                            api_key,\n                        )\n                    )\n            st.session_state[\"terms\"].update(terms_docs)\n\n        if \"terms\" in st.session_state and st.session_state[\"terms\"]:\n            st.markdown(\"Extracted terms\")\n            st.json(st.session_state[\"terms\"])\n\n            if st.button(\"Insert terms?\"):\n                with st.spinner(\"Inserting terms\"):\n                    insert_terms(st.session_state[\"terms\"])\n                st.session_state[\"all_terms\"].update(st.session_state[\"terms\"])\n                st.session_state[\"terms\"] = {}\n                st.experimental_rerun()\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Improvement 3 - Image Support\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere, we added the option to upload a file using Streamlit. Then the image is opened and saved to disk (this seems hacky but it keeps things simple).", "start_char_idx": 2250, "end_char_idx": 4940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68ae0c0f-f71c-44eb-bbf1-a2795f3db639": {"__data__": {"id_": "68ae0c0f-f71c-44eb-bbf1-a2795f3db639", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8307386e-c419-4780-bf97-9408b4967a9e", "node_type": "1", "metadata": {}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50fd7008-631d-4845-b3da-5825d697de42", "node_type": "1", "metadata": {}, "hash": "f83d66e98296c6a2beeecc1d49f56dd429697892ec3df1321d120520bf926546", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8307386e-c419-4780-bf97-9408b4967a9e", "node_type": "1", "metadata": {}, "hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "class_name": "RelatedNodeInfo"}}, "hash": "76b95f285c68c9df4f0a51ae33782fbeedcf320a689ee58beaf285d7d3fc92a3", "text": "Then we pass the image path to the reader, extract the documents/text, and remove our temp image file.\n\nNow that we have the documents, we can call `extract_terms()` the same as before.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nContent Type: text\nHeader Path: A Guide to Extracting Terms and Definitions/Dry Run Test/Conclusion/TLDR\nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md\nfile_name: terms_definitions_tutorial.md\nfile_type: None\nfile_size: 26452\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn this tutorial, we covered a ton of information, while solving some common issues and problems along the way:\n\n- Using different indexes for different use cases (List vs. Vector index)\n- Storing global state values with Streamlit's `session_state` concept\n- Customizing internal prompts with Llama Index\n- Reading text from images with Llama Index\n\nThe final version of this tutorial can be found here and a live hosted demo is available on Huggingface Spaces.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex offers a variety of different use cases.\n\nFor simple queries, we may want to use a single index data structure, such as a `VectorStoreIndex` for semantic search, or `ListIndex` for summarization.\n\nFor more complex queries, we may want to use a composable graph.\n\nBut how do we integrate indexes and graphs into our LLM application? Different indexes and graphs may be better suited for different types of queries that you may want to run.", "start_char_idx": 4941, "end_char_idx": 6884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eee0cf49-41ea-4605-93d2-6a985f5c870f": {"__data__": {"id_": "eee0cf49-41ea-4605-93d2-6a985f5c870f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b", "node_type": "1", "metadata": {}, "hash": "d6345df814f8c9f8800b9d5ef5385b81330418d3c20ae0a07ddcd5cd8252df77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "863991c9-7dd3-4688-bfa4-374dcf558157", "node_type": "1", "metadata": {}, "hash": "a9c285e31d052ba406cd9c578b09a45dc15a14ae4af60dbe30333ceb3a73bced", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b", "node_type": "1", "metadata": {}, "hash": "d6345df814f8c9f8800b9d5ef5385b81330418d3c20ae0a07ddcd5cd8252df77", "class_name": "RelatedNodeInfo"}}, "hash": "854e8b3b3597e09ec7ad4ec961ea962840fa9da3511c2a423c85199f97b0cc63", "text": "Different indexes and graphs may be better suited for different types of queries that you may want to run.\n\nIn this guide, we show how you can unify the diverse use cases of different index/graph structures under a **single** query framework.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn this example, we will analyze Wikipedia articles of different cities: Boston, Seattle, San Francisco, and more.\n\nThe below code snippet downloads the relevant data into files.", "start_char_idx": 0, "end_char_idx": 829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "863991c9-7dd3-4688-bfa4-374dcf558157": {"__data__": {"id_": "863991c9-7dd3-4688-bfa4-374dcf558157", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b", "node_type": "1", "metadata": {}, "hash": "d6345df814f8c9f8800b9d5ef5385b81330418d3c20ae0a07ddcd5cd8252df77", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eee0cf49-41ea-4605-93d2-6a985f5c870f", "node_type": "1", "metadata": {}, "hash": "854e8b3b3597e09ec7ad4ec961ea962840fa9da3511c2a423c85199f97b0cc63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ea697f8-6d10-4ff1-946a-cd3cb406e694", "node_type": "1", "metadata": {}, "hash": "4c1f4d135a51e7d53c527150c0f27eb6ec269ab8083039d161a679e500dbd4ea", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b", "node_type": "1", "metadata": {}, "hash": "d6345df814f8c9f8800b9d5ef5385b81330418d3c20ae0a07ddcd5cd8252df77", "class_name": "RelatedNodeInfo"}}, "hash": "a9c285e31d052ba406cd9c578b09a45dc15a14ae4af60dbe30333ceb3a73bced", "text": "The below code snippet downloads the relevant data into files.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom pathlib import Path\nimport requests\n\nwiki_titles = [\"Toronto\", \"Seattle\", \"Chicago\", \"Boston\", \"Houston\"]\n\nfor title in wiki_titles:\n    response = requests.get(\n        'https://en.wikipedia.org/w/api.php',\n        params={\n            'action': 'query',\n            'format': 'json',\n            'titles': title,\n            'prop': 'extracts',\n            # 'exintro': True,\n            'explaintext': True,\n        }\n    ).json()\n    page = next(iter(response['query']['pages'].values()))\n    wiki_text = page['extract']\n\n    data_path = Path('data')\n    if not data_path.exists():\n        Path.mkdir(data_path)\n\n    with open(data_path / f\"{title}.txt\", 'w') as fp:\n        fp.write(wiki_text)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe next snippet loads all files into Document objects.", "start_char_idx": 767, "end_char_idx": 2405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ea697f8-6d10-4ff1-946a-cd3cb406e694": {"__data__": {"id_": "9ea697f8-6d10-4ff1-946a-cd3cb406e694", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b", "node_type": "1", "metadata": {}, "hash": "d6345df814f8c9f8800b9d5ef5385b81330418d3c20ae0a07ddcd5cd8252df77", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "863991c9-7dd3-4688-bfa4-374dcf558157", "node_type": "1", "metadata": {}, "hash": "a9c285e31d052ba406cd9c578b09a45dc15a14ae4af60dbe30333ceb3a73bced", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b", "node_type": "1", "metadata": {}, "hash": "d6345df814f8c9f8800b9d5ef5385b81330418d3c20ae0a07ddcd5cd8252df77", "class_name": "RelatedNodeInfo"}}, "hash": "4c1f4d135a51e7d53c527150c0f27eb6ec269ab8083039d161a679e500dbd4ea", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Load all wiki documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncity_docs = {}\nfor wiki_title in wiki_titles:\n    city_docs[wiki_title] = SimpleDirectoryReader(input_files=[f\"data/{wiki_title}.txt\"]).load_data()\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining the Set of Indexes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe will now define a set of indexes and graphs over our data. You can think of each index/graph as a lightweight structure\nthat solves a distinct use case.\n\nWe will first define a vector index over the documents of each city.", "start_char_idx": 2407, "end_char_idx": 3635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99f07d67-47b4-478c-b87b-c7bbf90f7509": {"__data__": {"id_": "99f07d67-47b4-478c-b87b-c7bbf90f7509", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4", "node_type": "1", "metadata": {}, "hash": "af64eb5d1eef876fb19d2823481667aa718b452902b44283312ed84f276ce71e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ecb87cb-eb82-4e30-b344-16d287c03ed7", "node_type": "1", "metadata": {}, "hash": "6269de3f86e589349139aab3e41e5180e54d60c99289756fd964b4ffc6c8a5da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4", "node_type": "1", "metadata": {}, "hash": "af64eb5d1eef876fb19d2823481667aa718b452902b44283312ed84f276ce71e", "class_name": "RelatedNodeInfo"}}, "hash": "8204abe877593024ae848a4e071958ef2a1b5adee65c6a65162fd8768b6676d1", "text": "We will first define a vector index over the documents of each city.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining the Set of Indexes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, ServiceContext, StorageContext\nfrom langchain.llms.openai import OpenAIChat\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/set service context\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor_gpt4 = LLMPredictor(llm=OpenAIChat(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(\n    llm_predictor=llm_predictor_gpt4, chunk_size=1024\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Build city document index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nvector_indices = {}\nfor wiki_title in wiki_titles:\n    storage_context = StorageContext.", "start_char_idx": 0, "end_char_idx": 1744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ecb87cb-eb82-4e30-b344-16d287c03ed7": {"__data__": {"id_": "0ecb87cb-eb82-4e30-b344-16d287c03ed7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4", "node_type": "1", "metadata": {}, "hash": "af64eb5d1eef876fb19d2823481667aa718b452902b44283312ed84f276ce71e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99f07d67-47b4-478c-b87b-c7bbf90f7509", "node_type": "1", "metadata": {}, "hash": "8204abe877593024ae848a4e071958ef2a1b5adee65c6a65162fd8768b6676d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65c62462-8add-4bb8-9690-655bdcbddaf6", "node_type": "1", "metadata": {}, "hash": "5bc654278c1be413157897c3e1bdbbdfd9f46acab41abb72a611e0c1c54c18f7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4", "node_type": "1", "metadata": {}, "hash": "af64eb5d1eef876fb19d2823481667aa718b452902b44283312ed84f276ce71e", "class_name": "RelatedNodeInfo"}}, "hash": "6269de3f86e589349139aab3e41e5180e54d60c99289756fd964b4ffc6c8a5da", "text": "from_defaults()\n    # build vector index\n    vector_indices[wiki_title] = VectorStoreIndex.from_documents(\n        city_docs[wiki_title],\n        service_context=service_context,\n        storage_context=storage_context,\n    )\n    # set id for vector index\n    vector_indices[wiki_title].index_struct.index_id = wiki_title\n    # persist to disk\n    storage_context.persist(persist_dir=f'./storage/{wiki_title}')\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Build city document index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuerying a vector index lets us easily perform semantic search over a given city's documents.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Build city document index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = vector_indices[\"Toronto\"].as_query_engine().query(\"What are the sports teams in Toronto?\")", "start_char_idx": 1744, "end_char_idx": 3206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65c62462-8add-4bb8-9690-655bdcbddaf6": {"__data__": {"id_": "65c62462-8add-4bb8-9690-655bdcbddaf6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4", "node_type": "1", "metadata": {}, "hash": "af64eb5d1eef876fb19d2823481667aa718b452902b44283312ed84f276ce71e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ecb87cb-eb82-4e30-b344-16d287c03ed7", "node_type": "1", "metadata": {}, "hash": "6269de3f86e589349139aab3e41e5180e54d60c99289756fd964b4ffc6c8a5da", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4", "node_type": "1", "metadata": {}, "hash": "af64eb5d1eef876fb19d2823481667aa718b452902b44283312ed84f276ce71e", "class_name": "RelatedNodeInfo"}}, "hash": "5bc654278c1be413157897c3e1bdbbdfd9f46acab41abb72a611e0c1c54c18f7", "text": "print(str(response))\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Build city document index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nExample response:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Build city document index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe sports teams in Toronto are the Toronto Maple Leafs (NHL), Toronto Blue Jays (MLB), Toronto Raptors (NBA), Toronto Argonauts (CFL), Toronto FC (MLS), Toronto Rock (NLL), Toronto Wolfpack (RFL), and Toronto Rush (NARL).\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining a Graph for Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe will now define a composed graph in order to run **compare/contrast** queries (see use cases doc).\nThis graph contains a keyword table composed on top of existing vector indexes.\n\nTo do this, we first want to set the \"summary text\" for each vector index.", "start_char_idx": 3207, "end_char_idx": 5030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d64353b-2c93-472b-af1a-029696461c4a": {"__data__": {"id_": "1d64353b-2c93-472b-af1a-029696461c4a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6", "node_type": "1", "metadata": {}, "hash": "3e1c099691bb4e3bc479dee16f81cdf157d1b898a1e94ce0dd906a77ebfa0f20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ecd87b1-2ce9-4ad4-8864-361467317d67", "node_type": "1", "metadata": {}, "hash": "bda2e5ce6076b6d2f2c23eca605a3d9e692a9c2c4202c69dc4cd9cac729eff40", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6", "node_type": "1", "metadata": {}, "hash": "3e1c099691bb4e3bc479dee16f81cdf157d1b898a1e94ce0dd906a77ebfa0f20", "class_name": "RelatedNodeInfo"}}, "hash": "6a6fc46145a60e671570c12929f2de4b34da56acadfb8cfebf900a2f20709838", "text": "To do this, we first want to set the \"summary text\" for each vector index.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining a Graph for Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex_summaries = {}\nfor wiki_title in wiki_titles:\n    # set summary text for city\n    index_summaries[wiki_title] = (\n        f\"This content contains Wikipedia articles about {wiki_title}. \"\n        f\"Use this index if you need to lookup specific facts about {wiki_title}.\\n\"\n        \"Do not use this index if you want to analyze multiple cities.\"\n    )\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining a Graph for Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNext, we compose a keyword table on top of these vector indexes, with these indexes and summaries, in order to build the graph.", "start_char_idx": 0, "end_char_idx": 1454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ecd87b1-2ce9-4ad4-8864-361467317d67": {"__data__": {"id_": "6ecd87b1-2ce9-4ad4-8864-361467317d67", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6", "node_type": "1", "metadata": {}, "hash": "3e1c099691bb4e3bc479dee16f81cdf157d1b898a1e94ce0dd906a77ebfa0f20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d64353b-2c93-472b-af1a-029696461c4a", "node_type": "1", "metadata": {}, "hash": "6a6fc46145a60e671570c12929f2de4b34da56acadfb8cfebf900a2f20709838", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f0df44d-e555-49a8-9337-992e8f4cdd82", "node_type": "1", "metadata": {}, "hash": "b60926c12e02a0eea02ba30e506491327fe4379c19a4f073760dfc9a9fc18165", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6", "node_type": "1", "metadata": {}, "hash": "3e1c099691bb4e3bc479dee16f81cdf157d1b898a1e94ce0dd906a77ebfa0f20", "class_name": "RelatedNodeInfo"}}, "hash": "bda2e5ce6076b6d2f2c23eca605a3d9e692a9c2c4202c69dc4cd9cac729eff40", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining a Graph for Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.composability import ComposableGraph\n\ngraph = ComposableGraph.from_indices(\n    SimpleKeywordTableIndex,\n    [index for _, index in vector_indices.items()],\n    [summary for _, summary in index_summaries.items()],\n    max_keywords_per_chunk=50\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/get root index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nroot_index = graph.get_index(graph.index_struct.root_id, SimpleKeywordTableIndex)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/set id of root index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nroot_index.set_index_id(\"compare_contrast\")\nroot_summary = (\n    \"This index contains Wikipedia articles about multiple cities. \"\n    \"Use this index if you want to compare multiple cities. \"", "start_char_idx": 1456, "end_char_idx": 3287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f0df44d-e555-49a8-9337-992e8f4cdd82": {"__data__": {"id_": "0f0df44d-e555-49a8-9337-992e8f4cdd82", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6", "node_type": "1", "metadata": {}, "hash": "3e1c099691bb4e3bc479dee16f81cdf157d1b898a1e94ce0dd906a77ebfa0f20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ecd87b1-2ce9-4ad4-8864-361467317d67", "node_type": "1", "metadata": {}, "hash": "bda2e5ce6076b6d2f2c23eca605a3d9e692a9c2c4202c69dc4cd9cac729eff40", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6", "node_type": "1", "metadata": {}, "hash": "3e1c099691bb4e3bc479dee16f81cdf157d1b898a1e94ce0dd906a77ebfa0f20", "class_name": "RelatedNodeInfo"}}, "hash": "b60926c12e02a0eea02ba30e506491327fe4379c19a4f073760dfc9a9fc18165", "text": "\"Use this index if you want to compare multiple cities. \"\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/set id of root index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nQuerying this graph (with a query transform module), allows us to easily compare/contrast between different cities.\nAn example is shown below.", "start_char_idx": 3230, "end_char_idx": 3855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06959c18-5d5a-436c-94c2-c825c9fd19e4": {"__data__": {"id_": "06959c18-5d5a-436c-94c2-c825c9fd19e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "467795da-1869-46ce-8473-5f97601af966", "node_type": "1", "metadata": {}, "hash": "1e99cc7284d3cf6afc40e4c2246e19690ff3bb1dd8bc82a074522bb998cc4403", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "073fab27-83ad-447d-a4dd-1a986579cccd", "node_type": "1", "metadata": {}, "hash": "77877e8fe682ce5e129bfc70dab3393cbd0847dd62ccb24e23b285524acf5f3c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "467795da-1869-46ce-8473-5f97601af966", "node_type": "1", "metadata": {}, "hash": "1e99cc7284d3cf6afc40e4c2246e19690ff3bb1dd8bc82a074522bb998cc4403", "class_name": "RelatedNodeInfo"}}, "hash": "7867182d15b72fc605ee32a6e1e6e94822df50f37c5f7116bbe01209ed66270c", "text": "An example is shown below.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/define decompose_transform\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor_chatgpt, verbose=True\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/define custom query engines\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\ncustom_query_engines = {}\nfor index in vector_indices.values():\n    query_engine = index.as_query_engine(service_context=service_context)\n    query_engine = TransformQueryEngine(\n        query_engine,\n        query_transform=decompose_transform,\n        transform_extra_info={'index_summary': index.index_struct.summary},\n    )\n    custom_query_engines[index.index_id] = query_engine\ncustom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n    retriever_mode='simple',\n    response_mode='tree_summarize',\n    service_context=service_context,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.", "start_char_idx": 0, "end_char_idx": 1771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "073fab27-83ad-447d-a4dd-1a986579cccd": {"__data__": {"id_": "073fab27-83ad-447d-a4dd-1a986579cccd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "467795da-1869-46ce-8473-5f97601af966", "node_type": "1", "metadata": {}, "hash": "1e99cc7284d3cf6afc40e4c2246e19690ff3bb1dd8bc82a074522bb998cc4403", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06959c18-5d5a-436c-94c2-c825c9fd19e4", "node_type": "1", "metadata": {}, "hash": "7867182d15b72fc605ee32a6e1e6e94822df50f37c5f7116bbe01209ed66270c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "943e09d5-0632-48fc-a3e8-d9f514189867", "node_type": "1", "metadata": {}, "hash": "f86fe4f2e85207dab3de82b6a4f7d7c246625a7e51390a766ecc35dd55ddb94d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "467795da-1869-46ce-8473-5f97601af966", "node_type": "1", "metadata": {}, "hash": "1e99cc7284d3cf6afc40e4c2246e19690ff3bb1dd8bc82a074522bb998cc4403", "class_name": "RelatedNodeInfo"}}, "hash": "77877e8fe682ce5e129bfc70dab3393cbd0847dd62ccb24e23b285524acf5f3c", "text": "md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/define query engine\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/query the graph\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_str = (\n    \"Compare and contrast the arts and culture of Houston and Boston. \"\n)\nresponse_chatgpt = query_engine.query(query_str)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining the Unified Query Interface\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow that we've defined the set of indexes/graphs, we want to build an **outer abstraction** layer that provides a unified query interface\nto our data structures. This means that during query-time, we can query this outer abstraction layer and trust that the right index/graph\nwill be used for the job.\n\nThere are a few ways to do this, both within our framework as well as outside of it!", "start_char_idx": 1771, "end_char_idx": 3582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "943e09d5-0632-48fc-a3e8-d9f514189867": {"__data__": {"id_": "943e09d5-0632-48fc-a3e8-d9f514189867", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "467795da-1869-46ce-8473-5f97601af966", "node_type": "1", "metadata": {}, "hash": "1e99cc7284d3cf6afc40e4c2246e19690ff3bb1dd8bc82a074522bb998cc4403", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "073fab27-83ad-447d-a4dd-1a986579cccd", "node_type": "1", "metadata": {}, "hash": "77877e8fe682ce5e129bfc70dab3393cbd0847dd62ccb24e23b285524acf5f3c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "467795da-1869-46ce-8473-5f97601af966", "node_type": "1", "metadata": {}, "hash": "1e99cc7284d3cf6afc40e4c2246e19690ff3bb1dd8bc82a074522bb998cc4403", "class_name": "RelatedNodeInfo"}}, "hash": "f86fe4f2e85207dab3de82b6a4f7d7c246625a7e51390a766ecc35dd55ddb94d", "text": "There are a few ways to do this, both within our framework as well as outside of it!\n\n- Build a **router query engine** on top of your existing indexes/graphs\n- Define each index/graph as a Tool within an agent framework (e.g. LangChain).\n\nFor the purposes of this tutorial, we follow the former approach. If you want to take a look at how the latter approach works,\ntake a look at our example tutorial here.\n\nLet's take a look at an example of building a router query engine to automatically \"route\" any query to the set of indexes/graphs that you have define under the hood.\n\nFirst, we define the query engines for the set of indexes/graph that we want to route our query to. We also give each a description (about what data it holds and what it's useful for) to help the router choose between them depending on the specific query.", "start_char_idx": 3498, "end_char_idx": 4331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a1645f5-42af-445d-a6f9-f9151e840905": {"__data__": {"id_": "3a1645f5-42af-445d-a6f9-f9151e840905", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "node_type": "1", "metadata": {}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3532dee7-3488-402a-9323-c4ac4ab111f6", "node_type": "1", "metadata": {}, "hash": "16b749788442bc834ef66f0e68b596072fa407e2f49ea345dd6804e2505fdc30", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "node_type": "1", "metadata": {}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "class_name": "RelatedNodeInfo"}}, "hash": "c34c27e91c578a30fb86dc91bb1c469a2f967e01900c45656c67c3a1950aa579", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Defining the Unified Query Interface\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.tools.query_engine import QueryEngineTool\n\nquery_engine_tools = []\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/add vector index tools\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfor wiki_title in wiki_titles:\n    index = vector_indices[wiki_title]\n    summary = index_summaries[wiki_title]\n\n    query_engine = index.as_query_engine(service_context=service_context)\n    vector_tool = QueryEngineTool.from_defaults(query_engine, description=summary)\n    query_engine_tools.append(vector_tool)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/add graph tool\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ngraph_description = (\n    \"This tool contains Wikipedia articles about multiple cities. \"\n    \"Use this tool if you want to compare multiple cities. \"", "start_char_idx": 0, "end_char_idx": 1827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3532dee7-3488-402a-9323-c4ac4ab111f6": {"__data__": {"id_": "3532dee7-3488-402a-9323-c4ac4ab111f6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "node_type": "1", "metadata": {}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a1645f5-42af-445d-a6f9-f9151e840905", "node_type": "1", "metadata": {}, "hash": "c34c27e91c578a30fb86dc91bb1c469a2f967e01900c45656c67c3a1950aa579", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71589527-1312-4862-bf76-ddf5f4067218", "node_type": "1", "metadata": {}, "hash": "af1932bc0b9e2d9027e6c49cff65ed117afc9996fdba96f51973904720085f43", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "node_type": "1", "metadata": {}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "class_name": "RelatedNodeInfo"}}, "hash": "16b749788442bc834ef66f0e68b596072fa407e2f49ea345dd6804e2505fdc30", "text": "\"Use this tool if you want to compare multiple cities. \"\n)\ngraph_tool = QueryEngineTool.from_defaults(graph_query_engine, description=graph_description)\nquery_engine_tools.append(graph_tool)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/add graph tool\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow, we can define the routing logic and overall router query engine.\nHere, we use the `LLMSingleSelector`, which uses LLM to choose a underlying query engine to route the query to.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/add graph tool\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.query_engine.router_query_engine import RouterQueryEngine\nfrom llama_index.selectors.llm_selectors import LLMSingleSelector", "start_char_idx": 1771, "end_char_idx": 3118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71589527-1312-4862-bf76-ddf5f4067218": {"__data__": {"id_": "71589527-1312-4862-bf76-ddf5f4067218", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "node_type": "1", "metadata": {}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3532dee7-3488-402a-9323-c4ac4ab111f6", "node_type": "1", "metadata": {}, "hash": "16b749788442bc834ef66f0e68b596072fa407e2f49ea345dd6804e2505fdc30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94ce6c98-d7c0-42d5-b410-5f252c6e4add", "node_type": "1", "metadata": {}, "hash": "cb1c83e287f275759455cac5aeff5fd6db619d85ab82a09176f90c4bc56920dd", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "node_type": "1", "metadata": {}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "class_name": "RelatedNodeInfo"}}, "hash": "af1932bc0b9e2d9027e6c49cff65ed117afc9996fdba96f51973904720085f43", "text": "router_query_engine = RouterQueryEngine(\n    selector=LLMSingleSelector.from_defaults(service_context=service_context),\n    query_engine_tools=query_engine_tools\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/Querying our Unified Interface\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe advantage of a unified query interface is that it can now handle different types of queries.\n\nIt can now handle queries about specific cities (by routing to the specific city vector index), and also compare/contrast different cities.\n\nLet's take a look at a few examples!\n\n**Asking a Compare/Contrast Question**\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/ask a compare/contrast question\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = router_query_engine.query(\n    \"Compare and contrast the arts and culture of Houston and Boston.", "start_char_idx": 3121, "end_char_idx": 4575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94ce6c98-d7c0-42d5-b410-5f252c6e4add": {"__data__": {"id_": "94ce6c98-d7c0-42d5-b410-5f252c6e4add", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "node_type": "1", "metadata": {}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71589527-1312-4862-bf76-ddf5f4067218", "node_type": "1", "metadata": {}, "hash": "af1932bc0b9e2d9027e6c49cff65ed117afc9996fdba96f51973904720085f43", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "node_type": "1", "metadata": {}, "hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "class_name": "RelatedNodeInfo"}}, "hash": "cb1c83e287f275759455cac5aeff5fd6db619d85ab82a09176f90c4bc56920dd", "text": "\",\n)\nprint(str(response)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/ask a compare/contrast question\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Asking Questions about specific Cities**\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/ask a compare/contrast question\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = router_query_engine.query(\"What are the sports teams in Toronto?\")", "start_char_idx": 4575, "end_char_idx": 5588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "775cfdcb-ec0a-400b-bdff-380c5cd33594": {"__data__": {"id_": "775cfdcb-ec0a-400b-bdff-380c5cd33594", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57d82275-bdb8-4707-83cf-2f2442d7befa", "node_type": "1", "metadata": {}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bd244f0-784b-441e-a312-5854ad193643", "node_type": "1", "metadata": {}, "hash": "e3232e6cc4ede0f08002234f02ce0077c1125a425fa5aff98c8ebbae81e46206", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "57d82275-bdb8-4707-83cf-2f2442d7befa", "node_type": "1", "metadata": {}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "class_name": "RelatedNodeInfo"}}, "hash": "17ec6cbb3203e567273501704e2afbca55f257102bfa90404a31a76008a3f744", "text": "print(str(response))\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nContent Type: text\nHeader Path: A Guide to Creating a Unified Query Framework over your Indexes/ask a compare/contrast question\nfile_path: Docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md\nfile_name: unified_query.md\nfile_type: None\nfile_size: 9745\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis \"outer\" abstraction is able to handle different queries by routing to the right underlying abstractions.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAt a high-level, LlamaIndex gives you the ability to query your data for any downstream LLM use case,\nwhether it's question-answering, summarization, or a component in a chatbot.\n\nThis section describes the different ways you can query your data with LlamaIndex, roughly in order\nof simplest (top-k semantic search), to more advanced capabilities.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Semantic Search\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe most basic example usage of LlamaIndex is through semantic search.", "start_char_idx": 0, "end_char_idx": 1663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bd244f0-784b-441e-a312-5854ad193643": {"__data__": {"id_": "1bd244f0-784b-441e-a312-5854ad193643", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57d82275-bdb8-4707-83cf-2f2442d7befa", "node_type": "1", "metadata": {}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "775cfdcb-ec0a-400b-bdff-380c5cd33594", "node_type": "1", "metadata": {}, "hash": "17ec6cbb3203e567273501704e2afbca55f257102bfa90404a31a76008a3f744", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "233df86f-00fe-437b-ae64-54905559b456", "node_type": "1", "metadata": {}, "hash": "93b03e42f3cb747c999b20df84d37ad4b750d74a023114644604b426801111ce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "57d82275-bdb8-4707-83cf-2f2442d7befa", "node_type": "1", "metadata": {}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "class_name": "RelatedNodeInfo"}}, "hash": "e3232e6cc4ede0f08002234f02ce0077c1125a425fa5aff98c8ebbae81e46206", "text": "We provide\na simple in-memory vector store for you to get started, but you can also choose\nto use any one of our vector store integrations:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Semantic Search\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Semantic Search\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Tutorials**\n- Starter Tutorial\n- Basic Usage Pattern\n\n**Guides**\n- Example (Notebook)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Summarization\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer.\nFor instance, a summarization query could look like one of the following: \n- \"What is a summary of this collection of text?\"\n- \"Give me a summary of person X's experience with the company.\"", "start_char_idx": 1664, "end_char_idx": 3546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "233df86f-00fe-437b-ae64-54905559b456": {"__data__": {"id_": "233df86f-00fe-437b-ae64-54905559b456", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57d82275-bdb8-4707-83cf-2f2442d7befa", "node_type": "1", "metadata": {}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bd244f0-784b-441e-a312-5854ad193643", "node_type": "1", "metadata": {}, "hash": "e3232e6cc4ede0f08002234f02ce0077c1125a425fa5aff98c8ebbae81e46206", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68f1cda2-f23d-4779-babd-9a366faa6269", "node_type": "1", "metadata": {}, "hash": "9f5221e99287f77cb5368bfa27f0665e099658d32b259fdf8435a00262af779f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "57d82275-bdb8-4707-83cf-2f2442d7befa", "node_type": "1", "metadata": {}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "class_name": "RelatedNodeInfo"}}, "hash": "93b03e42f3cb747c999b20df84d37ad4b750d74a023114644604b426801111ce", "text": "- \"Give me a summary of person X's experience with the company.\"\n\nIn general, a list index would be suited for this use case. A list index by default goes through all the data.\n\nEmpirically, setting `response_mode=\"tree_summarize\"` also leads to better summarization results.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Summarization\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = ListIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(\n    response_mode=\"tree_summarize\"\n)\nresponse = query_engine.query(\"<summarization_query>\")\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Queries over Structured Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports queries over structured data, whether that's a Pandas DataFrame or a SQL Database.\n\nHere are some relevant resources:\n\n**Tutorials**\n\n- Guide on Text-to-SQL\n\n**Guides**\n- SQL Guide (Core) (Notebook)\n- Pandas Demo (Notebook)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Synthesis over Heterogeneous Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex supports synthesizing across heterogeneous data sources.", "start_char_idx": 3482, "end_char_idx": 5329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68f1cda2-f23d-4779-babd-9a366faa6269": {"__data__": {"id_": "68f1cda2-f23d-4779-babd-9a366faa6269", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57d82275-bdb8-4707-83cf-2f2442d7befa", "node_type": "1", "metadata": {}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "233df86f-00fe-437b-ae64-54905559b456", "node_type": "1", "metadata": {}, "hash": "93b03e42f3cb747c999b20df84d37ad4b750d74a023114644604b426801111ce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "57d82275-bdb8-4707-83cf-2f2442d7befa", "node_type": "1", "metadata": {}, "hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "class_name": "RelatedNodeInfo"}}, "hash": "9f5221e99287f77cb5368bfa27f0665e099658d32b259fdf8435a00262af779f", "text": "This can be done by composing a graph over your existing data.\nSpecifically, compose a list index over your subindices. A list index inherently combines information for each node; therefore\nit can synthesize information across your heterogeneous data sources.", "start_char_idx": 5330, "end_char_idx": 5589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61024bcd-7861-4906-a3fa-1a56a6bf9307": {"__data__": {"id_": "61024bcd-7861-4906-a3fa-1a56a6bf9307", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6", "node_type": "1", "metadata": {}, "hash": "2ad03f3872044972ec87f7f571a78e0a5013b71cefa01ebc67058f53533959b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14be06b7-297d-4ada-af94-28249b913c15", "node_type": "1", "metadata": {}, "hash": "907c8c841135a0fbf69392769beb2c7c2725df761bfedbb8aec3e2b852c6dbf8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6", "node_type": "1", "metadata": {}, "hash": "2ad03f3872044972ec87f7f571a78e0a5013b71cefa01ebc67058f53533959b8", "class_name": "RelatedNodeInfo"}}, "hash": "c7472c842671eb160804939e6e8cf1f2b2323669df511fdab4d8e72d6ae0a348", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Synthesis over Heterogeneous Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, ListIndex\nfrom llama_index.indices.composability import ComposableGraph\n\nindex1 = VectorStoreIndex.from_documents(notion_docs)\nindex2 = VectorStoreIndex.from_documents(slack_docs)\n\ngraph = ComposableGraph.from_indices(ListIndex, [index1, index2], index_summaries=[\"summary1\", \"summary2\"])\nquery_engine = graph.as_query_engine()\nresponse = query_engine.query(\"<query_str>\")\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Synthesis over Heterogeneous Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Guides**\n- Composability\n- City Analysis (Notebook)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Routing over Heterogeneous Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex also supports routing over heterogeneous data sources with `RouterQueryEngine` - for instance, if you want to \"route\" a query to an \nunderlying Document or a sub-index.\n\n\nTo do this, first build the sub-indices over different data sources.", "start_char_idx": 0, "end_char_idx": 1826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14be06b7-297d-4ada-af94-28249b913c15": {"__data__": {"id_": "14be06b7-297d-4ada-af94-28249b913c15", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6", "node_type": "1", "metadata": {}, "hash": "2ad03f3872044972ec87f7f571a78e0a5013b71cefa01ebc67058f53533959b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61024bcd-7861-4906-a3fa-1a56a6bf9307", "node_type": "1", "metadata": {}, "hash": "c7472c842671eb160804939e6e8cf1f2b2323669df511fdab4d8e72d6ae0a348", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54cc4e5b-f7e1-4202-b4a7-3beb688ea5c5", "node_type": "1", "metadata": {}, "hash": "9df7aaed7b6801b6421974870470e9041bf7d1907793f4bbe33c3d9c6a804232", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6", "node_type": "1", "metadata": {}, "hash": "2ad03f3872044972ec87f7f571a78e0a5013b71cefa01ebc67058f53533959b8", "class_name": "RelatedNodeInfo"}}, "hash": "907c8c841135a0fbf69392769beb2c7c2725df761bfedbb8aec3e2b852c6dbf8", "text": "To do this, first build the sub-indices over different data sources.\nThen construct the corresponding query engines, and give each query engine a description to obtain a `QueryEngineTool`.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Routing over Heterogeneous Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import TreeIndex, VectorStoreIndex\nfrom llama_index.tools import QueryEngineTool\n\n.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/define sub-indices\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex1 = VectorStoreIndex.from_documents(notion_docs)\nindex2 = VectorStoreIndex.from_documents(slack_docs)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/define query engines and tools\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntool1 = QueryEngineTool.from_defaults(\n    query_engine=index1.as_query_engine(), \n    description=\"Use this query engine to do.\",\n)\ntool2 = QueryEngineTool.from_defaults(\n    query_engine=index2.as_query_engine(), \n    description=\"Use this query engine for something else.\",\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54cc4e5b-f7e1-4202-b4a7-3beb688ea5c5": {"__data__": {"id_": "54cc4e5b-f7e1-4202-b4a7-3beb688ea5c5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6", "node_type": "1", "metadata": {}, "hash": "2ad03f3872044972ec87f7f571a78e0a5013b71cefa01ebc67058f53533959b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14be06b7-297d-4ada-af94-28249b913c15", "node_type": "1", "metadata": {}, "hash": "907c8c841135a0fbf69392769beb2c7c2725df761bfedbb8aec3e2b852c6dbf8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6", "node_type": "1", "metadata": {}, "hash": "2ad03f3872044972ec87f7f571a78e0a5013b71cefa01ebc67058f53533959b8", "class_name": "RelatedNodeInfo"}}, "hash": "9df7aaed7b6801b6421974870470e9041bf7d1907793f4bbe33c3d9c6a804232", "text": "\",\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/define query engines and tools\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen, we define a `RouterQueryEngine` over them.\nBy default, this uses a `LLMSingleSelector` as the router, which uses the LLM to choose the best sub-index to router the query to, given the descriptions.\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/define query engines and tools\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.query_engine import RouterQueryEngine\n\nquery_engine = RouterQueryEngine.from_defaults(\n    query_engine_tools=[tool1, tool2]\n)\n\nresponse = query_engine.query(\n    \"In Notion, give me a summary of the product roadmap.\"", "start_char_idx": 3525, "end_char_idx": 4702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33e31028-ed65-4bd1-a21d-aceb00b3b6cc": {"__data__": {"id_": "33e31028-ed65-4bd1-a21d-aceb00b3b6cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "efa0c9b7-e632-419d-8e96-465fde932bcd", "node_type": "1", "metadata": {}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bf7eee9-2e8f-47e6-8212-df37ee35c911", "node_type": "1", "metadata": {}, "hash": "59e9f53e7aef6df28d7654db88212737b161318a776e6f39e0b53065602138ac", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "efa0c9b7-e632-419d-8e96-465fde932bcd", "node_type": "1", "metadata": {}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "class_name": "RelatedNodeInfo"}}, "hash": "b83290c0aa73ec1bbd30160d4459253efbcdb5122dd2fa4aedb77d3ffc59272c", "text": ")\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/define query engines and tools\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Guides**\n- Router Query Engine Guide (Notebook)\n- City Analysis Unified Query Interface (Notebook)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can explicitly perform compare/contrast queries with a **query transformation** module within a ComposableGraph.", "start_char_idx": 0, "end_char_idx": 947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bf7eee9-2e8f-47e6-8212-df37ee35c911": {"__data__": {"id_": "7bf7eee9-2e8f-47e6-8212-df37ee35c911", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "efa0c9b7-e632-419d-8e96-465fde932bcd", "node_type": "1", "metadata": {}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33e31028-ed65-4bd1-a21d-aceb00b3b6cc", "node_type": "1", "metadata": {}, "hash": "b83290c0aa73ec1bbd30160d4459253efbcdb5122dd2fa4aedb77d3ffc59272c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fd0ebf0-ba68-481e-aa52-3df13b97d7bc", "node_type": "1", "metadata": {}, "hash": "a6ee9c9a7e23e7a2cead560f055185fc5d424672f68abe683b47e81bd52110c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "efa0c9b7-e632-419d-8e96-465fde932bcd", "node_type": "1", "metadata": {}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "class_name": "RelatedNodeInfo"}}, "hash": "59e9f53e7aef6df28d7654db88212737b161318a776e6f39e0b53065602138ac", "text": "File Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor_chatgpt, verbose=True\n)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Compare/Contrast Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis module will help break down a complex query into a simpler one over your existing index structure.\n\n**Guides**\n- Query Transformations\n- City Analysis Compare/Contrast Example (Notebook)\n\nYou can also rely on the LLM to *infer* whether to perform compare/contrast queries (see Multi-Document Queries below).\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Document Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBesides the explicit synthesis/routing flows described above, LlamaIndex can support more general multi-document queries as well. \nIt can do this through our `SubQuestionQueryEngine` class. Given a query, this query engine will generate a \"query plan\" containing\nsub-queries against sub-documents before synthesizing the final answer.\n\nTo do this,", "start_char_idx": 949, "end_char_idx": 2861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fd0ebf0-ba68-481e-aa52-3df13b97d7bc": {"__data__": {"id_": "7fd0ebf0-ba68-481e-aa52-3df13b97d7bc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "efa0c9b7-e632-419d-8e96-465fde932bcd", "node_type": "1", "metadata": {}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7bf7eee9-2e8f-47e6-8212-df37ee35c911", "node_type": "1", "metadata": {}, "hash": "59e9f53e7aef6df28d7654db88212737b161318a776e6f39e0b53065602138ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7f0528f-a8e7-4da6-92ff-25942d99c739", "node_type": "1", "metadata": {}, "hash": "f67c7d60eb3bb0ed85f03ec6e7dc3539b8b62195e60b6240eeffddcca13f8e2e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "efa0c9b7-e632-419d-8e96-465fde932bcd", "node_type": "1", "metadata": {}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "class_name": "RelatedNodeInfo"}}, "hash": "a6ee9c9a7e23e7a2cead560f055185fc5d424672f68abe683b47e81bd52110c3", "text": "To do this, first define an index for each document/data source, and wrap it with a `QueryEngineTool` (similar to above):\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Document Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.tools import QueryEngineTool, ToolMetadata\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sept_engine, \n        metadata=ToolMetadata(name='sept_22', description='Provides information about Uber quarterly financials ending September 2022')\n    ),\n    QueryEngineTool(\n        query_engine=june_engine, \n        metadata=ToolMetadata(name='june_22', description='Provides information about Uber quarterly financials ending June 2022')\n    ),\n    QueryEngineTool(\n        query_engine=march_engine, \n        metadata=ToolMetadata(name='march_22', description='Provides information about Uber quarterly financials ending March 2022')\n    ),\n]\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Document Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThen, we define a `SubQuestionQueryEngine` over these tools:\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Document Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.", "start_char_idx": 2850, "end_char_idx": 4661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7f0528f-a8e7-4da6-92ff-25942d99c739": {"__data__": {"id_": "d7f0528f-a8e7-4da6-92ff-25942d99c739", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "efa0c9b7-e632-419d-8e96-465fde932bcd", "node_type": "1", "metadata": {}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fd0ebf0-ba68-481e-aa52-3df13b97d7bc", "node_type": "1", "metadata": {}, "hash": "a6ee9c9a7e23e7a2cead560f055185fc5d424672f68abe683b47e81bd52110c3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "efa0c9b7-e632-419d-8e96-465fde932bcd", "node_type": "1", "metadata": {}, "hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "class_name": "RelatedNodeInfo"}}, "hash": "f67c7d60eb3bb0ed85f03ec6e7dc3539b8b62195e60b6240eeffddcca13f8e2e", "text": "md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.query_engine import SubQuestionQueryEngine\n\nquery_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Document Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis query engine can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer.\nThis makes it especially well-suited for compare/contrast queries across documents as well as queries pertaining to a specific document.", "start_char_idx": 4627, "end_char_idx": 5568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1489f6c5-e7b4-4b63-a335-da063a794271": {"__data__": {"id_": "1489f6c5-e7b4-4b63-a335-da063a794271", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "baf924cc-b96b-46fd-84c1-79d69be105f3", "node_type": "1", "metadata": {}, "hash": "c7ead2bc19323b9b676ceda4679afdcf0217af2f97346b1989813e0e8701e363", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d68d6704-95bb-4b0c-8753-6602d33b20e3", "node_type": "1", "metadata": {}, "hash": "0cfdafd780daedc38008c6d4836bce05f196a25414e2d75f4a1c39d057102e2f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "baf924cc-b96b-46fd-84c1-79d69be105f3", "node_type": "1", "metadata": {}, "hash": "c7ead2bc19323b9b676ceda4679afdcf0217af2f97346b1989813e0e8701e363", "class_name": "RelatedNodeInfo"}}, "hash": "384a79c4b666018e367ef34f382dfeb9204fdd0e0fc83db521610f787f4f23f3", "text": "**Guides**\n- Sub Question Query Engine (Intro)\n- 10Q Analysis (Uber)\n- 10K Analysis (Uber and Lyft)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Multi-Step Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can also support iterative multi-step queries. Given a complex query, break it down into an initial subquestions,\nand sequentially generate subquestions based on returned answers until the final answer is returned.\n\nFor instance, given a question \"Who was in the first batch of the accelerator program the author started?\",\nthe module will first decompose the query into a simpler initial question \"What was the accelerator program the author started?\",\nquery the index, and then ask followup questions.\n\n**Guides**\n- Query Transformations\n- Multi-Step Query Decomposition (Notebook)\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Temporal Queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex can support queries that require an understanding of time. It can do this in two ways:\n- Decide whether the query requires utilizing temporal relationships between nodes (prev/next relationships) in order to retrieve additional context to answer the question.\n- Sort by recency and filter outdated context.", "start_char_idx": 0, "end_char_idx": 1720, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d68d6704-95bb-4b0c-8753-6602d33b20e3": {"__data__": {"id_": "d68d6704-95bb-4b0c-8753-6602d33b20e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "baf924cc-b96b-46fd-84c1-79d69be105f3", "node_type": "1", "metadata": {}, "hash": "c7ead2bc19323b9b676ceda4679afdcf0217af2f97346b1989813e0e8701e363", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1489f6c5-e7b4-4b63-a335-da063a794271", "node_type": "1", "metadata": {}, "hash": "384a79c4b666018e367ef34f382dfeb9204fdd0e0fc83db521610f787f4f23f3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "baf924cc-b96b-46fd-84c1-79d69be105f3", "node_type": "1", "metadata": {}, "hash": "c7ead2bc19323b9b676ceda4679afdcf0217af2f97346b1989813e0e8701e363", "class_name": "RelatedNodeInfo"}}, "hash": "0cfdafd780daedc38008c6d4836bce05f196a25414e2d75f4a1c39d057102e2f", "text": "- Sort by recency and filter outdated context.\n\n**Guides**\n- Second-Stage Postprocessing Guide\n- Prev/Next Postprocessing\n- Recency Postprocessing\n\nFile Name: Docs\\end_to_end_tutorials\\question_and_answer.md\nContent Type: text\nHeader Path: Q&A over Documents/Additional Resources\nfile_path: Docs\\end_to_end_tutorials\\question_and_answer.md\nfile_name: question_and_answer.md\nfile_type: None\nfile_size: 10727\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n- A Guide to Creating a Unified Query Framework over your ndexes\n- A Guide to Extracting Terms and Definitions\n- SEC 10k Analysis\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA lot of modern data systems depend on structured data, such as a Postgres DB or a Snowflake data warehouse.\nLlamaIndex provides a lot of advanced features, powered by LLM's, to both create structured data from\nunstructured data, as well as analyze this structured data through augmented text-to-SQL capabilities.\n\nThis guide helps walk through each of these capabilities. Specifically, we cover the following topics:\n- **Setup**: Defining up our example SQL Table.\n- **Building our Table Index**: How to go from sql database to a Table Schema Index\n- **Using natural language SQL queries**: How to query our SQL database using natural language.\n\nWe will walk through a toy example table which contains city/population/country information.\nA notebook for this tutorial is available here.", "start_char_idx": 1674, "end_char_idx": 3445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4173ece5-0263-4f96-9601-b024f4520896": {"__data__": {"id_": "4173ece5-0263-4f96-9601-b024f4520896", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "node_type": "1", "metadata": {}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9944fcba-9019-4ba9-a3c5-574daf2cb5e3", "node_type": "1", "metadata": {}, "hash": "eb5f201de7fd6aafd1eca8be538edf7744b759ef216cf00636e6bab1eb9a095e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "node_type": "1", "metadata": {}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "class_name": "RelatedNodeInfo"}}, "hash": "235ba6ea2370be1a64e93359650bebd5ef52a128f2060f6a57d1a0e687eb93be", "text": "A notebook for this tutorial is available here.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFirst, we use SQLAlchemy to setup a simple sqlite db:\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n\nengine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData()\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Setup\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe then create a toy `city_stats` table:\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/create city SQL table\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.", "start_char_idx": 0, "end_char_idx": 1658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9944fcba-9019-4ba9-a3c5-574daf2cb5e3": {"__data__": {"id_": "9944fcba-9019-4ba9-a3c5-574daf2cb5e3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "node_type": "1", "metadata": {}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4173ece5-0263-4f96-9601-b024f4520896", "node_type": "1", "metadata": {}, "hash": "235ba6ea2370be1a64e93359650bebd5ef52a128f2060f6a57d1a0e687eb93be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e30dee4-2d8c-46d8-aec5-d1b76a642603", "node_type": "1", "metadata": {}, "hash": "9560c782a7dbd6026c087cfefa6f431004eb73775766389b8514a109e3cd8c6d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "node_type": "1", "metadata": {}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "class_name": "RelatedNodeInfo"}}, "hash": "eb5f201de7fd6aafd1eca8be538edf7744b759ef216cf00636e6bab1eb9a095e", "text": "md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntable_name = \"city_stats\"\ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column(\"city_name\", String(16), primary_key=True),\n    Column(\"population\", Integer),\n    Column(\"country\", String(16), nullable=False),\n)\nmetadata_obj.create_all(engine)\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/create city SQL table\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow it's time to insert some datapoints!\n\nIf you want to look into filling into this table by inferring structured datapoints\nfrom unstructured data, take a look at the below section. Otherwise, you can choose\nto directly populate this table:\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/create city SQL table\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom sqlalchemy import insert\nrows = [\n    {\"city_name\": \"Toronto\", \"population\": 2731571, \"country\": \"Canada\"},\n    {\"city_name\": \"Tokyo\", \"population\": 13929286, \"country\": \"Japan\"},\n    {\"city_name\": \"Berlin\", \"population\": 600000, \"country\": \"Germany\"},\n]\nfor row in rows:\n    stmt = insert(city_stats_table).", "start_char_idx": 1634, "end_char_idx": 3359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e30dee4-2d8c-46d8-aec5-d1b76a642603": {"__data__": {"id_": "0e30dee4-2d8c-46d8-aec5-d1b76a642603", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "node_type": "1", "metadata": {}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9944fcba-9019-4ba9-a3c5-574daf2cb5e3", "node_type": "1", "metadata": {}, "hash": "eb5f201de7fd6aafd1eca8be538edf7744b759ef216cf00636e6bab1eb9a095e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1aabb644-ea1d-4306-93c5-5bf6eb4611a9", "node_type": "1", "metadata": {}, "hash": "5c5b52c7d79744414c45b79d351ebe13ecaccc6840d3a202a25dd19a7e41d70a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "node_type": "1", "metadata": {}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "class_name": "RelatedNodeInfo"}}, "hash": "9560c782a7dbd6026c087cfefa6f431004eb73775766389b8514a109e3cd8c6d", "text": "]\nfor row in rows:\n    stmt = insert(city_stats_table).values(**row)\n    with engine.connect() as connection:\n        cursor = connection.execute(stmt)\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/create city SQL table\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFinally, we can wrap the SQLAlchemy engine with our SQLDatabase wrapper;\nthis allows the db to be used within LlamaIndex:\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/create city SQL table\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SQLDatabase\n\nsql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Natural language SQL\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOnce we have constructed our SQL database, we can use the NLSQLTableQueryEngine to\nconstruct natural language queries that are synthesized into SQL queries.\n\nNote that we need to specify the tables we want to use with this query engine.", "start_char_idx": 3304, "end_char_idx": 5056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1aabb644-ea1d-4306-93c5-5bf6eb4611a9": {"__data__": {"id_": "1aabb644-ea1d-4306-93c5-5bf6eb4611a9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "node_type": "1", "metadata": {}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e30dee4-2d8c-46d8-aec5-d1b76a642603", "node_type": "1", "metadata": {}, "hash": "9560c782a7dbd6026c087cfefa6f431004eb73775766389b8514a109e3cd8c6d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "node_type": "1", "metadata": {}, "hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "class_name": "RelatedNodeInfo"}}, "hash": "5c5b52c7d79744414c45b79d351ebe13ecaccc6840d3a202a25dd19a7e41d70a", "text": "Note that we need to specify the tables we want to use with this query engine.\nIf we don't the query engine will pull all the schema context, which could\noverflow the context window of the LLM.", "start_char_idx": 4978, "end_char_idx": 5171, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5aaada9-567d-4840-b2ef-5d4d18e90c54": {"__data__": {"id_": "a5aaada9-567d-4840-b2ef-5d4d18e90c54", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "node_type": "1", "metadata": {}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "372fe491-ee84-4139-bbd3-aeba672ceae2", "node_type": "1", "metadata": {}, "hash": "a385cf17507a9571e6f12094d6a8cfa3555e81dc8bb04b151715e5c3316e9f17", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "node_type": "1", "metadata": {}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "class_name": "RelatedNodeInfo"}}, "hash": "fe9fe0b67f2086a238b8d79345a1447abde1b9ce90155b04acea95b3ce13f222", "text": "File Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Natural language SQL\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"city_stats\"],\n)\nquery_str = (\n    \"Which city has the highest population?\"\n)\nresponse = query_engine.query(query_str)\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Natural language SQL\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis query engine should used in any case where you can specify the tables you want\nto query over beforehand, or the total size of all the table schema plus the rest of\nthe prompt fits your context window.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Building our Table Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf we don't know ahead of time which table we would like to use, and the total size of\nthe table schema overflows your context window size, we should store the table schema \nin an index so that during query time we can retrieve the right schema.", "start_char_idx": 0, "end_char_idx": 1791, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "372fe491-ee84-4139-bbd3-aeba672ceae2": {"__data__": {"id_": "372fe491-ee84-4139-bbd3-aeba672ceae2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "node_type": "1", "metadata": {}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5aaada9-567d-4840-b2ef-5d4d18e90c54", "node_type": "1", "metadata": {}, "hash": "fe9fe0b67f2086a238b8d79345a1447abde1b9ce90155b04acea95b3ce13f222", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf4e0f2a-c7a0-4209-9ddb-37dcbc926b81", "node_type": "1", "metadata": {}, "hash": "d993b4fa31b1a153803da83979e2f252d94a17fe881c5ed5247e72041e42602f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "node_type": "1", "metadata": {}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "class_name": "RelatedNodeInfo"}}, "hash": "a385cf17507a9571e6f12094d6a8cfa3555e81dc8bb04b151715e5c3316e9f17", "text": "The way we can do this is using the SQLTableNodeMapping object, which takes in a \nSQLDatabase and produces a Node object for each SQLTableSchema object passed \ninto the ObjectIndex constructor.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Building our Table Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ntable_node_mapping = SQLTableNodeMapping(sql_database)\ntable_schema_objs = [(SQLTableSchema(table_name=\"city_stats\")), ...] # one SQLTableSchema for each table\n\nobj_index = ObjectIndex.from_objects(\n    table_schema_objs,\n    table_node_mapping,\n    VectorStoreIndex,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Building our Table Index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere you can see we define our table_node_mapping, and a single SQLTableSchema with the\n\"city_stats\" table name. We pass these into the ObjectIndex constructor, along with the\nVectorStoreIndex class definition we want to use. This will give us a VectorStoreIndex where\neach Node contains table schema and other context information. You can also add any additional\ncontext information you'd like.", "start_char_idx": 1793, "end_char_idx": 3418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf4e0f2a-c7a0-4209-9ddb-37dcbc926b81": {"__data__": {"id_": "bf4e0f2a-c7a0-4209-9ddb-37dcbc926b81", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "node_type": "1", "metadata": {}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "372fe491-ee84-4139-bbd3-aeba672ceae2", "node_type": "1", "metadata": {}, "hash": "a385cf17507a9571e6f12094d6a8cfa3555e81dc8bb04b151715e5c3316e9f17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cd1fe65-7376-4018-957c-b12a0add9cb2", "node_type": "1", "metadata": {}, "hash": "b32e60d2c31bf705cfea083a8efe4c24941f00c528c388304ad7fb638b15e1d3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "node_type": "1", "metadata": {}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "class_name": "RelatedNodeInfo"}}, "hash": "d993b4fa31b1a153803da83979e2f252d94a17fe881c5ed5247e72041e42602f", "text": "You can also add any additional\ncontext information you'd like.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/manually set extra context text\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ncity_stats_text = (\n    \"This table gives information regarding the population and country of a given city.\\n\"\n    \"The user will query with codewords, where 'foo' corresponds to population and 'bar'\"\n    \"corresponds to city.\"\n)\n\ntable_node_mapping = SQLTableNodeMapping(sql_database)\ntable_schema_objs = [(SQLTableSchema(table_name=\"city_stats\", context_str=city_stats_text))]\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Using natural language SQL queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nOnce we have defined our table schema index obj_index, we can construct a SQLTableRetrieverQueryEngine\nby passing in our SQLDatabase, and a retriever constructed from our object index.", "start_char_idx": 3355, "end_char_idx": 4765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4cd1fe65-7376-4018-957c-b12a0add9cb2": {"__data__": {"id_": "4cd1fe65-7376-4018-957c-b12a0add9cb2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "node_type": "1", "metadata": {}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf4e0f2a-c7a0-4209-9ddb-37dcbc926b81", "node_type": "1", "metadata": {}, "hash": "d993b4fa31b1a153803da83979e2f252d94a17fe881c5ed5247e72041e42602f", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "node_type": "1", "metadata": {}, "hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "class_name": "RelatedNodeInfo"}}, "hash": "b32e60d2c31bf705cfea083a8efe4c24941f00c528c388304ad7fb638b15e1d3", "text": "File Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Using natural language SQL queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = SQLTableRetrieverQueryEngine(\n    sql_database, obj_index.as_retriever(similarity_top_k=1)\n)\nresponse = query_engine.query(\"Which city has the highest population?\")", "start_char_idx": 4767, "end_char_idx": 5338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e492ad7-038f-4098-ad95-c572129140aa": {"__data__": {"id_": "6e492ad7-038f-4098-ad95-c572129140aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "node_type": "1", "metadata": {}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a401e806-2317-44b7-b540-f06cc7515fdd", "node_type": "1", "metadata": {}, "hash": "3ffa1f82e91ee6367f53cbf832e945939661489fca5eb689780e3c59dfa0afdb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "node_type": "1", "metadata": {}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "class_name": "RelatedNodeInfo"}}, "hash": "0529a6d7344af2a258d18dfe5a87c620c86f5ed2fb689de7f29d73487c0cb103", "text": "print(response)\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Using natural language SQL queries\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNow when we query the retriever query engine, it will retrieve the relevant table schema\nand synthesize a SQL query and a response from the results of that query.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nContent Type: text\nHeader Path: A Guide to LlamaIndex + Structured Data/Concluding Thoughts\nfile_path: Docs\\end_to_end_tutorials\\structured_data\\sql_guide.md\nfile_name: sql_guide.md\nfile_type: None\nfile_size: 5724\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis is it for now! We're constantly looking for ways to improve our structured data support.\nIf you have any questions let us know in our Discord.", "start_char_idx": 0, "end_char_idx": 1089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a401e806-2317-44b7-b540-f06cc7515fdd": {"__data__": {"id_": "a401e806-2317-44b7-b540-f06cc7515fdd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "node_type": "1", "metadata": {}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e492ad7-038f-4098-ad95-c572129140aa", "node_type": "1", "metadata": {}, "hash": "0529a6d7344af2a258d18dfe5a87c620c86f5ed2fb689de7f29d73487c0cb103", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "856827b8-2ae2-4f33-bb4a-ff39be844c3a", "node_type": "1", "metadata": {}, "hash": "c9aa5f4a20747fa2989c5bbec04b9f0324aeedaaa4c810c1984bb1a31d21ef09", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "node_type": "1", "metadata": {}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "class_name": "RelatedNodeInfo"}}, "hash": "3ffa1f82e91ee6367f53cbf832e945939661489fca5eb689780e3c59dfa0afdb", "text": "If you have any questions let us know in our Discord.\n\nFile Name: Docs\\end_to_end_tutorials\\structured_data.md\nContent Type: text\nHeader Path: Structured Data\nfile_path: Docs\\end_to_end_tutorials\\structured_data.md\nfile_name: structured_data.md\nfile_type: None\nfile_size: 224\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRelevant Resources:\n- A Guide to LlamaIndex + Structured Data\n- Airbyte SQL Index Guide\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe general usage pattern of LlamaIndex is as follows:\n\n1. Load in documents (either manually, or through a data loader)\n2. Parse the Documents into Nodes\n3. Construct Index (from Nodes or Documents)\n4. [Optional, Advanced] Building indices on top of other indices\n5. Query the index\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/1. Load in Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe first step is to load in data. This data is represented in the form of `Document` objects.\nWe provide a variety of data loaders which will load in Documents\nthrough the `load_data` function, e.g.:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/1.", "start_char_idx": 1036, "end_char_idx": 2742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "856827b8-2ae2-4f33-bb4a-ff39be844c3a": {"__data__": {"id_": "856827b8-2ae2-4f33-bb4a-ff39be844c3a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "node_type": "1", "metadata": {}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a401e806-2317-44b7-b540-f06cc7515fdd", "node_type": "1", "metadata": {}, "hash": "3ffa1f82e91ee6367f53cbf832e945939661489fca5eb689780e3c59dfa0afdb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bee4ff0-8395-4897-8cb5-7510719e6322", "node_type": "1", "metadata": {}, "hash": "19f2226138ad840a4aeb4891654572abca4f9c236d19d7d77abb8e25f9730c56", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "node_type": "1", "metadata": {}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "class_name": "RelatedNodeInfo"}}, "hash": "c9aa5f4a20747fa2989c5bbec04b9f0324aeedaaa4c810c1984bb1a31d21ef09", "text": "Load in Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('./data').load_data()\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/1. Load in Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to construct documents manually. LlamaIndex exposes the `Document` struct.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/1. Load in Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import Document\n\ntext_list = [text1, text2, ...]\ndocuments = [Document(text=t) for t in text_list]\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/1. Load in Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nA Document represents a lightweight container around the data source. You can now choose to proceed with one of the\nfollowing steps:\n\n1. Feed the Document object directly into the index (see section 3).", "start_char_idx": 2743, "end_char_idx": 4509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bee4ff0-8395-4897-8cb5-7510719e6322": {"__data__": {"id_": "6bee4ff0-8395-4897-8cb5-7510719e6322", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "node_type": "1", "metadata": {}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "856827b8-2ae2-4f33-bb4a-ff39be844c3a", "node_type": "1", "metadata": {}, "hash": "c9aa5f4a20747fa2989c5bbec04b9f0324aeedaaa4c810c1984bb1a31d21ef09", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "node_type": "1", "metadata": {}, "hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "class_name": "RelatedNodeInfo"}}, "hash": "19f2226138ad840a4aeb4891654572abca4f9c236d19d7d77abb8e25f9730c56", "text": "Feed the Document object directly into the index (see section 3).\n2. First convert the Document into Node objects (see section 2).\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/2. Parse the Documents into Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe next step is to parse these Document objects into Node objects. Nodes represent \"chunks\" of source Documents,\nwhether that is a text chunk, an image, or more. They also contain metadata and relationship information\nwith other nodes and index structures.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes.", "start_char_idx": 4444, "end_char_idx": 5390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c266b44-9983-4b8f-9f3b-5cbaaceaa6b5": {"__data__": {"id_": "2c266b44-9983-4b8f-9f3b-5cbaaceaa6b5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "node_type": "1", "metadata": {}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cec635fa-2615-442b-b2c6-c429087ea181", "node_type": "1", "metadata": {}, "hash": "ccc28a8cc89f0c8182ac8e55cd0c488b4a08bf528d09d037933e4d3f86ed97ba", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "node_type": "1", "metadata": {}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "class_name": "RelatedNodeInfo"}}, "hash": "c0337d223b957d2b7062b1e383e66d1b22550c6dddbd042e243647184a2e3223", "text": "You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes.\n\nFor instance, you can do\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/2. Parse the Documents into Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.node_parser import SimpleNodeParser\n\nparser = SimpleNodeParser()\n\nnodes = parser.get_nodes_from_documents(documents)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/2. Parse the Documents into Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to construct Node objects manually and skip the first section. For instance,\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/2. Parse the Documents into Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n\nnode1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\nnode2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/set relationships\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.", "start_char_idx": 0, "end_char_idx": 1807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cec635fa-2615-442b-b2c6-c429087ea181": {"__data__": {"id_": "cec635fa-2615-442b-b2c6-c429087ea181", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "node_type": "1", "metadata": {}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c266b44-9983-4b8f-9f3b-5cbaaceaa6b5", "node_type": "1", "metadata": {}, "hash": "c0337d223b957d2b7062b1e383e66d1b22550c6dddbd042e243647184a2e3223", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50b69303-cb23-449a-8035-3f3052ba3c58", "node_type": "1", "metadata": {}, "hash": "8e393148857de11224c932ddea8ce6cb75b947a6ccdfdf76c957d90ae8319a68", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "node_type": "1", "metadata": {}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "class_name": "RelatedNodeInfo"}}, "hash": "ccc28a8cc89f0c8182ac8e55cd0c488b4a08bf528d09d037933e4d3f86ed97ba", "text": "md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnode1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id)\nnode2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id)\nnodes = [node1, node2]\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/set relationships\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe `RelatedNodeInfo` class can also store additional `metadata` if needed:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/set relationships\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnode2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=node1.node_id, metadata={\"key\": \"val\"})\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe can now build an index over these Document objects.", "start_char_idx": 1779, "end_char_idx": 3383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50b69303-cb23-449a-8035-3f3052ba3c58": {"__data__": {"id_": "50b69303-cb23-449a-8035-3f3052ba3c58", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "node_type": "1", "metadata": {}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cec635fa-2615-442b-b2c6-c429087ea181", "node_type": "1", "metadata": {}, "hash": "ccc28a8cc89f0c8182ac8e55cd0c488b4a08bf528d09d037933e4d3f86ed97ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7d815f1-6f92-4100-b2e6-076e82f1bcf3", "node_type": "1", "metadata": {}, "hash": "7bb4888b383063542cc48b5995c575a5f030635636ae4fdb34fb7c149902c808", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "node_type": "1", "metadata": {}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "class_name": "RelatedNodeInfo"}}, "hash": "8e393148857de11224c932ddea8ce6cb75b947a6ccdfdf76c957d90ae8319a68", "text": "The simplest high-level abstraction is to load-in the Document objects during index initialization (this is relevant if you came directly from step 1 and skipped step 2).\n\n`from_documents` also takes an optional argument `show_progress`. Set it to `True` to display a progress bar during index construction.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also choose to build an index over a set of Node objects directly (this is a continuation of step 2).\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex(nodes)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 3384, "end_char_idx": 5101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7d815f1-6f92-4100-b2e6-076e82f1bcf3": {"__data__": {"id_": "b7d815f1-6f92-4100-b2e6-076e82f1bcf3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "node_type": "1", "metadata": {}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50b69303-cb23-449a-8035-3f3052ba3c58", "node_type": "1", "metadata": {}, "hash": "8e393148857de11224c932ddea8ce6cb75b947a6ccdfdf76c957d90ae8319a68", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "node_type": "1", "metadata": {}, "hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "class_name": "RelatedNodeInfo"}}, "hash": "7bb4888b383063542cc48b5995c575a5f030635636ae4fdb34fb7c149902c808", "text": "Index Construction\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDepending on which index you use, LlamaIndex may make LLM calls in order to build the index.", "start_char_idx": 5102, "end_char_idx": 5425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec709318-af84-4a34-9912-b83cf1ffda69": {"__data__": {"id_": "ec709318-af84-4a34-9912-b83cf1ffda69", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "node_type": "1", "metadata": {}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdcb393b-f093-494d-8dd8-6d709f7f5457", "node_type": "1", "metadata": {}, "hash": "e462ba1deba7ab6f88e642edd49d86e9ecf20401b89e5cf52184e0548f8f709c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "node_type": "1", "metadata": {}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "class_name": "RelatedNodeInfo"}}, "hash": "091dfb4dddc38035492d4c52c829a3268effc68aa1c1638e5296eb20c2efef1c", "text": "File Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Reusing Nodes across Index Structures\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you have multiple Node objects defined, and wish to share these Node\nobjects across multiple index structures, you can do that.\nSimply instantiate a StorageContext object,\nadd the Node objects to the underlying DocumentStore,\nand pass the StorageContext around.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Reusing Nodes across Index Structures\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import StorageContext\n\nstorage_context = StorageContext.from_defaults()\nstorage_context.docstore.add_documents(nodes)\n\nindex1 = VectorStoreIndex(nodes, storage_context=storage_context)\nindex2 = ListIndex(nodes, storage_context=storage_context)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Reusing Nodes across Index Structures\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**NOTE**: If the `storage_context` argument isn't specified, then it is implicitly\ncreated for each index during index construction. You can access the docstore\nassociated with a given index through `index.storage_context`.", "start_char_idx": 0, "end_char_idx": 1885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdcb393b-f093-494d-8dd8-6d709f7f5457": {"__data__": {"id_": "cdcb393b-f093-494d-8dd8-6d709f7f5457", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "node_type": "1", "metadata": {}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec709318-af84-4a34-9912-b83cf1ffda69", "node_type": "1", "metadata": {}, "hash": "091dfb4dddc38035492d4c52c829a3268effc68aa1c1638e5296eb20c2efef1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79c66cc3-606d-4dfd-b2c1-9b28af6e3dd7", "node_type": "1", "metadata": {}, "hash": "d3d52b6f51abb242e35a00157d0c9c143fbb32142437956585af38e09495e834", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "node_type": "1", "metadata": {}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "class_name": "RelatedNodeInfo"}}, "hash": "e462ba1deba7ab6f88e642edd49d86e9ecf20401b89e5cf52184e0548f8f709c", "text": "You can access the docstore\nassociated with a given index through `index.storage_context`.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Inserting Documents or Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also take advantage of the `insert` capability of indices to insert Document objects\none at a time instead of during index construction.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Inserting Documents or Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex([])\nfor doc in documents:\n    index.insert(doc)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Inserting Documents or Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you want to insert nodes on directly you can use `insert_nodes` function\ninstead.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 1795, "end_char_idx": 3450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79c66cc3-606d-4dfd-b2c1-9b28af6e3dd7": {"__data__": {"id_": "79c66cc3-606d-4dfd-b2c1-9b28af6e3dd7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "node_type": "1", "metadata": {}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdcb393b-f093-494d-8dd8-6d709f7f5457", "node_type": "1", "metadata": {}, "hash": "e462ba1deba7ab6f88e642edd49d86e9ecf20401b89e5cf52184e0548f8f709c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d68c899-e84c-43ac-b263-e57a81b96d46", "node_type": "1", "metadata": {}, "hash": "bcaa8a6a28a8b09860862fe39b0f3ff66de0c9ff0d68ffb9527cfa75c004aff3", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "node_type": "1", "metadata": {}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "class_name": "RelatedNodeInfo"}}, "hash": "d3d52b6f51abb242e35a00157d0c9c143fbb32142437956585af38e09495e834", "text": "Index Construction/Inserting Documents or Nodes\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/nodes: Sequence[Node]\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex([])\nindex.insert_nodes(nodes)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/nodes: Sequence[Node]\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee the Document Management How-To for more details on managing documents and an example notebook.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Customizing Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWhen creating documents, you can also attach useful metadata. Any metadata added to a document will be copied to the nodes that get created from their respective source document.", "start_char_idx": 3451, "end_char_idx": 5173, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d68c899-e84c-43ac-b263-e57a81b96d46": {"__data__": {"id_": "8d68c899-e84c-43ac-b263-e57a81b96d46", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "node_type": "1", "metadata": {}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79c66cc3-606d-4dfd-b2c1-9b28af6e3dd7", "node_type": "1", "metadata": {}, "hash": "d3d52b6f51abb242e35a00157d0c9c143fbb32142437956585af38e09495e834", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "node_type": "1", "metadata": {}, "hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "class_name": "RelatedNodeInfo"}}, "hash": "bcaa8a6a28a8b09860862fe39b0f3ff66de0c9ff0d68ffb9527cfa75c004aff3", "text": "File Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 0, "end_char_idx": 108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b1cb693-9230-4c52-a786-35b978e4da8f": {"__data__": {"id_": "5b1cb693-9230-4c52-a786-35b978e4da8f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c691987-9616-4fb7-89a8-83dfc8587080", "node_type": "1", "metadata": {}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd8bb99a-8f6f-4474-9d17-f8c4db6e89db", "node_type": "1", "metadata": {}, "hash": "588442eae32ebadac2a4488d7ebbb3f04743e16730731767f3fc5d1beb83ec78", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5c691987-9616-4fb7-89a8-83dfc8587080", "node_type": "1", "metadata": {}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "class_name": "RelatedNodeInfo"}}, "hash": "c35cb847b1c0d684b2b2a66b0778b9f005125ac5c585bd48d6ab9f206c709dbb", "text": "Index Construction/Customizing Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\ndocument = Document(\n    text='text',\n    metadata={\n        'filename': '<doc_file_name>',\n        'category': '<category>'\n    }\n)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Customizing Documents\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nMore information and approaches to this are discussed in the section Customizing Documents.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Customizing LLM's\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, we use OpenAI's `text-davinci-003` model. You may choose to use another LLM when constructing\nan index.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 0, "end_char_idx": 1425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd8bb99a-8f6f-4474-9d17-f8c4db6e89db": {"__data__": {"id_": "fd8bb99a-8f6f-4474-9d17-f8c4db6e89db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c691987-9616-4fb7-89a8-83dfc8587080", "node_type": "1", "metadata": {}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b1cb693-9230-4c52-a786-35b978e4da8f", "node_type": "1", "metadata": {}, "hash": "c35cb847b1c0d684b2b2a66b0778b9f005125ac5c585bd48d6ab9f206c709dbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b159eb3-6f7b-47bd-9b1d-180e8743ec8f", "node_type": "1", "metadata": {}, "hash": "02f9e0bd9af588d1c22ebb50edb8d49fd18f175e1212b081a6e124062257d46d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5c691987-9616-4fb7-89a8-83dfc8587080", "node_type": "1", "metadata": {}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "class_name": "RelatedNodeInfo"}}, "hash": "588442eae32ebadac2a4488d7ebbb3f04743e16730731767f3fc5d1beb83ec78", "text": "Index Construction/Customizing LLM's\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import LLMPredictor, VectorStoreIndex, ServiceContext\nfrom langchain import OpenAI\n\n...\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/define LLM\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nllm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/configure service context\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 1426, "end_char_idx": 2771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b159eb3-6f7b-47bd-9b1d-180e8743ec8f": {"__data__": {"id_": "9b159eb3-6f7b-47bd-9b1d-180e8743ec8f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c691987-9616-4fb7-89a8-83dfc8587080", "node_type": "1", "metadata": {}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd8bb99a-8f6f-4474-9d17-f8c4db6e89db", "node_type": "1", "metadata": {}, "hash": "588442eae32ebadac2a4488d7ebbb3f04743e16730731767f3fc5d1beb83ec78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1550131-2086-43ce-832e-f350dfc70b2b", "node_type": "1", "metadata": {}, "hash": "22583802b3f983896b9d77df439574c8047366ba5ed1c802ba39a36657716f9c", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5c691987-9616-4fb7-89a8-83dfc8587080", "node_type": "1", "metadata": {}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "class_name": "RelatedNodeInfo"}}, "hash": "02f9e0bd9af588d1c22ebb50edb8d49fd18f175e1212b081a6e124062257d46d", "text": "Index Construction/build index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/build index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nSee the Custom LLM's How-To for more details.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Global ServiceContext\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIf you wanted the service context from the last section to always be the default, you can configure one like so:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Global ServiceContext\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import set_global_service_context\nset_global_service_context(service_context)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 2772, "end_char_idx": 4547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1550131-2086-43ce-832e-f350dfc70b2b": {"__data__": {"id_": "f1550131-2086-43ce-832e-f350dfc70b2b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c691987-9616-4fb7-89a8-83dfc8587080", "node_type": "1", "metadata": {}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b159eb3-6f7b-47bd-9b1d-180e8743ec8f", "node_type": "1", "metadata": {}, "hash": "02f9e0bd9af588d1c22ebb50edb8d49fd18f175e1212b081a6e124062257d46d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "5c691987-9616-4fb7-89a8-83dfc8587080", "node_type": "1", "metadata": {}, "hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "class_name": "RelatedNodeInfo"}}, "hash": "22583802b3f983896b9d77df439574c8047366ba5ed1c802ba39a36657716f9c", "text": "Index Construction/Global ServiceContext\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis service context will always be used as the default if not specified as a keyword argument in LlamaIndex functions.\n\nFor more details on the service context, including how to create a global service context, see the page Customizing the ServiceContext.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 4548, "end_char_idx": 5167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bbfdb31-657f-48e6-a55b-060470c3748b": {"__data__": {"id_": "7bbfdb31-657f-48e6-a55b-060470c3748b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "171bb256-7674-4c94-bfe7-2609e57291d1", "node_type": "1", "metadata": {}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89632ac4-256c-4965-bb79-9757bf15c38e", "node_type": "1", "metadata": {}, "hash": "41a075895de8349027278922d56f3158d092dc9b68db84caa98787f791c200f7", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "171bb256-7674-4c94-bfe7-2609e57291d1", "node_type": "1", "metadata": {}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "class_name": "RelatedNodeInfo"}}, "hash": "ff2f0cd64fe0da5f278410aa16122d304358c6392374e6a02dd3717239725aba", "text": "Index Construction/Customizing Prompts\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nDepending on the index used, we used default prompt templates for constructing the index (and also insertion/querying).\nSee Custom Prompts How-To for more details on how to customize your prompt.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Customizing embeddings\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nFor embedding-based indices, you can choose to pass in a custom embedding model. See\nCustom Embeddings How-To for more details.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Cost Analysis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCreating an index, inserting to an index, and querying an index may use tokens. We can track\ntoken usage through the outputs of these operations. When running operations,\nthe token usage will be printed.\n\nYou can also fetch the token usage through `index.llm_predictor.last_token_usage`.\nSee Cost Analysis How-To for more details.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 0, "end_char_idx": 1734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89632ac4-256c-4965-bb79-9757bf15c38e": {"__data__": {"id_": "89632ac4-256c-4965-bb79-9757bf15c38e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "171bb256-7674-4c94-bfe7-2609e57291d1", "node_type": "1", "metadata": {}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7bbfdb31-657f-48e6-a55b-060470c3748b", "node_type": "1", "metadata": {}, "hash": "ff2f0cd64fe0da5f278410aa16122d304358c6392374e6a02dd3717239725aba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a592c58d-7ca7-480e-80bd-65288a4c5d5c", "node_type": "1", "metadata": {}, "hash": "a552534d4ebe66504035d6badc622b3269ff2f37ce918de800ce18166d29fff8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "171bb256-7674-4c94-bfe7-2609e57291d1", "node_type": "1", "metadata": {}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "class_name": "RelatedNodeInfo"}}, "hash": "41a075895de8349027278922d56f3158d092dc9b68db84caa98787f791c200f7", "text": "Index Construction/[Optional] Save the index for future use\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, data is stored in-memory.\nTo persist to disk:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/[Optional] Save the index for future use\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/[Optional] Save the index for future use\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou may omit persist_dir to persist to `./storage` by default.\n\nTo reload from disk:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 1735, "end_char_idx": 3082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a592c58d-7ca7-480e-80bd-65288a4c5d5c": {"__data__": {"id_": "a592c58d-7ca7-480e-80bd-65288a4c5d5c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "171bb256-7674-4c94-bfe7-2609e57291d1", "node_type": "1", "metadata": {}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89632ac4-256c-4965-bb79-9757bf15c38e", "node_type": "1", "metadata": {}, "hash": "41a075895de8349027278922d56f3158d092dc9b68db84caa98787f791c200f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e38e707-5f98-47f9-8659-1ea63e7634b6", "node_type": "1", "metadata": {}, "hash": "73fe69a6a3e8d87c67b5515e47ae53f88da258b34c60e8e693e350022e833831", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "171bb256-7674-4c94-bfe7-2609e57291d1", "node_type": "1", "metadata": {}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "class_name": "RelatedNodeInfo"}}, "hash": "a552534d4ebe66504035d6badc622b3269ff2f37ce918de800ce18166d29fff8", "text": "Index Construction/[Optional] Save the index for future use\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import StorageContext, load_index_from_storage\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/rebuild storage context\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/load index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = load_index_from_storage(storage_context)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/load index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**NOTE**: If you had initialized the index with a custom\n`ServiceContext` object, you will also need to pass in the same\nServiceContext during `load_index_from_storage`.", "start_char_idx": 3083, "end_char_idx": 4782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e38e707-5f98-47f9-8659-1ea63e7634b6": {"__data__": {"id_": "2e38e707-5f98-47f9-8659-1ea63e7634b6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "171bb256-7674-4c94-bfe7-2609e57291d1", "node_type": "1", "metadata": {}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a592c58d-7ca7-480e-80bd-65288a4c5d5c", "node_type": "1", "metadata": {}, "hash": "a552534d4ebe66504035d6badc622b3269ff2f37ce918de800ce18166d29fff8", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "171bb256-7674-4c94-bfe7-2609e57291d1", "node_type": "1", "metadata": {}, "hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "class_name": "RelatedNodeInfo"}}, "hash": "73fe69a6a3e8d87c67b5515e47ae53f88da258b34c60e8e693e350022e833831", "text": "File Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/load index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 4784, "end_char_idx": 5320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9553bf60-44e2-4b30-b0f3-01015787894b": {"__data__": {"id_": "9553bf60-44e2-4b30-b0f3-01015787894b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "node_type": "1", "metadata": {}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e771a02-63d1-41fc-a448-0007671e9cae", "node_type": "1", "metadata": {}, "hash": "27d0e405c129d38cd5559803e0763619deafec62fd5d5e40858bc74d98492bd9", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "node_type": "1", "metadata": {}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "class_name": "RelatedNodeInfo"}}, "hash": "6658e6b4055229c954a6fcba43e8177945560d136bbefc5deb63575aec2c4361", "text": "Index Construction/when first building the index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)\n\n...\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/when loading the index from disk\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = load_index_from_storage(\n    service_context=service_context,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/4. [Optional, Advanced] Building indices on top of other indices\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can build indices on top of other indices!\nComposability gives you greater power in indexing your heterogeneous sources of data. For a discussion on relevant use cases,\nsee our Query Use Cases. For technical details and examples, see our Composability How-To.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/5. Query the index.", "start_char_idx": 0, "end_char_idx": 1620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e771a02-63d1-41fc-a448-0007671e9cae": {"__data__": {"id_": "3e771a02-63d1-41fc-a448-0007671e9cae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "node_type": "1", "metadata": {}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9553bf60-44e2-4b30-b0f3-01015787894b", "node_type": "1", "metadata": {}, "hash": "6658e6b4055229c954a6fcba43e8177945560d136bbefc5deb63575aec2c4361", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "881abcf2-7975-4eaf-b94d-d555990138ba", "node_type": "1", "metadata": {}, "hash": "e5a7d00bab4beeb4104e0e6a72f16fe4ac9475107a1f8e23988d53ee273ecdf6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "node_type": "1", "metadata": {}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "class_name": "RelatedNodeInfo"}}, "hash": "27d0e405c129d38cd5559803e0763619deafec62fd5d5e40858bc74d98492bd9", "text": "Index Construction/5. Query the index.\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAfter building the index, you can now query it with a `QueryEngine`. Note that a \"query\" is simply an input to an LLM -\nthis means that you can use the index for question-answering, but you can also do more than that!\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/High-level API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo start, you can query an index with the default `QueryEngine` (i.e., using default configs), as follows:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/High-level API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nresponse = query_engine.query(\"Write an email to the user given their background information.\")\nprint(response)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 1582, "end_char_idx": 3215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "881abcf2-7975-4eaf-b94d-d555990138ba": {"__data__": {"id_": "881abcf2-7975-4eaf-b94d-d555990138ba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "node_type": "1", "metadata": {}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e771a02-63d1-41fc-a448-0007671e9cae", "node_type": "1", "metadata": {}, "hash": "27d0e405c129d38cd5559803e0763619deafec62fd5d5e40858bc74d98492bd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da0e4cec-c02d-4b47-8b9e-e8cacd7eda8d", "node_type": "1", "metadata": {}, "hash": "a409eec7424f11201bf113bab1cc34cb078bf496c108e38e89e482dc5ed63a63", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "node_type": "1", "metadata": {}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "class_name": "RelatedNodeInfo"}}, "hash": "e5a7d00bab4beeb4104e0e6a72f16fe4ac9475107a1f8e23988d53ee273ecdf6", "text": "Index Construction/Low-level API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe also support a low-level composition API that gives you more granular control over the query logic.\nBelow we highlight a few of the possible customizations.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Low-level API\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.retrievers import VectorIndexRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/build index\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 3216, "end_char_idx": 4753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da0e4cec-c02d-4b47-8b9e-e8cacd7eda8d": {"__data__": {"id_": "da0e4cec-c02d-4b47-8b9e-e8cacd7eda8d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "node_type": "1", "metadata": {}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "881abcf2-7975-4eaf-b94d-d555990138ba", "node_type": "1", "metadata": {}, "hash": "e5a7d00bab4beeb4104e0e6a72f16fe4ac9475107a1f8e23988d53ee273ecdf6", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "node_type": "1", "metadata": {}, "hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "class_name": "RelatedNodeInfo"}}, "hash": "a409eec7424f11201bf113bab1cc34cb078bf496c108e38e89e482dc5ed63a63", "text": "Index Construction/configure retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=2,\n)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 4754, "end_char_idx": 5191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8ff3da0-6f08-4913-af29-ae21347f7cc7": {"__data__": {"id_": "a8ff3da0-6f08-4913-af29-ae21347f7cc7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "node_type": "1", "metadata": {}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4aaae6fe-08c6-44db-9fbc-aff16d3a2710", "node_type": "1", "metadata": {}, "hash": "fb7097a2b2dd8acc96713c4c7380305d037c8e5467d00619d83ce0d20a3698bb", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "node_type": "1", "metadata": {}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "class_name": "RelatedNodeInfo"}}, "hash": "2f4345fe18a98986f8cd1d95fb052be2341c7ea9f079b0c304a5d58c308392f4", "text": "Index Construction/configure response synthesizer\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse_synthesizer = get_response_synthesizer()\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/assemble query engine\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff=0.7)\n    ]\n\n)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/query\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/query\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou may also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.", "start_char_idx": 0, "end_char_idx": 1779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4aaae6fe-08c6-44db-9fbc-aff16d3a2710": {"__data__": {"id_": "4aaae6fe-08c6-44db-9fbc-aff16d3a2710", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "node_type": "1", "metadata": {}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8ff3da0-6f08-4913-af29-ae21347f7cc7", "node_type": "1", "metadata": {}, "hash": "2f4345fe18a98986f8cd1d95fb052be2341c7ea9f079b0c304a5d58c308392f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad0981f2-0977-4a76-bc37-1e4b826af027", "node_type": "1", "metadata": {}, "hash": "94bfb40ca8ffa7a7e8e5af8711a5336cb7f4999d6c915fdb318bf6a25b56ba3b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "node_type": "1", "metadata": {}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "class_name": "RelatedNodeInfo"}}, "hash": "fb7097a2b2dd8acc96713c4c7380305d037c8e5467d00619d83ce0d20a3698bb", "text": "For a full list of implemented components and the supported configurations, please see the detailed reference docs.\n\nIn the following, we discuss some commonly used configurations in detail.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn index can have a variety of index-specific retrieval modes.\nFor instance, a list index supports the default `ListIndexRetriever` that retrieves all nodes, and\n`ListIndexEmbeddingRetriever` that retrieves the top-k nodes by embedding similarity.\n\nFor convienience, you can also use the following shorthand:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n# ListIndexRetriever\n    retriever = index.as_retriever(retriever_mode='default')\n    # ListIndexEmbeddingRetriever\n    retriever = index.as_retriever(retriever_mode='embedding')\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 1781, "end_char_idx": 3295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad0981f2-0977-4a76-bc37-1e4b826af027": {"__data__": {"id_": "ad0981f2-0977-4a76-bc37-1e4b826af027", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "node_type": "1", "metadata": {}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4aaae6fe-08c6-44db-9fbc-aff16d3a2710", "node_type": "1", "metadata": {}, "hash": "fb7097a2b2dd8acc96713c4c7380305d037c8e5467d00619d83ce0d20a3698bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b6805b3-e806-46f4-8161-8a56124dd83b", "node_type": "1", "metadata": {}, "hash": "5751d89852790717f6c78313ac28a54947704baae180a203eafbcd836288e58d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "node_type": "1", "metadata": {}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "class_name": "RelatedNodeInfo"}}, "hash": "94bfb40ca8ffa7a7e8e5af8711a5336cb7f4999d6c915fdb318bf6a25b56ba3b", "text": "Index Construction/Configuring retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAfter choosing your desired retriever, you can construct your query engine:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine(retriever)\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring retriever\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe full list of retrievers for each index (and their shorthand) is documented in the Query Reference.\n\n(setting-response-mode)=\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring response synthesis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAfter a retriever fetches relevant nodes, a `BaseSynthesizer` synthesizes the final response by combining the information.", "start_char_idx": 3296, "end_char_idx": 5089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b6805b3-e806-46f4-8161-8a56124dd83b": {"__data__": {"id_": "1b6805b3-e806-46f4-8161-8a56124dd83b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "node_type": "1", "metadata": {}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad0981f2-0977-4a76-bc37-1e4b826af027", "node_type": "1", "metadata": {}, "hash": "94bfb40ca8ffa7a7e8e5af8711a5336cb7f4999d6c915fdb318bf6a25b56ba3b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f", "node_type": "1", "metadata": {}, "hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "class_name": "RelatedNodeInfo"}}, "hash": "5751d89852790717f6c78313ac28a54947704baae180a203eafbcd836288e58d", "text": "You can configure it via\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 5091, "end_char_idx": 5225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1309865-2516-42cb-b374-9811c3cb803f": {"__data__": {"id_": "f1309865-2516-42cb-b374-9811c3cb803f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "node_type": "1", "metadata": {}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d26219ad-4f57-48b4-a730-1fefa3530c91", "node_type": "1", "metadata": {}, "hash": "610769980b9237744200fdc6f1e7ad81eae41b7e0674bb58833f288abd3773ce", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "node_type": "1", "metadata": {}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "class_name": "RelatedNodeInfo"}}, "hash": "7bf86fb0bc4dea30130bbfc91086bdce25b94ef46da03b1ae131a75b5d2e7662", "text": "Index Construction/Configuring response synthesis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode=<response_mode>)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring response synthesis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRight now, we support the following options:\n\n- `default`: \"create and refine\" an answer by sequentially going through each retrieved `Node`;\n  This makes a separate LLM call per Node. Good for more detailed answers.\n- `compact`: \"compact\" the prompt during each LLM call by stuffing as\n  many `Node` text chunks that can fit within the maximum prompt size. If there are\n  too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n  multiple prompts.\n- `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree\n  and return the root node as the response. Good for summarization purposes.\n- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,\n  without actually sending them. Then can be inspected by checking `response.source_nodes`.\n  The response object is covered in more detail in Section 5.\n- `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text\n  chunk while accumulating the responses into an array. Returns a concatenated string of all\n  responses. Good for when you need to run the same query separately against each text\n  chunk.", "start_char_idx": 0, "end_char_idx": 1906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d26219ad-4f57-48b4-a730-1fefa3530c91": {"__data__": {"id_": "d26219ad-4f57-48b4-a730-1fefa3530c91", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "node_type": "1", "metadata": {}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1309865-2516-42cb-b374-9811c3cb803f", "node_type": "1", "metadata": {}, "hash": "7bf86fb0bc4dea30130bbfc91086bdce25b94ef46da03b1ae131a75b5d2e7662", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d80aaf95-855c-4fd3-94f1-e90606cd8f27", "node_type": "1", "metadata": {}, "hash": "15a8e78581c83a5aa137eb32da9ed644846e534c639eff86d4c180680d1c762e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "node_type": "1", "metadata": {}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "class_name": "RelatedNodeInfo"}}, "hash": "610769980b9237744200fdc6f1e7ad81eae41b7e0674bb58833f288abd3773ce", "text": "Good for when you need to run the same query separately against each text\n  chunk.\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring response synthesis\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = ListIndex.from_documents(documents)\nretriever = index.as_retriever()\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/default\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='default')\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/compact\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='compact')\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 1824, "end_char_idx": 3463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d80aaf95-855c-4fd3-94f1-e90606cd8f27": {"__data__": {"id_": "d80aaf95-855c-4fd3-94f1-e90606cd8f27", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "node_type": "1", "metadata": {}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d26219ad-4f57-48b4-a730-1fefa3530c91", "node_type": "1", "metadata": {}, "hash": "610769980b9237744200fdc6f1e7ad81eae41b7e0674bb58833f288abd3773ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3da92898-d25d-4a1f-8c47-d2c71b3db512", "node_type": "1", "metadata": {}, "hash": "8f45d8e14c8ff2fac17a64af98b3fea357b04122582af7386f882152c8e37a4a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "node_type": "1", "metadata": {}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "class_name": "RelatedNodeInfo"}}, "hash": "15a8e78581c83a5aa137eb32da9ed644846e534c639eff86d4c180680d1c762e", "text": "Index Construction/tree summarize\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='tree_summarize')\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/no text\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='no_text')\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring node postprocessors (i.e. filtering and augmentation)\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe also support advanced `Node` filtering and augmentation that can further improve the relevancy of the retrieved `Node` objects.\nThis can help reduce the time/number of LLM calls/cost or improve response quality.\n\nFor example:\n\n- `KeywordNodePostprocessor`: filters nodes by `required_keywords` and `exclude_keywords`.\n- `SimilarityPostprocessor`: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers)\n- `PrevNextNodePostprocessor`: augments retrieved `Node` objects with additional relevant context based on `Node` relationships.", "start_char_idx": 3464, "end_char_idx": 5364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3da92898-d25d-4a1f-8c47-d2c71b3db512": {"__data__": {"id_": "3da92898-d25d-4a1f-8c47-d2c71b3db512", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "node_type": "1", "metadata": {}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d80aaf95-855c-4fd3-94f1-e90606cd8f27", "node_type": "1", "metadata": {}, "hash": "15a8e78581c83a5aa137eb32da9ed644846e534c639eff86d4c180680d1c762e", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "node_type": "1", "metadata": {}, "hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "class_name": "RelatedNodeInfo"}}, "hash": "8f45d8e14c8ff2fac17a64af98b3fea357b04122582af7386f882152c8e37a4a", "text": "The full list of node postprocessors is documented in the Node Postprocessor Reference.\n\nTo configure the desired node postprocessors:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/Configuring node postprocessors (i.e.", "start_char_idx": 5366, "end_char_idx": 5667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c6787bb-f3de-4152-b236-e5adb248eef5": {"__data__": {"id_": "5c6787bb-f3de-4152-b236-e5adb248eef5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "node_type": "1", "metadata": {}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "702a4dc3-2218-45fb-a918-ae81910bb597", "node_type": "1", "metadata": {}, "hash": "b689429ca991e0f06ce1d34d3fa4fe7a8de3a8c04552652c23e6bed25cd5f457", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "node_type": "1", "metadata": {}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "class_name": "RelatedNodeInfo"}}, "hash": "ac63e5ba7bfa1642c09ba30f1d2802d4db035db136f8f3b486a067042827de8f", "text": "Index Construction/Configuring node postprocessors (i.e. filtering and augmentation)\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nnode_postprocessors = [\n    KeywordNodePostprocessor(\n        required_keywords=[\"Combinator\"],\n        exclude_keywords=[\"Italy\"]\n    )\n]\nquery_engine = RetrieverQueryEngine.from_args(\n    retriever, node_postprocessors=node_postprocessors\n)\nresponse = query_engine.query(\"What did the author do growing up?\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/5. Parsing the response\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThe object returned is a `Response` object.\nThe object contains both the response text as well as the \"sources\" of the response:\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/5. Parsing the response\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse = query_engine.query(\"<query_str>\")\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3.", "start_char_idx": 0, "end_char_idx": 1621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "702a4dc3-2218-45fb-a918-ae81910bb597": {"__data__": {"id_": "702a4dc3-2218-45fb-a918-ae81910bb597", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "node_type": "1", "metadata": {}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c6787bb-f3de-4152-b236-e5adb248eef5", "node_type": "1", "metadata": {}, "hash": "ac63e5ba7bfa1642c09ba30f1d2802d4db035db136f8f3b486a067042827de8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbfa05e3-53b0-4be3-bf05-9c83f8471bb0", "node_type": "1", "metadata": {}, "hash": "1f6a53dec9c1bcc60d09370fce7336cfe78810ded0b7ee3f63a0fd14e46fbf31", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "node_type": "1", "metadata": {}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "class_name": "RelatedNodeInfo"}}, "hash": "b689429ca991e0f06ce1d34d3fa4fe7a8de3a8c04552652c23e6bed25cd5f457", "text": "Index Construction/response.response\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstr(response)\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/get sources\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse.source_nodes\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/formatted sources\nLinks: \nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nresponse.get_formatted_sources()\n\nFile Name: Docs\\end_to_end_tutorials\\usage_pattern.md\nContent Type: text\nHeader Path: Basic Usage Pattern/3. Index Construction/formatted sources\nfile_path: Docs\\end_to_end_tutorials\\usage_pattern.md\nfile_name: usage_pattern.md\nfile_type: None\nfile_size: 16177\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nAn example is shown below.\n!", "start_char_idx": 1622, "end_char_idx": 3031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbfa05e3-53b0-4be3-bf05-9c83f8471bb0": {"__data__": {"id_": "fbfa05e3-53b0-4be3-bf05-9c83f8471bb0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "node_type": "1", "metadata": {}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "702a4dc3-2218-45fb-a918-ae81910bb597", "node_type": "1", "metadata": {}, "hash": "b689429ca991e0f06ce1d34d3fa4fe7a8de3a8c04552652c23e6bed25cd5f457", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ba6c70f-6801-497a-bc71-79b34146ac45", "node_type": "1", "metadata": {}, "hash": "c1c1a71a8a7bb69bad7604f30213511db0803d5661fd94e961b1182e20eecba4", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "node_type": "1", "metadata": {}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "class_name": "RelatedNodeInfo"}}, "hash": "1f6a53dec9c1bcc60d09370fce7336cfe78810ded0b7ee3f63a0fd14e46fbf31", "text": "!\n\nFile Name: Docs\\end_to_end_tutorials\\use_cases.md\nContent Type: code\nHeader Path: Use Cases\nfile_path: Docs\\end_to_end_tutorials\\use_cases.md\nfile_name: use_cases.md\nfile_type: None\nfile_size: 286\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/end_to_end_tutorials/question_and_answer.md\n/end_to_end_tutorials/chatbots.md\n/end_to_end_tutorials/agents.md\n/end_to_end_tutorials/structured_data.md\n/end_to_end_tutorials/apps.md\n/end_to_end_tutorials/privacy.md\n```\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: code\nHeader Path: High-Level Concepts\nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nIf you haven't, install and complete starter tutorial before you read this. It will make a lot more sense!\n```\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex helps you build LLM-powered applications (e.g. Q&A, chatbot, and agents) over custom data.\n\nIn this high-level concepts guide, you will learn:\n* the retrieval augmented generation (RAG) paradigm for combining LLM with custom data,\n* key concepts and modules in LlamaIndex for composing your own RAG pipeline.", "start_char_idx": 3030, "end_char_idx": 4588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ba6c70f-6801-497a-bc71-79b34146ac45": {"__data__": {"id_": "5ba6c70f-6801-497a-bc71-79b34146ac45", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "node_type": "1", "metadata": {}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbfa05e3-53b0-4be3-bf05-9c83f8471bb0", "node_type": "1", "metadata": {}, "hash": "1f6a53dec9c1bcc60d09370fce7336cfe78810ded0b7ee3f63a0fd14e46fbf31", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "node_type": "1", "metadata": {}, "hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "class_name": "RelatedNodeInfo"}}, "hash": "c1c1a71a8a7bb69bad7604f30213511db0803d5661fd94e961b1182e20eecba4", "text": "File Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nRetrieval augmented generation (RAG) is a paradigm for augmenting LLM with custom data.\nIt generally consists of two stages: \n1) **indexing stage**: preparing a knowledge base, and\n2) **querying stage**: retrieving relevant context from the knowledge to assist the LLM in responding to a question\n\n!", "start_char_idx": 4590, "end_char_idx": 5218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fea4afe-a138-4675-92c5-f6415ca97313": {"__data__": {"id_": "2fea4afe-a138-4675-92c5-f6415ca97313", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "53b92866-0f8e-48d1-bb79-a2d943008412", "node_type": "1", "metadata": {}, "hash": "76a39cd8d7738e591c73d577b1d61a76c9ad29cf7077dec16ea41a05f6d2ad35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a0d5a48-9818-4626-bb90-e6d0fbc3ac7d", "node_type": "1", "metadata": {}, "hash": "e1f3566396c24264a86417c6f1554804b6274be2c092f92b05799daed8f8e570", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "53b92866-0f8e-48d1-bb79-a2d943008412", "node_type": "1", "metadata": {}, "hash": "76a39cd8d7738e591c73d577b1d61a76c9ad29cf7077dec16ea41a05f6d2ad35", "class_name": "RelatedNodeInfo"}}, "hash": "ab5c68d6620d09e659234d1d36e3f4d83bb1e06ffdc3d74e845800f6b36d7484", "text": "LlamaIndex provides the essential toolkit for making both steps super easy.\nLet's explore each stage in detail.\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)/Indexing Stage\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex help you prepare the knowledge base with a suite of data connectors and indexes.\n! \n\n**Data Connectors**:\nA data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).\n\n**Documents / Nodes**: A `Document` is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A `Node` is the atomic unit of data in LlamaIndex and represents a \"chunk\" of a source `Document`. It's a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations.\n\n**Data Indexes**: \nOnce you've ingested your data, LlamaIndex help you index data into a format that's easy to retrieve.\nUnder the hood, LlamaIndex parse the raw documents into intermediate representations, calculate vector embeddings, and infer metadata, etc.\nThe most commonly used index is the VectorStoreIndex\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)/Querying Stage\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn the querying stage, the RAG pipeline retrieves the most relevant context given a user query,\nand pass that to the LLM (along with the query) to synthesize a response.\nThis gives the LLM up-to-date knowledge that is not in its original training data,\n(also reducing hallucination).", "start_char_idx": 0, "end_char_idx": 2071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a0d5a48-9818-4626-bb90-e6d0fbc3ac7d": {"__data__": {"id_": "9a0d5a48-9818-4626-bb90-e6d0fbc3ac7d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "53b92866-0f8e-48d1-bb79-a2d943008412", "node_type": "1", "metadata": {}, "hash": "76a39cd8d7738e591c73d577b1d61a76c9ad29cf7077dec16ea41a05f6d2ad35", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fea4afe-a138-4675-92c5-f6415ca97313", "node_type": "1", "metadata": {}, "hash": "ab5c68d6620d09e659234d1d36e3f4d83bb1e06ffdc3d74e845800f6b36d7484", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edff671d-9458-4bdb-ac51-0a7d9d7f7d42", "node_type": "1", "metadata": {}, "hash": "b74591e8d5f15a42e91b0c980d57b1f1f4c4b54798b0e78de95a024a56e1350a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "53b92866-0f8e-48d1-bb79-a2d943008412", "node_type": "1", "metadata": {}, "hash": "76a39cd8d7738e591c73d577b1d61a76c9ad29cf7077dec16ea41a05f6d2ad35", "class_name": "RelatedNodeInfo"}}, "hash": "e1f3566396c24264a86417c6f1554804b6274be2c092f92b05799daed8f8e570", "text": "The key challenge in the querying stage is retrieval, orchestration, and reasoning over (potentially many) knowledge bases.\n\nLlamaIndex provides composable modules that help you build and integrate RAG pipelines for Q&A (query engine), chatbot (chat engine), or as part of an agent.\nThese building blocks can be customized to reflect ranking preferences, as well as composed to reason over multiple knowledge bases in a structured way.\n\n!\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)/Querying Stage/Building Blocks\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Retrievers**: \nA retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query.\nThe specific retrieval logic differs for difference indices, the most popular being dense retrieval against a vector index.\n\n**Node Postprocessors**:\nA node postprocessor takes in a set of nodes, then apply transformation, filtering, or re-ranking logic to them. \n\n**Response Synthesizers**:\nA response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)/Querying Stage/Pipelines\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n**Query Engines**:\nA query engine is an end-to-end pipeline that allow you to ask question over your data.\nIt takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.", "start_char_idx": 2072, "end_char_idx": 4004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edff671d-9458-4bdb-ac51-0a7d9d7f7d42": {"__data__": {"id_": "edff671d-9458-4bdb-ac51-0a7d9d7f7d42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "53b92866-0f8e-48d1-bb79-a2d943008412", "node_type": "1", "metadata": {}, "hash": "76a39cd8d7738e591c73d577b1d61a76c9ad29cf7077dec16ea41a05f6d2ad35", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a0d5a48-9818-4626-bb90-e6d0fbc3ac7d", "node_type": "1", "metadata": {}, "hash": "e1f3566396c24264a86417c6f1554804b6274be2c092f92b05799daed8f8e570", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "53b92866-0f8e-48d1-bb79-a2d943008412", "node_type": "1", "metadata": {}, "hash": "76a39cd8d7738e591c73d577b1d61a76c9ad29cf7077dec16ea41a05f6d2ad35", "class_name": "RelatedNodeInfo"}}, "hash": "b74591e8d5f15a42e91b0c980d57b1f1f4c4b54798b0e78de95a024a56e1350a", "text": "**Chat Engines**: \nA chat engine is an end-to-end pipeline for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer).\n\n**Agents**: \nAn agent is an automated decision maker (powered by an LLM) that interacts with the world via a set of tools.\nAgent may be used in the same fashion as query engines or chat engines. \nThe main distinction is that an agent dynamically decides the best sequence of actions, instead of following a predetermined logic.\nThis gives it additional flexibility to tackle more complex tasks.\n\nFile Name: Docs\\getting_started\\concepts.md\nContent Type: text\nHeader Path: High-Level Concepts/Retrieval Augmented Generation (RAG)/Querying Stage/Pipelines\nLinks: \nfile_path: Docs\\getting_started\\concepts.md\nfile_name: concepts.md\nfile_type: None\nfile_size: 5127\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n* tell me how to customize things.\n* curious about a specific module? Check out the module guides \ud83d\udc48\n* have a use case in mind?", "start_char_idx": 4007, "end_char_idx": 5049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c76cac1f-267c-4a55-8d5f-c50ff3f2cc54": {"__data__": {"id_": "c76cac1f-267c-4a55-8d5f-c50ff3f2cc54", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9206bf00-08cf-4199-9276-96ef119a8e61", "node_type": "1", "metadata": {}, "hash": "741492fa2ea72962132f7b8aea6ab7e5dd917c0ca02d513b93a78277ee28fd82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "517e8542-2a5f-4174-ac04-97dd59ac391c", "node_type": "1", "metadata": {}, "hash": "02a5d5d7d301e024b522cba7394044cb4f0a69ff2734ae96b86a2b99aa8e6f44", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9206bf00-08cf-4199-9276-96ef119a8e61", "node_type": "1", "metadata": {}, "hash": "741492fa2ea72962132f7b8aea6ab7e5dd917c0ca02d513b93a78277ee28fd82", "class_name": "RelatedNodeInfo"}}, "hash": "726295230836e04a450f0606d41f631e6d6fa38344c6a4786f32442d6ca1e5ee", "text": "Check out the module guides \ud83d\udc48\n* have a use case in mind? Check out the end-to-end tutorials\n\nFile Name: Docs\\getting_started\\installation.md\nContent Type: text\nHeader Path: Installation and Setup/Installation from Pip\nLinks: \nfile_path: Docs\\getting_started\\installation.md\nfile_name: installation.md\nfile_type: None\nfile_size: 987\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can simply do:\n\nFile Name: Docs\\getting_started\\installation.md\nContent Type: text\nHeader Path: Installation and Setup/Installation from Pip\nLinks: \nfile_path: Docs\\getting_started\\installation.md\nfile_name: installation.md\nfile_type: None\nfile_size: 987\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\npip install llama-index\n\nFile Name: Docs\\getting_started\\installation.md\nContent Type: text\nHeader Path: Installation and Setup/Installation from Source\nLinks: \nfile_path: Docs\\getting_started\\installation.md\nfile_name: installation.md\nfile_type: None\nfile_size: 987\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nGit clone this repository: `git clone https://github.com/jerryjliu/llama_index.git`. Then do:\n\n- `pip install -e .` if you want to do an editable install (you can modify source files) of just the package itself.\n- `pip install -r requirements.txt` if you want to install optional dependencies + dependencies used for development (e.g. unit testing).", "start_char_idx": 0, "end_char_idx": 1474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "517e8542-2a5f-4174-ac04-97dd59ac391c": {"__data__": {"id_": "517e8542-2a5f-4174-ac04-97dd59ac391c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9206bf00-08cf-4199-9276-96ef119a8e61", "node_type": "1", "metadata": {}, "hash": "741492fa2ea72962132f7b8aea6ab7e5dd917c0ca02d513b93a78277ee28fd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c76cac1f-267c-4a55-8d5f-c50ff3f2cc54", "node_type": "1", "metadata": {}, "hash": "726295230836e04a450f0606d41f631e6d6fa38344c6a4786f32442d6ca1e5ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02097577-3e0b-4ef8-bac8-942c1468a056", "node_type": "1", "metadata": {}, "hash": "27208c3fdf2ea4bf8a1b67ea95e79db1b4669fb7b88a583fdb652d2fdaf05c06", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9206bf00-08cf-4199-9276-96ef119a8e61", "node_type": "1", "metadata": {}, "hash": "741492fa2ea72962132f7b8aea6ab7e5dd917c0ca02d513b93a78277ee28fd82", "class_name": "RelatedNodeInfo"}}, "hash": "02a5d5d7d301e024b522cba7394044cb4f0a69ff2734ae96b86a2b99aa8e6f44", "text": "unit testing).\n\nFile Name: Docs\\getting_started\\installation.md\nContent Type: text\nHeader Path: Installation and Setup/Environment Setup\nLinks: \nfile_path: Docs\\getting_started\\installation.md\nfile_name: installation.md\nfile_type: None\nfile_size: 987\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, we use the OpenAI GPT-3 `text-davinci-003` model. In order to use this, you must have an OPENAI_API_KEY setup.\nYou can register an API key by logging into OpenAI's page and creating a new API token.\n\nFile Name: Docs\\getting_started\\installation.md\nContent Type: text\nHeader Path: Installation and Setup/Environment Setup\nLinks: \nfile_path: Docs\\getting_started\\installation.md\nfile_name: installation.md\nfile_type: None\nfile_size: 987\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can also customize the underlying LLM. You may\nneed additional environment keys + tokens setup depending on the LLM provider.\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: code\nHeader Path: Starter Tutorial\nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{tip}\nMake sure you've followed the installation steps first.\n```\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nHere is a starter example for using LlamaIndex.", "start_char_idx": 1460, "end_char_idx": 3196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02097577-3e0b-4ef8-bac8-942c1468a056": {"__data__": {"id_": "02097577-3e0b-4ef8-bac8-942c1468a056", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9206bf00-08cf-4199-9276-96ef119a8e61", "node_type": "1", "metadata": {}, "hash": "741492fa2ea72962132f7b8aea6ab7e5dd917c0ca02d513b93a78277ee28fd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "517e8542-2a5f-4174-ac04-97dd59ac391c", "node_type": "1", "metadata": {}, "hash": "02a5d5d7d301e024b522cba7394044cb4f0a69ff2734ae96b86a2b99aa8e6f44", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "9206bf00-08cf-4199-9276-96ef119a8e61", "node_type": "1", "metadata": {}, "hash": "741492fa2ea72962132f7b8aea6ab7e5dd917c0ca02d513b93a78277ee28fd82", "class_name": "RelatedNodeInfo"}}, "hash": "27208c3fdf2ea4bf8a1b67ea95e79db1b4669fb7b88a583fdb652d2fdaf05c06", "text": "File Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nLlamaIndex examples can be found in the `examples` folder of the LlamaIndex repository.\nWe first want to download this `examples` folder.", "start_char_idx": 3198, "end_char_idx": 3654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ac65bc7-bb7b-4414-b9f9-04d8bbab3889": {"__data__": {"id_": "0ac65bc7-bb7b-4414-b9f9-04d8bbab3889", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8b3d4168-8e37-453a-9138-87c7e58ab499", "node_type": "1", "metadata": {}, "hash": "8d3fc5859de66e460483ed769451d0743888fed2c106a1a0f724674bf51830c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95efa0d0-2f98-4265-a077-0af2b5891a0c", "node_type": "1", "metadata": {}, "hash": "65bfc6ba6644a57d838b1e54cc4d5d781e71b239bb0a15a81a61c3bb2e91928d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8b3d4168-8e37-453a-9138-87c7e58ab499", "node_type": "1", "metadata": {}, "hash": "8d3fc5859de66e460483ed769451d0743888fed2c106a1a0f724674bf51830c6", "class_name": "RelatedNodeInfo"}}, "hash": "73136f2f360ee94c992a38f3ea369d4ab9185a3e362f4c733202fc3bea90c43b", "text": "We first want to download this `examples` folder. An easy way to do this is to just clone the repo:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n$ git clone https://github.com/jerryjliu/llama_index.git\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nNext, navigate to your newly-cloned repository, and verify the contents:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n$ cd llama_index\n$ ls\nLICENSE                data_requirements.txt  tests/\nMANIFEST.in            examples/              pyproject.toml\nMakefile               experimental/          requirements.txt\nREADME.md              llama_index/             setup.py\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.", "start_char_idx": 0, "end_char_idx": 1642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95efa0d0-2f98-4265-a077-0af2b5891a0c": {"__data__": {"id_": "95efa0d0-2f98-4265-a077-0af2b5891a0c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8b3d4168-8e37-453a-9138-87c7e58ab499", "node_type": "1", "metadata": {}, "hash": "8d3fc5859de66e460483ed769451d0743888fed2c106a1a0f724674bf51830c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ac65bc7-bb7b-4414-b9f9-04d8bbab3889", "node_type": "1", "metadata": {}, "hash": "73136f2f360ee94c992a38f3ea369d4ab9185a3e362f4c733202fc3bea90c43b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd53061c-f28d-44b2-a91a-b9fcb039faf8", "node_type": "1", "metadata": {}, "hash": "032ad4adecf264ac58c15a66f66b9dd0a58da127ca05fb0ab8d75ebb7ce6710a", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8b3d4168-8e37-453a-9138-87c7e58ab499", "node_type": "1", "metadata": {}, "hash": "8d3fc5859de66e460483ed769451d0743888fed2c106a1a0f724674bf51830c6", "class_name": "RelatedNodeInfo"}}, "hash": "65bfc6ba6644a57d838b1e54cc4d5d781e71b239bb0a15a81a61c3bb2e91928d", "text": "md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nWe now want to navigate to the following folder:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n$ cd examples/paul_graham_essay\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Download\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis contains LlamaIndex examples around Paul Graham's essay, \"What I Worked On\". A comprehensive set of examples are already provided in `TestEssay.ipynb`. For the purposes of this tutorial, we can focus on a simple example of getting LlamaIndex up and running.", "start_char_idx": 1612, "end_char_idx": 2749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd53061c-f28d-44b2-a91a-b9fcb039faf8": {"__data__": {"id_": "dd53061c-f28d-44b2-a91a-b9fcb039faf8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8b3d4168-8e37-453a-9138-87c7e58ab499", "node_type": "1", "metadata": {}, "hash": "8d3fc5859de66e460483ed769451d0743888fed2c106a1a0f724674bf51830c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95efa0d0-2f98-4265-a077-0af2b5891a0c", "node_type": "1", "metadata": {}, "hash": "65bfc6ba6644a57d838b1e54cc4d5d781e71b239bb0a15a81a61c3bb2e91928d", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "8b3d4168-8e37-453a-9138-87c7e58ab499", "node_type": "1", "metadata": {}, "hash": "8d3fc5859de66e460483ed769451d0743888fed2c106a1a0f724674bf51830c6", "class_name": "RelatedNodeInfo"}}, "hash": "032ad4adecf264ac58c15a66f66b9dd0a58da127ca05fb0ab8d75ebb7ce6710a", "text": "File Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Build and Query Index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nCreate a new `.py` file with the following:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Build and Query Index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Build and Query Index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nThis builds an index over the documents in the `data` folder (which in this case just consists of the essay text). We then run the following\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Build and Query Index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")", "start_char_idx": 2751, "end_char_idx": 4543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de5151fa-e027-4b87-afd3-6ac0abd85d55": {"__data__": {"id_": "de5151fa-e027-4b87-afd3-6ac0abd85d55", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "265d7b64-ddd6-4507-a17a-f8649b7bca3f", "node_type": "1", "metadata": {}, "hash": "8573ecbb93516da074d72677f0b4a5e3bde8ccb655a72442dc61709d06819bcf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b75f2d32-d00c-4f20-82fd-ddc51d695bd8", "node_type": "1", "metadata": {}, "hash": "f637d27b000b9fba516037ae5b932384fb045eab183360bdf58904d80b606e88", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "265d7b64-ddd6-4507-a17a-f8649b7bca3f", "node_type": "1", "metadata": {}, "hash": "8573ecbb93516da074d72677f0b4a5e3bde8ccb655a72442dc61709d06819bcf", "class_name": "RelatedNodeInfo"}}, "hash": "3b40291a3e8c5e857368f9a8632a6463308c7feb5762f6f555944ebef7b5b52f", "text": "print(response)\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Build and Query Index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou should get back a response similar to the following: `The author wrote short stories and tried to program on an IBM 1401.`\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Viewing Queries and Events Using Logging\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nIn a Jupyter notebook, you can view info and/or debugging logging using the following snippet:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Viewing Queries and Events Using Logging\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Viewing Queries and Events Using Logging\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nYou can set the level to `DEBUG` for verbose output, or use `level=logging.INFO` for less.", "start_char_idx": 0, "end_char_idx": 1877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b75f2d32-d00c-4f20-82fd-ddc51d695bd8": {"__data__": {"id_": "b75f2d32-d00c-4f20-82fd-ddc51d695bd8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "265d7b64-ddd6-4507-a17a-f8649b7bca3f", "node_type": "1", "metadata": {}, "hash": "8573ecbb93516da074d72677f0b4a5e3bde8ccb655a72442dc61709d06819bcf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de5151fa-e027-4b87-afd3-6ac0abd85d55", "node_type": "1", "metadata": {}, "hash": "3b40291a3e8c5e857368f9a8632a6463308c7feb5762f6f555944ebef7b5b52f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cb61500-a2fb-4865-a15c-61be52c02e8f", "node_type": "1", "metadata": {}, "hash": "0a52c034583f1c548128e541a3916898db1c3c95a2789e45e73439217fedc19b", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "265d7b64-ddd6-4507-a17a-f8649b7bca3f", "node_type": "1", "metadata": {}, "hash": "8573ecbb93516da074d72677f0b4a5e3bde8ccb655a72442dc61709d06819bcf", "class_name": "RelatedNodeInfo"}}, "hash": "f637d27b000b9fba516037ae5b932384fb045eab183360bdf58904d80b606e88", "text": "File Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Saving and Loading\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nBy default, data is stored in-memory.\nTo persist to disk (under `./storage`):\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Saving and Loading\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex.storage_context.persist()\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Saving and Loading\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nTo reload from disk:\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/Saving and Loading\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nfrom llama_index import StorageContext, load_index_from_storage\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/rebuild storage context\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.", "start_char_idx": 1879, "end_char_idx": 3604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4cb61500-a2fb-4865-a15c-61be52c02e8f": {"__data__": {"id_": "4cb61500-a2fb-4865-a15c-61be52c02e8f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "265d7b64-ddd6-4507-a17a-f8649b7bca3f", "node_type": "1", "metadata": {}, "hash": "8573ecbb93516da074d72677f0b4a5e3bde8ccb655a72442dc61709d06819bcf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b75f2d32-d00c-4f20-82fd-ddc51d695bd8", "node_type": "1", "metadata": {}, "hash": "f637d27b000b9fba516037ae5b932384fb045eab183360bdf58904d80b606e88", "class_name": "RelatedNodeInfo"}, "4": {"node_id": "265d7b64-ddd6-4507-a17a-f8649b7bca3f", "node_type": "1", "metadata": {}, "hash": "8573ecbb93516da074d72677f0b4a5e3bde8ccb655a72442dc61709d06819bcf", "class_name": "RelatedNodeInfo"}}, "hash": "0a52c034583f1c548128e541a3916898db1c3c95a2789e45e73439217fedc19b", "text": "md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: text\nHeader Path: Starter Tutorial/load index\nLinks: \nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\nindex = load_index_from_storage(storage_context)\n\nFile Name: Docs\\getting_started\\starter_example.md\nContent Type: code\nHeader Path: Starter Tutorial/load index\nfile_path: Docs\\getting_started\\starter_example.md\nfile_name: starter_example.md\nfile_type: None\nfile_size: 3097\ncreation_date: 2023-12-26\nlast_modified_date: 2023-12-23\nlast_accessed_date: 2023-12-30\n\n```{admonition} Next Steps\n* learn more about the high-level concepts.\n* tell me how to customize things.\n* curious about a specific module? check out the guides \ud83d\udc48\n* have a use case in mind? check out the end-to-end tutorials\n```", "start_char_idx": 3574, "end_char_idx": 4714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"f27f584b-2834-48c7-ad55-f273c21f7dd7": {"node_ids": ["d9281baa-c7fb-427b-b9a6-2871fde16930", "115b0331-8aa7-44bf-a5c5-2fbc49584b6b", "193c1e07-03ac-45f6-bd70-5bb0b54f1236", "2495892e-f217-42d3-b47c-f13cc073d7ac", "8877944a-5895-49cd-8397-2777043dd11b", "5f95c61d-1036-4ace-93d4-24d8d50f6634", "42cb046b-75bc-4943-b5f6-5b179a24a6d9", "8c986777-e62e-473a-92dc-acdb96c9c9d2", "10810e34-526c-413d-98e3-255cac0f8ee1", "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa", "fd509e3e-640c-40f0-99e9-4bb7dcfd378a", "392fdffd-9b49-404d-888c-bba04b0af715", "64e62721-99d2-4653-89a4-327bce7cb75b", "83735c20-50e7-493a-a9a3-af0c3d909a4e", "c1af9863-e7a3-460a-b4c2-6ded2595f8df", "73c5cb41-444a-497b-9712-8883e99a63c9", "6bf84f43-dde5-4866-89d0-bcab22576f63", "887ff996-3d20-466f-8a43-7ccf4043ebb0", "9b7c415a-d0cb-4a25-8639-1d943d367102", "b3399da4-ee87-4514-84fb-805c1728e658", "f326b656-9b2d-4837-abd3-67e6877c7439", "07802572-8cc3-4488-a4fd-6135f0cc3045", "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd", "652a770b-edf5-4968-bf1c-1e7289800dce", "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae", "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a", "b61dd542-0ed4-4a31-9612-cd7b9fa77581", "97474367-637c-42cb-bdbe-82a982b8ddac", "ad6dd30e-799b-4385-b420-d59372953192", "2e223e35-663d-45b7-bcfb-566a4b26d9b9", "fa8855d7-51de-40ee-842f-c3acdf2ce809", "0f5fdbba-1016-4d60-aeca-e98227fe9ea7", "0b07194c-2acc-4baa-8e31-a367741156c1", "2dcb8839-8d3f-474c-8f9b-724ab0904611", "767d8353-d5b7-4dda-ab2b-13adb4d57fc3", "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f", "b5cc1e57-4660-4db9-97e9-9c60e9e7c159", "4542dbfd-0d9c-4e95-8501-681dc1e404af", "8dd123d3-c5d6-475a-b90b-2cd8e959aeac", "156f0050-4cf2-4295-8684-c1d16f36c709", "93d40a64-cd24-4423-9b76-38270c080399", "c521edf8-0e9f-40d7-a411-07c241fb6733", "8054fe31-12d5-45a2-93e6-2592350a9cc6", "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03", "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1", "8779a8b9-760f-4de7-b49c-0df95d1bcd8b", "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c", "cd1ce237-0bf3-493e-b318-08c55024cba4", "45256c7f-e83f-4f1b-9943-5d93df44beef", "1536d61c-50cb-49fa-97c4-0aae79f4e538", "1075c947-32af-4c99-8c75-4f87b980f909", "2cccb9e7-9a10-4487-a24a-550528d54562", "96b953dd-0f15-42fa-a00b-ecc0e554907c", "fd7538f0-7953-4038-8b82-42031ff86fb1", "54ef6afd-0d9a-4358-a1dc-d317c9225537", "cae4deaf-d284-4a2f-a529-c51f448188a0", "a707dfb3-b0ee-47e4-a50b-c8f43d560b91", "cf87247e-da28-4d97-827f-4beaef959aa9", "e539d691-178a-42b3-a0a5-e415d86e6bf8", "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe", "3b0400e2-b023-44f0-b993-a89f6e3903c6", "bcec9c19-8725-42e9-b91f-a355a6a0389a", "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c", "e0a12723-8a55-44cf-ac43-2688613ee018", "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2", "3807ce52-317a-494a-9b60-f0cc6ec111e4", "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec", "e8ea7021-af99-4bc8-b505-811562e86599", "d0d05493-b438-49a8-8171-2da3c170a2a4", "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b", "178e7a4c-422c-4c79-8e54-119e2e05c739", "d7f4e18c-f12c-4440-bec3-960141c48bea", "8d1ffae8-1523-447e-9879-92deafe8423c", "88a26da0-22e2-444b-9df6-8f01c0e24203", "7c7a0d7b-2e71-4380-8cd7-706ee035b617", "7bbacaf0-99f4-4ae6-9740-b7774c835b80", "46c525df-09b7-4fe1-a4df-c7f1676daedf", "cedbeb90-5626-4faf-ac24-de412008c78c", "3e66155c-ea3c-4435-9d90-ef6444c9f514", "f99b44ac-ef51-4064-b11f-376000a8d227", "ef046785-afb0-44f2-b971-a11f2947b864", "e55610d1-f38c-4556-ab21-8baa89ea8746", "49f45f3d-b52b-4ba6-94bd-073ae87610c0", "19e9220c-9515-4e08-9b2d-339a09be6cba", "bc007a85-6883-4a66-afa8-30af77b854cd", "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4", "fcd9a011-a035-41e2-87a7-23f1405a9dfc", "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b", "6c7685ea-8047-4290-ac9a-498322fa81da", "6a8f9c4b-5ccd-4765-9848-990d17ef768e", "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1", "f9450061-cd10-4103-8133-a4093c11c82f", "82d6e145-bda6-4955-8139-0ebd6de31919", "16bd0293-7c88-4fb9-8a72-d944ffe333fb", "aed5c07e-3ab7-444d-8c32-f894592fe2b2", "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9", "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484", "bd01209c-9c3a-4eb0-8594-239574f103e7", "39f0cb49-3072-4e99-824e-39b89862f502", "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb", "ec5952e9-f4da-452a-9210-1509bb76996d", "07e1b2c7-9b3f-4256-93a1-db834bab04d8", "422218a9-2dc5-4297-8cb4-280ebd599a91", "7a933fe9-3bf3-469b-b381-0fde2c77d1af", "c58fa53f-1bec-4b92-b3c0-28261fffc47e", "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe", "25f0891c-e28f-439f-b61f-356860391fab", "d69b2eb2-01f2-449a-99a7-588f7dc9d67d", "c16d2161-9918-4b03-96b7-ab1531cde427", "5213a7ed-3bfb-42b0-99f8-4f7882537a25", "28cc8849-c8d0-4678-b1d6-5a94c066f2a0", "df036a43-245d-4c3e-b036-d33f56dabd94", "ce98334c-2023-441b-acea-e0729b9bee09", "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85", "f546fdee-56fd-4002-9441-746ae6318f44", "d607c9f6-64f3-4c83-a796-b211f91fa8c5", "8821cf26-edd5-435a-9f1a-b0248cf8d822", "6184d959-1551-4667-8767-3dd54a1351c3", "522de47a-97a6-4d73-948f-ad40577a9d06", "716fffcb-e467-4b3f-b39b-40b128c950fa", "dad26612-babc-4a44-84ef-3f98b427ac0c", "14ddcb99-9b21-4ce8-8cc8-38a1abfea400", "14c27008-5937-4330-9775-61c1868104c5", "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94", "7f4c1d8c-f359-481a-b841-e364b1ace56c", "936e4894-59f4-4fae-812f-9cedd308bd4a", "57d98462-a7ab-4c26-8cfa-c07b48f6f17d", "ff0056b9-5f40-4b65-b337-973ab0014686", "c51945eb-302c-4d86-90c2-dd965b7f1868", "6c9c6365-4eb9-4dce-bdbc-ece01038550d", "56119b9c-ea21-4227-b3e6-1493babf84d6", "3453be55-5f04-4dd1-8870-191f9c5fd82e", "e4044733-7eda-41ed-ab44-516f207a5536", "6dd9730e-36d5-4009-88e1-5e237e7767c8", "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441", "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa", "c5b23877-b4ba-41c2-9081-12725a1b5b47", "2e87e40a-d75c-4bca-bf66-2cb20b1e1201", "f6efea28-e895-40cf-9ca9-21a5b32921bb", "30ff2ffa-0405-4b62-b988-ea7cd4226bd7", "8307386e-c419-4780-bf97-9408b4967a9e", "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b", "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4", "b7e94e2e-e638-416c-b26b-6f00d0deb2d6", "467795da-1869-46ce-8473-5f97601af966", "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4", "57d82275-bdb8-4707-83cf-2f2442d7befa", "9fa1858f-bb91-4e38-8aca-8cead6f750f6", "efa0c9b7-e632-419d-8e96-465fde932bcd", "baf924cc-b96b-46fd-84c1-79d69be105f3", "6a3fee34-1975-4ed2-aa31-79f5dce3ba13", "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2", "fe11e513-a41b-435b-a4c3-5ff192d6feb0", "25e4fcbf-2304-4368-a573-0dd04a2fdc9e", "ccef981f-9e59-48d8-bb73-711d3aabdfe1", "5c691987-9616-4fb7-89a8-83dfc8587080", "171bb256-7674-4c94-bfe7-2609e57291d1", "7342149e-ef54-4ae0-ab55-aa6c2bec14a0", "b38af219-da17-45df-a80a-fe8ae2a44c6f", "9d2789b5-4e82-42b9-9a53-33bc52a366b5", "767c4308-525a-46c7-b3dd-1c7a3a792ad9", "53b92866-0f8e-48d1-bb79-a2d943008412", "9206bf00-08cf-4199-9276-96ef119a8e61", "8b3d4168-8e37-453a-9138-87c7e58ab499", "265d7b64-ddd6-4507-a17a-f8649b7bca3f"], "metadata": {}}, "d9281baa-c7fb-427b-b9a6-2871fde16930": {"node_ids": ["caa3d881-2b6a-4a21-897c-9040f14abbd0", "9a6b808b-c800-4276-bb45-e411144750c8", "c0b79f38-f54d-49a8-b5a3-5827fb3d638b", "129e80a0-343f-4e02-ad24-7da18d5945c5"], "metadata": {}}, "115b0331-8aa7-44bf-a5c5-2fbc49584b6b": {"node_ids": ["60b5bfaf-706e-42d7-a723-9df702496b39", "85cd9d1c-f2ed-4826-98e8-8f352c020431", "5fa49686-6f5f-4a1a-89b0-93ffe41006a4"], "metadata": {}}, "193c1e07-03ac-45f6-bd70-5bb0b54f1236": {"node_ids": ["0ed32ab1-3669-4106-ac1d-2f66c3bdb29d", "b30ebacc-9abf-42ad-9f40-9e5e88833711", "6910eea8-072f-42ba-8c11-43f5781d409e"], "metadata": {}}, "2495892e-f217-42d3-b47c-f13cc073d7ac": {"node_ids": ["b784149f-4d79-4495-baf7-04894159cac1", "b3527ee3-d586-45cf-ac35-6ed9dae27802", "e9ad55ed-47aa-4c1b-b760-c898291c8c33", "037a136f-42d9-4099-8139-a859c61c9690"], "metadata": {}}, "8877944a-5895-49cd-8397-2777043dd11b": {"node_ids": ["c58a5832-c6e1-4660-bb8b-0b305cf97f2c", "84c56aae-2e36-40d8-949c-d689fdcd64f0", "e6c79f20-6e57-4dbc-922a-e3074073104f", "2cb5bde9-973e-4fd5-a8cd-b012b5e49ed4"], "metadata": {}}, "5f95c61d-1036-4ace-93d4-24d8d50f6634": {"node_ids": ["b38d046d-ae64-4c9b-8144-06d6834a1819", "a1b126a3-f651-4ec8-8868-9bed2e78f7b1", "63f07a12-74d3-4d22-ae66-9d3d5ab4399e"], "metadata": {}}, "42cb046b-75bc-4943-b5f6-5b179a24a6d9": {"node_ids": ["f8284800-a627-47ba-a595-2bf621e5bfcc", "d676ddeb-3565-4c6a-985b-c1161668436c", "4077c693-e2f5-4093-ac5a-85f8435a7afc", "ff83f3c5-dab5-4b42-9742-1341cd52b835"], "metadata": {}}, "8c986777-e62e-473a-92dc-acdb96c9c9d2": {"node_ids": ["d8210398-936d-4da0-afaa-cafbda9d4703", "8aad2f35-e698-41fa-87ed-7173b63d324d", "29f19384-7403-42eb-b681-19b889439afb", "5302d647-66eb-4181-83fb-4b6b2a0f3ffd"], "metadata": {}}, "10810e34-526c-413d-98e3-255cac0f8ee1": {"node_ids": ["2f911b85-4221-47f6-a216-26ec753f3446", "436fd2be-6597-4d79-896d-58c726d0f4f0", "4ed0193f-b2bd-4f01-9a63-53e5d3fba802", "18ed6626-69f0-4378-be4f-1280d22459c6"], "metadata": {}}, "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa": {"node_ids": ["45e3ce5a-8dbe-4e8a-a391-cc2c29bd9f96", "b84b4357-5200-409c-bf2d-0be1fab0441f", "e84b17a5-8c32-44dc-8fe0-fdb1ad963e08", "37dc6775-88ee-4f77-9ee4-e4932ea9bdec"], "metadata": {}}, "fd509e3e-640c-40f0-99e9-4bb7dcfd378a": {"node_ids": ["b8d60a4c-25cf-43e9-a388-952342591041", "277d968a-88eb-491c-a9e5-85bf55f39f11", "f0cf3648-44c2-4393-8319-4f5a36faed00", "9886d89a-600e-4752-b244-e82a88999cf6"], "metadata": {}}, "392fdffd-9b49-404d-888c-bba04b0af715": {"node_ids": ["d056ad7a-d759-48e5-9bb4-fe59e7230f79", "032b4796-6f12-4c57-95de-5e6287b9b309", "726bab5c-341d-4653-a1b7-568794b28cdb", "b703bcbb-235e-4a9a-adae-ae27ffc8b7b2"], "metadata": {}}, "64e62721-99d2-4653-89a4-327bce7cb75b": {"node_ids": ["96f54233-b16b-40fe-aae0-385b9a91fc34", "03ab4346-2c32-4920-a14a-79b02cfe75dd", "fdb7123d-fc53-41b2-8a65-ef26675c6dfc"], "metadata": {}}, "83735c20-50e7-493a-a9a3-af0c3d909a4e": {"node_ids": ["fe988892-00ed-40d6-bd55-5575565ad14c", "cb240758-46ff-40f9-99ef-1f67ac5345fc", "a96841ca-0274-4186-bdd7-2dd8aecf02a9", "d0f157a3-5025-4f7a-9fd0-f00ef7b15858"], "metadata": {}}, "c1af9863-e7a3-460a-b4c2-6ded2595f8df": {"node_ids": ["775d675f-dbdf-42ee-9d5e-c725e9b7fdc5", "336c0ae4-4ee7-43a9-b2d5-94924f8947ed", "5fac2a33-b3b9-43a6-85e5-5557ebc23269", "86135e2e-a847-4de5-b87e-868ac0f17e97"], "metadata": {}}, "73c5cb41-444a-497b-9712-8883e99a63c9": {"node_ids": ["1f631b18-8411-4438-9307-c9038c6fc816", "5a662638-6ad4-494d-8676-f6274686a19f", "851aefec-48d1-4bdc-b4ed-ff3f727f0f0a"], "metadata": {}}, "6bf84f43-dde5-4866-89d0-bcab22576f63": {"node_ids": ["7f92ac4c-c144-4662-bd84-33d3ffb867bc", "a06a4f97-9432-4d71-9126-26efe752e422", "ad4eaa11-b7d7-4892-bc18-6db5c9b48769", "04394911-77be-4d35-b5e7-431af7087cc3"], "metadata": {}}, "887ff996-3d20-466f-8a43-7ccf4043ebb0": {"node_ids": ["469b13f6-1879-4e2d-86f4-58608a2fe361", "16fce20f-be0d-4dc9-bc9c-44e1842c6e06", "5a46f800-df81-4eda-b5a0-a2244ca97e03"], "metadata": {}}, "9b7c415a-d0cb-4a25-8639-1d943d367102": {"node_ids": ["bbce3433-c5aa-4437-918a-55b061bcef40", "f64c7dde-ee0f-44fe-9359-6c2291604a98", "72cc8a69-5259-4f17-9dab-ef0f12c531b3", "3050f0c7-639e-433c-a51e-360048a44846"], "metadata": {}}, "b3399da4-ee87-4514-84fb-805c1728e658": {"node_ids": ["1f701998-d2dc-40f8-95ef-eed75c2be17a", "56fba29e-2ab2-4832-87cd-c149d6a32fe6"], "metadata": {}}, "f326b656-9b2d-4837-abd3-67e6877c7439": {"node_ids": ["db5a729d-92c2-412b-a5a1-a622ee0f629d", "07ef6ba2-1212-4620-9a64-d97e427b2584", "cce549ec-b777-4de0-a696-011295bbe324", "64124226-5ec9-4b0e-a4cf-82c44b8a7e08"], "metadata": {}}, "07802572-8cc3-4488-a4fd-6135f0cc3045": {"node_ids": ["19811015-7b6e-4227-a8e1-71300febfdec", "e42e97d4-89d8-42fb-aab2-a01085e26abf", "82277ba7-e9d4-451e-8936-bdf713e2781f"], "metadata": {}}, "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd": {"node_ids": ["44b5d802-dbc2-48bb-9f6a-3ceb0c3dcc93", "ce343639-2638-4fc0-8fc4-b83afb10564a", "6e49b319-be00-40e6-af4b-5c0a8d2ad0f3"], "metadata": {}}, "652a770b-edf5-4968-bf1c-1e7289800dce": {"node_ids": ["29988d7a-5ee7-4ad8-81c9-c83f972d0697", "e24cf0ac-8186-4727-84ed-6c07832eab80", "0c2c25f4-13ca-43ed-8f97-c793f711ea4b"], "metadata": {}}, "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae": {"node_ids": ["35952454-60da-4b15-868b-24884a0f8a0d", "d1330cb2-d00f-4890-857d-9b0aac7ead05", "1c5741a9-c6fc-47a9-a2cd-3c45be6dc465", "ffe26393-c9d4-4612-bbfc-391cdb7af831", "b3d6a034-cbd5-43c5-9022-1715c05ef1ff"], "metadata": {}}, "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a": {"node_ids": ["f4263475-b3a1-4f60-84a0-ca4ccdc7482c", "a54d98eb-d4ad-4125-90ec-4a38c95483e7", "4acfe805-ad5c-4dd3-86e4-3ed23d752c4e", "6af4dded-1e29-4b14-9289-f2cf8af37bbf"], "metadata": {}}, "b61dd542-0ed4-4a31-9612-cd7b9fa77581": {"node_ids": ["467759b4-efac-438a-9c9c-d79a54126a79", "32f45229-cca8-4dd5-a6e0-7ebf62badeaf", "9f014092-2ab1-476d-ba96-e5fd449c45e3", "c1fc7dc8-1fe2-4476-b2e9-7e2134603e7c"], "metadata": {}}, "97474367-637c-42cb-bdbe-82a982b8ddac": {"node_ids": ["bdebdcf2-d10d-48f8-bed0-afdc55ddbffa", "7554fb30-8d54-4acd-a4de-dbbe5da79084", "898e0035-4dc6-4af1-87c5-b5a3f8436aaf", "47a8ccc3-e483-403c-87ab-2fd898383d67"], "metadata": {}}, "ad6dd30e-799b-4385-b420-d59372953192": {"node_ids": ["3dec0a73-1cc9-4602-a065-f9142880083e", "47ef4582-31ec-46fe-8d19-566262c354e5", "9322d884-94e3-45db-a0e1-ff6d3602ccf6"], "metadata": {}}, "2e223e35-663d-45b7-bcfb-566a4b26d9b9": {"node_ids": ["de6ba78a-53e3-4297-967c-9d0490004a20", "e853cb59-60ac-4f2a-99d1-828b877e5ab3", "ecfd65a4-f974-4554-ba86-4805cab833b1"], "metadata": {}}, "fa8855d7-51de-40ee-842f-c3acdf2ce809": {"node_ids": ["e846f65e-f37d-46c8-868e-ea192deecbb1", "64b66bb3-6599-463d-b7d6-5913638e26e0"], "metadata": {}}, "0f5fdbba-1016-4d60-aeca-e98227fe9ea7": {"node_ids": ["4e87c606-e67e-4afb-b738-5a26b6e05584", "00d0fc93-663f-4feb-8c25-03e5d2998334", "bd4513ae-9145-41ff-8297-a26e911309ff"], "metadata": {}}, "0b07194c-2acc-4baa-8e31-a367741156c1": {"node_ids": ["d8aa81e5-9c1a-4994-9576-f420e4c763a4", "889388cb-178c-4046-9c58-3b9e2a38b89c", "f2f975fc-d497-40fc-83df-27ee09d7dc64", "1335bd5a-3236-4891-9db8-379da4c1ca05"], "metadata": {}}, "2dcb8839-8d3f-474c-8f9b-724ab0904611": {"node_ids": ["fd274941-455a-48b8-b720-cee0ae61075d", "9504f2b8-a7b0-4ab2-960a-0c704dcf9bee", "bfed0e6f-d2bd-4f6d-8cb6-f200ee70a83a", "0321e342-6af1-4315-a0e3-a299b091a7e1", "686fb86d-ffbb-46f0-819e-3495e7ff4547"], "metadata": {}}, "767d8353-d5b7-4dda-ab2b-13adb4d57fc3": {"node_ids": ["1843d26d-92bb-4f9c-ba21-bfb76b1ed4b0", "3d49d111-1bfc-4311-bc44-8a6c151f6156", "1abf506b-8336-41bc-b55b-f13581d2a7da", "406cd354-54c5-4304-bbfd-b3d99e3b37a1"], "metadata": {}}, "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f": {"node_ids": ["ffbf1723-e016-411a-86f7-05f2442a7283", "f03a32ac-46e0-444a-8581-738e7d5e040b", "b9146699-a47a-4169-b3fc-d029991dec71"], "metadata": {}}, "b5cc1e57-4660-4db9-97e9-9c60e9e7c159": {"node_ids": ["2d3e1f2a-7c5e-4655-8fb6-f3e37c9f09a4", "88e956ba-bb12-4885-8d75-71185ec887e6", "96f11461-5762-44d2-b517-888dcc2de650", "35cb2b8d-cc7f-49ca-b85b-6ce401dc2336"], "metadata": {}}, "4542dbfd-0d9c-4e95-8501-681dc1e404af": {"node_ids": ["6f083448-5542-4404-acee-af96835a9db8", "48a96332-cc2f-4081-8f7d-c36963ea928b", "1227e1e5-4a48-4130-9f78-43623f4c1c86", "e16341dd-63c3-4997-81b2-e69224f7c731"], "metadata": {}}, "8dd123d3-c5d6-475a-b90b-2cd8e959aeac": {"node_ids": ["663fb4ad-8fa4-4850-b686-345f93efcd36", "5cc27566-ccb0-4f57-a305-15c89e85721d", "842b2d1f-a046-4f59-9141-9f44f22b7120", "72843296-d34d-46e7-afc0-7dfd104a6dc5"], "metadata": {}}, "156f0050-4cf2-4295-8684-c1d16f36c709": {"node_ids": ["32e90751-5bfc-459b-8414-aa67fa4a00c1", "21da08f4-4d26-4ac5-9798-5c10f3b8333c"], "metadata": {}}, "93d40a64-cd24-4423-9b76-38270c080399": {"node_ids": ["fee806fd-9986-4970-aa40-62f2682f5601", "4f4d8270-900d-42eb-9090-f93304ecac87", "b3a4aeb7-f12f-44fb-bb8b-206d05e237b0", "f7c25916-5a5c-4dff-8eb6-0e33191ae75e"], "metadata": {}}, "c521edf8-0e9f-40d7-a411-07c241fb6733": {"node_ids": ["f6aeac0a-9ebc-4847-8c85-8cbbee3e663b", "31da94f6-90b8-4595-8f2e-88d702dac292", "b5cbe7b5-d45d-474b-bfcc-10fe061780a2", "7053c6b9-a453-48d7-aefc-2353774c4bbc"], "metadata": {}}, "8054fe31-12d5-45a2-93e6-2592350a9cc6": {"node_ids": ["68cd4ecc-145c-4942-a673-54b67b2ca4c5", "ad95958f-2a96-4b93-be43-2c1a8a62ca06", "f0a628c5-0903-4d58-ab2f-171f2f0b930e"], "metadata": {}}, "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03": {"node_ids": ["0ce373c6-b769-4ae8-9dc6-74d32bee6687", "89724f49-aa7d-4b16-a4ce-d074cdf4fb9d", "0ccbc9fc-beff-42f9-a50d-7ff02530331a", "3629e574-e58e-476f-9354-2a8b1649d84b"], "metadata": {}}, "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1": {"node_ids": ["08c56a70-26b6-41d3-bee8-2e0e3e0f81de", "bd3420d8-9621-49b1-acfc-52208e8049bf", "5bcc33da-c757-4943-a190-3856c64392f1", "f1e23c7e-3eb7-4888-a2b8-fe8975b68a8c"], "metadata": {}}, "8779a8b9-760f-4de7-b49c-0df95d1bcd8b": {"node_ids": ["e805909e-8d09-43df-a9c6-dc53f9d770b6"], "metadata": {}}, "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c": {"node_ids": ["f89aa930-dca9-40e5-b5ef-8f084a3ac5e4", "28fd1901-5401-42bb-8b1d-26ae73c1afad", "2ed1fe30-5069-4a0e-89cf-347e756b4cdb", "3ed20e50-d408-40db-b36b-8488d813aafd"], "metadata": {}}, "cd1ce237-0bf3-493e-b318-08c55024cba4": {"node_ids": ["20101a59-a024-4649-bac5-9059bfdd5587", "b9e67033-f226-4fd3-8b78-793a4ff61056", "aa47c07e-5519-415a-98b9-998b1eb33242"], "metadata": {}}, "45256c7f-e83f-4f1b-9943-5d93df44beef": {"node_ids": ["e4e81f8d-42e5-4864-b424-4c84f2be118a", "cf6f149b-9c66-4a34-bdbd-10d460b9a92d", "ab467314-fca3-4e77-9f26-7cb7378e39c7"], "metadata": {}}, "1536d61c-50cb-49fa-97c4-0aae79f4e538": {"node_ids": ["e66c707a-63e2-4e3d-9b6d-99e26513393e", "7f07022f-8d04-4293-a052-3d0640606654", "11f1ac58-f0cb-4b48-a6bc-3a23d4f90a0f"], "metadata": {}}, "1075c947-32af-4c99-8c75-4f87b980f909": {"node_ids": ["c5838bac-77c4-4377-b9ab-07ea8231d1fe", "0f6091dc-cb54-4eb4-9763-bc6001f95072", "8f721465-dba3-4b27-b3ea-531a2aedb1f4"], "metadata": {}}, "2cccb9e7-9a10-4487-a24a-550528d54562": {"node_ids": ["7fcf52bd-c5be-4e21-a4fb-93e2af5e06c9", "f20e29cb-25e5-4d56-a2e2-e2d612ff07f1", "5a8f23cd-6466-4b33-97f8-d52f17345761", "6e27bbfc-12a0-446d-b4e2-feaf0b9c1066"], "metadata": {}}, "96b953dd-0f15-42fa-a00b-ecc0e554907c": {"node_ids": ["8a1d7807-aa41-47c8-9dd7-084d4097483a", "9344c360-d43a-4ece-a321-d6e98c311d4b"], "metadata": {}}, "fd7538f0-7953-4038-8b82-42031ff86fb1": {"node_ids": ["5c0a5cf8-a554-4b45-8532-aa25ceb3d911", "a11f277d-1983-4066-aace-321e61588680", "711f400f-f144-4aba-bbd7-f0ef888b36bf"], "metadata": {}}, "54ef6afd-0d9a-4358-a1dc-d317c9225537": {"node_ids": ["7165700b-0795-438a-9394-5943ce730da2", "4d438c52-7882-4870-b7d7-9786dad54f7f", "aa59b324-bced-42a5-bfc5-4d9e93f4e35d"], "metadata": {}}, "cae4deaf-d284-4a2f-a529-c51f448188a0": {"node_ids": ["4e11970d-2737-408a-9880-82efb9f30500", "b02504ef-9496-4b5d-9803-34e6bb4c4b20", "665b86e0-639d-4207-a483-3a874ea9d39a", "3fc3f90d-1a37-403c-a257-0f84b3e46ea0"], "metadata": {}}, "a707dfb3-b0ee-47e4-a50b-c8f43d560b91": {"node_ids": ["97faf9c1-4ef6-43f9-b68d-dc9eb12cace0", "6537a827-6b22-4ea0-909d-d40605457dc8", "d74b1e06-4f5d-43f4-9481-acbad0dc7f1a", "2b1b51ae-3158-40ed-8b5e-0d17adac50ec"], "metadata": {}}, "cf87247e-da28-4d97-827f-4beaef959aa9": {"node_ids": ["1c67d148-7cec-4ba2-8970-d6584100e4d9", "b00879d8-1aa8-4cbb-ac15-51f6e436f2e6", "7f14ce69-f3dd-4132-84b0-206a96d9de9b", "d098b1ae-02c9-4f62-bb8d-f5ed9e288914"], "metadata": {}}, "e539d691-178a-42b3-a0a5-e415d86e6bf8": {"node_ids": ["60447b3a-237a-4a20-bdf4-02d73d9c5b70", "8502831f-6b0c-4d66-ae71-6a5e4f0b9158", "4ca2153a-bb41-4cd3-8eba-f9d3efec4352", "877e0a46-7397-4bfc-a924-0ac24e0c8eb7"], "metadata": {}}, "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe": {"node_ids": ["f55e0d52-7922-46fc-adca-747d0dbdb6f5", "066d5c34-faad-4abb-a00f-6a9ccdf982d7", "4eba65f0-8080-4260-8cdf-c806124d8292", "8dc13219-c223-4f21-9482-cdebddf4b7e3"], "metadata": {}}, "3b0400e2-b023-44f0-b993-a89f6e3903c6": {"node_ids": ["1364720d-f74c-454b-9505-be0425082e42", "a31672c0-029c-49ea-83ef-ebea6bf21349", "7ed14c17-a43b-4cbf-9b74-74e78cabdade", "fb69fa0a-3c79-4a62-bcd7-ea2a95975d9d"], "metadata": {}}, "bcec9c19-8725-42e9-b91f-a355a6a0389a": {"node_ids": ["d78599f9-78e8-4ebc-b7b7-b1d6d7307702", "47f69943-ccf3-4bf2-864b-b7ed32b85e9c", "9876e684-21c6-48a3-bb99-01f70cdb6908"], "metadata": {}}, "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c": {"node_ids": ["8ddc107e-02a7-4d69-a976-a15b237b97f8", "82aac71e-f8f6-41b5-86fc-cc647f68c6b6", "fecefe8c-f135-4ecd-b1f4-bcf27763cbce"], "metadata": {}}, "e0a12723-8a55-44cf-ac43-2688613ee018": {"node_ids": ["8eefb2ba-49eb-4091-a4a8-5bc8a0765803", "a96e2d52-a0a9-4f0b-b7e8-7cbefefa2ff4", "5ce6af19-6c4e-49aa-8530-527c1776d181", "053a299d-174e-4691-be85-64399ab269a1"], "metadata": {}}, "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2": {"node_ids": ["d0f715d0-e1de-4622-b474-01547e080bf3", "0f3972c2-9ca0-4dc0-9c58-481c82e443d3", "19e99400-c7d2-40fb-8687-098392f49704", "1f9963d1-6a38-4220-8617-93a5cb9789bd", "ecc26c36-b4b2-4a93-80b2-61fb57afdbd0"], "metadata": {}}, "3807ce52-317a-494a-9b60-f0cc6ec111e4": {"node_ids": ["b857577b-3e79-4099-b044-63d4274fbb69", "c0490741-1793-41f1-a0cf-e480098af30b", "618c9782-52ac-4364-b4e9-75de2b313e4f"], "metadata": {}}, "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec": {"node_ids": ["cad62ac2-f538-411f-8ab6-d6a0c7dc1afd", "c969a149-819d-44e5-a53e-fe3b9c7bf3b2"], "metadata": {}}, "e8ea7021-af99-4bc8-b505-811562e86599": {"node_ids": ["8a7ea6ca-717b-4b09-a88b-6def15e14eef", "ba35fd49-1e5b-487c-9605-98df0fbd0e46", "5ba4363f-c41e-4e89-8b36-96ef866d2c2b", "457677b5-d8c1-420f-8ea1-83b56cf3b153"], "metadata": {}}, "d0d05493-b438-49a8-8171-2da3c170a2a4": {"node_ids": ["52b153c9-e550-4a6d-8325-254c3a866d9a", "e502ed84-b3d9-4265-af70-95d5738e1807", "055491d4-1246-43bf-8f94-54c2a555705f", "3aae0939-41d6-4a37-a417-c6da7c0fb284"], "metadata": {}}, "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b": {"node_ids": ["8919ae46-fb0f-4130-bc8d-2cddb2a74f73", "e9e43751-02d0-4621-b39d-2e44418f4455", "abf16a18-0c5e-4efd-8898-60b1c0832f83", "05040174-34f9-4d79-a1be-b1cba73de84e"], "metadata": {}}, "178e7a4c-422c-4c79-8e54-119e2e05c739": {"node_ids": ["ae435429-cf0c-42e8-8050-c9ac1b8c91df", "30c86132-8708-43b2-b30b-5fa311f3bac5", "1462f6cf-d6f6-42b4-bd56-1bcc7ab4db0d"], "metadata": {}}, "d7f4e18c-f12c-4440-bec3-960141c48bea": {"node_ids": ["717e1455-26c2-4e94-8828-0aafc65fe714", "43d90206-228a-42f7-8038-dec0902d6fae", "fe3b84fa-7467-48fa-969e-b9fba40b30c2"], "metadata": {}}, "8d1ffae8-1523-447e-9879-92deafe8423c": {"node_ids": ["c485a396-c235-463c-b215-ebde1980ff28", "139ad006-31e5-407f-ac67-e31c4c44b392", "9ec911a2-ad2e-4b5d-acc2-5da8be39c8e5"], "metadata": {}}, "88a26da0-22e2-444b-9df6-8f01c0e24203": {"node_ids": ["006ee14e-9068-48ac-83c7-dcc4b99c69c9", "2f217501-fdde-4300-8766-590ca19de1e3", "38317e39-4b36-458a-bd0a-afe652b6a453"], "metadata": {}}, "7c7a0d7b-2e71-4380-8cd7-706ee035b617": {"node_ids": ["f0e1077f-7df1-4be1-a15e-cf519bc7e007", "6e3c8471-87b5-4e0b-9653-9ec23b2e6cad", "4c338303-12d9-4c81-b8de-e12c64d598d3", "886e3f81-c5dc-4b0f-98ef-26b73704602f"], "metadata": {}}, "7bbacaf0-99f4-4ae6-9740-b7774c835b80": {"node_ids": ["ecb74226-c19d-4a62-940b-a190429915a2", "bc3772b8-cd45-489d-9821-b1e072034617"], "metadata": {}}, "46c525df-09b7-4fe1-a4df-c7f1676daedf": {"node_ids": ["4c2568d9-9d74-46f8-9494-0f6c1208b1e7", "ff288ad3-f67e-453f-80a2-36e8df06acf8", "7b8d6b4d-877f-4f5a-bc3c-d15e7eaae96f"], "metadata": {}}, "cedbeb90-5626-4faf-ac24-de412008c78c": {"node_ids": ["dc004b59-b2ee-4071-aa1d-845bddf28a4d", "4f5d255e-1c3c-45e5-afe0-4bd6b947188d", "818cb288-4336-4115-846e-24de9ebecb88"], "metadata": {}}, "3e66155c-ea3c-4435-9d90-ef6444c9f514": {"node_ids": ["2172d386-222e-4632-979d-c43cf53eef8a", "496f6e3d-6497-4e4d-8f64-7bc846728080", "64c581be-ffaf-4820-9cbe-60e7faa0d31d", "60564498-bab5-47c1-9a8c-b991f672b4e4"], "metadata": {}}, "f99b44ac-ef51-4064-b11f-376000a8d227": {"node_ids": ["4dbd9c5f-178b-4ecd-aa3e-41f2824140f4", "9d66814f-bd9e-4e5b-a7be-fe9801064c3e", "4f53b62a-6b12-4eb0-ade0-061997ded610"], "metadata": {}}, "ef046785-afb0-44f2-b971-a11f2947b864": {"node_ids": ["3c11af20-f2f5-4904-9253-0de11f04df45", "91e5b4a4-c742-4dc8-9acb-32493d892ec9", "e8976cc4-4f5c-40ba-9df1-f095f713aed6", "6b2690bf-b5cb-41b0-929b-8fd7b832cfee"], "metadata": {}}, "e55610d1-f38c-4556-ab21-8baa89ea8746": {"node_ids": ["872aad94-9da9-4557-97bd-abd802849708", "c5d7c57a-f321-4824-aff1-31e89e24375a", "050c6272-35a7-41be-b9c7-a044c487524f"], "metadata": {}}, "49f45f3d-b52b-4ba6-94bd-073ae87610c0": {"node_ids": ["3c931fa1-b7bc-4665-8296-b69e8ec7022b"], "metadata": {}}, "19e9220c-9515-4e08-9b2d-339a09be6cba": {"node_ids": ["c8d66d8d-f99e-4cac-a2ae-208fa8323598", "1d0352ec-83b8-458d-9d7e-dd1dd5e57aff", "44dbce79-8521-48a7-beea-de14b5c6383c"], "metadata": {}}, "bc007a85-6883-4a66-afa8-30af77b854cd": {"node_ids": ["1a35a11a-8fd9-45d7-ad98-06a245c10326", "3a1f5112-7bc8-4e99-becc-4cf7fc32a113"], "metadata": {}}, "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4": {"node_ids": ["41e35ee7-a929-4f68-af37-634800af56ca", "cefcf65a-e0d8-4249-8784-d1b253b6d370", "46d530a0-a244-4710-9007-f0264c1aa5e2", "32eef4f1-7aa6-4f39-b7b0-c6dcc26185f7"], "metadata": {}}, "fcd9a011-a035-41e2-87a7-23f1405a9dfc": {"node_ids": ["9291fcf4-5c26-46d4-8daf-f632e94482a6"], "metadata": {}}, "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b": {"node_ids": ["b8bff74e-ae63-4b2f-84a2-10cd31bcef79", "298e40fd-0964-4fca-930f-cf5b053eb630", "09c96574-11c8-4d52-a6f4-0226d2004eb9"], "metadata": {}}, "6c7685ea-8047-4290-ac9a-498322fa81da": {"node_ids": ["00b09109-7e4e-4405-81d9-eac4ceb4bbe1", "b1626bfc-e490-40e1-9a72-b01ed133699d", "8d38fd5d-8264-45fc-8417-3cce56944380"], "metadata": {}}, "6a8f9c4b-5ccd-4765-9848-990d17ef768e": {"node_ids": ["ac33dca7-0115-4896-97d2-a1da715c057f", "85014ae4-1598-4cbc-adb0-282440392bb0", "94979dcc-a9c3-41a5-a731-31d54ad85a5b"], "metadata": {}}, "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1": {"node_ids": ["2cc59327-43fd-46e2-b2d0-ec9c077beac5", "b1cc8bdd-50ee-41c8-8b3a-ab587e4cb9d5", "2ef03dc7-3c9d-40db-9e66-e4473620b4b1", "a14e5911-54bd-4998-aac3-9591f2f97113"], "metadata": {}}, "f9450061-cd10-4103-8133-a4093c11c82f": {"node_ids": ["038a69b4-043e-45c7-9ce7-aea38a9c30ae", "30d4603a-906a-457a-8787-b958f30381a3", "a99b73bc-0a2a-487c-b417-566d106bfbe3", "14292a30-78f8-4036-bb75-12df73a72eed"], "metadata": {}}, "82d6e145-bda6-4955-8139-0ebd6de31919": {"node_ids": ["f2e09c03-26b5-4fad-8415-c20bd10525d2", "3eb032c3-57ef-4e7a-af31-7b7df20da4a3", "4ae5f656-cd55-48e5-abc6-78b2ff30365c"], "metadata": {}}, "16bd0293-7c88-4fb9-8a72-d944ffe333fb": {"node_ids": ["88eba848-fa30-49f3-8783-7ae042d90552", "98f295d9-66cf-4771-88ea-20417403d118", "60c02106-6a4a-482c-ab2a-8df3532b474b", "727954f9-af6d-4d68-a515-0e1b00bbc39f"], "metadata": {}}, "aed5c07e-3ab7-444d-8c32-f894592fe2b2": {"node_ids": ["d87db227-b408-45bb-acb5-86b12488fd18", "3daabe54-04ed-4728-bff1-58bd74f9cdb9", "8120e5b1-0e04-4361-acc5-40389a3d4a23"], "metadata": {}}, "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9": {"node_ids": ["70840c13-c0a8-4197-acfc-40bea23adbe0", "83722899-33b6-4e87-9f1a-884ed7627d55", "ddd83ffd-eabe-4b99-90af-124ba80a5438"], "metadata": {}}, "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484": {"node_ids": ["b8c86f8b-5eb4-45ab-a1ec-52fd2593b30f", "cb9484de-8627-4501-906e-291343164f4b", "cc8303a3-e113-46f4-a075-42b6a782e8e3"], "metadata": {}}, "bd01209c-9c3a-4eb0-8594-239574f103e7": {"node_ids": ["36616633-4d3b-481b-ac71-c52d704cf5da", "aa7f8c98-34f1-4ec6-9841-cbce006ebae2", "1efbc0eb-eb89-47db-8965-39835610c7bb"], "metadata": {}}, "39f0cb49-3072-4e99-824e-39b89862f502": {"node_ids": ["2c420b97-581e-4f93-a7da-d3b136554c80", "f60cd002-f417-4f3f-a2bc-1a1666a150ec", "7b2f6c81-0a0f-4da3-8a2c-12c0ac7d710f", "3c629b58-2558-4626-87d6-aca4236b12da"], "metadata": {}}, "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb": {"node_ids": ["2ea2aa38-021e-4b6b-89c7-527a836a0c27", "5a023095-45e1-4385-b446-c75328fdfe72", "fc476e0c-e354-4a0e-8eca-6c98817c9859", "8d7fb08f-830e-48d5-bf7b-256fb6afa197"], "metadata": {}}, "ec5952e9-f4da-452a-9210-1509bb76996d": {"node_ids": ["62b532e5-e99c-4d07-a113-8924ff067a71", "9e11533d-8fd3-461d-83fc-83f699cbd98c"], "metadata": {}}, "07e1b2c7-9b3f-4256-93a1-db834bab04d8": {"node_ids": ["1df1131e-791b-4c13-b2b9-fac7e3f0fbe7", "06c9b143-e5af-4e8e-8354-26cdab4080ca"], "metadata": {}}, "422218a9-2dc5-4297-8cb4-280ebd599a91": {"node_ids": ["bbe39360-85bc-48ea-bb0b-5cabbdf5a707", "f1be8183-5f77-4bac-9853-97f8c646e8e0"], "metadata": {}}, "7a933fe9-3bf3-469b-b381-0fde2c77d1af": {"node_ids": ["c9f022c4-2428-4af6-8818-20c27f67d2c7", "220062a2-05f1-4532-97b9-c9e378f4eda5"], "metadata": {}}, "c58fa53f-1bec-4b92-b3c0-28261fffc47e": {"node_ids": ["6eed3470-65ae-4a30-94e4-cefb8b399503", "617168b3-3ae1-40f2-9e9a-0e2410dbd36a"], "metadata": {}}, "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe": {"node_ids": ["4c1b9aa9-2e93-4c24-9677-b9fe35aecb55", "ff7170af-6d0f-4f65-8b68-6e8b25416e3b", "37e90aa5-f3f4-4fdb-904a-f7e2546c2cdc", "9c2ae263-8111-4b0a-aa1a-8499e26a67f5"], "metadata": {}}, "25f0891c-e28f-439f-b61f-356860391fab": {"node_ids": ["a5f910fc-19c2-4e85-b99c-cb02ca9f570a", "037cd9ea-c523-48fd-a581-855307260406", "2f22326b-f3fd-4295-b2b1-41c05122c2c3"], "metadata": {}}, "d69b2eb2-01f2-449a-99a7-588f7dc9d67d": {"node_ids": ["37f4e6bc-d13d-494c-a5c3-8729a1bf22f9", "813844cc-9bd9-4731-8bdc-a67f5cbb84d6", "7c825415-8a8a-497c-9631-0480cd167818", "07fab882-9814-41a5-939b-40b9eae8261a"], "metadata": {}}, "c16d2161-9918-4b03-96b7-ab1531cde427": {"node_ids": ["45ff6027-0e9a-468d-9b73-1858960cb1ca", "b6e663ab-d603-46b9-bc95-5ed81ab2ff82", "d63fad17-aaf3-4edb-bb99-0714cce6906e"], "metadata": {}}, "5213a7ed-3bfb-42b0-99f8-4f7882537a25": {"node_ids": ["920566e4-b853-445f-b94f-1401e4c45891", "d2353a21-f9bd-4f00-95ba-52430bf65476", "4b1a051e-a7ff-4cea-9bc1-a872e4de5f09"], "metadata": {}}, "28cc8849-c8d0-4678-b1d6-5a94c066f2a0": {"node_ids": ["0d1dac4c-03c6-4dd6-ae47-d58444ea7dc2", "7dbfa10c-386f-44aa-bf06-299b0b13eb72", "2a72a30f-f122-48e7-b2b5-25813db6d3ae", "cad717e1-fa4c-4cac-8db5-ab62967e12cb"], "metadata": {}}, "df036a43-245d-4c3e-b036-d33f56dabd94": {"node_ids": ["ef51f9c8-e76c-4b1d-9fb9-da90bdf12754", "ef4efee0-7de1-49f9-84df-f7d840d2995a", "14ceac11-7853-42c0-a772-1d5f0cdafd76", "f077122e-bead-4974-9c39-e43fb995e396"], "metadata": {}}, "ce98334c-2023-441b-acea-e0729b9bee09": {"node_ids": ["b1523a33-fb8a-45e3-bda8-62185f5f82bd", "f74083dc-d459-4b03-b3a9-5f954eaa7326", "16219dff-01fe-46d5-91f1-c548caf59882", "9db7b730-8131-417b-b359-4a3669318a5e"], "metadata": {}}, "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85": {"node_ids": ["e400ea9e-3d87-462c-b6e2-a5c578191c6c", "c6ee80ea-65f5-42b7-9644-5eedf61423be", "a161bf29-9df1-4011-b4ef-153fd555897f", "e4cfde42-c780-45e2-889d-ad88f7171da1", "525b7a92-a6db-4210-b68b-2d2d4bfc0bdc"], "metadata": {}}, "f546fdee-56fd-4002-9441-746ae6318f44": {"node_ids": ["89e8b848-6f96-432b-a008-633b169734ce", "d5d05ee3-c578-46b5-a3da-44e3c54c24f9", "69f8fca8-4d0c-4eca-927c-83001797ad85", "066f7152-6f38-4f0f-a501-5600d66e757d"], "metadata": {}}, "d607c9f6-64f3-4c83-a796-b211f91fa8c5": {"node_ids": ["8b0061c9-30b8-4557-8a86-ddda10331ccc", "739efa70-5c4d-4903-9b7d-7ae0e5b434bf", "620307eb-ce1e-4985-98aa-594dc9bdc5c7"], "metadata": {}}, "8821cf26-edd5-435a-9f1a-b0248cf8d822": {"node_ids": ["1ae0fcd0-1f6f-42f9-94ee-8382150cea36", "52c768ef-e147-4809-84f2-b35c6c9b4a1b", "99e103ce-f5fa-4bb6-afce-e0d061dfd115"], "metadata": {}}, "6184d959-1551-4667-8767-3dd54a1351c3": {"node_ids": ["cfc542d7-8752-42ab-a833-8d9277ad678c", "f66f044e-3eee-4483-8344-ece21a635523", "0e35aa78-97d4-4392-9225-2fac96eab48d", "76b3aeb2-6333-4b90-b673-a51c9a7427f6"], "metadata": {}}, "522de47a-97a6-4d73-948f-ad40577a9d06": {"node_ids": ["e12cb0b4-bbc6-4d53-b25d-b522cc0a075d", "93f420f1-19c2-4478-9b9d-e12cb1d27eb1", "3e21a4a8-1405-4c27-8d5f-0de975f26508", "e00d7077-fbdc-410e-88a2-dbfd059192e8", "09770b69-569d-422e-89e6-b71b288c5ae1"], "metadata": {}}, "716fffcb-e467-4b3f-b39b-40b128c950fa": {"node_ids": ["710f2056-4774-4562-9322-e9bd4e56e47f", "7e46d80c-1192-4e95-9212-1c040333c957", "9cf4a007-4214-431f-acab-37e91d82c897"], "metadata": {}}, "dad26612-babc-4a44-84ef-3f98b427ac0c": {"node_ids": ["92b82f76-caaa-4ca6-811c-8c208c13734c", "2cd5c2bd-963a-4ba5-9615-7c0016d9970c", "915f2f95-9027-49f7-a8e8-9910c2c23fe6", "d4e4aca4-1228-412c-a869-47566bb7958b"], "metadata": {}}, "14ddcb99-9b21-4ce8-8cc8-38a1abfea400": {"node_ids": ["9e69cfef-1cb9-4673-aadb-5718751b3c8a", "70fc7448-eb16-4b30-bf32-459082292708", "81e9c672-8e58-419e-a558-49379373a75d"], "metadata": {}}, "14c27008-5937-4330-9775-61c1868104c5": {"node_ids": ["5fa243cd-4c13-470e-85e8-6cc591c573e0", "81f733f2-318d-48cd-b64c-6dd5478fbd63", "bc39d683-788b-4286-8c03-a2237a41817c"], "metadata": {}}, "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94": {"node_ids": ["984ee8d9-3183-4a7e-9648-0c93d1a97c3a", "806b115b-52e6-46b9-a17c-8d3f98522927", "589174b0-b27e-46e7-acfb-3c0f5105cfbc"], "metadata": {}}, "7f4c1d8c-f359-481a-b841-e364b1ace56c": {"node_ids": ["eb1d53a1-ab5e-4e33-913a-7cf24c76b83f", "0990ce95-ea2e-463b-a0eb-aeb5aaa435ae", "1efa5e14-1c8a-4255-940c-32d59f1c2d60"], "metadata": {}}, "936e4894-59f4-4fae-812f-9cedd308bd4a": {"node_ids": ["2a56c91e-5fd2-4fbb-908d-1ec63cdb2351", "3ef5868c-09ff-4247-919d-ec84260604a5", "748bfeef-d12b-42d3-874e-8737ab6b65ab"], "metadata": {}}, "57d98462-a7ab-4c26-8cfa-c07b48f6f17d": {"node_ids": ["8066023d-e814-45de-bdc2-b331be7e8df4", "1a716a35-a27d-46ef-bd71-7c33323126a4"], "metadata": {}}, "ff0056b9-5f40-4b65-b337-973ab0014686": {"node_ids": ["437e4f47-ffdd-40dd-a21f-5d074f720231", "8619cde6-3639-40f4-b458-81a52505ab74"], "metadata": {}}, "c51945eb-302c-4d86-90c2-dd965b7f1868": {"node_ids": ["3e7db9ab-0518-496c-bde8-9b1e0f2955d5", "72077ad7-27c8-4e4d-a54c-097473a95004", "d23269e7-9c04-4e98-a766-dee3f17a884a", "58719532-4381-4a45-9db6-9944aa10f590"], "metadata": {}}, "6c9c6365-4eb9-4dce-bdbc-ece01038550d": {"node_ids": ["edd8b01c-c34b-4ca6-b23a-aeb0a80d8e26", "66dafd3b-417d-44f2-9cb8-e17eff5347be"], "metadata": {}}, "56119b9c-ea21-4227-b3e6-1493babf84d6": {"node_ids": ["4b41fc5f-6cd9-4ffc-aa78-c6d8e54bbe39", "e47dfe0e-495d-440f-9241-cc0c14b80566", "7759f0d7-7e25-4712-a447-3adbdb4dafbb", "1ffa6103-38a6-42ec-afd2-efd386dd31dc"], "metadata": {}}, "3453be55-5f04-4dd1-8870-191f9c5fd82e": {"node_ids": ["394f6166-dfb9-459b-94e6-3efd3836717b", "2dc63329-0589-4bdc-98f1-81d61a1c2796", "8a03e044-a964-482d-b48e-4eef196677a9", "faf180dc-3423-4bfe-a347-42687a9736a1"], "metadata": {}}, "e4044733-7eda-41ed-ab44-516f207a5536": {"node_ids": ["7755ee58-4020-43b4-93de-6884e6fb133b", "a4bdb967-6c11-42dd-89b1-fd0eb2b8fb1b", "30ac8e2f-551e-4d9d-bd5e-8766930322fb", "01d84a18-fafb-4941-967f-3112652eb5a6"], "metadata": {}}, "6dd9730e-36d5-4009-88e1-5e237e7767c8": {"node_ids": ["f09fe913-223f-431a-a64c-75f594606793", "9912ac21-cfcb-4e72-8552-12e6c54aa10a", "1153b072-6b34-4917-b560-7868b64009ff"], "metadata": {}}, "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441": {"node_ids": ["6096b9e3-fcbc-4aad-bbc1-ba2ba633f284", "6bf3ee39-9515-4002-a9f6-049f1d3b26ec", "ca4d5a00-f5ab-47a8-97c9-00f8f7660b47"], "metadata": {}}, "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa": {"node_ids": ["55916261-7870-443d-8cbd-4b51467a77d6", "76ead195-2cbb-415d-8494-0bcb751662f0", "6ac06fb4-4158-4fa2-b621-af3966e83d18", "c9f6c430-aa2b-43f7-a063-8888fe5435b3"], "metadata": {}}, "c5b23877-b4ba-41c2-9081-12725a1b5b47": {"node_ids": ["26917647-8356-409e-b1d2-f587e7ad7b88", "2d135c7d-8eaa-478b-bdd1-ad2578ab3094", "6e5af7bb-9eff-44b7-a85a-279d750c3dc2"], "metadata": {}}, "2e87e40a-d75c-4bca-bf66-2cb20b1e1201": {"node_ids": ["a1b98ca6-819a-4536-81f2-b981441c17c1", "24995949-e50d-41b4-80ed-8a7b44de0c9d", "c8341126-de12-4e80-9eb5-b118eca3e5a4"], "metadata": {}}, "f6efea28-e895-40cf-9ca9-21a5b32921bb": {"node_ids": ["a0b19d2d-3f10-4e64-9d84-79b1e7539cf3", "d530c033-2e49-4ed0-8be0-10fb13ac6abf", "1fd96e80-2333-4431-9fd6-c05da53d4fd6", "c0764f8c-3df3-4eb5-b37e-a0ac7708814c"], "metadata": {}}, "30ff2ffa-0405-4b62-b988-ea7cd4226bd7": {"node_ids": ["862af2ce-1189-47b6-98a3-ddb8ae2e94d8", "fe5fcf31-cc29-4408-8f3f-6cdf1fabac33", "294dca04-e918-4d16-81ce-784ed3a08e3f"], "metadata": {}}, "8307386e-c419-4780-bf97-9408b4967a9e": {"node_ids": ["9b7e4e7c-911c-4c48-a02f-cb33dac04468", "ddcf50e2-c728-4522-bbb6-b18e29fb2a26", "50fd7008-631d-4845-b3da-5825d697de42", "68ae0c0f-f71c-44eb-bbf1-a2795f3db639"], "metadata": {}}, "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b": {"node_ids": ["eee0cf49-41ea-4605-93d2-6a985f5c870f", "863991c9-7dd3-4688-bfa4-374dcf558157", "9ea697f8-6d10-4ff1-946a-cd3cb406e694"], "metadata": {}}, "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4": {"node_ids": ["99f07d67-47b4-478c-b87b-c7bbf90f7509", "0ecb87cb-eb82-4e30-b344-16d287c03ed7", "65c62462-8add-4bb8-9690-655bdcbddaf6"], "metadata": {}}, "b7e94e2e-e638-416c-b26b-6f00d0deb2d6": {"node_ids": ["1d64353b-2c93-472b-af1a-029696461c4a", "6ecd87b1-2ce9-4ad4-8864-361467317d67", "0f0df44d-e555-49a8-9337-992e8f4cdd82"], "metadata": {}}, "467795da-1869-46ce-8473-5f97601af966": {"node_ids": ["06959c18-5d5a-436c-94c2-c825c9fd19e4", "073fab27-83ad-447d-a4dd-1a986579cccd", "943e09d5-0632-48fc-a3e8-d9f514189867"], "metadata": {}}, "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4": {"node_ids": ["3a1645f5-42af-445d-a6f9-f9151e840905", "3532dee7-3488-402a-9323-c4ac4ab111f6", "71589527-1312-4862-bf76-ddf5f4067218", "94ce6c98-d7c0-42d5-b410-5f252c6e4add"], "metadata": {}}, "57d82275-bdb8-4707-83cf-2f2442d7befa": {"node_ids": ["775cfdcb-ec0a-400b-bdff-380c5cd33594", "1bd244f0-784b-441e-a312-5854ad193643", "233df86f-00fe-437b-ae64-54905559b456", "68f1cda2-f23d-4779-babd-9a366faa6269"], "metadata": {}}, "9fa1858f-bb91-4e38-8aca-8cead6f750f6": {"node_ids": ["61024bcd-7861-4906-a3fa-1a56a6bf9307", "14be06b7-297d-4ada-af94-28249b913c15", "54cc4e5b-f7e1-4202-b4a7-3beb688ea5c5"], "metadata": {}}, "efa0c9b7-e632-419d-8e96-465fde932bcd": {"node_ids": ["33e31028-ed65-4bd1-a21d-aceb00b3b6cc", "7bf7eee9-2e8f-47e6-8212-df37ee35c911", "7fd0ebf0-ba68-481e-aa52-3df13b97d7bc", "d7f0528f-a8e7-4da6-92ff-25942d99c739"], "metadata": {}}, "baf924cc-b96b-46fd-84c1-79d69be105f3": {"node_ids": ["1489f6c5-e7b4-4b63-a335-da063a794271", "d68d6704-95bb-4b0c-8753-6602d33b20e3"], "metadata": {}}, "6a3fee34-1975-4ed2-aa31-79f5dce3ba13": {"node_ids": ["4173ece5-0263-4f96-9601-b024f4520896", "9944fcba-9019-4ba9-a3c5-574daf2cb5e3", "0e30dee4-2d8c-46d8-aec5-d1b76a642603", "1aabb644-ea1d-4306-93c5-5bf6eb4611a9"], "metadata": {}}, "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2": {"node_ids": ["a5aaada9-567d-4840-b2ef-5d4d18e90c54", "372fe491-ee84-4139-bbd3-aeba672ceae2", "bf4e0f2a-c7a0-4209-9ddb-37dcbc926b81", "4cd1fe65-7376-4018-957c-b12a0add9cb2"], "metadata": {}}, "fe11e513-a41b-435b-a4c3-5ff192d6feb0": {"node_ids": ["6e492ad7-038f-4098-ad95-c572129140aa", "a401e806-2317-44b7-b540-f06cc7515fdd", "856827b8-2ae2-4f33-bb4a-ff39be844c3a", "6bee4ff0-8395-4897-8cb5-7510719e6322"], "metadata": {}}, "25e4fcbf-2304-4368-a573-0dd04a2fdc9e": {"node_ids": ["2c266b44-9983-4b8f-9f3b-5cbaaceaa6b5", "cec635fa-2615-442b-b2c6-c429087ea181", "50b69303-cb23-449a-8035-3f3052ba3c58", "b7d815f1-6f92-4100-b2e6-076e82f1bcf3"], "metadata": {}}, "ccef981f-9e59-48d8-bb73-711d3aabdfe1": {"node_ids": ["ec709318-af84-4a34-9912-b83cf1ffda69", "cdcb393b-f093-494d-8dd8-6d709f7f5457", "79c66cc3-606d-4dfd-b2c1-9b28af6e3dd7", "8d68c899-e84c-43ac-b263-e57a81b96d46"], "metadata": {}}, "5c691987-9616-4fb7-89a8-83dfc8587080": {"node_ids": ["5b1cb693-9230-4c52-a786-35b978e4da8f", "fd8bb99a-8f6f-4474-9d17-f8c4db6e89db", "9b159eb3-6f7b-47bd-9b1d-180e8743ec8f", "f1550131-2086-43ce-832e-f350dfc70b2b"], "metadata": {}}, "171bb256-7674-4c94-bfe7-2609e57291d1": {"node_ids": ["7bbfdb31-657f-48e6-a55b-060470c3748b", "89632ac4-256c-4965-bb79-9757bf15c38e", "a592c58d-7ca7-480e-80bd-65288a4c5d5c", "2e38e707-5f98-47f9-8659-1ea63e7634b6"], "metadata": {}}, "7342149e-ef54-4ae0-ab55-aa6c2bec14a0": {"node_ids": ["9553bf60-44e2-4b30-b0f3-01015787894b", "3e771a02-63d1-41fc-a448-0007671e9cae", "881abcf2-7975-4eaf-b94d-d555990138ba", "da0e4cec-c02d-4b47-8b9e-e8cacd7eda8d"], "metadata": {}}, "b38af219-da17-45df-a80a-fe8ae2a44c6f": {"node_ids": ["a8ff3da0-6f08-4913-af29-ae21347f7cc7", "4aaae6fe-08c6-44db-9fbc-aff16d3a2710", "ad0981f2-0977-4a76-bc37-1e4b826af027", "1b6805b3-e806-46f4-8161-8a56124dd83b"], "metadata": {}}, "9d2789b5-4e82-42b9-9a53-33bc52a366b5": {"node_ids": ["f1309865-2516-42cb-b374-9811c3cb803f", "d26219ad-4f57-48b4-a730-1fefa3530c91", "d80aaf95-855c-4fd3-94f1-e90606cd8f27", "3da92898-d25d-4a1f-8c47-d2c71b3db512"], "metadata": {}}, "767c4308-525a-46c7-b3dd-1c7a3a792ad9": {"node_ids": ["5c6787bb-f3de-4152-b236-e5adb248eef5", "702a4dc3-2218-45fb-a918-ae81910bb597", "fbfa05e3-53b0-4be3-bf05-9c83f8471bb0", "5ba6c70f-6801-497a-bc71-79b34146ac45"], "metadata": {}}, "53b92866-0f8e-48d1-bb79-a2d943008412": {"node_ids": ["2fea4afe-a138-4675-92c5-f6415ca97313", "9a0d5a48-9818-4626-bb90-e6d0fbc3ac7d", "edff671d-9458-4bdb-ac51-0a7d9d7f7d42"], "metadata": {}}, "9206bf00-08cf-4199-9276-96ef119a8e61": {"node_ids": ["c76cac1f-267c-4a55-8d5f-c50ff3f2cc54", "517e8542-2a5f-4174-ac04-97dd59ac391c", "02097577-3e0b-4ef8-bac8-942c1468a056"], "metadata": {}}, "8b3d4168-8e37-453a-9138-87c7e58ab499": {"node_ids": ["0ac65bc7-bb7b-4414-b9f9-04d8bbab3889", "95efa0d0-2f98-4265-a077-0af2b5891a0c", "dd53061c-f28d-44b2-a91a-b9fcb039faf8"], "metadata": {}}, "265d7b64-ddd6-4507-a17a-f8649b7bca3f": {"node_ids": ["de5151fa-e027-4b87-afd3-6ac0abd85d55", "b75f2d32-d00c-4f20-82fd-ddc51d695bd8", "4cb61500-a2fb-4865-a15c-61be52c02e8f"], "metadata": {}}}, "docstore/metadata": {"d9281baa-c7fb-427b-b9a6-2871fde16930": {"doc_hash": "b19d918a425443ecb85ae629c14fca1d497bfebe4ecae6054afd93dcbfbb9965", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "115b0331-8aa7-44bf-a5c5-2fbc49584b6b": {"doc_hash": "4520438bbb63add46db8092c10706b2a7f98bbecf23496be1d3866be12bf1b92", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "193c1e07-03ac-45f6-bd70-5bb0b54f1236": {"doc_hash": "ef9c76e6c0c10e120aa1805a956c3532e019b02b7a02de48252084024805b4ba", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "2495892e-f217-42d3-b47c-f13cc073d7ac": {"doc_hash": "7e23f50d94a0742ba9b7a8494a477e53cd68c82c98f270b27b41155c973ef317", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "8877944a-5895-49cd-8397-2777043dd11b": {"doc_hash": "e0dab7d5e98e0e185d60ba1602ce9de9f9b5e584c17ad185568381bd30211e11", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "5f95c61d-1036-4ace-93d4-24d8d50f6634": {"doc_hash": "c219a0cf1ce7ea6fb6019c24f8f6ab08129b4f50db9a4c46f7828629906eeff3", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "42cb046b-75bc-4943-b5f6-5b179a24a6d9": {"doc_hash": "2bf03cb87b4034dac427cadbb6745b0cebc78b92b0e45f56c0e88667d6cde748", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "8c986777-e62e-473a-92dc-acdb96c9c9d2": {"doc_hash": "55edb756a80b8dd2dce89f9499c140b2c7a4dde337dc52cc45b2aeebbc371718", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "10810e34-526c-413d-98e3-255cac0f8ee1": {"doc_hash": "336f9ca1ef4dbe6adeaeef34c0c77884914e562418dc6e4435776a3269886533", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa": {"doc_hash": "38d83558f5025872a47d73d6c8694c28f9ae78adc1ef927a722d90d23a7a0649", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "fd509e3e-640c-40f0-99e9-4bb7dcfd378a": {"doc_hash": "e8001a9028b7d731099276185824e99ca7812078edebc4e0c4941f038f8dbbe6", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "392fdffd-9b49-404d-888c-bba04b0af715": {"doc_hash": "c5ab75a2e5568e368ec06b4b2d78917195462e552b1471dc2ae4f21ec9757208", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "64e62721-99d2-4653-89a4-327bce7cb75b": {"doc_hash": "01313ab8cdfdfc9475fb755f41c81aee633f4976305f0aff0bdf06376688116c", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "83735c20-50e7-493a-a9a3-af0c3d909a4e": {"doc_hash": "b3f13ff1ecc76f1828a71d9579ef460cf0aab6f451814a228a91cbdce73643e2", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "c1af9863-e7a3-460a-b4c2-6ded2595f8df": {"doc_hash": "f47d77376b0ae0a6f2fa86ee8e9dbd60a32416367cc312744ee844f58476cba3", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "73c5cb41-444a-497b-9712-8883e99a63c9": {"doc_hash": "76ee8df3cb51834a2cb3f3a660c7e9a907fb1b8a92a406410183bc869c10f0c7", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "6bf84f43-dde5-4866-89d0-bcab22576f63": {"doc_hash": "04f8bd9cd306dc388a37a3ef914cfdb1b70edb150299ca2b6989560033eb1f95", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "887ff996-3d20-466f-8a43-7ccf4043ebb0": {"doc_hash": "6319320dc240344335701df12777a3160bcb93f3f0e66382a213fd9d52a94a16", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "9b7c415a-d0cb-4a25-8639-1d943d367102": {"doc_hash": "94e8ae4fd21c91601b9f0571d5cdb5230613b869da4498e904f2519a9fb76fd9", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "b3399da4-ee87-4514-84fb-805c1728e658": {"doc_hash": "b184212a41d9892e8649c3391792c691507b74d39e6d6e949a723976c63ae5cf", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "f326b656-9b2d-4837-abd3-67e6877c7439": {"doc_hash": "442bf4c119d31459127a7c6a356119f4e0657fd7601707b4a44fc4aba03785c9", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "07802572-8cc3-4488-a4fd-6135f0cc3045": {"doc_hash": "f44f2532ad9efd4160a02b65b972bea2c66a323ebc0e40e94781c65978c6bd85", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd": {"doc_hash": "a7c902e17e614210a0e6aa290102eca75a25b2cdf0f7753c249ee78c27c37520", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "652a770b-edf5-4968-bf1c-1e7289800dce": {"doc_hash": "76ed119e7fb6d90417a3ec226117314d7d2d067ad641aaf7b2fe72d61e421851", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae": {"doc_hash": "4b37bb9b789fd5aa33e249f930922f3a7d11d5c08e7ec12d99afa3ff5a97b525", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a": {"doc_hash": "2922f4f399b53fce4492f4015f2fc409ce0aa31f6bc9c55b4e4e0436d36a0369", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "b61dd542-0ed4-4a31-9612-cd7b9fa77581": {"doc_hash": "f5b660325c6bad6bae119f7e57f7c958c7b26d5e42b7d410c3a0d307e3e7cbf7", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "97474367-637c-42cb-bdbe-82a982b8ddac": {"doc_hash": "ffb3c82649d3d94c92286479fd3d7d79af759c2cdf19ede2a36658b09da35ac5", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "ad6dd30e-799b-4385-b420-d59372953192": {"doc_hash": "a7e433526e10b0bd8ddf7cc047e74cb5f2dd09378f9f522c2414f4b3f52cfe0e", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "2e223e35-663d-45b7-bcfb-566a4b26d9b9": {"doc_hash": "cf0dcf293c895e4b6aae5368bf837f89c99af86f74ebf04689f0a2f6b57b923e", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "fa8855d7-51de-40ee-842f-c3acdf2ce809": {"doc_hash": "ec60c4d7677f32291466d5d8fb00d539015b65a4ccfc4e5fd9c7003ff4dba02a", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "0f5fdbba-1016-4d60-aeca-e98227fe9ea7": {"doc_hash": "26f84ce32d42a77c94f10e0aec56543bdd5b5aaf79f3d51e97b5103a638f71d6", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "0b07194c-2acc-4baa-8e31-a367741156c1": {"doc_hash": "dfb90d7d53dcd3cc24ec96ac568798a39d9aab56e31b66d114e9c23f123b9c96", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "2dcb8839-8d3f-474c-8f9b-724ab0904611": {"doc_hash": "a87dfe920b69147f9d578694017b896054ec7abb2377ae568a9f3965dae2103c", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "767d8353-d5b7-4dda-ab2b-13adb4d57fc3": {"doc_hash": "702adb52abb27bd6556ddc53de7c6bb165e77b45c82055a7de4fe67c53a4422c", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f": {"doc_hash": "61a26eec879d891a2a323e46275b58c8696878d3ddbcc8c9c858d6f0055e7ff0", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "b5cc1e57-4660-4db9-97e9-9c60e9e7c159": {"doc_hash": "a6cadbbaa1e38c5a7df9572989055b5dfef755e67c496eb061298b6ed318df40", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "4542dbfd-0d9c-4e95-8501-681dc1e404af": {"doc_hash": "59184eccf8f03ecf5e628ae40b08747e04516c55c713c628168282e1c357c2aa", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "8dd123d3-c5d6-475a-b90b-2cd8e959aeac": {"doc_hash": "a72638383f2bfff4e1af0e868f7e5cc10e71a63f8b2a87170137045efb88d24f", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "156f0050-4cf2-4295-8684-c1d16f36c709": {"doc_hash": "39e6a05c133d33dae399ddb62ba9101934294dab2a9578a00b26ba8107b37ec4", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "93d40a64-cd24-4423-9b76-38270c080399": {"doc_hash": "8d5cebb150cdc182c1a0548a840c1927d48b4cf18032605c1faf4cecef8894e8", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "c521edf8-0e9f-40d7-a411-07c241fb6733": {"doc_hash": "4ec3651661b20152e29f71ae79e49b22e24c50a031b0949e097d6de23c2ce76a", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "8054fe31-12d5-45a2-93e6-2592350a9cc6": {"doc_hash": "0b63e68ac1162371428a3f1cad68e21b77b247d754a513b382d57787363ec247", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03": {"doc_hash": "ac6c9bd8737d93d65f954840f825635e9340411822a8d7255fe1122bcd8cd4f8", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1": {"doc_hash": "2e28307baac0780054c00b0cad788836d080e802f588c9a2538f3c1f46a6d47d", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "8779a8b9-760f-4de7-b49c-0df95d1bcd8b": {"doc_hash": "21215dc15b2e69800bb35d61983507f66418ee4c56d71e753d07733816b50eab", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c": {"doc_hash": "ddb1df06281494aa03f8a2ffedecfbfc42cf21743365a8179f98e05bd9f2fa06", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "cd1ce237-0bf3-493e-b318-08c55024cba4": {"doc_hash": "f4c3c478c1f46a44b06ef0f2776cc3dc4bdd684b0676026672b09fdb6e79e7fc", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "45256c7f-e83f-4f1b-9943-5d93df44beef": {"doc_hash": "c43ba2531e2c721dee02361971b76eac954ea5f5ce1a137be507d9353e9dbff2", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "1536d61c-50cb-49fa-97c4-0aae79f4e538": {"doc_hash": "d86b971b5a48e95f197d11500c39125c644077c162add32d9ebf22631f3252da", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "1075c947-32af-4c99-8c75-4f87b980f909": {"doc_hash": "f143b93edef9fa1e87134984f4e10697b652de6b5dba7b3abdb8d8c8a0ec8224", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "2cccb9e7-9a10-4487-a24a-550528d54562": {"doc_hash": "a132146986905a3f66285702f87ad1f6790a03afb21270fd678882cc74950d59", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "96b953dd-0f15-42fa-a00b-ecc0e554907c": {"doc_hash": "df94adb5aa2b2f90688857b542f2b2b10c554a3e169fc64642073cdeeb466041", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "fd7538f0-7953-4038-8b82-42031ff86fb1": {"doc_hash": "a137a1c4a08fb5334a8f0c6127e5140526090e2d54fb96acf6916bc5447361f1", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "54ef6afd-0d9a-4358-a1dc-d317c9225537": {"doc_hash": "3c4e3f57d8d8eeb02c6f8c3bfc094d56481ac66edba4b1acee241552182c8444", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "cae4deaf-d284-4a2f-a529-c51f448188a0": {"doc_hash": "1bdfd950c867c7647e852f461d6efff594a749fa335c38ccffa0d2bb305147c1", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "a707dfb3-b0ee-47e4-a50b-c8f43d560b91": {"doc_hash": "5b98be79aaffa8d13b1c4b52eb320e9e8f73df4d4c6286fbc6b97d77800eb29b", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "cf87247e-da28-4d97-827f-4beaef959aa9": {"doc_hash": "5e4843d7261abb3968878f39f60dd99783a97fefabc75973dd8e291696483a3f", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "e539d691-178a-42b3-a0a5-e415d86e6bf8": {"doc_hash": "09fb6ce4dd7643cd94ee25e8c7d5f7c8e08b0a8793d9a8fedda8f4b262316333", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe": {"doc_hash": "6351cf21b7134e8f08cdc68b621dd40c73f06887a34fa96545ceb77680af59c4", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "3b0400e2-b023-44f0-b993-a89f6e3903c6": {"doc_hash": "450d63e7e75332db2c67d56c364be21671809c9cec2472267f17e607530790f0", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "bcec9c19-8725-42e9-b91f-a355a6a0389a": {"doc_hash": "e993cae5ebc3cb0bd0f20104b76fc036c729a07a74ece8f4a7543bee076343b7", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c": {"doc_hash": "c0641090de36ac0cf8d00d2ab57ef7bdbf433a45416289a3918828aa606a42a9", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "e0a12723-8a55-44cf-ac43-2688613ee018": {"doc_hash": "0ac829ad4f4a92a11a7fa30ba7910163f0c651ec3f2dfb19f94300491d4b62fb", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2": {"doc_hash": "e1a3f397f74a161c40a64237d89f361f9b062d03db09c710d877ab2a7a3da04a", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "3807ce52-317a-494a-9b60-f0cc6ec111e4": {"doc_hash": "1f2969c151e8e590987b04579d8802598ae3d17b17ded6ea334bec5bf6e71f5e", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec": {"doc_hash": "63458a56aa3fc79bf06244416fc0fdd25738e6a24ac2ffe262277c44e61576bb", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "e8ea7021-af99-4bc8-b505-811562e86599": {"doc_hash": "7dbf384da5263fe0317bd8ab6a7c3689259c924e18e1a3cff25462b7d2b51ac7", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "d0d05493-b438-49a8-8171-2da3c170a2a4": {"doc_hash": "c23e642698ee3c8b6df3a97f56c6baf32e23711f0bf6b0568238d9ce5502f934", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b": {"doc_hash": "a525612d74fb13cf932d06a4dddcef0a5da851aae1356cbcfee5da012227d93c", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "178e7a4c-422c-4c79-8e54-119e2e05c739": {"doc_hash": "f3542e49f8d2cb1d95f9b322b0e3fcf3894852f6e22c5bf1ac10faf515f29913", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "d7f4e18c-f12c-4440-bec3-960141c48bea": {"doc_hash": "ca492d2c5c08edadd2435c94f6ac4de35cf5745049a75e1ecfbacf2adb4de0e1", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "8d1ffae8-1523-447e-9879-92deafe8423c": {"doc_hash": "69405ea491b03796819a1192f7395d251090e6c9723db4dd4e9d2a53f3c1cd6d", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "88a26da0-22e2-444b-9df6-8f01c0e24203": {"doc_hash": "9e26995e97574a5611b450946caa3a36830ba558282f15189073f61e799d13c5", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "7c7a0d7b-2e71-4380-8cd7-706ee035b617": {"doc_hash": "3718a86648eb8f93605a9e868e51fd012d7a454d1ed8e86a2633a9c0155e11df", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "7bbacaf0-99f4-4ae6-9740-b7774c835b80": {"doc_hash": "09c72f3249a95e391f5cb9ba0f0f49ec5645ee86e151ece31a7f14f141158518", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "46c525df-09b7-4fe1-a4df-c7f1676daedf": {"doc_hash": "a8e47cc50e6e4a90dc3ecf90a80320bdc25455d894dc0095d9bac38bac7ec2dc", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "cedbeb90-5626-4faf-ac24-de412008c78c": {"doc_hash": "331b266488459b207e1643ed788f071467f6f531f195502932f3391d94a00ce5", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "3e66155c-ea3c-4435-9d90-ef6444c9f514": {"doc_hash": "b0c009b00a9d28690ad1754660685d676f6a6195b89b939cf29913443b433d92", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "f99b44ac-ef51-4064-b11f-376000a8d227": {"doc_hash": "56da6f91d81b42c0bcdd08a6fbb6456a7f243f2ae32d5b910461d8ccb5a6c72e", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "ef046785-afb0-44f2-b971-a11f2947b864": {"doc_hash": "9753faf7cd834ee9413f8e23333e4acd8c9bfe165c9f8a3dbbd54edb872bdcda", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "e55610d1-f38c-4556-ab21-8baa89ea8746": {"doc_hash": "5a184d2afc52e38ef816783ed5d05f9044cceb3b0107e208121b3a7788b1a4e7", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "49f45f3d-b52b-4ba6-94bd-073ae87610c0": {"doc_hash": "741f829103d87723bad5d1f5d01b9ff474001ab22bbec0914af554f35a5c4547", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "19e9220c-9515-4e08-9b2d-339a09be6cba": {"doc_hash": "dee63d8f226759fde5f74ade948b85f36884f06333e49c583c5b56d88d23cf98", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "bc007a85-6883-4a66-afa8-30af77b854cd": {"doc_hash": "b974c2210558ff9f02865dd3b29d3bb5175893a955e3c16017b0d20712de61ed", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4": {"doc_hash": "d1452b51ba2affc92374c4c05c37805d96152ceff54f8b5b341a3efb47ad31a7", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "fcd9a011-a035-41e2-87a7-23f1405a9dfc": {"doc_hash": "67d6a49a3605f68a4dc05d6ae7e5e3f596b72391c637b340510e824dbf901f51", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b": {"doc_hash": "de8679944de3bdb9de7f5a3dc0360ecc21480fcb375a28cf0b1e1671e3a1c17d", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "6c7685ea-8047-4290-ac9a-498322fa81da": {"doc_hash": "950e12c152d44fd3ff8a79a58274466279aae9513e19d2c2d87eb400f47ddb89", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "6a8f9c4b-5ccd-4765-9848-990d17ef768e": {"doc_hash": "718e2dcdd8d1aa111011d3ce529dc7010abf815229ba66d9395fcf924bf0f38a", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1": {"doc_hash": "de838560c6e0dd2317105e8b2bc93508ccc434a45fbfdfe54e335965dbc5574c", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "f9450061-cd10-4103-8133-a4093c11c82f": {"doc_hash": "15257f56612557ae4d72e1fa0be92a08d7621b1ff88211e96fe94e3d70264402", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "82d6e145-bda6-4955-8139-0ebd6de31919": {"doc_hash": "1f082b70c7ec1e1691b6db6e13e333bcdccd65d4f36b4af8a1128f1c4c5f9efe", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "16bd0293-7c88-4fb9-8a72-d944ffe333fb": {"doc_hash": "91a9ba033679f715c524537c548a1886dd8c590b3e3c629b58e0109c1b7de41a", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "aed5c07e-3ab7-444d-8c32-f894592fe2b2": {"doc_hash": "c61b7484b150d732bcb42e7c9074f65e085dce210897002e1e3a04cd47c80ce3", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9": {"doc_hash": "189ae8fea1540694ea3ba1fda1c13a9104e66df13d9d11990c18ae003ada3c9d", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484": {"doc_hash": "80e78fffd7b5fa2cbf81c024a795f520cfdc746c6d7b13f609dcc1ed7ba0e3cb", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "bd01209c-9c3a-4eb0-8594-239574f103e7": {"doc_hash": "7d811f93b41cad65008c0176862140e8e18267816f04ecf320884101d51ea033", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "39f0cb49-3072-4e99-824e-39b89862f502": {"doc_hash": "7720f36af0d7627f7f2c1c1677cd1ec50cdf9c4c764f197b5857fe9b4ca1d522", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb": {"doc_hash": "d30b69e21c6c05899392dbfafd611525c550a5835a147278555a3aa64bed56b3", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "ec5952e9-f4da-452a-9210-1509bb76996d": {"doc_hash": "7c9418cadbe2a894f1312ac989c2464a536fc0613af17f8cfd2383eeac54332c", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "07e1b2c7-9b3f-4256-93a1-db834bab04d8": {"doc_hash": "b94644963a0d9e7a5ec557bfe45d6d47f59bd8badc004a01e279445195d2b350", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "422218a9-2dc5-4297-8cb4-280ebd599a91": {"doc_hash": "47fc36050e751d1f22daaeb938476c167d4ec84f7d9872fb689621df57b80c3d", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "7a933fe9-3bf3-469b-b381-0fde2c77d1af": {"doc_hash": "a82967ac4a25c0427e823e185c9427d37d2b3985459baae4a693fa91b2763329", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "c58fa53f-1bec-4b92-b3c0-28261fffc47e": {"doc_hash": "a72b0fb285709b9aa500c22f866cf120574de7e8a743ce66828e299d8ed999e6", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe": {"doc_hash": "fb0d57b6a3393108cb68ee7943a83595c564e1e64c73004edac5abd735848b81", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "25f0891c-e28f-439f-b61f-356860391fab": {"doc_hash": "7b32858201113b02d3a93d6a3fb16e6913c7a9e842583cdd811c7c0aeb26e9ba", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "d69b2eb2-01f2-449a-99a7-588f7dc9d67d": {"doc_hash": "17b1f63d35ab1b6e3bec62dea2bd85fe1155d5776b1e2eced7cf6a8b2e8bcbe8", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "c16d2161-9918-4b03-96b7-ab1531cde427": {"doc_hash": "f6d16c8b8b4e92f06bcee35859f5af75a4f91abb7d7b9828cdc21912e9e47d1e", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "5213a7ed-3bfb-42b0-99f8-4f7882537a25": {"doc_hash": "727857fa392684cfd622f209b870ddbbb15cb5cdbacace82d4896f7f863cdcf1", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "28cc8849-c8d0-4678-b1d6-5a94c066f2a0": {"doc_hash": "53593eb51c201cc1a98ab27cc3b1c22b7e3ae9f44b9a120783c7d6d45bfc4d65", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "df036a43-245d-4c3e-b036-d33f56dabd94": {"doc_hash": "3db94a78a1558a2911b1f6487aa6f170b940a42333bfa38fa9f5b3dc163aff8b", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "ce98334c-2023-441b-acea-e0729b9bee09": {"doc_hash": "0fd0e34cbde42f002326b49782b365e6bb45b889d04ac5b44c7c5f711784b73b", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85": {"doc_hash": "0a0098c8ee9ea8e19804e2aecf77f100e25776bd4a132bb73a623ac21bcfc5dd", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "f546fdee-56fd-4002-9441-746ae6318f44": {"doc_hash": "3c7b2b741e72a94d6b63185aa8cf012cb6f5edcbfa6fc7154f21fd0b93f68ba7", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "d607c9f6-64f3-4c83-a796-b211f91fa8c5": {"doc_hash": "566d0db4152fa794e55cb242d1a9fa3d486cbecf1c321e21d85f17730e3294bb", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "8821cf26-edd5-435a-9f1a-b0248cf8d822": {"doc_hash": "48af84f1c7b675ada7b29bd678427298414d187f71728887fa2e7cfb36d3df19", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "6184d959-1551-4667-8767-3dd54a1351c3": {"doc_hash": "c86712a891a9aad25289b1ae519687e42770baf288de691bd48df3c593cc6da6", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "522de47a-97a6-4d73-948f-ad40577a9d06": {"doc_hash": "6227980c05f7f0bbba1ca195a450615b7eb09b86e53c005fbe36a8f3e8eb8d03", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "716fffcb-e467-4b3f-b39b-40b128c950fa": {"doc_hash": "bc1f11fe85f292099878a8af572638629b18b9280808e41971daef1e47e7bd29", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "dad26612-babc-4a44-84ef-3f98b427ac0c": {"doc_hash": "f8deb10662d98b653ff128bdbf7aa55e314dad16f434c5ffa25998fcf625c6eb", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "14ddcb99-9b21-4ce8-8cc8-38a1abfea400": {"doc_hash": "dbb8412828fb2951465dbfd852768f350087a020e6e9e05b443ad1d53a4a8442", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "14c27008-5937-4330-9775-61c1868104c5": {"doc_hash": "43ed4273bd08cd6ce2d235818fa84cd3b440ff95d3eb937e7239f491abcf7a06", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94": {"doc_hash": "596a282cd8020eed7a0e3e979a696cdcc65fe1221906cd27acd8a7902b1d2f34", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "7f4c1d8c-f359-481a-b841-e364b1ace56c": {"doc_hash": "03de927b2bbd47b32970de07588b746f6cf27b4800b8a086bf7ea480effcda95", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "936e4894-59f4-4fae-812f-9cedd308bd4a": {"doc_hash": "67a772a64c948b00adc97c3bc5203ff012636751548eecb5a11ad1bb177a5675", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "57d98462-a7ab-4c26-8cfa-c07b48f6f17d": {"doc_hash": "23e160dfe08668d48eae9b90ee4f6b4e2a157446b9f8d5fb2298082058dec76d", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "ff0056b9-5f40-4b65-b337-973ab0014686": {"doc_hash": "acade153dff820885cbce8848a7524b9ddbe32f3022cf27fbc0b889763845ebc", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "c51945eb-302c-4d86-90c2-dd965b7f1868": {"doc_hash": "95650e77b3b1e2f98297407514692006d54bd8a120facf754215559d49b693b5", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "6c9c6365-4eb9-4dce-bdbc-ece01038550d": {"doc_hash": "b30bec469e11d89dc619bc0ee351932910bd8346b0fee593b11f6d0701193dd5", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "56119b9c-ea21-4227-b3e6-1493babf84d6": {"doc_hash": "dd8abd20c1e33ec3824d28ebdcda4d25665e9c72769d2245ae128b5b54ba15a5", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "3453be55-5f04-4dd1-8870-191f9c5fd82e": {"doc_hash": "2e8e64b71fba5bda89f19f37e9ddfdc9b143aa4c1cc2b3e1f4c9f3adb62a0a95", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "e4044733-7eda-41ed-ab44-516f207a5536": {"doc_hash": "84db5a1c017c92851391e44a9c1ec08949e3c38ba357368aa65f35025f044ec7", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "6dd9730e-36d5-4009-88e1-5e237e7767c8": {"doc_hash": "4f258d579fc345c1ae314aa72d27d5a525a23370b1ab6a23f1fe7273d096d1b2", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441": {"doc_hash": "8ba80fd34c4d6077da47a0dd44f97bb9e35b7441e2b96dda4986c048db106e01", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa": {"doc_hash": "51f4adc68169ffa99406a3291731c3ae1691377a64a4f13355e4185a02c32126", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "c5b23877-b4ba-41c2-9081-12725a1b5b47": {"doc_hash": "49defd404fba505f60b24efbb9033e99a730a0c06bf60ce5ff71441855e30fc1", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "2e87e40a-d75c-4bca-bf66-2cb20b1e1201": {"doc_hash": "fd598698587a580a0391bf9cc9c05ce5d28d41129bf3d7ad454c42b7a8a9cf11", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "f6efea28-e895-40cf-9ca9-21a5b32921bb": {"doc_hash": "3f2340807d8643139c5df754379e4c24b4ebcfc582983256ce02bbbefd29769a", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "30ff2ffa-0405-4b62-b988-ea7cd4226bd7": {"doc_hash": "18ebaf1f87fd810a5d9741c3d51fd342383fc58d0e6e1108bffba8205383627e", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "8307386e-c419-4780-bf97-9408b4967a9e": {"doc_hash": "2444f2275e1d1fdfe3b226d537555eff0093c248ba53404119bcc78b2bb144fe", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b": {"doc_hash": "d6345df814f8c9f8800b9d5ef5385b81330418d3c20ae0a07ddcd5cd8252df77", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4": {"doc_hash": "af64eb5d1eef876fb19d2823481667aa718b452902b44283312ed84f276ce71e", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "b7e94e2e-e638-416c-b26b-6f00d0deb2d6": {"doc_hash": "3e1c099691bb4e3bc479dee16f81cdf157d1b898a1e94ce0dd906a77ebfa0f20", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "467795da-1869-46ce-8473-5f97601af966": {"doc_hash": "1e99cc7284d3cf6afc40e4c2246e19690ff3bb1dd8bc82a074522bb998cc4403", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4": {"doc_hash": "1f4c04156e3620d5b3293a5e1ef2593cbf6378bd4ce4cf28f165aa63e7cbca50", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "57d82275-bdb8-4707-83cf-2f2442d7befa": {"doc_hash": "fe3afc010af008d842febf6977c424d24a017617255bfad750f72438e3e8cf4b", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "9fa1858f-bb91-4e38-8aca-8cead6f750f6": {"doc_hash": "2ad03f3872044972ec87f7f571a78e0a5013b71cefa01ebc67058f53533959b8", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "efa0c9b7-e632-419d-8e96-465fde932bcd": {"doc_hash": "50df0fb7af020e6de18ed3f1882a9d8708fdd32285246e7f6f9ab4a169ad58ae", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "baf924cc-b96b-46fd-84c1-79d69be105f3": {"doc_hash": "c7ead2bc19323b9b676ceda4679afdcf0217af2f97346b1989813e0e8701e363", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "6a3fee34-1975-4ed2-aa31-79f5dce3ba13": {"doc_hash": "85c91c730deb9b9a54b7ed8ad97a20e88d060ba434f2d1fac7647459f8a208d4", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2": {"doc_hash": "740bf12cfb795da6ad97c6366190a4ab602f4fa84803c54018add41ddbda98fc", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "fe11e513-a41b-435b-a4c3-5ff192d6feb0": {"doc_hash": "81665e0735d269729c757cd19dfc6cd5b985af87a9dada141e37bb18c6ab4fac", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "25e4fcbf-2304-4368-a573-0dd04a2fdc9e": {"doc_hash": "e8323f5c85860a7f1c604c77c4d10ba71db2d56b3a22dce879741b3b78cff653", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "ccef981f-9e59-48d8-bb73-711d3aabdfe1": {"doc_hash": "6098f2f61f6bbacfaa3d5e6f01bc281adf24fc8aea1fcb26d2fa6509151fc2f0", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "5c691987-9616-4fb7-89a8-83dfc8587080": {"doc_hash": "209e9251486e40b5b42bf8e152192f740e207bd09ac76aba9036716a180fe681", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "171bb256-7674-4c94-bfe7-2609e57291d1": {"doc_hash": "010fcd7c23dadb4ef9cd723318ad6b46d3bf5b9198b506dddf31f052113d7431", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "7342149e-ef54-4ae0-ab55-aa6c2bec14a0": {"doc_hash": "bf270d45d8d040e0f266f3619288577dc170e8408f2f64f8f8152ea18036537d", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "b38af219-da17-45df-a80a-fe8ae2a44c6f": {"doc_hash": "a96166a30d722e06207a97e7083f9e348b82b648d6723ffcce39c021663da92c", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "9d2789b5-4e82-42b9-9a53-33bc52a366b5": {"doc_hash": "f895d75af44021c6568e763c45aab3e67d2e28064ac0c7ecf1995e59e199173c", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "767c4308-525a-46c7-b3dd-1c7a3a792ad9": {"doc_hash": "37139dbc21aa2acd1cf977293afcf1df4c37c96873b5adcec366d3a28ad166a3", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "53b92866-0f8e-48d1-bb79-a2d943008412": {"doc_hash": "76a39cd8d7738e591c73d577b1d61a76c9ad29cf7077dec16ea41a05f6d2ad35", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "9206bf00-08cf-4199-9276-96ef119a8e61": {"doc_hash": "741492fa2ea72962132f7b8aea6ab7e5dd917c0ca02d513b93a78277ee28fd82", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "8b3d4168-8e37-453a-9138-87c7e58ab499": {"doc_hash": "8d3fc5859de66e460483ed769451d0743888fed2c106a1a0f724674bf51830c6", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "265d7b64-ddd6-4507-a17a-f8649b7bca3f": {"doc_hash": "8573ecbb93516da074d72677f0b4a5e3bde8ccb655a72442dc61709d06819bcf", "ref_doc_id": "f27f584b-2834-48c7-ad55-f273c21f7dd7"}, "caa3d881-2b6a-4a21-897c-9040f14abbd0": {"doc_hash": "74ac7475481c990ffe046c550864909cb409d022b732827a4c9dcf2efedc567b", "ref_doc_id": "d9281baa-c7fb-427b-b9a6-2871fde16930"}, "9a6b808b-c800-4276-bb45-e411144750c8": {"doc_hash": "385546f8839bcd57982e313be5ef2559532a3a74a5be12eedc313d3125cd74de", "ref_doc_id": "d9281baa-c7fb-427b-b9a6-2871fde16930"}, "c0b79f38-f54d-49a8-b5a3-5827fb3d638b": {"doc_hash": "862f7ca88c65c3a4572e372531f6d618ee9de4f208912d303ddbde4838815099", "ref_doc_id": "d9281baa-c7fb-427b-b9a6-2871fde16930"}, "129e80a0-343f-4e02-ad24-7da18d5945c5": {"doc_hash": "ecb1412810d8511072d1ea14642472dd639349713592c4c2b6af2eaa5a4db3ce", "ref_doc_id": "d9281baa-c7fb-427b-b9a6-2871fde16930"}, "60b5bfaf-706e-42d7-a723-9df702496b39": {"doc_hash": "9ec5c9e0e5fe3be8845573ca4c79c3390b03cc2020700b23a225c4bc26d32b92", "ref_doc_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b"}, "85cd9d1c-f2ed-4826-98e8-8f352c020431": {"doc_hash": "c0821ea1a5be7ff15030d1d1ad49a477e91ce5add07577d7bbfae6a1760d4ced", "ref_doc_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b"}, "5fa49686-6f5f-4a1a-89b0-93ffe41006a4": {"doc_hash": "6676bbcc56ba85b210a3a8fe824e508ad4e2262447f8538c665a6b93f2a6b2bf", "ref_doc_id": "115b0331-8aa7-44bf-a5c5-2fbc49584b6b"}, "0ed32ab1-3669-4106-ac1d-2f66c3bdb29d": {"doc_hash": "44b50cb9c25d6163f3ff7d64eb124f97295ae175c296072257efed15974ca1f3", "ref_doc_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236"}, "b30ebacc-9abf-42ad-9f40-9e5e88833711": {"doc_hash": "35f9c836cad41909017022efaf9e93d1cb5d76157b6b708618ee0188e8e6b536", "ref_doc_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236"}, "6910eea8-072f-42ba-8c11-43f5781d409e": {"doc_hash": "6ae082112a9a3bce10ab77503d8dcdb7ef3eb8fd7b6f0940a368d54fccc94f55", "ref_doc_id": "193c1e07-03ac-45f6-bd70-5bb0b54f1236"}, "b784149f-4d79-4495-baf7-04894159cac1": {"doc_hash": "03391ff7326c6a47d68f6b0c6235851852408757c5d83ec827d09ca2ef60e4b7", "ref_doc_id": "2495892e-f217-42d3-b47c-f13cc073d7ac"}, "b3527ee3-d586-45cf-ac35-6ed9dae27802": {"doc_hash": "5e99c994f1c48176472a14344b48a5c517ea0234284b62c9a8d298414e2cff33", "ref_doc_id": "2495892e-f217-42d3-b47c-f13cc073d7ac"}, "e9ad55ed-47aa-4c1b-b760-c898291c8c33": {"doc_hash": "783ddefd6c786fb0f1f3cfa759ab5414439ee105ae04b2917ab9021c2aaec554", "ref_doc_id": "2495892e-f217-42d3-b47c-f13cc073d7ac"}, "037a136f-42d9-4099-8139-a859c61c9690": {"doc_hash": "555ee5bc897781941f99b545d7d3ed76c4148d6261a238cdcc2a746c7c772454", "ref_doc_id": "2495892e-f217-42d3-b47c-f13cc073d7ac"}, "c58a5832-c6e1-4660-bb8b-0b305cf97f2c": {"doc_hash": "84d5bf069d69d7757dd6e3296db165f4e5d02d11325fc891ba01ff366e7dc709", "ref_doc_id": "8877944a-5895-49cd-8397-2777043dd11b"}, "84c56aae-2e36-40d8-949c-d689fdcd64f0": {"doc_hash": "9da881b544e6c4afa4a9a46baf05da1cbf41557ffb592006034474e8ce81718b", "ref_doc_id": "8877944a-5895-49cd-8397-2777043dd11b"}, "e6c79f20-6e57-4dbc-922a-e3074073104f": {"doc_hash": "b0171cc53f83dbca35c5013ec2c3799f6c482ad822bc310936588d975c517cbd", "ref_doc_id": "8877944a-5895-49cd-8397-2777043dd11b"}, "2cb5bde9-973e-4fd5-a8cd-b012b5e49ed4": {"doc_hash": "6a00c494ef4dec7389d5d6188c9e86e88187bae9aafd47bbcd073d7a3b9ac493", "ref_doc_id": "8877944a-5895-49cd-8397-2777043dd11b"}, "b38d046d-ae64-4c9b-8144-06d6834a1819": {"doc_hash": "c7eb3dc0b8ff4c1bea48efce4f34e9dd3169fe54566e8e46aaed115cc383ed9e", "ref_doc_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634"}, "a1b126a3-f651-4ec8-8868-9bed2e78f7b1": {"doc_hash": "16dc86e3511ab3361a1aa7f2b1e94c6ca292ceaf5c59ae716ccfb92fbd14427b", "ref_doc_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634"}, "63f07a12-74d3-4d22-ae66-9d3d5ab4399e": {"doc_hash": "55ec3913793bef189844a6d5a771c99af8cf8b8e83bec4eb0ab57e9e6dbdd9a5", "ref_doc_id": "5f95c61d-1036-4ace-93d4-24d8d50f6634"}, "f8284800-a627-47ba-a595-2bf621e5bfcc": {"doc_hash": "dabff93b9974353d35367350c249525c653c74d731a2a10e886725addc7f780a", "ref_doc_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9"}, "d676ddeb-3565-4c6a-985b-c1161668436c": {"doc_hash": "209904f1692154051c100b4049308b5c191b4ac3f8391cf6d218a0a016c61b72", "ref_doc_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9"}, "4077c693-e2f5-4093-ac5a-85f8435a7afc": {"doc_hash": "69f4124539afdbe1eca9ede5a912589325fa6c4005a9412433e1f9896856b485", "ref_doc_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9"}, "ff83f3c5-dab5-4b42-9742-1341cd52b835": {"doc_hash": "478aa8a2805f16cc6c6d04051b73f315293aae6b956e3f03a0e4883bb87f8236", "ref_doc_id": "42cb046b-75bc-4943-b5f6-5b179a24a6d9"}, "d8210398-936d-4da0-afaa-cafbda9d4703": {"doc_hash": "ea0702b0772383062e345b5a8ac82c92e944a0c12fb15148ac644c2ecc055a56", "ref_doc_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2"}, "8aad2f35-e698-41fa-87ed-7173b63d324d": {"doc_hash": "9d82e3b9b8509be9bd173a82634199b269e2b67ddf26abd596dfa190bb76d086", "ref_doc_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2"}, "29f19384-7403-42eb-b681-19b889439afb": {"doc_hash": "ea3727cffc782d6ab1f454e1740abddc701d3b2520184b9bf18b5f6597c30a45", "ref_doc_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2"}, "5302d647-66eb-4181-83fb-4b6b2a0f3ffd": {"doc_hash": "60ae2fcfacb8a40b02946a7d4e114ab0e8caae9b73246fd17f912a7c52dc1327", "ref_doc_id": "8c986777-e62e-473a-92dc-acdb96c9c9d2"}, "2f911b85-4221-47f6-a216-26ec753f3446": {"doc_hash": "1db0515f685e963440fac925e697ffefedce17c950798c649fa11fb98c411f92", "ref_doc_id": "10810e34-526c-413d-98e3-255cac0f8ee1"}, "436fd2be-6597-4d79-896d-58c726d0f4f0": {"doc_hash": "d480d39d5ef1c6bf263d11bfda71f9478f5bcc6de211998f99b14413cf6d14e8", "ref_doc_id": "10810e34-526c-413d-98e3-255cac0f8ee1"}, "4ed0193f-b2bd-4f01-9a63-53e5d3fba802": {"doc_hash": "67cedb04a562b962d95412a5c76ba82773ec4ce271c9935b7a0b81e6cdd130f8", "ref_doc_id": "10810e34-526c-413d-98e3-255cac0f8ee1"}, "18ed6626-69f0-4378-be4f-1280d22459c6": {"doc_hash": "0494715f4b948fddb927c9b18fba19b6c186761e2c2b996ebfe6175c2bfb431b", "ref_doc_id": "10810e34-526c-413d-98e3-255cac0f8ee1"}, "45e3ce5a-8dbe-4e8a-a391-cc2c29bd9f96": {"doc_hash": "d1e15cbcc7cdb74330f528818cec49772b771c22c2fdf70028d0096cb17716e8", "ref_doc_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa"}, "b84b4357-5200-409c-bf2d-0be1fab0441f": {"doc_hash": "bb521dcd0e7d7a6bcbe8e0480877537c2fed32d07d2fc63cf25f2bda6f2f6a03", "ref_doc_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa"}, "e84b17a5-8c32-44dc-8fe0-fdb1ad963e08": {"doc_hash": "8654b763b7d9d11aad3c1cf4eef968c822082fe350af0fd6c147e9b6c8e67c7f", "ref_doc_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa"}, "37dc6775-88ee-4f77-9ee4-e4932ea9bdec": {"doc_hash": "a4ef6e7fd00288e9fdc3848af49cdd18638687d46d761eb8ec487e15ad3f8672", "ref_doc_id": "78fc6b18-1c43-48b6-9e38-0fe5ec77dbfa"}, "b8d60a4c-25cf-43e9-a388-952342591041": {"doc_hash": "2feb79328dacdf8cdee63520441762c4f0e821971d6f4082aa9c91cbabfa0bc1", "ref_doc_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a"}, "277d968a-88eb-491c-a9e5-85bf55f39f11": {"doc_hash": "993ccdf411673d8eebe7b0ff5446842942c2e08c9ad02c9a7275710bcd727156", "ref_doc_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a"}, "f0cf3648-44c2-4393-8319-4f5a36faed00": {"doc_hash": "66086896f7512f4eaa09458e6b2643bcdadff9f6c2687c2fec37a62ddb44addc", "ref_doc_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a"}, "9886d89a-600e-4752-b244-e82a88999cf6": {"doc_hash": "308eaec48ee03d3d79475876f0fd39956265ee9ec5d99a94ec3ee7ee76e12727", "ref_doc_id": "fd509e3e-640c-40f0-99e9-4bb7dcfd378a"}, "d056ad7a-d759-48e5-9bb4-fe59e7230f79": {"doc_hash": "e459f789e94432cb2c21d094b88585fc1a98b730ff6a8c8b129045c10f1d8ce5", "ref_doc_id": "392fdffd-9b49-404d-888c-bba04b0af715"}, "032b4796-6f12-4c57-95de-5e6287b9b309": {"doc_hash": "7227483d02e6c60057be6156405b43444290a5d765ed62e471a3a5bba0434ffc", "ref_doc_id": "392fdffd-9b49-404d-888c-bba04b0af715"}, "726bab5c-341d-4653-a1b7-568794b28cdb": {"doc_hash": "ca57768a24dc6e495f4ab4cec3ea76f0eff05255c9cea742a950784bdee771de", "ref_doc_id": "392fdffd-9b49-404d-888c-bba04b0af715"}, "b703bcbb-235e-4a9a-adae-ae27ffc8b7b2": {"doc_hash": "377da729cc7e5233dc3c06140aed8e949384c1f3893202413c3615cad981555b", "ref_doc_id": "392fdffd-9b49-404d-888c-bba04b0af715"}, "96f54233-b16b-40fe-aae0-385b9a91fc34": {"doc_hash": "ab2b63d08d97ca1d6781cd5ef4b71cd33b5920136e6f3eeb7ae34950942cc1cc", "ref_doc_id": "64e62721-99d2-4653-89a4-327bce7cb75b"}, "03ab4346-2c32-4920-a14a-79b02cfe75dd": {"doc_hash": "7a588769e2e28e14af6c27d05e05268fc1546b42e95dc24d62eb8fae6ee40581", "ref_doc_id": "64e62721-99d2-4653-89a4-327bce7cb75b"}, "fdb7123d-fc53-41b2-8a65-ef26675c6dfc": {"doc_hash": "23ed4bee7a44176a65d2eab7aab5ff6df16efe12d5a92e651e8b84f1624af6c3", "ref_doc_id": "64e62721-99d2-4653-89a4-327bce7cb75b"}, "fe988892-00ed-40d6-bd55-5575565ad14c": {"doc_hash": "bbb06337933b0800e1f3708b8aacdb048bf08a0ac1d4cfb1d45f4da020160740", "ref_doc_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e"}, "cb240758-46ff-40f9-99ef-1f67ac5345fc": {"doc_hash": "1c200eeb8fdccb8e1bdde273e532d69ec5dbd0ab3c48217656fa5e22e2fe05e2", "ref_doc_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e"}, "a96841ca-0274-4186-bdd7-2dd8aecf02a9": {"doc_hash": "33abbfd1c0bbede017f316316b87aefa30dea2198479bc1945783c243ce44033", "ref_doc_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e"}, "d0f157a3-5025-4f7a-9fd0-f00ef7b15858": {"doc_hash": "d544b353d7a6da68e88caae5e9e37397508cd71695b34b0ce380081e97672de9", "ref_doc_id": "83735c20-50e7-493a-a9a3-af0c3d909a4e"}, "775d675f-dbdf-42ee-9d5e-c725e9b7fdc5": {"doc_hash": "cb0d6d45f66737149a5ca1b44d7888c1dccf1a147417926039429a7710899974", "ref_doc_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df"}, "336c0ae4-4ee7-43a9-b2d5-94924f8947ed": {"doc_hash": "21b88c3cb34c57a0a4b39919578bff95dbf4e366148aa5263342c1bf05566864", "ref_doc_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df"}, "5fac2a33-b3b9-43a6-85e5-5557ebc23269": {"doc_hash": "c2df17712f95abf8988588650f524e344eb8c11077a6964ad19f1c3ad038e77f", "ref_doc_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df"}, "86135e2e-a847-4de5-b87e-868ac0f17e97": {"doc_hash": "cc60ce768021769d3476b0c39d499435d23ca6e6bdcddc0405dca4b94951e774", "ref_doc_id": "c1af9863-e7a3-460a-b4c2-6ded2595f8df"}, "1f631b18-8411-4438-9307-c9038c6fc816": {"doc_hash": "f85704546583957a5f9de63218c93038650fcb0e457b7dff43147ca345277183", "ref_doc_id": "73c5cb41-444a-497b-9712-8883e99a63c9"}, "5a662638-6ad4-494d-8676-f6274686a19f": {"doc_hash": "48a489cab05ed0363eb88169192662b95af93622b8e708d16da60f6ad9371d32", "ref_doc_id": "73c5cb41-444a-497b-9712-8883e99a63c9"}, "851aefec-48d1-4bdc-b4ed-ff3f727f0f0a": {"doc_hash": "15948ee16243e8fc86e52c250ac2e877fef28351ccfcccde4f1a9f468b48b96b", "ref_doc_id": "73c5cb41-444a-497b-9712-8883e99a63c9"}, "7f92ac4c-c144-4662-bd84-33d3ffb867bc": {"doc_hash": "1fea7a8d80596b5f2834d1b8b3827badce28d0bbb919f35aa82af867ebd24ffd", "ref_doc_id": "6bf84f43-dde5-4866-89d0-bcab22576f63"}, "a06a4f97-9432-4d71-9126-26efe752e422": {"doc_hash": "31b86f35b4100cd3b32e710fa26c9755f1d3be2401002d2f9b131d40ad218f9f", "ref_doc_id": "6bf84f43-dde5-4866-89d0-bcab22576f63"}, "ad4eaa11-b7d7-4892-bc18-6db5c9b48769": {"doc_hash": "5696f37eca9a1654661fa85075c265d797bca1445783c1a061719f666a0daf97", "ref_doc_id": "6bf84f43-dde5-4866-89d0-bcab22576f63"}, "04394911-77be-4d35-b5e7-431af7087cc3": {"doc_hash": "ce465f270ed2c87a78408aec316aab6f0773661aaf387dbada713661e45b0182", "ref_doc_id": "6bf84f43-dde5-4866-89d0-bcab22576f63"}, "469b13f6-1879-4e2d-86f4-58608a2fe361": {"doc_hash": "77d7838cd18dbbc272c15ececc81aa73819ab4d239dc179450e6a6fd960e88f9", "ref_doc_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0"}, "16fce20f-be0d-4dc9-bc9c-44e1842c6e06": {"doc_hash": "9bf20cb927d30f415046eeadb4fc3dac18b2cd24298f3ae856d675cfbc2d6159", "ref_doc_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0"}, "5a46f800-df81-4eda-b5a0-a2244ca97e03": {"doc_hash": "6d671394c3aca276eb52e60329e4a92df1aa087a19c0e5445d401e3fc2f33a3f", "ref_doc_id": "887ff996-3d20-466f-8a43-7ccf4043ebb0"}, "bbce3433-c5aa-4437-918a-55b061bcef40": {"doc_hash": "98ee78e9f0350368ac64a604e0f64831becfb77be72d48a1d19d7ce56afd1170", "ref_doc_id": "9b7c415a-d0cb-4a25-8639-1d943d367102"}, "f64c7dde-ee0f-44fe-9359-6c2291604a98": {"doc_hash": "ca0b8dab7c699c4f023cdd8850df82c1f4efd6ed913f527342ee05614dfe07d1", "ref_doc_id": "9b7c415a-d0cb-4a25-8639-1d943d367102"}, "72cc8a69-5259-4f17-9dab-ef0f12c531b3": {"doc_hash": "aad49d7eb54d918d43ccb503c7af3edad1972476b0a1ae513a7ec482e3578a85", "ref_doc_id": "9b7c415a-d0cb-4a25-8639-1d943d367102"}, "3050f0c7-639e-433c-a51e-360048a44846": {"doc_hash": "d3247939dcdf59c20c3b15ee78268f356c9c5bd14660b0cd9a3b0d13e5872755", "ref_doc_id": "9b7c415a-d0cb-4a25-8639-1d943d367102"}, "1f701998-d2dc-40f8-95ef-eed75c2be17a": {"doc_hash": "e24e2ab95a9e2284ae4fd2a9f33514fce87bf701d475ff83b71a1a0829cc79da", "ref_doc_id": "b3399da4-ee87-4514-84fb-805c1728e658"}, "56fba29e-2ab2-4832-87cd-c149d6a32fe6": {"doc_hash": "0075b8d4fa6e3a5fbbbbd1d2e9ce1f0981f7fde32d7738483f40b4fd148572b8", "ref_doc_id": "b3399da4-ee87-4514-84fb-805c1728e658"}, "db5a729d-92c2-412b-a5a1-a622ee0f629d": {"doc_hash": "7b3c6398de10eedb5e0eafb7b891fb7a75b9e63e2164add8238655e59a0d90f1", "ref_doc_id": "f326b656-9b2d-4837-abd3-67e6877c7439"}, "07ef6ba2-1212-4620-9a64-d97e427b2584": {"doc_hash": "f3247709ab321d44b6b7a07d17f82dfee0b9c97b4bb76c56fb711efb52ba079b", "ref_doc_id": "f326b656-9b2d-4837-abd3-67e6877c7439"}, "cce549ec-b777-4de0-a696-011295bbe324": {"doc_hash": "d415f1114829514bf9b5b631c9faefe43a68d68d2bd31cb54f04fffc75cd0638", "ref_doc_id": "f326b656-9b2d-4837-abd3-67e6877c7439"}, "64124226-5ec9-4b0e-a4cf-82c44b8a7e08": {"doc_hash": "3f21d108f04f55a7efc7f5793956639e8f609d7e0126052c1f830c85221771bd", "ref_doc_id": "f326b656-9b2d-4837-abd3-67e6877c7439"}, "19811015-7b6e-4227-a8e1-71300febfdec": {"doc_hash": "bf8a006929ca2e2595a32510816dc65c87d32e2efcf7d27485fbf6188c51fe6b", "ref_doc_id": "07802572-8cc3-4488-a4fd-6135f0cc3045"}, "e42e97d4-89d8-42fb-aab2-a01085e26abf": {"doc_hash": "7dfd0da7f19f501659c295cd94cb24b9a1f714967c9334fd48c182851faa1c2a", "ref_doc_id": "07802572-8cc3-4488-a4fd-6135f0cc3045"}, "82277ba7-e9d4-451e-8936-bdf713e2781f": {"doc_hash": "ecb0ea18a34cc3f2a2ca1f227e32caa4c65585a1b607fa66070bccc5dd631d04", "ref_doc_id": "07802572-8cc3-4488-a4fd-6135f0cc3045"}, "44b5d802-dbc2-48bb-9f6a-3ceb0c3dcc93": {"doc_hash": "cd4aa6a6bfd3808825408c4eb726b068daf9e59adbec4d641f63ba48adba4245", "ref_doc_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd"}, "ce343639-2638-4fc0-8fc4-b83afb10564a": {"doc_hash": "beaa9f5d8b34d3227c0b6d6d46094e3b10b43dc76e253be1cadbe14a8ba49624", "ref_doc_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd"}, "6e49b319-be00-40e6-af4b-5c0a8d2ad0f3": {"doc_hash": "5bba92a70333a3659924a1231f0b2a742999cfbd79adacbc238959294e21e26f", "ref_doc_id": "9767d7b7-33cf-4bf4-9fa3-cd8f2ad487fd"}, "29988d7a-5ee7-4ad8-81c9-c83f972d0697": {"doc_hash": "cd4e9c5bf64d00f506f5b73b75977590c3face787bd666ee217b7bc677cabe12", "ref_doc_id": "652a770b-edf5-4968-bf1c-1e7289800dce"}, "e24cf0ac-8186-4727-84ed-6c07832eab80": {"doc_hash": "63b75e57436987c21b6ccbe1b8b0e89cda2fc39a9528584f85994083a5a60e10", "ref_doc_id": "652a770b-edf5-4968-bf1c-1e7289800dce"}, "0c2c25f4-13ca-43ed-8f97-c793f711ea4b": {"doc_hash": "633e28bd176c37a75988779638f42aa5c2463cec129fb99a8f1bba1ff36e1e32", "ref_doc_id": "652a770b-edf5-4968-bf1c-1e7289800dce"}, "35952454-60da-4b15-868b-24884a0f8a0d": {"doc_hash": "d8535206f8b6117400bd9773dfc7ef06929be30946b773af97b3c641902bcde9", "ref_doc_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae"}, "d1330cb2-d00f-4890-857d-9b0aac7ead05": {"doc_hash": "2428f1e6dd08348a2f2253b78efd023bb2dfd2d686d4b7066949412c888c3322", "ref_doc_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae"}, "1c5741a9-c6fc-47a9-a2cd-3c45be6dc465": {"doc_hash": "b63d31f2813e0fc9d4f5d2467a38a7ffd119a7f15f22483f872bb4fb6521be10", "ref_doc_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae"}, "ffe26393-c9d4-4612-bbfc-391cdb7af831": {"doc_hash": "5aacfcd9628a7441d56a1a5bd09a744a45f53154a38d603a024097dc65ace382", "ref_doc_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae"}, "b3d6a034-cbd5-43c5-9022-1715c05ef1ff": {"doc_hash": "6e1ee1923377b895a9ad3d3a7eab6c9dd9052d228b4f5e6817a4c5c7cd5acac5", "ref_doc_id": "1acf1fe9-15ab-4ea8-ab8e-f40250e19aae"}, "f4263475-b3a1-4f60-84a0-ca4ccdc7482c": {"doc_hash": "d85b8f3d96526ad0bc43cc481b1e148eb65ca97d6ee5aa519b550adb32144e60", "ref_doc_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a"}, "a54d98eb-d4ad-4125-90ec-4a38c95483e7": {"doc_hash": "c016d9c06ffa72224ac57a9c0d227b433f0debadf9fb172394981c26aa4025aa", "ref_doc_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a"}, "4acfe805-ad5c-4dd3-86e4-3ed23d752c4e": {"doc_hash": "c84ac99be091b050e2a31a0e234a8896ded86084f8f3a4fb3c438ea43c8a1bcd", "ref_doc_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a"}, "6af4dded-1e29-4b14-9289-f2cf8af37bbf": {"doc_hash": "e2b6cd9e2a9e3821b58ae7fec74132b4e5d446fb0e44bc0200439e4deb4fe304", "ref_doc_id": "9ab5a92d-366d-4f0f-aa1d-a8507bb5c50a"}, "467759b4-efac-438a-9c9c-d79a54126a79": {"doc_hash": "2d79dd0295430e816caaf76780d3f5704ed0ad3e97757cfcfb92b58dd78f7de1", "ref_doc_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581"}, "32f45229-cca8-4dd5-a6e0-7ebf62badeaf": {"doc_hash": "e09cbf435c77025b37993dae0915f6ccb23dd2664a3c19cf17fb173c0f4376b2", "ref_doc_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581"}, "9f014092-2ab1-476d-ba96-e5fd449c45e3": {"doc_hash": "9ce7b3078731baccbf62890878b2c451660f59cd355584fb461921298ed1ad14", "ref_doc_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581"}, "c1fc7dc8-1fe2-4476-b2e9-7e2134603e7c": {"doc_hash": "240837e08b10a0b91de30f4e5eb52d9a9192ac27c1bf76968e6dd9e20b033c17", "ref_doc_id": "b61dd542-0ed4-4a31-9612-cd7b9fa77581"}, "bdebdcf2-d10d-48f8-bed0-afdc55ddbffa": {"doc_hash": "1485ccea8bec3f956bf5be3314acc728117260cdf85080d7b0c7bb57167e2563", "ref_doc_id": "97474367-637c-42cb-bdbe-82a982b8ddac"}, "7554fb30-8d54-4acd-a4de-dbbe5da79084": {"doc_hash": "c7f2b5a8787b41210ee5d2ef3d7f647c81f04aa93c5b76a8cb9c48c14a5cf714", "ref_doc_id": "97474367-637c-42cb-bdbe-82a982b8ddac"}, "898e0035-4dc6-4af1-87c5-b5a3f8436aaf": {"doc_hash": "aa092b08e564d7a8dc2a9d8a26d6680b96789d907c12c7cb1de28fe65203f2f5", "ref_doc_id": "97474367-637c-42cb-bdbe-82a982b8ddac"}, "47a8ccc3-e483-403c-87ab-2fd898383d67": {"doc_hash": "aa6f7d4cabd4656fba93bfda137b64338a03e65487b2a3f0ac2a51bfece825c7", "ref_doc_id": "97474367-637c-42cb-bdbe-82a982b8ddac"}, "3dec0a73-1cc9-4602-a065-f9142880083e": {"doc_hash": "bb0200e54267e5cfa2b7d8f9d3eabe762ecd6d476575ca05f30fdb9cdab06957", "ref_doc_id": "ad6dd30e-799b-4385-b420-d59372953192"}, "47ef4582-31ec-46fe-8d19-566262c354e5": {"doc_hash": "b2fb198eac297fdf122a23477e11cc31011859148a9ce71a1b94ea1925376ac9", "ref_doc_id": "ad6dd30e-799b-4385-b420-d59372953192"}, "9322d884-94e3-45db-a0e1-ff6d3602ccf6": {"doc_hash": "f85e4db56f2f5d713240df54682e831531dcd4101ed8160ddea1bb7f7d293f05", "ref_doc_id": "ad6dd30e-799b-4385-b420-d59372953192"}, "de6ba78a-53e3-4297-967c-9d0490004a20": {"doc_hash": "886a8f54155e24d7d6ad38fd59a990663c6bb2d2b4b28a8baccb6a039db5b2b4", "ref_doc_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9"}, "e853cb59-60ac-4f2a-99d1-828b877e5ab3": {"doc_hash": "9cf0d2ea3ee005f558102927c271090077993d7170f9c93690c6dcc58a14e720", "ref_doc_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9"}, "ecfd65a4-f974-4554-ba86-4805cab833b1": {"doc_hash": "f602a0d021d6c79350be9a199e05aa6772eca1228b6be3da61a53466900b8faf", "ref_doc_id": "2e223e35-663d-45b7-bcfb-566a4b26d9b9"}, "e846f65e-f37d-46c8-868e-ea192deecbb1": {"doc_hash": "3170ea2caffc5b4fe183246f09635145b055b4a41960a0f4f67bdf1e29dd0590", "ref_doc_id": "fa8855d7-51de-40ee-842f-c3acdf2ce809"}, "64b66bb3-6599-463d-b7d6-5913638e26e0": {"doc_hash": "c3e2a8a853b3f5232854222fec32b8db36afa3012336710aeb0aa8be3aaeb754", "ref_doc_id": "fa8855d7-51de-40ee-842f-c3acdf2ce809"}, "4e87c606-e67e-4afb-b738-5a26b6e05584": {"doc_hash": "5c0bc19293900a5a1b909a87af247bf297325d27f8bf0d34120bde066c022819", "ref_doc_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7"}, "00d0fc93-663f-4feb-8c25-03e5d2998334": {"doc_hash": "3dd9daf956dec3cfddd70c33c78ee2de61360f4021bf80a9ce06513df20bbbed", "ref_doc_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7"}, "bd4513ae-9145-41ff-8297-a26e911309ff": {"doc_hash": "b028581e5325709c8bd179b89d320db983aba6306d5583330c12f1da2c1a60cf", "ref_doc_id": "0f5fdbba-1016-4d60-aeca-e98227fe9ea7"}, "d8aa81e5-9c1a-4994-9576-f420e4c763a4": {"doc_hash": "b1645a4fab6a91a09f8e9ad50a08dff831181affbab7b249dc5143e739f78a20", "ref_doc_id": "0b07194c-2acc-4baa-8e31-a367741156c1"}, "889388cb-178c-4046-9c58-3b9e2a38b89c": {"doc_hash": "ae6e486b71a0f5eb295639db51f837d5e1d0cc7ada0b4e792edc5df69d82c3ef", "ref_doc_id": "0b07194c-2acc-4baa-8e31-a367741156c1"}, "f2f975fc-d497-40fc-83df-27ee09d7dc64": {"doc_hash": "e685d78d2f666b77e2c958ccfd8832729c84efafeecc43e0115f34fa44074ce4", "ref_doc_id": "0b07194c-2acc-4baa-8e31-a367741156c1"}, "1335bd5a-3236-4891-9db8-379da4c1ca05": {"doc_hash": "7190196b248df9825743b9d1aea45c516bc4b8a483cecd2d819d934eb10831c6", "ref_doc_id": "0b07194c-2acc-4baa-8e31-a367741156c1"}, "fd274941-455a-48b8-b720-cee0ae61075d": {"doc_hash": "326cb317ee85019efd15db2f6c1c97ab31a14a4cf8737fa0af2a17c9a1161b5c", "ref_doc_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611"}, "9504f2b8-a7b0-4ab2-960a-0c704dcf9bee": {"doc_hash": "3f94fa8b77888623c16a8e1bd1b4a48100e8b448942e5e88cdb37e7b3c54941f", "ref_doc_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611"}, "bfed0e6f-d2bd-4f6d-8cb6-f200ee70a83a": {"doc_hash": "5f701fa1370144d71e782ff0921f921a7173268acca95e8b083f5f13d3871e63", "ref_doc_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611"}, "0321e342-6af1-4315-a0e3-a299b091a7e1": {"doc_hash": "cdd6be7e959b5f7c33012810258816f86b7f1175a87e66d8b69afe665148b9cd", "ref_doc_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611"}, "686fb86d-ffbb-46f0-819e-3495e7ff4547": {"doc_hash": "71d602335d61014dd592ed125e398a0bf56eb87e2cf789c13e28c0f17f7568f8", "ref_doc_id": "2dcb8839-8d3f-474c-8f9b-724ab0904611"}, "1843d26d-92bb-4f9c-ba21-bfb76b1ed4b0": {"doc_hash": "65aaf6ff6fdc8312600f6dbb238a14543558917669b03787fde1bc83765b179b", "ref_doc_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3"}, "3d49d111-1bfc-4311-bc44-8a6c151f6156": {"doc_hash": "149efcb818412eb826620a2b3b86afee8e7af8be25b984d482201d9cec1e3ba5", "ref_doc_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3"}, "1abf506b-8336-41bc-b55b-f13581d2a7da": {"doc_hash": "72c321a9cbf19b5de7c8f00ea84c98cb073d1d871f8cdff28bfa0574c73b3090", "ref_doc_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3"}, "406cd354-54c5-4304-bbfd-b3d99e3b37a1": {"doc_hash": "43c525080a6f4dc102501ee14e1544b395e0d0b5aa10ca87d19bb24c4ff42324", "ref_doc_id": "767d8353-d5b7-4dda-ab2b-13adb4d57fc3"}, "ffbf1723-e016-411a-86f7-05f2442a7283": {"doc_hash": "2971877059a96b230ab45d2853d131c473d842e80fc9b1c560a0cf5173fb48ac", "ref_doc_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f"}, "f03a32ac-46e0-444a-8581-738e7d5e040b": {"doc_hash": "f10c217862d994157a2c8675c5052ca6fdf1c1226adfc88b0c63cb3d9978f215", "ref_doc_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f"}, "b9146699-a47a-4169-b3fc-d029991dec71": {"doc_hash": "0a55c006619d879cb395c69932aee7785b537c954c21543732368ca881810e99", "ref_doc_id": "aad4c5ce-d268-47a9-8b8e-41e25e7a5b8f"}, "2d3e1f2a-7c5e-4655-8fb6-f3e37c9f09a4": {"doc_hash": "9dc5064226ecdb9b6da472fcd2fc87e53124c0b4a9682a8cfb1ce9ed28bb1fd7", "ref_doc_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159"}, "88e956ba-bb12-4885-8d75-71185ec887e6": {"doc_hash": "795dd27439477e0564863c08bd786dd04dc1c4ead75dc1cb64157ec47b3d5a27", "ref_doc_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159"}, "96f11461-5762-44d2-b517-888dcc2de650": {"doc_hash": "d0a43e75fb20a32de7b78439400a06c47bafc145d32d2726678a7c5aac0aaa45", "ref_doc_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159"}, "35cb2b8d-cc7f-49ca-b85b-6ce401dc2336": {"doc_hash": "204f109aa3b3b0f49a1703fdcced28733ba2381b5341b7d4cb86376f509d9227", "ref_doc_id": "b5cc1e57-4660-4db9-97e9-9c60e9e7c159"}, "6f083448-5542-4404-acee-af96835a9db8": {"doc_hash": "899357273069354443b6dfd7f76c9b99e692a24d0b5266f75dd79d21b6bd9a4c", "ref_doc_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af"}, "48a96332-cc2f-4081-8f7d-c36963ea928b": {"doc_hash": "df28d3af2a029afb525d37c57809c4ce64933699a89686c95996c177058acca6", "ref_doc_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af"}, "1227e1e5-4a48-4130-9f78-43623f4c1c86": {"doc_hash": "adaec6e390e15c9c88b6aae2c11bd52f64cac5030268bb3c0324a3cf1162755b", "ref_doc_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af"}, "e16341dd-63c3-4997-81b2-e69224f7c731": {"doc_hash": "3abfc045f326987c76114346f2bbf82b90b06dabad4ca3924a3571e5665428af", "ref_doc_id": "4542dbfd-0d9c-4e95-8501-681dc1e404af"}, "663fb4ad-8fa4-4850-b686-345f93efcd36": {"doc_hash": "dc7e700360a7f0d3a00b1183494e05abcc251c8c67f4f04369258d8bef6e3348", "ref_doc_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac"}, "5cc27566-ccb0-4f57-a305-15c89e85721d": {"doc_hash": "7cee225c44e41cd18bcd5c577026c328ef378dc6a294e0e2add55adf428bf410", "ref_doc_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac"}, "842b2d1f-a046-4f59-9141-9f44f22b7120": {"doc_hash": "f51c1f2c0b5d82cb92870c81aa9c2450061c8ebf56adb8594a1595238d82cad1", "ref_doc_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac"}, "72843296-d34d-46e7-afc0-7dfd104a6dc5": {"doc_hash": "81e62fe5696cc1e43d8bdadf02519521ad9ea2632944153e74037cae7ebaaa1b", "ref_doc_id": "8dd123d3-c5d6-475a-b90b-2cd8e959aeac"}, "32e90751-5bfc-459b-8414-aa67fa4a00c1": {"doc_hash": "de244f1bb4a5f5e43bd48ef8d9ac0fbac11ab4526a5b7e0c221581b4dd58a504", "ref_doc_id": "156f0050-4cf2-4295-8684-c1d16f36c709"}, "21da08f4-4d26-4ac5-9798-5c10f3b8333c": {"doc_hash": "266f1b1a4687ac0541d1b3e7707efaebe8bf37a974ff73f35f83b1c45d2b7190", "ref_doc_id": "156f0050-4cf2-4295-8684-c1d16f36c709"}, "fee806fd-9986-4970-aa40-62f2682f5601": {"doc_hash": "73bd043351bab60887420a6612dc1cd170f99f6845042d3d1ec993d81ed8d4d5", "ref_doc_id": "93d40a64-cd24-4423-9b76-38270c080399"}, "4f4d8270-900d-42eb-9090-f93304ecac87": {"doc_hash": "49bdd696a8dd831f0875c4b30a6ac779e0a4496c507fbbb06ecbc9de536c2d7c", "ref_doc_id": "93d40a64-cd24-4423-9b76-38270c080399"}, "b3a4aeb7-f12f-44fb-bb8b-206d05e237b0": {"doc_hash": "a86052b9a40eed5699529d04d8f3e61be99c3d8a84adc1c9db3e8f736efdd6f2", "ref_doc_id": "93d40a64-cd24-4423-9b76-38270c080399"}, "f7c25916-5a5c-4dff-8eb6-0e33191ae75e": {"doc_hash": "cf54d456d9c4a0a91438169886481c6c8b799e8c728bf8e82d956f3ba2eeee75", "ref_doc_id": "93d40a64-cd24-4423-9b76-38270c080399"}, "f6aeac0a-9ebc-4847-8c85-8cbbee3e663b": {"doc_hash": "eaebff7ca74ac43bdae8e016fef5595a602ffff7f234b033696d5a792138d5f6", "ref_doc_id": "c521edf8-0e9f-40d7-a411-07c241fb6733"}, "31da94f6-90b8-4595-8f2e-88d702dac292": {"doc_hash": "1a579ecaba612f5beaf6b3b398740c015052f070368442c7f4a66db4c11749b4", "ref_doc_id": "c521edf8-0e9f-40d7-a411-07c241fb6733"}, "b5cbe7b5-d45d-474b-bfcc-10fe061780a2": {"doc_hash": "1a833459ccf16f1e2c55e4f89267d4bf797f82165a6e1e2dc2c04afe8ddeb498", "ref_doc_id": "c521edf8-0e9f-40d7-a411-07c241fb6733"}, "7053c6b9-a453-48d7-aefc-2353774c4bbc": {"doc_hash": "bc18fe21506dfd58ed0494542fac1314ba3acd82a8a84fdac905e3be7d4ea003", "ref_doc_id": "c521edf8-0e9f-40d7-a411-07c241fb6733"}, "68cd4ecc-145c-4942-a673-54b67b2ca4c5": {"doc_hash": "ecc749583cacaa8dd70b6c69d326f70903f4cfa784adcba48d3474bfb22ce0a4", "ref_doc_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6"}, "ad95958f-2a96-4b93-be43-2c1a8a62ca06": {"doc_hash": "842da23bd464499fcd5d82052e7c87bd3459254b09471826660624e91f224a8f", "ref_doc_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6"}, "f0a628c5-0903-4d58-ab2f-171f2f0b930e": {"doc_hash": "04c885c289aff7aa1f418911466ca64120ec5c762d462bddb5b6fedf321eadaf", "ref_doc_id": "8054fe31-12d5-45a2-93e6-2592350a9cc6"}, "0ce373c6-b769-4ae8-9dc6-74d32bee6687": {"doc_hash": "8b90438801cc9dbd84fb3e50a212b4650e2b07903df9e9402019bf589f49c4aa", "ref_doc_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03"}, "89724f49-aa7d-4b16-a4ce-d074cdf4fb9d": {"doc_hash": "edbfd0cac5f3dc4426dfb6247ae65858b3697f7a65c02f34476bbad97fd52059", "ref_doc_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03"}, "0ccbc9fc-beff-42f9-a50d-7ff02530331a": {"doc_hash": "9116082ff49e02072354f70761a66a65d3d24dc933cd82674fef4cfeb21b06e8", "ref_doc_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03"}, "3629e574-e58e-476f-9354-2a8b1649d84b": {"doc_hash": "401b008930bd9609833297090654f8db126973a44ee380bb5cacc798d16b44fa", "ref_doc_id": "64f8299c-8ea9-46a7-a7ce-7316ea4f9e03"}, "08c56a70-26b6-41d3-bee8-2e0e3e0f81de": {"doc_hash": "1609017a897c407245e9b4f340c0dbe47028394db382e54a486e2a563ed56276", "ref_doc_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1"}, "bd3420d8-9621-49b1-acfc-52208e8049bf": {"doc_hash": "483a3ec6e425186e688ef85dbffff201b59c1f10119fc004fa3acac12c906aa2", "ref_doc_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1"}, "5bcc33da-c757-4943-a190-3856c64392f1": {"doc_hash": "099c091dda7abb29b801b9b1aac6151d186fa4db49063e905c23b07b22027e77", "ref_doc_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1"}, "f1e23c7e-3eb7-4888-a2b8-fe8975b68a8c": {"doc_hash": "0cd1b9efdace84bb7f5e30d274e06a0a4390855ce59b70e1df84c3dad331e0e1", "ref_doc_id": "6a9effbc-a631-47d0-a9d4-c6bd3110a7b1"}, "e805909e-8d09-43df-a9c6-dc53f9d770b6": {"doc_hash": "21215dc15b2e69800bb35d61983507f66418ee4c56d71e753d07733816b50eab", "ref_doc_id": "8779a8b9-760f-4de7-b49c-0df95d1bcd8b"}, "f89aa930-dca9-40e5-b5ef-8f084a3ac5e4": {"doc_hash": "c3d3c2035f941135406c79f196342970c8cdf29e09d9b663dad708de1de443c5", "ref_doc_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c"}, "28fd1901-5401-42bb-8b1d-26ae73c1afad": {"doc_hash": "82d8b41e6aaa0ec8aa7d6c9b746e4804da099500ec01dc32de606c1a6a2c1724", "ref_doc_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c"}, "2ed1fe30-5069-4a0e-89cf-347e756b4cdb": {"doc_hash": "145145b4260658c2dc929ff9eb6687246cbcad9c9120024d2d72246e4ee7fb2e", "ref_doc_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c"}, "3ed20e50-d408-40db-b36b-8488d813aafd": {"doc_hash": "93697686d45eb7ea3455180d3dd74b356d0e2f13a39e92d4c01e8d6d4a22b450", "ref_doc_id": "0d00dc4e-4ae9-430d-8f3f-8f4cf06da13c"}, "20101a59-a024-4649-bac5-9059bfdd5587": {"doc_hash": "c9c3a3f9648edb351c0e1196a12a68322c4655efbbf17f1fbb6f833f6b0dc5d9", "ref_doc_id": "cd1ce237-0bf3-493e-b318-08c55024cba4"}, "b9e67033-f226-4fd3-8b78-793a4ff61056": {"doc_hash": "50b422e96aa5564e04be9d5c3027f84ff7deb1b32cda192a744a140a95e103ac", "ref_doc_id": "cd1ce237-0bf3-493e-b318-08c55024cba4"}, "aa47c07e-5519-415a-98b9-998b1eb33242": {"doc_hash": "08dc7f2bba0677767e8d5bede45f4dbb7f1f0a0c01bf5d802bb228e868adbaab", "ref_doc_id": "cd1ce237-0bf3-493e-b318-08c55024cba4"}, "e4e81f8d-42e5-4864-b424-4c84f2be118a": {"doc_hash": "0c608f25c41cb97ed1dc3672bd65676fa06a1a7710e8cd62a25af0d2f5d2dc0c", "ref_doc_id": "45256c7f-e83f-4f1b-9943-5d93df44beef"}, "cf6f149b-9c66-4a34-bdbd-10d460b9a92d": {"doc_hash": "3ff483d831c053b457f0da395f385f56345d1a3e5226f6ae1fd633b55e53ea7e", "ref_doc_id": "45256c7f-e83f-4f1b-9943-5d93df44beef"}, "ab467314-fca3-4e77-9f26-7cb7378e39c7": {"doc_hash": "ea85eff7d1ae394d2af2b37bd26b6f9566410dab2de06dbdba5735864aa432c0", "ref_doc_id": "45256c7f-e83f-4f1b-9943-5d93df44beef"}, "e66c707a-63e2-4e3d-9b6d-99e26513393e": {"doc_hash": "4496384e8762fd1d55a0f21749b63b5ceb62450032112f0f944aee7a09984405", "ref_doc_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538"}, "7f07022f-8d04-4293-a052-3d0640606654": {"doc_hash": "0e9a7c16c2b7c49b360f12c6c69bc040388b7b31425491fc2a5a969ff0b5fa0b", "ref_doc_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538"}, "11f1ac58-f0cb-4b48-a6bc-3a23d4f90a0f": {"doc_hash": "6d7af20b737f1fddf06b00b5da26923b741f5b7b89d3d9ede38dd89302720aee", "ref_doc_id": "1536d61c-50cb-49fa-97c4-0aae79f4e538"}, "c5838bac-77c4-4377-b9ab-07ea8231d1fe": {"doc_hash": "221a064b4876faef97a9ce2e3adf707e7f3b00e8663733cc56195a2892ccee07", "ref_doc_id": "1075c947-32af-4c99-8c75-4f87b980f909"}, "0f6091dc-cb54-4eb4-9763-bc6001f95072": {"doc_hash": "467cd5acd425894745fb0986d379da6fd98cd1da8036e9b703e38146dc3adf2d", "ref_doc_id": "1075c947-32af-4c99-8c75-4f87b980f909"}, "8f721465-dba3-4b27-b3ea-531a2aedb1f4": {"doc_hash": "6b000d10e55e8419055786c0ce8a3099b218896e7ed3f03ec048824ce65aba75", "ref_doc_id": "1075c947-32af-4c99-8c75-4f87b980f909"}, "7fcf52bd-c5be-4e21-a4fb-93e2af5e06c9": {"doc_hash": "5ba4a14802321b14532588b0d18816fc4438723e1de064828bcf4d6f74c43539", "ref_doc_id": "2cccb9e7-9a10-4487-a24a-550528d54562"}, "f20e29cb-25e5-4d56-a2e2-e2d612ff07f1": {"doc_hash": "ca758cda474e739667ca84f065284d5a83e76b553a932e7aef72925e3776a72b", "ref_doc_id": "2cccb9e7-9a10-4487-a24a-550528d54562"}, "5a8f23cd-6466-4b33-97f8-d52f17345761": {"doc_hash": "673ef040f393b4588ff5efb750a6b54b27832c6565f969fd3b8c7ab59120222f", "ref_doc_id": "2cccb9e7-9a10-4487-a24a-550528d54562"}, "6e27bbfc-12a0-446d-b4e2-feaf0b9c1066": {"doc_hash": "7adbce3a0a26bb7cdb559a38b4f31a24f0839acae29d5a9ead5a84156f03e6b3", "ref_doc_id": "2cccb9e7-9a10-4487-a24a-550528d54562"}, "8a1d7807-aa41-47c8-9dd7-084d4097483a": {"doc_hash": "e5af6cc85b86529fa88691ce6a8899e69674b33a0776939b355b60d8f761bb3d", "ref_doc_id": "96b953dd-0f15-42fa-a00b-ecc0e554907c"}, "9344c360-d43a-4ece-a321-d6e98c311d4b": {"doc_hash": "751da0c2085d592c95fea605117ed1bef7a545b5ba3e4320496041c35eebe61a", "ref_doc_id": "96b953dd-0f15-42fa-a00b-ecc0e554907c"}, "5c0a5cf8-a554-4b45-8532-aa25ceb3d911": {"doc_hash": "8e32897bbc6a48ce40450a1c0b1f9070ce5fc9595c6332ac90cb1cd725bb834c", "ref_doc_id": "fd7538f0-7953-4038-8b82-42031ff86fb1"}, "a11f277d-1983-4066-aace-321e61588680": {"doc_hash": "3d6ccc61dd186370a04136a0d91cd64600da5e16f14ee8d151ce165654aaf185", "ref_doc_id": "fd7538f0-7953-4038-8b82-42031ff86fb1"}, "711f400f-f144-4aba-bbd7-f0ef888b36bf": {"doc_hash": "172771c06ba470d8001018cf84a886ba8c5ad955e1a4231210204044ab802636", "ref_doc_id": "fd7538f0-7953-4038-8b82-42031ff86fb1"}, "7165700b-0795-438a-9394-5943ce730da2": {"doc_hash": "6dc836a2c7f279ee3fa90b5adac3c30d676bcce909efb814fdff44b3f7e7dc82", "ref_doc_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537"}, "4d438c52-7882-4870-b7d7-9786dad54f7f": {"doc_hash": "6bb05212b99c7498a2402e8bf260fc253186bf99ab38924b8b4cd83c714b381b", "ref_doc_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537"}, "aa59b324-bced-42a5-bfc5-4d9e93f4e35d": {"doc_hash": "05f331d9ad376f3460e7c4795db651d3024011e60ec7d580a16401d97ede21a3", "ref_doc_id": "54ef6afd-0d9a-4358-a1dc-d317c9225537"}, "4e11970d-2737-408a-9880-82efb9f30500": {"doc_hash": "4420f98742af652388587db35a0cf3531de04a164b805fdd4ae86ca6fcbf5d70", "ref_doc_id": "cae4deaf-d284-4a2f-a529-c51f448188a0"}, "b02504ef-9496-4b5d-9803-34e6bb4c4b20": {"doc_hash": "f7df85472e7839c85ffd3bfc64fd2fd59a5ec4e80b0ab781a50d1378e298025a", "ref_doc_id": "cae4deaf-d284-4a2f-a529-c51f448188a0"}, "665b86e0-639d-4207-a483-3a874ea9d39a": {"doc_hash": "a2cac16372549a7ad03186064cd451e496bcdd13ba07a467b4df9c17dc3fb87b", "ref_doc_id": "cae4deaf-d284-4a2f-a529-c51f448188a0"}, "3fc3f90d-1a37-403c-a257-0f84b3e46ea0": {"doc_hash": "8e601881c37eb7ae0a17b019926cd4e4f4fbff7c4e1223659c4ead5870598b1c", "ref_doc_id": "cae4deaf-d284-4a2f-a529-c51f448188a0"}, "97faf9c1-4ef6-43f9-b68d-dc9eb12cace0": {"doc_hash": "784c063b454a5de45458286de31877fd6071cd7a3e02e187630c4568c274ea6d", "ref_doc_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91"}, "6537a827-6b22-4ea0-909d-d40605457dc8": {"doc_hash": "e0e5afa98659fa8ab13de585b76ce8cb3eab601c2759f74736c2ad2ac9e19b0d", "ref_doc_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91"}, "d74b1e06-4f5d-43f4-9481-acbad0dc7f1a": {"doc_hash": "725f97fcdc31ec7b0903c1e238efd0f669bc136291df750fdb1a5d1d2b295433", "ref_doc_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91"}, "2b1b51ae-3158-40ed-8b5e-0d17adac50ec": {"doc_hash": "7d01f9c6ad9afdac055dc9bd736169a3df140c124903502169c356163f100df5", "ref_doc_id": "a707dfb3-b0ee-47e4-a50b-c8f43d560b91"}, "1c67d148-7cec-4ba2-8970-d6584100e4d9": {"doc_hash": "5191e763e3d59e63fe90a7e4481e9f32306062f82a27ed0f900ef9859324de59", "ref_doc_id": "cf87247e-da28-4d97-827f-4beaef959aa9"}, "b00879d8-1aa8-4cbb-ac15-51f6e436f2e6": {"doc_hash": "6a81ce7d9beaea315c3ef86b11acc06a0a3426535f56d1f01d631a4412e6c88e", "ref_doc_id": "cf87247e-da28-4d97-827f-4beaef959aa9"}, "7f14ce69-f3dd-4132-84b0-206a96d9de9b": {"doc_hash": "cd493143f16f2bd00085287f100bef08839972dc19881af1b5e715725a7591d3", "ref_doc_id": "cf87247e-da28-4d97-827f-4beaef959aa9"}, "d098b1ae-02c9-4f62-bb8d-f5ed9e288914": {"doc_hash": "7928ced9d521975bdecb52fdf8a980f27dca5e7400724d9e541457bf65a58a0a", "ref_doc_id": "cf87247e-da28-4d97-827f-4beaef959aa9"}, "60447b3a-237a-4a20-bdf4-02d73d9c5b70": {"doc_hash": "ffcdd0847d257ba804898c0b2fa7140e035f04baa3c896200d1326b932c69bef", "ref_doc_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8"}, "8502831f-6b0c-4d66-ae71-6a5e4f0b9158": {"doc_hash": "a8be8269f220de0aab2524e4346305eb52b9ec9a31ef1a0494d5dfb7c4bf1b94", "ref_doc_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8"}, "4ca2153a-bb41-4cd3-8eba-f9d3efec4352": {"doc_hash": "8d92950d7529071c6f77909e063c182be4240fe75bbbc65c1ee91dc3fc720e6d", "ref_doc_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8"}, "877e0a46-7397-4bfc-a924-0ac24e0c8eb7": {"doc_hash": "e747b0eba6d307af451ff80eb13761f1ad94165fb7748b5c8237f970f80267b3", "ref_doc_id": "e539d691-178a-42b3-a0a5-e415d86e6bf8"}, "f55e0d52-7922-46fc-adca-747d0dbdb6f5": {"doc_hash": "f361664e23d522a282e75224507b3b5c3aab078fd252e667e1955d86c6f28b29", "ref_doc_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe"}, "066d5c34-faad-4abb-a00f-6a9ccdf982d7": {"doc_hash": "5d3b290f6b30b050356fe8f6f853d8acd6641f1d19d1ae65477ee6a1143e2d07", "ref_doc_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe"}, "4eba65f0-8080-4260-8cdf-c806124d8292": {"doc_hash": "b89ec8b6f91dad05f2c0cd5c9a1cce0a1026a979c2b5146cc3a09000d50f3c7e", "ref_doc_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe"}, "8dc13219-c223-4f21-9482-cdebddf4b7e3": {"doc_hash": "90417415a65efa77c4f9937a7056d6c1864f203c190b075b96e64035af8d6e9f", "ref_doc_id": "69cf1a5e-7d1d-4d6a-bb29-27e2fe1d12fe"}, "1364720d-f74c-454b-9505-be0425082e42": {"doc_hash": "19f415fe0cfd6406f3eaf29d2e4ecd1159855efe0b943ed3e1c37c6db23ef49f", "ref_doc_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6"}, "a31672c0-029c-49ea-83ef-ebea6bf21349": {"doc_hash": "121dc53a9f2e5499ebeb227e5703fb5453c6bba4220c06e7c8ef739f6f6dfd8c", "ref_doc_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6"}, "7ed14c17-a43b-4cbf-9b74-74e78cabdade": {"doc_hash": "1207e634db08727ad7d02e5a12a526f21a967a63fb61db3558eb377dbd0a7cdb", "ref_doc_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6"}, "fb69fa0a-3c79-4a62-bcd7-ea2a95975d9d": {"doc_hash": "10a939e1204b7a00932dcaa4192e624413f25c91ba32beec24a0fb8bf4bc65f0", "ref_doc_id": "3b0400e2-b023-44f0-b993-a89f6e3903c6"}, "d78599f9-78e8-4ebc-b7b7-b1d6d7307702": {"doc_hash": "34ccd23fc0d486dc28930baeac3fb8815a7ebdb016b6e058c515fdfd1ed9bdaa", "ref_doc_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a"}, "47f69943-ccf3-4bf2-864b-b7ed32b85e9c": {"doc_hash": "3269ee61e7262ef3eb6b34fb5ca758b6e00cbdb3e6c64245abda640a0521cd58", "ref_doc_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a"}, "9876e684-21c6-48a3-bb99-01f70cdb6908": {"doc_hash": "78daf13efc20062afa1ab83d46df286176801cb9177288cb31ed244858c0d8b5", "ref_doc_id": "bcec9c19-8725-42e9-b91f-a355a6a0389a"}, "8ddc107e-02a7-4d69-a976-a15b237b97f8": {"doc_hash": "60333aca05813aa542a9889f82d07a793cbf6e363466fe9efc3daf97ac3fe8ef", "ref_doc_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c"}, "82aac71e-f8f6-41b5-86fc-cc647f68c6b6": {"doc_hash": "326faad75b59cd94ed683f37a7a54cda6f310134e282d7356791cbabbdb58768", "ref_doc_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c"}, "fecefe8c-f135-4ecd-b1f4-bcf27763cbce": {"doc_hash": "bf319d317f552e2c6f9c72efa14daad3e5c627c8e92b2a30184e2d0b3c782053", "ref_doc_id": "a0c6c78c-675e-45b4-91a7-1a0c1db2a69c"}, "8eefb2ba-49eb-4091-a4a8-5bc8a0765803": {"doc_hash": "200ad72b96b0d94fc6de636ea312e3bf1a893561ec921b4c30bab66ccaf688e4", "ref_doc_id": "e0a12723-8a55-44cf-ac43-2688613ee018"}, "a96e2d52-a0a9-4f0b-b7e8-7cbefefa2ff4": {"doc_hash": "dc021e4c0aa0f17c8de2246ff4d4f1fad7123de8c725e7b631f60b3683ef4eb2", "ref_doc_id": "e0a12723-8a55-44cf-ac43-2688613ee018"}, "5ce6af19-6c4e-49aa-8530-527c1776d181": {"doc_hash": "ffb4574b5c3ce9df058b25c71bd2fd49e4a61a3f6d6c80b95a174ca2cf32e030", "ref_doc_id": "e0a12723-8a55-44cf-ac43-2688613ee018"}, "053a299d-174e-4691-be85-64399ab269a1": {"doc_hash": "f81c33c64247c60b01fb32f55770ed732aa5750df6b77c1f7370ab92caa5e4c2", "ref_doc_id": "e0a12723-8a55-44cf-ac43-2688613ee018"}, "d0f715d0-e1de-4622-b474-01547e080bf3": {"doc_hash": "06823781c2c32ab0bc68aa74ba87757fa38f888916aeb09364f84e38508844cd", "ref_doc_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2"}, "0f3972c2-9ca0-4dc0-9c58-481c82e443d3": {"doc_hash": "a9380cb22cffcc8982cbd6077d9973b3bd04dba78ba07fa37357b1d82d85defb", "ref_doc_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2"}, "19e99400-c7d2-40fb-8687-098392f49704": {"doc_hash": "6223020496945b6321e7968eea6262a58425b2fbae844f1b9fef9cf59f809429", "ref_doc_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2"}, "1f9963d1-6a38-4220-8617-93a5cb9789bd": {"doc_hash": "6bd034b2de3d298909ba1c958e277076b0f01652d067513d4721b399af3fcd27", "ref_doc_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2"}, "ecc26c36-b4b2-4a93-80b2-61fb57afdbd0": {"doc_hash": "2ebeecc3087fabe235965196530db325b8ba31cf13dab190b5fae1cd41590f59", "ref_doc_id": "10f661ed-a73f-4cf0-9877-f0dd0e4e02f2"}, "b857577b-3e79-4099-b044-63d4274fbb69": {"doc_hash": "4d7b8806b7407221c2f7901dcfddd52637567150fef24a77cbaeefe87249eefa", "ref_doc_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4"}, "c0490741-1793-41f1-a0cf-e480098af30b": {"doc_hash": "960effeef9bc1135375a5bb9a31aa500f85ca2ca983d377e12603a5112e49e83", "ref_doc_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4"}, "618c9782-52ac-4364-b4e9-75de2b313e4f": {"doc_hash": "583a0cfe950528b8bee03b0768b85375db6b699b13f6d153e982f1b331e54dbb", "ref_doc_id": "3807ce52-317a-494a-9b60-f0cc6ec111e4"}, "cad62ac2-f538-411f-8ab6-d6a0c7dc1afd": {"doc_hash": "0a19811f3c89236becd6d45947cb0510b2581e4f2f2d452c2ef2010b8dcaae7f", "ref_doc_id": "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec"}, "c969a149-819d-44e5-a53e-fe3b9c7bf3b2": {"doc_hash": "713892e8ad7e4ff6fbdb38aab5caa1547496e599a5d0a65afa716e331926027a", "ref_doc_id": "d89bb9ca-4b05-4bd7-ac2d-b43d5b7356ec"}, "8a7ea6ca-717b-4b09-a88b-6def15e14eef": {"doc_hash": "a9d90ab8757e484cf8365d1070837c35750f46c40bf5ea0ed545bc7f7cdc37ae", "ref_doc_id": "e8ea7021-af99-4bc8-b505-811562e86599"}, "ba35fd49-1e5b-487c-9605-98df0fbd0e46": {"doc_hash": "e665f6e6619f2a34f78d3a1eb9b34e38364adfe3bed1282c8583ef01e8db046d", "ref_doc_id": "e8ea7021-af99-4bc8-b505-811562e86599"}, "5ba4363f-c41e-4e89-8b36-96ef866d2c2b": {"doc_hash": "24c42c11803af77b67e461a70f092954e86cd52aaa741d245f6a9003f8f44e35", "ref_doc_id": "e8ea7021-af99-4bc8-b505-811562e86599"}, "457677b5-d8c1-420f-8ea1-83b56cf3b153": {"doc_hash": "277b8262cc061b136630fc8a9650ddcc287d7be7677c6d6d50d42204f7d6397f", "ref_doc_id": "e8ea7021-af99-4bc8-b505-811562e86599"}, "52b153c9-e550-4a6d-8325-254c3a866d9a": {"doc_hash": "4764f3bad01cf2dcf1d52954345eab86671d018fc97f4d578f7aef76c81c1e36", "ref_doc_id": "d0d05493-b438-49a8-8171-2da3c170a2a4"}, "e502ed84-b3d9-4265-af70-95d5738e1807": {"doc_hash": "d92e3f49269f701d59464f14f013005ffa4f2b551313e358046282db5adf353b", "ref_doc_id": "d0d05493-b438-49a8-8171-2da3c170a2a4"}, "055491d4-1246-43bf-8f94-54c2a555705f": {"doc_hash": "584f5f1a814d1e354db54aabcd54a3e1c2511a4536abe63fcbe7f96f6cf96032", "ref_doc_id": "d0d05493-b438-49a8-8171-2da3c170a2a4"}, "3aae0939-41d6-4a37-a417-c6da7c0fb284": {"doc_hash": "697fe89f3b35e0f5f8df427d6a7d1b17ecb55ffcb861baeae4140c3495589721", "ref_doc_id": "d0d05493-b438-49a8-8171-2da3c170a2a4"}, "8919ae46-fb0f-4130-bc8d-2cddb2a74f73": {"doc_hash": "9517ba1eb27e4ce5ecceedee38d93312863fb8654c44dc0be0ef4a13fa7b1c11", "ref_doc_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b"}, "e9e43751-02d0-4621-b39d-2e44418f4455": {"doc_hash": "42fb258fbba7e9ccca9ae276e70dd4aeccbbd2aebdfc4b4115d711b679d5f8fc", "ref_doc_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b"}, "abf16a18-0c5e-4efd-8898-60b1c0832f83": {"doc_hash": "13105deaa880b1567e2d8c04a7738dff9bbaad99ec9e552cb3ec550f6ba4f660", "ref_doc_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b"}, "05040174-34f9-4d79-a1be-b1cba73de84e": {"doc_hash": "fc339a165d14c367721680b6baa6008a350a854b045e9d18d3fa45f8edf82996", "ref_doc_id": "bc4b1ed2-8c7a-4c40-83ed-5291658c2b4b"}, "ae435429-cf0c-42e8-8050-c9ac1b8c91df": {"doc_hash": "fb01da5b0c7bdc1c1a1f4d8457662c13a07cfae247b720d88d272c1376372593", "ref_doc_id": "178e7a4c-422c-4c79-8e54-119e2e05c739"}, "30c86132-8708-43b2-b30b-5fa311f3bac5": {"doc_hash": "077ec10394eabac712658efa27c47b486e21b084aa29a85ae7d398955fb97781", "ref_doc_id": "178e7a4c-422c-4c79-8e54-119e2e05c739"}, "1462f6cf-d6f6-42b4-bd56-1bcc7ab4db0d": {"doc_hash": "a76892c88fd471842c16da77cb4f7c47112f02e620fdd4f18d5d7bc984c21cfe", "ref_doc_id": "178e7a4c-422c-4c79-8e54-119e2e05c739"}, "717e1455-26c2-4e94-8828-0aafc65fe714": {"doc_hash": "420fc35f633728fbce99d3072a5c858d6397acbd1cd9307f1eba5f48ad2cdbac", "ref_doc_id": "d7f4e18c-f12c-4440-bec3-960141c48bea"}, "43d90206-228a-42f7-8038-dec0902d6fae": {"doc_hash": "28043d4ffd660547c1ecbd08bd6877564c4d823f972fef478904dbcf3a134348", "ref_doc_id": "d7f4e18c-f12c-4440-bec3-960141c48bea"}, "fe3b84fa-7467-48fa-969e-b9fba40b30c2": {"doc_hash": "fd51068037a1ecce0a10b24a41c49e9111fe355461036292b366aa3bffd4a874", "ref_doc_id": "d7f4e18c-f12c-4440-bec3-960141c48bea"}, "c485a396-c235-463c-b215-ebde1980ff28": {"doc_hash": "a96c2f4a771b2d1b091e0c8df0413c6d4c1d2aeaaf82989ddda97714c0999640", "ref_doc_id": "8d1ffae8-1523-447e-9879-92deafe8423c"}, "139ad006-31e5-407f-ac67-e31c4c44b392": {"doc_hash": "ef6d8e81675850480a30ada9bdf5098db266f4dfde9e3d0fa1003fbfeb9819f0", "ref_doc_id": "8d1ffae8-1523-447e-9879-92deafe8423c"}, "9ec911a2-ad2e-4b5d-acc2-5da8be39c8e5": {"doc_hash": "b6603d3edeb94c85b70ac8f113d6390bc89a1f821bad111e9baa0a1830a5c37c", "ref_doc_id": "8d1ffae8-1523-447e-9879-92deafe8423c"}, "006ee14e-9068-48ac-83c7-dcc4b99c69c9": {"doc_hash": "3b0b47d35b9193c2b7ca0adc4f8097be111bfb896dd4cf536bc54a25e7089dbe", "ref_doc_id": "88a26da0-22e2-444b-9df6-8f01c0e24203"}, "2f217501-fdde-4300-8766-590ca19de1e3": {"doc_hash": "aefacc9987210ffa4d9e25744ee6bd560abc011c2efbe9588fa4d47b5bda72b6", "ref_doc_id": "88a26da0-22e2-444b-9df6-8f01c0e24203"}, "38317e39-4b36-458a-bd0a-afe652b6a453": {"doc_hash": "4a6f72cf81eeb5e90d1955af55c20c8603241ce9511c98d05350353c5839b9aa", "ref_doc_id": "88a26da0-22e2-444b-9df6-8f01c0e24203"}, "f0e1077f-7df1-4be1-a15e-cf519bc7e007": {"doc_hash": "0f4976fa48845f8fd1cb25744ba0f34ff7ae63dbea25eaa7c1bd61ebe5341bd4", "ref_doc_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617"}, "6e3c8471-87b5-4e0b-9653-9ec23b2e6cad": {"doc_hash": "9867512f3ac55930e3a1494cef0ad9984b97e2484586c74c5f1663686a6f3cd0", "ref_doc_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617"}, "4c338303-12d9-4c81-b8de-e12c64d598d3": {"doc_hash": "a13fcd75e7966fa6660dae984b810eb2581ea1f1faa490bb3e47b74de1864940", "ref_doc_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617"}, "886e3f81-c5dc-4b0f-98ef-26b73704602f": {"doc_hash": "e4c25e6e24fdcc151ce355864c2efcf16c46e3474592e613b73faf0a5062023b", "ref_doc_id": "7c7a0d7b-2e71-4380-8cd7-706ee035b617"}, "ecb74226-c19d-4a62-940b-a190429915a2": {"doc_hash": "599ed6488ad4438f9d64afe6f9be1661fb6bec7b2904629b20c9e84f757598af", "ref_doc_id": "7bbacaf0-99f4-4ae6-9740-b7774c835b80"}, "bc3772b8-cd45-489d-9821-b1e072034617": {"doc_hash": "d865974a25914e5cd9ff3931e738fb2f6c22323759608ec5e873689d133f88a3", "ref_doc_id": "7bbacaf0-99f4-4ae6-9740-b7774c835b80"}, "4c2568d9-9d74-46f8-9494-0f6c1208b1e7": {"doc_hash": "3686e6638541a01366d76c7c6d1b0bcfa75b59f897b4b892c74835bcc2438604", "ref_doc_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf"}, "ff288ad3-f67e-453f-80a2-36e8df06acf8": {"doc_hash": "5477578229295ebd519be0616cac87fef33038a765db12d17856e7af3c3463cf", "ref_doc_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf"}, "7b8d6b4d-877f-4f5a-bc3c-d15e7eaae96f": {"doc_hash": "ad8d8b948996e894668f9b4c1a9ad7c4ffacd2a9b16b5db1324055a4dbd6c0c8", "ref_doc_id": "46c525df-09b7-4fe1-a4df-c7f1676daedf"}, "dc004b59-b2ee-4071-aa1d-845bddf28a4d": {"doc_hash": "8079ebce2d5c235539a4e5f1c2b19f61b99c01cace03a0830928398b1cf74887", "ref_doc_id": "cedbeb90-5626-4faf-ac24-de412008c78c"}, "4f5d255e-1c3c-45e5-afe0-4bd6b947188d": {"doc_hash": "d62608b9243ec1dc4e382915753d7a7bb4e978f4b4ed076362073018197d1a5f", "ref_doc_id": "cedbeb90-5626-4faf-ac24-de412008c78c"}, "818cb288-4336-4115-846e-24de9ebecb88": {"doc_hash": "5ff509539b3a2f8b13ad463bc090103df9ad7ce953fed8a059514bee97d2bfcf", "ref_doc_id": "cedbeb90-5626-4faf-ac24-de412008c78c"}, "2172d386-222e-4632-979d-c43cf53eef8a": {"doc_hash": "ba2cd19dc540177ae35f4b61e5c80240ad0f1acf2ab6618b0c3d325317fee17b", "ref_doc_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514"}, "496f6e3d-6497-4e4d-8f64-7bc846728080": {"doc_hash": "51b79ba348a14ad62ad28b7fcff4d62ce1fe52ecf0c3bd2aabb96f23edf7f687", "ref_doc_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514"}, "64c581be-ffaf-4820-9cbe-60e7faa0d31d": {"doc_hash": "4043b1d3cdb38733215bd235a9e96ce0381ccaa897b1211803816e6c6976efd6", "ref_doc_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514"}, "60564498-bab5-47c1-9a8c-b991f672b4e4": {"doc_hash": "6a9f3c15c9a1072d0c9e066aa62427dd0b7992d765cde196e62b64521a5dda2d", "ref_doc_id": "3e66155c-ea3c-4435-9d90-ef6444c9f514"}, "4dbd9c5f-178b-4ecd-aa3e-41f2824140f4": {"doc_hash": "05476a86e0c33954b5d3cb140dc4540548ff16afd96c5600b73c78cab406a6ae", "ref_doc_id": "f99b44ac-ef51-4064-b11f-376000a8d227"}, "9d66814f-bd9e-4e5b-a7be-fe9801064c3e": {"doc_hash": "ee020d95fe29d768d51f7d6bf4681f1b47b65f98380c2b53172a7dd14f2abc37", "ref_doc_id": "f99b44ac-ef51-4064-b11f-376000a8d227"}, "4f53b62a-6b12-4eb0-ade0-061997ded610": {"doc_hash": "b6f2b7f4a6404d0134ec387a87daf826681e632acc4cb6c670e7d3197cffc5b2", "ref_doc_id": "f99b44ac-ef51-4064-b11f-376000a8d227"}, "3c11af20-f2f5-4904-9253-0de11f04df45": {"doc_hash": "0565e80081af0e57493ce657a5c668404adc2bd3f053b13dd369a502a2015f62", "ref_doc_id": "ef046785-afb0-44f2-b971-a11f2947b864"}, "91e5b4a4-c742-4dc8-9acb-32493d892ec9": {"doc_hash": "0d6c73e0baf744d2596f42ae3aff4850ae1d7d0073b1fa016cf96f176916c336", "ref_doc_id": "ef046785-afb0-44f2-b971-a11f2947b864"}, "e8976cc4-4f5c-40ba-9df1-f095f713aed6": {"doc_hash": "ba8cf6892bad5aef013a5903da34527ee94a97f5f359c08f219bce10256bd18a", "ref_doc_id": "ef046785-afb0-44f2-b971-a11f2947b864"}, "6b2690bf-b5cb-41b0-929b-8fd7b832cfee": {"doc_hash": "a9115d3dacb1cff2d46e3710f0b59568d17965dce7961a57060e78b4e9816e81", "ref_doc_id": "ef046785-afb0-44f2-b971-a11f2947b864"}, "872aad94-9da9-4557-97bd-abd802849708": {"doc_hash": "34861dd5b1749d45e3d1f7c8ccccc1f4e1a4b7ff5026ba661a5cc9798ee08ade", "ref_doc_id": "e55610d1-f38c-4556-ab21-8baa89ea8746"}, "c5d7c57a-f321-4824-aff1-31e89e24375a": {"doc_hash": "0f1c13e9fad0dfcc0f473116347ed53b2dccecb1ef08b8270592317f4b3e3bdb", "ref_doc_id": "e55610d1-f38c-4556-ab21-8baa89ea8746"}, "050c6272-35a7-41be-b9c7-a044c487524f": {"doc_hash": "f4c756cbd9d06f8a1c8656e46711ccc5b4c10af36236c5c9f38c2de8f94f69f2", "ref_doc_id": "e55610d1-f38c-4556-ab21-8baa89ea8746"}, "3c931fa1-b7bc-4665-8296-b69e8ec7022b": {"doc_hash": "741f829103d87723bad5d1f5d01b9ff474001ab22bbec0914af554f35a5c4547", "ref_doc_id": "49f45f3d-b52b-4ba6-94bd-073ae87610c0"}, "c8d66d8d-f99e-4cac-a2ae-208fa8323598": {"doc_hash": "e414025695bfaa10c2ef859de942dd85040b80d00395f49de1c34d197d7e8e01", "ref_doc_id": "19e9220c-9515-4e08-9b2d-339a09be6cba"}, "1d0352ec-83b8-458d-9d7e-dd1dd5e57aff": {"doc_hash": "fdbb91daff0424a9d5c731a8144fca314b80a3e0ab042269f25f9b1672b9ca05", "ref_doc_id": "19e9220c-9515-4e08-9b2d-339a09be6cba"}, "44dbce79-8521-48a7-beea-de14b5c6383c": {"doc_hash": "a363d96b66f0060257e1788bdd4240b36659923a79ca3f8e700ae29b6195cf2b", "ref_doc_id": "19e9220c-9515-4e08-9b2d-339a09be6cba"}, "1a35a11a-8fd9-45d7-ad98-06a245c10326": {"doc_hash": "a4ea8317fd20061437fd74f75d12fd2555ed37a0a17eaad1c8a417b732e85683", "ref_doc_id": "bc007a85-6883-4a66-afa8-30af77b854cd"}, "3a1f5112-7bc8-4e99-becc-4cf7fc32a113": {"doc_hash": "c7ac254c0c5548d042fe48378d78076aa2d91f94676f1d20a4298ab414e0215d", "ref_doc_id": "bc007a85-6883-4a66-afa8-30af77b854cd"}, "41e35ee7-a929-4f68-af37-634800af56ca": {"doc_hash": "a96f50d50412a686a498b199dd881097792befa3bc904e996972ae74b944e33f", "ref_doc_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4"}, "cefcf65a-e0d8-4249-8784-d1b253b6d370": {"doc_hash": "babdd53092261df92ff07b828d983f08dfc44cb128306dad491aa52e0656c52d", "ref_doc_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4"}, "46d530a0-a244-4710-9007-f0264c1aa5e2": {"doc_hash": "b2f83f426c617440838f1f3873b87d404fddd36a8f694327015e131490fcb642", "ref_doc_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4"}, "32eef4f1-7aa6-4f39-b7b0-c6dcc26185f7": {"doc_hash": "f04fe1c00907969ac63902208a054b58a5eee434dd63ae98240f7bda60455f33", "ref_doc_id": "a9954ec2-d91a-46fd-ba1f-38646c4bd4b4"}, "9291fcf4-5c26-46d4-8daf-f632e94482a6": {"doc_hash": "67d6a49a3605f68a4dc05d6ae7e5e3f596b72391c637b340510e824dbf901f51", "ref_doc_id": "fcd9a011-a035-41e2-87a7-23f1405a9dfc"}, "b8bff74e-ae63-4b2f-84a2-10cd31bcef79": {"doc_hash": "c2b96f437a911cb039627eab097318134ab140296b8af1be602263c27df4b19f", "ref_doc_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b"}, "298e40fd-0964-4fca-930f-cf5b053eb630": {"doc_hash": "7906a00b38356f84c3193f0b599d37b6573662a6d937ef3c03d029fa3f4eeee3", "ref_doc_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b"}, "09c96574-11c8-4d52-a6f4-0226d2004eb9": {"doc_hash": "1c83d30ea01a86b84c379fa65084817ab640d7edfb817c31fecd6fd89ce8062a", "ref_doc_id": "2b6cd84e-ce54-44ea-b8eb-733b219f4a1b"}, "00b09109-7e4e-4405-81d9-eac4ceb4bbe1": {"doc_hash": "bfb9050957beedcf8a107173953b2cd6c1afe7f7667023dc8d3ae910032967dd", "ref_doc_id": "6c7685ea-8047-4290-ac9a-498322fa81da"}, "b1626bfc-e490-40e1-9a72-b01ed133699d": {"doc_hash": "4efeb17a5358f02c9012041326babfc41640dc46505551700ad9d2a9319266ee", "ref_doc_id": "6c7685ea-8047-4290-ac9a-498322fa81da"}, "8d38fd5d-8264-45fc-8417-3cce56944380": {"doc_hash": "421b15082007f3833143e12bc469f19ed9f7e94a8d056707d9b303015c38c643", "ref_doc_id": "6c7685ea-8047-4290-ac9a-498322fa81da"}, "ac33dca7-0115-4896-97d2-a1da715c057f": {"doc_hash": "a0f39a6c8260bd69ac2e2499a4242bb0ab1d2abf49a59d3a6bf11789c7a6b22f", "ref_doc_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e"}, "85014ae4-1598-4cbc-adb0-282440392bb0": {"doc_hash": "aeaa2ed4d2e72c53c149d6c106703533379ee0167666fc448a5308f7e295e682", "ref_doc_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e"}, "94979dcc-a9c3-41a5-a731-31d54ad85a5b": {"doc_hash": "4cc770f0f039c3cec2aeaa19fe18b624314cd789cdca471ce33d9dca9d41efa0", "ref_doc_id": "6a8f9c4b-5ccd-4765-9848-990d17ef768e"}, "2cc59327-43fd-46e2-b2d0-ec9c077beac5": {"doc_hash": "38e240a498f979439f24d35f2224ae467159772f41ab3e449146cc00e78e46b4", "ref_doc_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1"}, "b1cc8bdd-50ee-41c8-8b3a-ab587e4cb9d5": {"doc_hash": "d6454df9562c24cb4a9084e8644ab5cccdba2c71e684eb46ac48b6cb888cb5c3", "ref_doc_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1"}, "2ef03dc7-3c9d-40db-9e66-e4473620b4b1": {"doc_hash": "ca1354f8e51336ccb860f8be13ea8217d1b6108b5931aa19caee2e546aa373af", "ref_doc_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1"}, "a14e5911-54bd-4998-aac3-9591f2f97113": {"doc_hash": "5da99b9720b492903aa20ebff30c68ff22faaa0d44b2198700d3bc908b56f54e", "ref_doc_id": "eac1c5b3-e02d-4cf9-b3f0-c9c2eb1d68f1"}, "038a69b4-043e-45c7-9ce7-aea38a9c30ae": {"doc_hash": "bf85aace88fc80e444b181f21ca8f49aa42fcff35ea72d60849a1e2db3ac2847", "ref_doc_id": "f9450061-cd10-4103-8133-a4093c11c82f"}, "30d4603a-906a-457a-8787-b958f30381a3": {"doc_hash": "e83a7c0c248e85447e8aa15198840ce72d531d1604ae2a1d6a2ea0f9983bf5de", "ref_doc_id": "f9450061-cd10-4103-8133-a4093c11c82f"}, "a99b73bc-0a2a-487c-b417-566d106bfbe3": {"doc_hash": "998a0393e6dcb8811fe8619711877c83c1eeaa67ecd585face46046d1019662f", "ref_doc_id": "f9450061-cd10-4103-8133-a4093c11c82f"}, "14292a30-78f8-4036-bb75-12df73a72eed": {"doc_hash": "7453d184385fad3c9a03355ba1e752ec1f284207db45229ef0e333bcdc0c1066", "ref_doc_id": "f9450061-cd10-4103-8133-a4093c11c82f"}, "f2e09c03-26b5-4fad-8415-c20bd10525d2": {"doc_hash": "1a454bc8220d8464335a868b75fd94d5452bf18553415d20b4f354e41693364e", "ref_doc_id": "82d6e145-bda6-4955-8139-0ebd6de31919"}, "3eb032c3-57ef-4e7a-af31-7b7df20da4a3": {"doc_hash": "64509cc0262dc5a1df2f191b79698a96731c6593e1ce1b46f90a72040afdcc99", "ref_doc_id": "82d6e145-bda6-4955-8139-0ebd6de31919"}, "4ae5f656-cd55-48e5-abc6-78b2ff30365c": {"doc_hash": "65e778f862369d124bd7ba3ab08b0dbbc23eecceecf1486fa611829de250eb57", "ref_doc_id": "82d6e145-bda6-4955-8139-0ebd6de31919"}, "88eba848-fa30-49f3-8783-7ae042d90552": {"doc_hash": "e7aa315076f0e4b6a745878458d312703806d58c7bef4c832cd2c9e1a44056be", "ref_doc_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb"}, "98f295d9-66cf-4771-88ea-20417403d118": {"doc_hash": "cc3298f6f7b7039b9a5b3fd9e41cfb4ae2ab7ed95f464759afb93f8ee976a5dd", "ref_doc_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb"}, "60c02106-6a4a-482c-ab2a-8df3532b474b": {"doc_hash": "2b2b42c7e00665eedb74ec681242bc68c59df1bdd4c6de69fb2845805eb80a64", "ref_doc_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb"}, "727954f9-af6d-4d68-a515-0e1b00bbc39f": {"doc_hash": "f021cf69754f7233064f912f4acdbf21f0e2e5233c47370548a05784437ed75d", "ref_doc_id": "16bd0293-7c88-4fb9-8a72-d944ffe333fb"}, "d87db227-b408-45bb-acb5-86b12488fd18": {"doc_hash": "3044ef47fd268ca12c96f7f62f9f70dcbc0567e9a6d4149de8bc9c0391c193ec", "ref_doc_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2"}, "3daabe54-04ed-4728-bff1-58bd74f9cdb9": {"doc_hash": "17f7fdcb725494ecda3a6faa05222bbb7ec97687f17bf6d0e37901a9f565bbf0", "ref_doc_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2"}, "8120e5b1-0e04-4361-acc5-40389a3d4a23": {"doc_hash": "a5107b2740f71571e7963e40b36c293d0db2b13b35cbfd0d5ef13d6467cb5bdb", "ref_doc_id": "aed5c07e-3ab7-444d-8c32-f894592fe2b2"}, "70840c13-c0a8-4197-acfc-40bea23adbe0": {"doc_hash": "954c6c872ea77ad559408f33e34bb054714d6d7ac7233ef1460bef441ea17ddc", "ref_doc_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9"}, "83722899-33b6-4e87-9f1a-884ed7627d55": {"doc_hash": "98de344efb058cb74b019867ee8ceb9432e31dc2596805e3fa33c27e447658a5", "ref_doc_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9"}, "ddd83ffd-eabe-4b99-90af-124ba80a5438": {"doc_hash": "2a75f2713160344c4f73e1ccad485b3d6b9f6246342db2da46f018dd935d94d6", "ref_doc_id": "e3f1ee62-a681-4f19-9ac1-d6957aa3b6e9"}, "b8c86f8b-5eb4-45ab-a1ec-52fd2593b30f": {"doc_hash": "c05581f92dfd81f82818e047c3807c9cf33cb3d809c6ae156017f24f987821e5", "ref_doc_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484"}, "cb9484de-8627-4501-906e-291343164f4b": {"doc_hash": "3489a5933e175a2d830ad71908d55e63497c60412eff15669bd9ff629cd9289a", "ref_doc_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484"}, "cc8303a3-e113-46f4-a075-42b6a782e8e3": {"doc_hash": "36b62737306258159a4aadf4a41a54dc948042c5f33b9dd3ebe535dcd85dba61", "ref_doc_id": "8c9b0d7a-b5e8-42ff-8d61-5f65ad2af484"}, "36616633-4d3b-481b-ac71-c52d704cf5da": {"doc_hash": "8da9dbab48d5b0c6b80240c50c128bf7f49db91cf181bea650653424458d3d84", "ref_doc_id": "bd01209c-9c3a-4eb0-8594-239574f103e7"}, "aa7f8c98-34f1-4ec6-9841-cbce006ebae2": {"doc_hash": "a527c81ea27be46ad52da3d255cda4ad3689a1b296648093eb684dbbebe0ae8d", "ref_doc_id": "bd01209c-9c3a-4eb0-8594-239574f103e7"}, "1efbc0eb-eb89-47db-8965-39835610c7bb": {"doc_hash": "bd14bd05180dcc08fa90591e1d213be7b63a90e6b384c6740b283f7089106d79", "ref_doc_id": "bd01209c-9c3a-4eb0-8594-239574f103e7"}, "2c420b97-581e-4f93-a7da-d3b136554c80": {"doc_hash": "f5fc8393dcd64e38a48252f96ead2f2d2aef6bacbeb13af5d86e1b78bde52963", "ref_doc_id": "39f0cb49-3072-4e99-824e-39b89862f502"}, "f60cd002-f417-4f3f-a2bc-1a1666a150ec": {"doc_hash": "f64fe6946e6e33356ce362b42b0127d8c1fcd85d23e41f8a6e50cafc9c130b08", "ref_doc_id": "39f0cb49-3072-4e99-824e-39b89862f502"}, "7b2f6c81-0a0f-4da3-8a2c-12c0ac7d710f": {"doc_hash": "6adca613739d7bbe325eb877c7165ae7761f46d27bcdaf0b6e55554a2c8f9180", "ref_doc_id": "39f0cb49-3072-4e99-824e-39b89862f502"}, "3c629b58-2558-4626-87d6-aca4236b12da": {"doc_hash": "7ab613def70f57af6de2c2ff0fafb53e2fdaf011e1dde6bd38d2fc7942259ef3", "ref_doc_id": "39f0cb49-3072-4e99-824e-39b89862f502"}, "2ea2aa38-021e-4b6b-89c7-527a836a0c27": {"doc_hash": "18295918124cf1396f321d852db952f16a9b5a3515aca0aeea70ef8958b257f4", "ref_doc_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb"}, "5a023095-45e1-4385-b446-c75328fdfe72": {"doc_hash": "5d72397fc462605e7d8731096b836d16830770844602c4799267196811638f00", "ref_doc_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb"}, "fc476e0c-e354-4a0e-8eca-6c98817c9859": {"doc_hash": "e1e9e1563a4a9134fe8095fe40af4eb19f2c6611a697301ca8c786873a3a6e8b", "ref_doc_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb"}, "8d7fb08f-830e-48d5-bf7b-256fb6afa197": {"doc_hash": "98b337a1974c9d61d35f8fce9585dcb55350650da09de460e86daed6d8f63da0", "ref_doc_id": "4e88e8d4-25ed-47fa-a7ee-96e19cf3f2cb"}, "62b532e5-e99c-4d07-a113-8924ff067a71": {"doc_hash": "7a86e404b5db1a8bb8bf375748623f77928bb5deece337f7841e111873803413", "ref_doc_id": "ec5952e9-f4da-452a-9210-1509bb76996d"}, "9e11533d-8fd3-461d-83fc-83f699cbd98c": {"doc_hash": "f52fc51af468c6671811dceac293ff82fa12351c69a72f99a203e3836c723a35", "ref_doc_id": "ec5952e9-f4da-452a-9210-1509bb76996d"}, "1df1131e-791b-4c13-b2b9-fac7e3f0fbe7": {"doc_hash": "44460d72f5bf74b319952500b633d4896bd081e14c47841b528803c0268ac65f", "ref_doc_id": "07e1b2c7-9b3f-4256-93a1-db834bab04d8"}, "06c9b143-e5af-4e8e-8354-26cdab4080ca": {"doc_hash": "99cce85546cc279317abb9f54cc43ea56a6c6f603926d90628acfbfd4e4fc6e1", "ref_doc_id": "07e1b2c7-9b3f-4256-93a1-db834bab04d8"}, "bbe39360-85bc-48ea-bb0b-5cabbdf5a707": {"doc_hash": "b360eb2d395ab7be20489da4a654901825bc229e2cfe94339d14c677930f4b1d", "ref_doc_id": "422218a9-2dc5-4297-8cb4-280ebd599a91"}, "f1be8183-5f77-4bac-9853-97f8c646e8e0": {"doc_hash": "5b5c37b170c00a6cdb7f78839cf01e36bff96fa5c0d89f2d95bfd6ba6f311790", "ref_doc_id": "422218a9-2dc5-4297-8cb4-280ebd599a91"}, "c9f022c4-2428-4af6-8818-20c27f67d2c7": {"doc_hash": "f4f3547cd228572f88bca22fdb76dab36f12267a923684d20c75df1317d9e860", "ref_doc_id": "7a933fe9-3bf3-469b-b381-0fde2c77d1af"}, "220062a2-05f1-4532-97b9-c9e378f4eda5": {"doc_hash": "1cf7a1db25ceb5a1e66c309e2a07d817c595531a34bcf000430b8431c042aec3", "ref_doc_id": "7a933fe9-3bf3-469b-b381-0fde2c77d1af"}, "6eed3470-65ae-4a30-94e4-cefb8b399503": {"doc_hash": "1783a6eb1cc1b6b987c4ef83f1ef53e62d143a3c29b5506f150a81f95869cf53", "ref_doc_id": "c58fa53f-1bec-4b92-b3c0-28261fffc47e"}, "617168b3-3ae1-40f2-9e9a-0e2410dbd36a": {"doc_hash": "e326b3024b1dd9ea3d35d01fc2dcf2d0b1ae403947149dac811583900cce1d0f", "ref_doc_id": "c58fa53f-1bec-4b92-b3c0-28261fffc47e"}, "4c1b9aa9-2e93-4c24-9677-b9fe35aecb55": {"doc_hash": "e5601d3637ebe625b425eebe7449bb42aa333fb878a6b0a7590bcdc9c0d2af02", "ref_doc_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe"}, "ff7170af-6d0f-4f65-8b68-6e8b25416e3b": {"doc_hash": "818ddb8090dcfa4137cea2e7eb896774696c95c0dc203d5497c418832ebd9bf8", "ref_doc_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe"}, "37e90aa5-f3f4-4fdb-904a-f7e2546c2cdc": {"doc_hash": "c13d1964eeb1e406a8d921b3decf181839a5ad5e7d5c6ba8eba105c99f0ef5b2", "ref_doc_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe"}, "9c2ae263-8111-4b0a-aa1a-8499e26a67f5": {"doc_hash": "d0c03c0a05b47571ee21ddde5ec85c9f1b3bf05eb62e9dc6e2cf5d45e55feb4c", "ref_doc_id": "18c51b69-b3ac-4fc1-b907-6f1b312ec3fe"}, "a5f910fc-19c2-4e85-b99c-cb02ca9f570a": {"doc_hash": "35d12bbf9c32bfaf6ede5b951cc1b9ab8ace9eb0f3ab27bf0c2106a17705583c", "ref_doc_id": "25f0891c-e28f-439f-b61f-356860391fab"}, "037cd9ea-c523-48fd-a581-855307260406": {"doc_hash": "88200e44b9c1f5ddc07862d48b7ba279ce7645bd9cfb5cf6fef24ac3860bfaca", "ref_doc_id": "25f0891c-e28f-439f-b61f-356860391fab"}, "2f22326b-f3fd-4295-b2b1-41c05122c2c3": {"doc_hash": "582ad4b5686adbf2ee3f2fcdcee5169a67bc82caf51a41a29f0da663de2a8b25", "ref_doc_id": "25f0891c-e28f-439f-b61f-356860391fab"}, "37f4e6bc-d13d-494c-a5c3-8729a1bf22f9": {"doc_hash": "69ba5059fe71bfe6d98f05cb867aee330860d1401a7781b2d3d8ea3e6dcc431c", "ref_doc_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d"}, "813844cc-9bd9-4731-8bdc-a67f5cbb84d6": {"doc_hash": "be08d262634a7d3ca8d5bad5395a91493fa35fd1ab0ab24ba2aec821bb9f3bd4", "ref_doc_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d"}, "7c825415-8a8a-497c-9631-0480cd167818": {"doc_hash": "e22e476aa2849e803bc373c7c7d668e0d1536abdd8810685da4f372b7c6bc57d", "ref_doc_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d"}, "07fab882-9814-41a5-939b-40b9eae8261a": {"doc_hash": "42a75123789ea8d2445a009c21d908a7abceef9f9759442781d1bfa6d25826bc", "ref_doc_id": "d69b2eb2-01f2-449a-99a7-588f7dc9d67d"}, "45ff6027-0e9a-468d-9b73-1858960cb1ca": {"doc_hash": "7b98b83d80ee8fdf4d193843511508b67b0c3fd603787822e6662f279058f9e4", "ref_doc_id": "c16d2161-9918-4b03-96b7-ab1531cde427"}, "b6e663ab-d603-46b9-bc95-5ed81ab2ff82": {"doc_hash": "1c79c2e5d6ea3499579723425ac48637dd363e4c4e40b9c308b18dd97a94f437", "ref_doc_id": "c16d2161-9918-4b03-96b7-ab1531cde427"}, "d63fad17-aaf3-4edb-bb99-0714cce6906e": {"doc_hash": "1bf3937e3401880e14190381752becf6d85c82b3c853fed4432e1f5a7874b06a", "ref_doc_id": "c16d2161-9918-4b03-96b7-ab1531cde427"}, "920566e4-b853-445f-b94f-1401e4c45891": {"doc_hash": "b71104176f2495a51a15aacc0cf8612afa3d79f5e52c4b69fa3b09f216183c9a", "ref_doc_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25"}, "d2353a21-f9bd-4f00-95ba-52430bf65476": {"doc_hash": "3291a8795d102006855734236c69889116b8e6a9cb4d62ae4eebebbd576a048a", "ref_doc_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25"}, "4b1a051e-a7ff-4cea-9bc1-a872e4de5f09": {"doc_hash": "586bc07df23caaf05e44c7b71b924377dbe59b4ea607201a7f18077a22078a7a", "ref_doc_id": "5213a7ed-3bfb-42b0-99f8-4f7882537a25"}, "0d1dac4c-03c6-4dd6-ae47-d58444ea7dc2": {"doc_hash": "607532325b18a650ee3d9a0f5236e070ec14c9a1b52f6e2ebeffb6ae213dfd1d", "ref_doc_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0"}, "7dbfa10c-386f-44aa-bf06-299b0b13eb72": {"doc_hash": "58faf1ed6f6b190d8c0b351059dbdafc95079adeb31921afd9002f61c6e42437", "ref_doc_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0"}, "2a72a30f-f122-48e7-b2b5-25813db6d3ae": {"doc_hash": "ff56a794c1d94d170fba5baf39848312ac621c148ae544efe278d1f72abfbc60", "ref_doc_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0"}, "cad717e1-fa4c-4cac-8db5-ab62967e12cb": {"doc_hash": "91d12b18b7150e8fdc52fc045728227dde02b2e6991a003abc282deeec1b586d", "ref_doc_id": "28cc8849-c8d0-4678-b1d6-5a94c066f2a0"}, "ef51f9c8-e76c-4b1d-9fb9-da90bdf12754": {"doc_hash": "00835070339ed821bba6c348cc1e5c0ba1ac93f10839614653c9351b35495197", "ref_doc_id": "df036a43-245d-4c3e-b036-d33f56dabd94"}, "ef4efee0-7de1-49f9-84df-f7d840d2995a": {"doc_hash": "2f2d1b5a31c21478e278f568f96c62c7dff4c39282604a4029cafa083ae4df9c", "ref_doc_id": "df036a43-245d-4c3e-b036-d33f56dabd94"}, "14ceac11-7853-42c0-a772-1d5f0cdafd76": {"doc_hash": "1feec6011aa6811f0b71888297617ace38092a336c567097a6e172b821115589", "ref_doc_id": "df036a43-245d-4c3e-b036-d33f56dabd94"}, "f077122e-bead-4974-9c39-e43fb995e396": {"doc_hash": "183ac8a82478974e856f9af92dfbfdb3ab5b759427c44ea87633c754359d56a4", "ref_doc_id": "df036a43-245d-4c3e-b036-d33f56dabd94"}, "b1523a33-fb8a-45e3-bda8-62185f5f82bd": {"doc_hash": "f250baf79e559546473875e6784bbdccdf6b4f3dfa354d0793863a8e1ca3d5a6", "ref_doc_id": "ce98334c-2023-441b-acea-e0729b9bee09"}, "f74083dc-d459-4b03-b3a9-5f954eaa7326": {"doc_hash": "c851892fbf793ffa7b102d5036febe0a5f464d55422e61de535d6f538fe3d596", "ref_doc_id": "ce98334c-2023-441b-acea-e0729b9bee09"}, "16219dff-01fe-46d5-91f1-c548caf59882": {"doc_hash": "b0748124b88a87c4f6ed752184c44924543e8d079cc6b41ec9d6f5a15b02ee70", "ref_doc_id": "ce98334c-2023-441b-acea-e0729b9bee09"}, "9db7b730-8131-417b-b359-4a3669318a5e": {"doc_hash": "ae7d0f82b3a7b4ddfcc7c7d546cb3cbbbe986613579e054daf92354fd870445c", "ref_doc_id": "ce98334c-2023-441b-acea-e0729b9bee09"}, "e400ea9e-3d87-462c-b6e2-a5c578191c6c": {"doc_hash": "31a5b41b420f10728ce298555ddf54f31fee278bf06a91e1cd41c57e5414bb4d", "ref_doc_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85"}, "c6ee80ea-65f5-42b7-9644-5eedf61423be": {"doc_hash": "e5f23e8a83a7089dab7e70e13f6672cef099c89a76632bdb542a0069cc5853e8", "ref_doc_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85"}, "a161bf29-9df1-4011-b4ef-153fd555897f": {"doc_hash": "cd28a9a8f0ff53576017fc30fdcc631baa255e33a32763432a4a8e02e3704d54", "ref_doc_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85"}, "e4cfde42-c780-45e2-889d-ad88f7171da1": {"doc_hash": "7b589cb43e2ce37e62666a93ca188998d2b1f048cb2fbfcca3b323da652188c1", "ref_doc_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85"}, "525b7a92-a6db-4210-b68b-2d2d4bfc0bdc": {"doc_hash": "1a9b226dfe85feb5e7e2310f39b578af5c239891aa82e84425cd17be3103d092", "ref_doc_id": "62a6cb42-d744-4edf-9cbc-f41f0d0a0d85"}, "89e8b848-6f96-432b-a008-633b169734ce": {"doc_hash": "a8e5549d313144caea015f7aba737bb0fcbd89061c1b126a348bb979793c8e61", "ref_doc_id": "f546fdee-56fd-4002-9441-746ae6318f44"}, "d5d05ee3-c578-46b5-a3da-44e3c54c24f9": {"doc_hash": "5de44f6e70d8aa14b21d00c1a9f66226d613cd6ff031e4816522c641a9dd04e6", "ref_doc_id": "f546fdee-56fd-4002-9441-746ae6318f44"}, "69f8fca8-4d0c-4eca-927c-83001797ad85": {"doc_hash": "a31566b03bc2eccac19a12cab94eec73883d18513fcb0b777dee3a3786e50b66", "ref_doc_id": "f546fdee-56fd-4002-9441-746ae6318f44"}, "066f7152-6f38-4f0f-a501-5600d66e757d": {"doc_hash": "1bdce6ac6fe5478262e02bd40e913c318a2c71cf3deb0ee028225ee31736659d", "ref_doc_id": "f546fdee-56fd-4002-9441-746ae6318f44"}, "8b0061c9-30b8-4557-8a86-ddda10331ccc": {"doc_hash": "9b608f07f5722b5d23c1706ac7a2d3b5b4256d7444d0c0c70acbe296cdfea301", "ref_doc_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5"}, "739efa70-5c4d-4903-9b7d-7ae0e5b434bf": {"doc_hash": "5b25258bc2d443d17f40d011f135b4ce49e8fe5a3be21b7b8dc1b8ae3067d612", "ref_doc_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5"}, "620307eb-ce1e-4985-98aa-594dc9bdc5c7": {"doc_hash": "1cbf1b394237fa64efdd496653203761da05a656bf9df125d9196d533b44dc55", "ref_doc_id": "d607c9f6-64f3-4c83-a796-b211f91fa8c5"}, "1ae0fcd0-1f6f-42f9-94ee-8382150cea36": {"doc_hash": "85c9ff20b7e3d35acac5ca4f6338407ba9b491a7c25550faeb258796d078ca72", "ref_doc_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822"}, "52c768ef-e147-4809-84f2-b35c6c9b4a1b": {"doc_hash": "22c3440071b256314eeba7c95c27fd6c123f4cd7131b4c23b6e0963727fc67b4", "ref_doc_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822"}, "99e103ce-f5fa-4bb6-afce-e0d061dfd115": {"doc_hash": "7290c4330d22865ad724f6a438ad024d8e6a3fc91f59c433c4db28d603987e97", "ref_doc_id": "8821cf26-edd5-435a-9f1a-b0248cf8d822"}, "cfc542d7-8752-42ab-a833-8d9277ad678c": {"doc_hash": "e6413f2553a06884361590a1ee5bf4cc70ff9b0d35d37886e9284d52958559fb", "ref_doc_id": "6184d959-1551-4667-8767-3dd54a1351c3"}, "f66f044e-3eee-4483-8344-ece21a635523": {"doc_hash": "38e5f7b893a2df1d1454fdbe3a1d7652a1d68e6c962ee2d93fa517c997fbb48e", "ref_doc_id": "6184d959-1551-4667-8767-3dd54a1351c3"}, "0e35aa78-97d4-4392-9225-2fac96eab48d": {"doc_hash": "88b699596ee3beb45dff4a98770bd836f988cfa1cb1e65a04035f501d7c5bb28", "ref_doc_id": "6184d959-1551-4667-8767-3dd54a1351c3"}, "76b3aeb2-6333-4b90-b673-a51c9a7427f6": {"doc_hash": "d2620f0e490e8aba1f72e181f072dd8ff52a5c73b9ed7291d5028c465a91e4bd", "ref_doc_id": "6184d959-1551-4667-8767-3dd54a1351c3"}, "e12cb0b4-bbc6-4d53-b25d-b522cc0a075d": {"doc_hash": "e4bceb5b413b0d138e6282e24dcdb6f70b2e51a4d341fe47a01829f3e5f4954a", "ref_doc_id": "522de47a-97a6-4d73-948f-ad40577a9d06"}, "93f420f1-19c2-4478-9b9d-e12cb1d27eb1": {"doc_hash": "2a03923d06ae800c8f9fac91c2d9bb1422a9b469fd6fc24aa44a1bd1d72427ff", "ref_doc_id": "522de47a-97a6-4d73-948f-ad40577a9d06"}, "3e21a4a8-1405-4c27-8d5f-0de975f26508": {"doc_hash": "759923acb33b279a82ba76ac10e4a5cf4a8bc943a8808aa6745eddb3ae311e79", "ref_doc_id": "522de47a-97a6-4d73-948f-ad40577a9d06"}, "e00d7077-fbdc-410e-88a2-dbfd059192e8": {"doc_hash": "92fd6c66bb50454d98646ed7fa849b194914a14c4ac173910fc7ed3d84160dc9", "ref_doc_id": "522de47a-97a6-4d73-948f-ad40577a9d06"}, "09770b69-569d-422e-89e6-b71b288c5ae1": {"doc_hash": "9ea998e60bc700d38817a711ca13064a0b448dc8298c8617348a1ae30ebaf99a", "ref_doc_id": "522de47a-97a6-4d73-948f-ad40577a9d06"}, "710f2056-4774-4562-9322-e9bd4e56e47f": {"doc_hash": "27e568ed32d6da847614a6fffe329845f6d707f33e8ea4aaa31094133907d0c4", "ref_doc_id": "716fffcb-e467-4b3f-b39b-40b128c950fa"}, "7e46d80c-1192-4e95-9212-1c040333c957": {"doc_hash": "25049273d985ca30366bd2b3f03812fe6d910677343bd69235c8e6441834a4e9", "ref_doc_id": "716fffcb-e467-4b3f-b39b-40b128c950fa"}, "9cf4a007-4214-431f-acab-37e91d82c897": {"doc_hash": "73b09ad8eb672f367f4688a9fd60f191c20c3619a31f7dd53f6b857df4dcc7a1", "ref_doc_id": "716fffcb-e467-4b3f-b39b-40b128c950fa"}, "92b82f76-caaa-4ca6-811c-8c208c13734c": {"doc_hash": "639bfe734f2a88b0110be2ef9013ef30dcf1ffbbed92abd588103006ed56ee2f", "ref_doc_id": "dad26612-babc-4a44-84ef-3f98b427ac0c"}, "2cd5c2bd-963a-4ba5-9615-7c0016d9970c": {"doc_hash": "3e70abe3b1cb13dca53e3718aaa420440e67dedb2424ffde92a73e759a93b921", "ref_doc_id": "dad26612-babc-4a44-84ef-3f98b427ac0c"}, "915f2f95-9027-49f7-a8e8-9910c2c23fe6": {"doc_hash": "d2a6e992b350c20d4b35cc304584cf15a2019c72427a44dbb788ab2d830720a7", "ref_doc_id": "dad26612-babc-4a44-84ef-3f98b427ac0c"}, "d4e4aca4-1228-412c-a869-47566bb7958b": {"doc_hash": "75c417795abfac11f22a6edcc2048221572b458ca0d39da45abd4452ed123214", "ref_doc_id": "dad26612-babc-4a44-84ef-3f98b427ac0c"}, "9e69cfef-1cb9-4673-aadb-5718751b3c8a": {"doc_hash": "3fcd5abd19cb5b7d3a6574c10bbccd39d08a445aa69178a8e1cc2ae042a2dab5", "ref_doc_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400"}, "70fc7448-eb16-4b30-bf32-459082292708": {"doc_hash": "0414475b61aacf66f7b41cad8a2b1acb68d631c807b45ce18aa162187f5a264b", "ref_doc_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400"}, "81e9c672-8e58-419e-a558-49379373a75d": {"doc_hash": "a6e97de4e9fca0100f3c9f3e7df404a61f99b3e107266f0bc4bcccfc41cf1aae", "ref_doc_id": "14ddcb99-9b21-4ce8-8cc8-38a1abfea400"}, "5fa243cd-4c13-470e-85e8-6cc591c573e0": {"doc_hash": "f5d73266a3264fa5befc9df822af751b8aa111e30e5002cc4d06cc35663f5306", "ref_doc_id": "14c27008-5937-4330-9775-61c1868104c5"}, "81f733f2-318d-48cd-b64c-6dd5478fbd63": {"doc_hash": "4f1a70ed93695c911ac863d09b929d88865d8a884df57ce1b02e813c67530bd9", "ref_doc_id": "14c27008-5937-4330-9775-61c1868104c5"}, "bc39d683-788b-4286-8c03-a2237a41817c": {"doc_hash": "761fe765126de7e161e0f951e8e21a8a016a49a38b74fff2ade5518ad3d3683d", "ref_doc_id": "14c27008-5937-4330-9775-61c1868104c5"}, "984ee8d9-3183-4a7e-9648-0c93d1a97c3a": {"doc_hash": "c0696ecb6e7a7e99ffa17aecc83a3b9261e882be14eeb9900ead618b581fad05", "ref_doc_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94"}, "806b115b-52e6-46b9-a17c-8d3f98522927": {"doc_hash": "8e25a79220d9f5175f6a4bd491daa781eeb65ecf702713f8b71f575ab95b61d2", "ref_doc_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94"}, "589174b0-b27e-46e7-acfb-3c0f5105cfbc": {"doc_hash": "befd931d431fa7fa99755595701baf4092967b74a8af07a93aae8b602ef2aa0a", "ref_doc_id": "ba15b127-f6fe-4c5d-8df0-fb35c5c98d94"}, "eb1d53a1-ab5e-4e33-913a-7cf24c76b83f": {"doc_hash": "10857ada186dc32f49f4b563908b062023ba849d2f0c12bb9374f7ae6a1aeb76", "ref_doc_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c"}, "0990ce95-ea2e-463b-a0eb-aeb5aaa435ae": {"doc_hash": "33fc09321bf9423eb9270691720880d3bc0d92d6daea32c0085439989d78d266", "ref_doc_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c"}, "1efa5e14-1c8a-4255-940c-32d59f1c2d60": {"doc_hash": "979482d022fd53b1631eda14492e83ebb25e6c756640d7cd7a0ad73fd9bcc253", "ref_doc_id": "7f4c1d8c-f359-481a-b841-e364b1ace56c"}, "2a56c91e-5fd2-4fbb-908d-1ec63cdb2351": {"doc_hash": "52271a0a7b0c5eaaf4d7b71b5f9c33acdaf6de00922ce4fb3ad0a971cf1b20c0", "ref_doc_id": "936e4894-59f4-4fae-812f-9cedd308bd4a"}, "3ef5868c-09ff-4247-919d-ec84260604a5": {"doc_hash": "e50037f5b7e3ed4fa4d02e6c478a2737f3ffe4009e74693b43b1f5902870afd6", "ref_doc_id": "936e4894-59f4-4fae-812f-9cedd308bd4a"}, "748bfeef-d12b-42d3-874e-8737ab6b65ab": {"doc_hash": "938048aaa13e2086dfc23f59935b0b0c3816ca9cd73c3aca9ed0294f7d452da0", "ref_doc_id": "936e4894-59f4-4fae-812f-9cedd308bd4a"}, "8066023d-e814-45de-bdc2-b331be7e8df4": {"doc_hash": "3fed6bcfd8ed393fadc8380d82f135cfd32d6b7d3437de21136d4030ffcc3537", "ref_doc_id": "57d98462-a7ab-4c26-8cfa-c07b48f6f17d"}, "1a716a35-a27d-46ef-bd71-7c33323126a4": {"doc_hash": "5edfc4a059dbe561c3cec3af0edf6b867e7366149b84e3eb406b84d60c1ca9b4", "ref_doc_id": "57d98462-a7ab-4c26-8cfa-c07b48f6f17d"}, "437e4f47-ffdd-40dd-a21f-5d074f720231": {"doc_hash": "c83ae1fc5b6e3e4310bc9cf4ce8b72de5b6c286e71a5aa635fdd60e7dc252ef5", "ref_doc_id": "ff0056b9-5f40-4b65-b337-973ab0014686"}, "8619cde6-3639-40f4-b458-81a52505ab74": {"doc_hash": "8b97e9091dbf7f201d7ebe07a1be3b507ebccbe157c759a5c9d8393ceee932d7", "ref_doc_id": "ff0056b9-5f40-4b65-b337-973ab0014686"}, "3e7db9ab-0518-496c-bde8-9b1e0f2955d5": {"doc_hash": "27b20d12ad0764a920f13817ce057194500e5325883a110c7218e49f7716b53a", "ref_doc_id": "c51945eb-302c-4d86-90c2-dd965b7f1868"}, "72077ad7-27c8-4e4d-a54c-097473a95004": {"doc_hash": "0cbcc028a378662bc86a357c042405b1d16feeb240bb91739bf220d84b36548c", "ref_doc_id": "c51945eb-302c-4d86-90c2-dd965b7f1868"}, "d23269e7-9c04-4e98-a766-dee3f17a884a": {"doc_hash": "0b33792f6c885e0c443d8ed7057621cbc9ecb5ad0084b43368477b3f89204a7f", "ref_doc_id": "c51945eb-302c-4d86-90c2-dd965b7f1868"}, "58719532-4381-4a45-9db6-9944aa10f590": {"doc_hash": "adef584f01e6af59e2d7e76c2109a1f1a881a50ececec3e3b29da13a99ab432a", "ref_doc_id": "c51945eb-302c-4d86-90c2-dd965b7f1868"}, "edd8b01c-c34b-4ca6-b23a-aeb0a80d8e26": {"doc_hash": "58667287208b1a9e0508abf65ba4b93b36fe5d3f5719576e0bcec8660a34dc3d", "ref_doc_id": "6c9c6365-4eb9-4dce-bdbc-ece01038550d"}, "66dafd3b-417d-44f2-9cb8-e17eff5347be": {"doc_hash": "f99156030e53ba4d94d4e838af4b177ab3d28d624b24af4d2f31bec7de363984", "ref_doc_id": "6c9c6365-4eb9-4dce-bdbc-ece01038550d"}, "4b41fc5f-6cd9-4ffc-aa78-c6d8e54bbe39": {"doc_hash": "a6c60eee815c7989c80ab8b432e9e1dd7158603deb7f416624e7ae8d05272c30", "ref_doc_id": "56119b9c-ea21-4227-b3e6-1493babf84d6"}, "e47dfe0e-495d-440f-9241-cc0c14b80566": {"doc_hash": "51a7d6cf9fa9dea0837c1962826599c21aa1ccfefa887b936858134edb87a003", "ref_doc_id": "56119b9c-ea21-4227-b3e6-1493babf84d6"}, "7759f0d7-7e25-4712-a447-3adbdb4dafbb": {"doc_hash": "d745e21ac19485d2ba920f486a5a5565994d8965e71f436a9ca63d284b09a7bb", "ref_doc_id": "56119b9c-ea21-4227-b3e6-1493babf84d6"}, "1ffa6103-38a6-42ec-afd2-efd386dd31dc": {"doc_hash": "f106a22ad4c509c919246c575588045de51b35395d598c73d384619867dd0f19", "ref_doc_id": "56119b9c-ea21-4227-b3e6-1493babf84d6"}, "394f6166-dfb9-459b-94e6-3efd3836717b": {"doc_hash": "5998e617e287e74a5c98d2a8beea209c4a1ae01cab0175188675b42bb9a910d1", "ref_doc_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e"}, "2dc63329-0589-4bdc-98f1-81d61a1c2796": {"doc_hash": "d396abc0d2a7954867b490612cf643782d2d6ad10e9c34e463f4da59641514f2", "ref_doc_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e"}, "8a03e044-a964-482d-b48e-4eef196677a9": {"doc_hash": "5600f256804a7386823f9602e7e982c154a9c6e93bf0fb1f756df26724b39f03", "ref_doc_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e"}, "faf180dc-3423-4bfe-a347-42687a9736a1": {"doc_hash": "7bd5ddb2adc0265c8adf9526e336f5098463ddbd67a46e80c3ed51cace8cb9ec", "ref_doc_id": "3453be55-5f04-4dd1-8870-191f9c5fd82e"}, "7755ee58-4020-43b4-93de-6884e6fb133b": {"doc_hash": "f4a3c8297b27aebdbfb082b63d0e114aec3d6379d33532a12adf9889bdfba7ef", "ref_doc_id": "e4044733-7eda-41ed-ab44-516f207a5536"}, "a4bdb967-6c11-42dd-89b1-fd0eb2b8fb1b": {"doc_hash": "9a7c2a389cbc6e995f4f7cb621e09194f4528bcafc478473fb2af7400c6c560a", "ref_doc_id": "e4044733-7eda-41ed-ab44-516f207a5536"}, "30ac8e2f-551e-4d9d-bd5e-8766930322fb": {"doc_hash": "9a07e902a59b7002fe4bb60a018ecea2629688e0cd434d8ae9f4074e32258b98", "ref_doc_id": "e4044733-7eda-41ed-ab44-516f207a5536"}, "01d84a18-fafb-4941-967f-3112652eb5a6": {"doc_hash": "318cac52d3ad2c95d507a25c382c26474204b7670893bcc48418b031550ae6fe", "ref_doc_id": "e4044733-7eda-41ed-ab44-516f207a5536"}, "f09fe913-223f-431a-a64c-75f594606793": {"doc_hash": "e962278af8fe51e7970f6da1b748c971aa7d7b419d652bf7fe25c173d7780b16", "ref_doc_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8"}, "9912ac21-cfcb-4e72-8552-12e6c54aa10a": {"doc_hash": "abc91f0c220d89ea534e595591d637b829c08944209c395449596a64fd8c910d", "ref_doc_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8"}, "1153b072-6b34-4917-b560-7868b64009ff": {"doc_hash": "9cfd71943c0bed86681cd5d18c258a378b6993839810a7117958fa5b0b343a0f", "ref_doc_id": "6dd9730e-36d5-4009-88e1-5e237e7767c8"}, "6096b9e3-fcbc-4aad-bbc1-ba2ba633f284": {"doc_hash": "60e558ee03e6f032bf7befad319be4157403bf56b6f6f74079fd67ba584deb0b", "ref_doc_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441"}, "6bf3ee39-9515-4002-a9f6-049f1d3b26ec": {"doc_hash": "75e19492e328af0e653d6217a1960ac0a3a21cd94e7a8d93a5556e941d3b5abf", "ref_doc_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441"}, "ca4d5a00-f5ab-47a8-97c9-00f8f7660b47": {"doc_hash": "46c9dbb47802daf7343e30af7872af0c00e8c5b1d504afb0a6ebf84bad034688", "ref_doc_id": "e6d56a9a-cc7d-49a7-929c-d6bf60d6f441"}, "55916261-7870-443d-8cbd-4b51467a77d6": {"doc_hash": "832af99c6255ffca1f7494dfe1a03194b4c301546a9dcedc3a0ced00d79db5fe", "ref_doc_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa"}, "76ead195-2cbb-415d-8494-0bcb751662f0": {"doc_hash": "0b8340ad54712d5501d2b8c96bf935cd56aa21baddf7fd1dd33b5efacf7e9fa6", "ref_doc_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa"}, "6ac06fb4-4158-4fa2-b621-af3966e83d18": {"doc_hash": "ffbafe46a741ae995be77c63257db2865862a47653043c84e4a38b37e9f4e92b", "ref_doc_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa"}, "c9f6c430-aa2b-43f7-a063-8888fe5435b3": {"doc_hash": "40a91e2f74e267146128ca32c39300d6dfc3e30f4c177854347d1b99b1338f45", "ref_doc_id": "56143f32-0b4a-4fb6-a7a0-2852f8bdc7aa"}, "26917647-8356-409e-b1d2-f587e7ad7b88": {"doc_hash": "80c017d2c18b86e0d25937a58ca35edad55185351eb0f7a21dd4a37912e05258", "ref_doc_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47"}, "2d135c7d-8eaa-478b-bdd1-ad2578ab3094": {"doc_hash": "f68f1ccfc523b8bac200e4e155065f005e11a23b1c6424910bb6c67198547cc7", "ref_doc_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47"}, "6e5af7bb-9eff-44b7-a85a-279d750c3dc2": {"doc_hash": "d5395ade6f5a94df0aeadd66a22b1cebc4396c4cacd3a85ec4bc08bf2841eed6", "ref_doc_id": "c5b23877-b4ba-41c2-9081-12725a1b5b47"}, "a1b98ca6-819a-4536-81f2-b981441c17c1": {"doc_hash": "777c60a53b55932834eb4efdfb02f1371a49893f9de0fd3c8239bcd430c3ba95", "ref_doc_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201"}, "24995949-e50d-41b4-80ed-8a7b44de0c9d": {"doc_hash": "e5253e4e6c6bcae38b8d49d777700a90eb75e26b7db0561a73f6b11861690b12", "ref_doc_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201"}, "c8341126-de12-4e80-9eb5-b118eca3e5a4": {"doc_hash": "c3521d5953a0a69dae1745e32ab578d92da60e78e0eb773330e7bc9abf63dd38", "ref_doc_id": "2e87e40a-d75c-4bca-bf66-2cb20b1e1201"}, "a0b19d2d-3f10-4e64-9d84-79b1e7539cf3": {"doc_hash": "7895101163952bb86ec5b2f5602e5c1c63a6c1955174767d7d54f98a836a307e", "ref_doc_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb"}, "d530c033-2e49-4ed0-8be0-10fb13ac6abf": {"doc_hash": "82a5c348eac6fa43988a07e26cbabc7a6d6af015a37cfd859baf4fe9a0c18854", "ref_doc_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb"}, "1fd96e80-2333-4431-9fd6-c05da53d4fd6": {"doc_hash": "299fdc802caabb2a8ca2829e4aa13df1c2939deb7fdc4cbc6574b952bbfe45e7", "ref_doc_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb"}, "c0764f8c-3df3-4eb5-b37e-a0ac7708814c": {"doc_hash": "33b26bacf1a6879e0b9713b4cf56f9c88b67d0410246fdaa0c934f5c8594b6cc", "ref_doc_id": "f6efea28-e895-40cf-9ca9-21a5b32921bb"}, "862af2ce-1189-47b6-98a3-ddb8ae2e94d8": {"doc_hash": "18d47bb2cebcf2de83e7eeddc501521a04173a4286dadac27d348e93246c0111", "ref_doc_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7"}, "fe5fcf31-cc29-4408-8f3f-6cdf1fabac33": {"doc_hash": "72d060dc8fe3fcd2c2bfc9877b86b07d5805f4d7a3c4386e5f5e618cfa908ece", "ref_doc_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7"}, "294dca04-e918-4d16-81ce-784ed3a08e3f": {"doc_hash": "6c34e4e2e7874f9199159ab27c7ce5554c884a420b716fea7b983e32668217d8", "ref_doc_id": "30ff2ffa-0405-4b62-b988-ea7cd4226bd7"}, "9b7e4e7c-911c-4c48-a02f-cb33dac04468": {"doc_hash": "5899c104c946157d35d1ffb145da3f0c32f21a1af536feb179e5f2aff05b915b", "ref_doc_id": "8307386e-c419-4780-bf97-9408b4967a9e"}, "ddcf50e2-c728-4522-bbb6-b18e29fb2a26": {"doc_hash": "41651a9a2abe4c2083069fca5c5c9c14df67e86b298f6c27b11d3c183445c092", "ref_doc_id": "8307386e-c419-4780-bf97-9408b4967a9e"}, "50fd7008-631d-4845-b3da-5825d697de42": {"doc_hash": "f83d66e98296c6a2beeecc1d49f56dd429697892ec3df1321d120520bf926546", "ref_doc_id": "8307386e-c419-4780-bf97-9408b4967a9e"}, "68ae0c0f-f71c-44eb-bbf1-a2795f3db639": {"doc_hash": "76b95f285c68c9df4f0a51ae33782fbeedcf320a689ee58beaf285d7d3fc92a3", "ref_doc_id": "8307386e-c419-4780-bf97-9408b4967a9e"}, "eee0cf49-41ea-4605-93d2-6a985f5c870f": {"doc_hash": "854e8b3b3597e09ec7ad4ec961ea962840fa9da3511c2a423c85199f97b0cc63", "ref_doc_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b"}, "863991c9-7dd3-4688-bfa4-374dcf558157": {"doc_hash": "a9c285e31d052ba406cd9c578b09a45dc15a14ae4af60dbe30333ceb3a73bced", "ref_doc_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b"}, "9ea697f8-6d10-4ff1-946a-cd3cb406e694": {"doc_hash": "4c1f4d135a51e7d53c527150c0f27eb6ec269ab8083039d161a679e500dbd4ea", "ref_doc_id": "2c051a76-b76f-4afa-8ef0-4e8ac7df5b3b"}, "99f07d67-47b4-478c-b87b-c7bbf90f7509": {"doc_hash": "8204abe877593024ae848a4e071958ef2a1b5adee65c6a65162fd8768b6676d1", "ref_doc_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4"}, "0ecb87cb-eb82-4e30-b344-16d287c03ed7": {"doc_hash": "6269de3f86e589349139aab3e41e5180e54d60c99289756fd964b4ffc6c8a5da", "ref_doc_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4"}, "65c62462-8add-4bb8-9690-655bdcbddaf6": {"doc_hash": "5bc654278c1be413157897c3e1bdbbdfd9f46acab41abb72a611e0c1c54c18f7", "ref_doc_id": "fbc4cea4-73f2-4eaf-8cf3-320dc6cc53e4"}, "1d64353b-2c93-472b-af1a-029696461c4a": {"doc_hash": "6a6fc46145a60e671570c12929f2de4b34da56acadfb8cfebf900a2f20709838", "ref_doc_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6"}, "6ecd87b1-2ce9-4ad4-8864-361467317d67": {"doc_hash": "bda2e5ce6076b6d2f2c23eca605a3d9e692a9c2c4202c69dc4cd9cac729eff40", "ref_doc_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6"}, "0f0df44d-e555-49a8-9337-992e8f4cdd82": {"doc_hash": "b60926c12e02a0eea02ba30e506491327fe4379c19a4f073760dfc9a9fc18165", "ref_doc_id": "b7e94e2e-e638-416c-b26b-6f00d0deb2d6"}, "06959c18-5d5a-436c-94c2-c825c9fd19e4": {"doc_hash": "7867182d15b72fc605ee32a6e1e6e94822df50f37c5f7116bbe01209ed66270c", "ref_doc_id": "467795da-1869-46ce-8473-5f97601af966"}, "073fab27-83ad-447d-a4dd-1a986579cccd": {"doc_hash": "77877e8fe682ce5e129bfc70dab3393cbd0847dd62ccb24e23b285524acf5f3c", "ref_doc_id": "467795da-1869-46ce-8473-5f97601af966"}, "943e09d5-0632-48fc-a3e8-d9f514189867": {"doc_hash": "f86fe4f2e85207dab3de82b6a4f7d7c246625a7e51390a766ecc35dd55ddb94d", "ref_doc_id": "467795da-1869-46ce-8473-5f97601af966"}, "3a1645f5-42af-445d-a6f9-f9151e840905": {"doc_hash": "c34c27e91c578a30fb86dc91bb1c469a2f967e01900c45656c67c3a1950aa579", "ref_doc_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4"}, "3532dee7-3488-402a-9323-c4ac4ab111f6": {"doc_hash": "16b749788442bc834ef66f0e68b596072fa407e2f49ea345dd6804e2505fdc30", "ref_doc_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4"}, "71589527-1312-4862-bf76-ddf5f4067218": {"doc_hash": "af1932bc0b9e2d9027e6c49cff65ed117afc9996fdba96f51973904720085f43", "ref_doc_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4"}, "94ce6c98-d7c0-42d5-b410-5f252c6e4add": {"doc_hash": "cb1c83e287f275759455cac5aeff5fd6db619d85ab82a09176f90c4bc56920dd", "ref_doc_id": "5b8adf1e-0b8f-4998-8d79-6c2fc82397a4"}, "775cfdcb-ec0a-400b-bdff-380c5cd33594": {"doc_hash": "17ec6cbb3203e567273501704e2afbca55f257102bfa90404a31a76008a3f744", "ref_doc_id": "57d82275-bdb8-4707-83cf-2f2442d7befa"}, "1bd244f0-784b-441e-a312-5854ad193643": {"doc_hash": "e3232e6cc4ede0f08002234f02ce0077c1125a425fa5aff98c8ebbae81e46206", "ref_doc_id": "57d82275-bdb8-4707-83cf-2f2442d7befa"}, "233df86f-00fe-437b-ae64-54905559b456": {"doc_hash": "93b03e42f3cb747c999b20df84d37ad4b750d74a023114644604b426801111ce", "ref_doc_id": "57d82275-bdb8-4707-83cf-2f2442d7befa"}, "68f1cda2-f23d-4779-babd-9a366faa6269": {"doc_hash": "9f5221e99287f77cb5368bfa27f0665e099658d32b259fdf8435a00262af779f", "ref_doc_id": "57d82275-bdb8-4707-83cf-2f2442d7befa"}, "61024bcd-7861-4906-a3fa-1a56a6bf9307": {"doc_hash": "c7472c842671eb160804939e6e8cf1f2b2323669df511fdab4d8e72d6ae0a348", "ref_doc_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6"}, "14be06b7-297d-4ada-af94-28249b913c15": {"doc_hash": "907c8c841135a0fbf69392769beb2c7c2725df761bfedbb8aec3e2b852c6dbf8", "ref_doc_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6"}, "54cc4e5b-f7e1-4202-b4a7-3beb688ea5c5": {"doc_hash": "9df7aaed7b6801b6421974870470e9041bf7d1907793f4bbe33c3d9c6a804232", "ref_doc_id": "9fa1858f-bb91-4e38-8aca-8cead6f750f6"}, "33e31028-ed65-4bd1-a21d-aceb00b3b6cc": {"doc_hash": "b83290c0aa73ec1bbd30160d4459253efbcdb5122dd2fa4aedb77d3ffc59272c", "ref_doc_id": "efa0c9b7-e632-419d-8e96-465fde932bcd"}, "7bf7eee9-2e8f-47e6-8212-df37ee35c911": {"doc_hash": "59e9f53e7aef6df28d7654db88212737b161318a776e6f39e0b53065602138ac", "ref_doc_id": "efa0c9b7-e632-419d-8e96-465fde932bcd"}, "7fd0ebf0-ba68-481e-aa52-3df13b97d7bc": {"doc_hash": "a6ee9c9a7e23e7a2cead560f055185fc5d424672f68abe683b47e81bd52110c3", "ref_doc_id": "efa0c9b7-e632-419d-8e96-465fde932bcd"}, "d7f0528f-a8e7-4da6-92ff-25942d99c739": {"doc_hash": "f67c7d60eb3bb0ed85f03ec6e7dc3539b8b62195e60b6240eeffddcca13f8e2e", "ref_doc_id": "efa0c9b7-e632-419d-8e96-465fde932bcd"}, "1489f6c5-e7b4-4b63-a335-da063a794271": {"doc_hash": "384a79c4b666018e367ef34f382dfeb9204fdd0e0fc83db521610f787f4f23f3", "ref_doc_id": "baf924cc-b96b-46fd-84c1-79d69be105f3"}, "d68d6704-95bb-4b0c-8753-6602d33b20e3": {"doc_hash": "0cfdafd780daedc38008c6d4836bce05f196a25414e2d75f4a1c39d057102e2f", "ref_doc_id": "baf924cc-b96b-46fd-84c1-79d69be105f3"}, "4173ece5-0263-4f96-9601-b024f4520896": {"doc_hash": "235ba6ea2370be1a64e93359650bebd5ef52a128f2060f6a57d1a0e687eb93be", "ref_doc_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13"}, "9944fcba-9019-4ba9-a3c5-574daf2cb5e3": {"doc_hash": "eb5f201de7fd6aafd1eca8be538edf7744b759ef216cf00636e6bab1eb9a095e", "ref_doc_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13"}, "0e30dee4-2d8c-46d8-aec5-d1b76a642603": {"doc_hash": "9560c782a7dbd6026c087cfefa6f431004eb73775766389b8514a109e3cd8c6d", "ref_doc_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13"}, "1aabb644-ea1d-4306-93c5-5bf6eb4611a9": {"doc_hash": "5c5b52c7d79744414c45b79d351ebe13ecaccc6840d3a202a25dd19a7e41d70a", "ref_doc_id": "6a3fee34-1975-4ed2-aa31-79f5dce3ba13"}, "a5aaada9-567d-4840-b2ef-5d4d18e90c54": {"doc_hash": "fe9fe0b67f2086a238b8d79345a1447abde1b9ce90155b04acea95b3ce13f222", "ref_doc_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2"}, "372fe491-ee84-4139-bbd3-aeba672ceae2": {"doc_hash": "a385cf17507a9571e6f12094d6a8cfa3555e81dc8bb04b151715e5c3316e9f17", "ref_doc_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2"}, "bf4e0f2a-c7a0-4209-9ddb-37dcbc926b81": {"doc_hash": "d993b4fa31b1a153803da83979e2f252d94a17fe881c5ed5247e72041e42602f", "ref_doc_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2"}, "4cd1fe65-7376-4018-957c-b12a0add9cb2": {"doc_hash": "b32e60d2c31bf705cfea083a8efe4c24941f00c528c388304ad7fb638b15e1d3", "ref_doc_id": "bb91e285-3cd3-4ee7-b37a-8e7630cf00e2"}, "6e492ad7-038f-4098-ad95-c572129140aa": {"doc_hash": "0529a6d7344af2a258d18dfe5a87c620c86f5ed2fb689de7f29d73487c0cb103", "ref_doc_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0"}, "a401e806-2317-44b7-b540-f06cc7515fdd": {"doc_hash": "3ffa1f82e91ee6367f53cbf832e945939661489fca5eb689780e3c59dfa0afdb", "ref_doc_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0"}, "856827b8-2ae2-4f33-bb4a-ff39be844c3a": {"doc_hash": "c9aa5f4a20747fa2989c5bbec04b9f0324aeedaaa4c810c1984bb1a31d21ef09", "ref_doc_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0"}, "6bee4ff0-8395-4897-8cb5-7510719e6322": {"doc_hash": "19f2226138ad840a4aeb4891654572abca4f9c236d19d7d77abb8e25f9730c56", "ref_doc_id": "fe11e513-a41b-435b-a4c3-5ff192d6feb0"}, "2c266b44-9983-4b8f-9f3b-5cbaaceaa6b5": {"doc_hash": "c0337d223b957d2b7062b1e383e66d1b22550c6dddbd042e243647184a2e3223", "ref_doc_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e"}, "cec635fa-2615-442b-b2c6-c429087ea181": {"doc_hash": "ccc28a8cc89f0c8182ac8e55cd0c488b4a08bf528d09d037933e4d3f86ed97ba", "ref_doc_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e"}, "50b69303-cb23-449a-8035-3f3052ba3c58": {"doc_hash": "8e393148857de11224c932ddea8ce6cb75b947a6ccdfdf76c957d90ae8319a68", "ref_doc_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e"}, "b7d815f1-6f92-4100-b2e6-076e82f1bcf3": {"doc_hash": "7bb4888b383063542cc48b5995c575a5f030635636ae4fdb34fb7c149902c808", "ref_doc_id": "25e4fcbf-2304-4368-a573-0dd04a2fdc9e"}, "ec709318-af84-4a34-9912-b83cf1ffda69": {"doc_hash": "091dfb4dddc38035492d4c52c829a3268effc68aa1c1638e5296eb20c2efef1c", "ref_doc_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1"}, "cdcb393b-f093-494d-8dd8-6d709f7f5457": {"doc_hash": "e462ba1deba7ab6f88e642edd49d86e9ecf20401b89e5cf52184e0548f8f709c", "ref_doc_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1"}, "79c66cc3-606d-4dfd-b2c1-9b28af6e3dd7": {"doc_hash": "d3d52b6f51abb242e35a00157d0c9c143fbb32142437956585af38e09495e834", "ref_doc_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1"}, "8d68c899-e84c-43ac-b263-e57a81b96d46": {"doc_hash": "bcaa8a6a28a8b09860862fe39b0f3ff66de0c9ff0d68ffb9527cfa75c004aff3", "ref_doc_id": "ccef981f-9e59-48d8-bb73-711d3aabdfe1"}, "5b1cb693-9230-4c52-a786-35b978e4da8f": {"doc_hash": "c35cb847b1c0d684b2b2a66b0778b9f005125ac5c585bd48d6ab9f206c709dbb", "ref_doc_id": "5c691987-9616-4fb7-89a8-83dfc8587080"}, "fd8bb99a-8f6f-4474-9d17-f8c4db6e89db": {"doc_hash": "588442eae32ebadac2a4488d7ebbb3f04743e16730731767f3fc5d1beb83ec78", "ref_doc_id": "5c691987-9616-4fb7-89a8-83dfc8587080"}, "9b159eb3-6f7b-47bd-9b1d-180e8743ec8f": {"doc_hash": "02f9e0bd9af588d1c22ebb50edb8d49fd18f175e1212b081a6e124062257d46d", "ref_doc_id": "5c691987-9616-4fb7-89a8-83dfc8587080"}, "f1550131-2086-43ce-832e-f350dfc70b2b": {"doc_hash": "22583802b3f983896b9d77df439574c8047366ba5ed1c802ba39a36657716f9c", "ref_doc_id": "5c691987-9616-4fb7-89a8-83dfc8587080"}, "7bbfdb31-657f-48e6-a55b-060470c3748b": {"doc_hash": "ff2f0cd64fe0da5f278410aa16122d304358c6392374e6a02dd3717239725aba", "ref_doc_id": "171bb256-7674-4c94-bfe7-2609e57291d1"}, "89632ac4-256c-4965-bb79-9757bf15c38e": {"doc_hash": "41a075895de8349027278922d56f3158d092dc9b68db84caa98787f791c200f7", "ref_doc_id": "171bb256-7674-4c94-bfe7-2609e57291d1"}, "a592c58d-7ca7-480e-80bd-65288a4c5d5c": {"doc_hash": "a552534d4ebe66504035d6badc622b3269ff2f37ce918de800ce18166d29fff8", "ref_doc_id": "171bb256-7674-4c94-bfe7-2609e57291d1"}, "2e38e707-5f98-47f9-8659-1ea63e7634b6": {"doc_hash": "73fe69a6a3e8d87c67b5515e47ae53f88da258b34c60e8e693e350022e833831", "ref_doc_id": "171bb256-7674-4c94-bfe7-2609e57291d1"}, "9553bf60-44e2-4b30-b0f3-01015787894b": {"doc_hash": "6658e6b4055229c954a6fcba43e8177945560d136bbefc5deb63575aec2c4361", "ref_doc_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0"}, "3e771a02-63d1-41fc-a448-0007671e9cae": {"doc_hash": "27d0e405c129d38cd5559803e0763619deafec62fd5d5e40858bc74d98492bd9", "ref_doc_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0"}, "881abcf2-7975-4eaf-b94d-d555990138ba": {"doc_hash": "e5a7d00bab4beeb4104e0e6a72f16fe4ac9475107a1f8e23988d53ee273ecdf6", "ref_doc_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0"}, "da0e4cec-c02d-4b47-8b9e-e8cacd7eda8d": {"doc_hash": "a409eec7424f11201bf113bab1cc34cb078bf496c108e38e89e482dc5ed63a63", "ref_doc_id": "7342149e-ef54-4ae0-ab55-aa6c2bec14a0"}, "a8ff3da0-6f08-4913-af29-ae21347f7cc7": {"doc_hash": "2f4345fe18a98986f8cd1d95fb052be2341c7ea9f079b0c304a5d58c308392f4", "ref_doc_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f"}, "4aaae6fe-08c6-44db-9fbc-aff16d3a2710": {"doc_hash": "fb7097a2b2dd8acc96713c4c7380305d037c8e5467d00619d83ce0d20a3698bb", "ref_doc_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f"}, "ad0981f2-0977-4a76-bc37-1e4b826af027": {"doc_hash": "94bfb40ca8ffa7a7e8e5af8711a5336cb7f4999d6c915fdb318bf6a25b56ba3b", "ref_doc_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f"}, "1b6805b3-e806-46f4-8161-8a56124dd83b": {"doc_hash": "5751d89852790717f6c78313ac28a54947704baae180a203eafbcd836288e58d", "ref_doc_id": "b38af219-da17-45df-a80a-fe8ae2a44c6f"}, "f1309865-2516-42cb-b374-9811c3cb803f": {"doc_hash": "7bf86fb0bc4dea30130bbfc91086bdce25b94ef46da03b1ae131a75b5d2e7662", "ref_doc_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5"}, "d26219ad-4f57-48b4-a730-1fefa3530c91": {"doc_hash": "610769980b9237744200fdc6f1e7ad81eae41b7e0674bb58833f288abd3773ce", "ref_doc_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5"}, "d80aaf95-855c-4fd3-94f1-e90606cd8f27": {"doc_hash": "15a8e78581c83a5aa137eb32da9ed644846e534c639eff86d4c180680d1c762e", "ref_doc_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5"}, "3da92898-d25d-4a1f-8c47-d2c71b3db512": {"doc_hash": "8f45d8e14c8ff2fac17a64af98b3fea357b04122582af7386f882152c8e37a4a", "ref_doc_id": "9d2789b5-4e82-42b9-9a53-33bc52a366b5"}, "5c6787bb-f3de-4152-b236-e5adb248eef5": {"doc_hash": "ac63e5ba7bfa1642c09ba30f1d2802d4db035db136f8f3b486a067042827de8f", "ref_doc_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9"}, "702a4dc3-2218-45fb-a918-ae81910bb597": {"doc_hash": "b689429ca991e0f06ce1d34d3fa4fe7a8de3a8c04552652c23e6bed25cd5f457", "ref_doc_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9"}, "fbfa05e3-53b0-4be3-bf05-9c83f8471bb0": {"doc_hash": "1f6a53dec9c1bcc60d09370fce7336cfe78810ded0b7ee3f63a0fd14e46fbf31", "ref_doc_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9"}, "5ba6c70f-6801-497a-bc71-79b34146ac45": {"doc_hash": "c1c1a71a8a7bb69bad7604f30213511db0803d5661fd94e961b1182e20eecba4", "ref_doc_id": "767c4308-525a-46c7-b3dd-1c7a3a792ad9"}, "2fea4afe-a138-4675-92c5-f6415ca97313": {"doc_hash": "ab5c68d6620d09e659234d1d36e3f4d83bb1e06ffdc3d74e845800f6b36d7484", "ref_doc_id": "53b92866-0f8e-48d1-bb79-a2d943008412"}, "9a0d5a48-9818-4626-bb90-e6d0fbc3ac7d": {"doc_hash": "e1f3566396c24264a86417c6f1554804b6274be2c092f92b05799daed8f8e570", "ref_doc_id": "53b92866-0f8e-48d1-bb79-a2d943008412"}, "edff671d-9458-4bdb-ac51-0a7d9d7f7d42": {"doc_hash": "b74591e8d5f15a42e91b0c980d57b1f1f4c4b54798b0e78de95a024a56e1350a", "ref_doc_id": "53b92866-0f8e-48d1-bb79-a2d943008412"}, "c76cac1f-267c-4a55-8d5f-c50ff3f2cc54": {"doc_hash": "726295230836e04a450f0606d41f631e6d6fa38344c6a4786f32442d6ca1e5ee", "ref_doc_id": "9206bf00-08cf-4199-9276-96ef119a8e61"}, "517e8542-2a5f-4174-ac04-97dd59ac391c": {"doc_hash": "02a5d5d7d301e024b522cba7394044cb4f0a69ff2734ae96b86a2b99aa8e6f44", "ref_doc_id": "9206bf00-08cf-4199-9276-96ef119a8e61"}, "02097577-3e0b-4ef8-bac8-942c1468a056": {"doc_hash": "27208c3fdf2ea4bf8a1b67ea95e79db1b4669fb7b88a583fdb652d2fdaf05c06", "ref_doc_id": "9206bf00-08cf-4199-9276-96ef119a8e61"}, "0ac65bc7-bb7b-4414-b9f9-04d8bbab3889": {"doc_hash": "73136f2f360ee94c992a38f3ea369d4ab9185a3e362f4c733202fc3bea90c43b", "ref_doc_id": "8b3d4168-8e37-453a-9138-87c7e58ab499"}, "95efa0d0-2f98-4265-a077-0af2b5891a0c": {"doc_hash": "65bfc6ba6644a57d838b1e54cc4d5d781e71b239bb0a15a81a61c3bb2e91928d", "ref_doc_id": "8b3d4168-8e37-453a-9138-87c7e58ab499"}, "dd53061c-f28d-44b2-a91a-b9fcb039faf8": {"doc_hash": "032ad4adecf264ac58c15a66f66b9dd0a58da127ca05fb0ab8d75ebb7ce6710a", "ref_doc_id": "8b3d4168-8e37-453a-9138-87c7e58ab499"}, "de5151fa-e027-4b87-afd3-6ac0abd85d55": {"doc_hash": "3b40291a3e8c5e857368f9a8632a6463308c7feb5762f6f555944ebef7b5b52f", "ref_doc_id": "265d7b64-ddd6-4507-a17a-f8649b7bca3f"}, "b75f2d32-d00c-4f20-82fd-ddc51d695bd8": {"doc_hash": "f637d27b000b9fba516037ae5b932384fb045eab183360bdf58904d80b606e88", "ref_doc_id": "265d7b64-ddd6-4507-a17a-f8649b7bca3f"}, "4cb61500-a2fb-4865-a15c-61be52c02e8f": {"doc_hash": "0a52c034583f1c548128e541a3916898db1c3c95a2789e45e73439217fedc19b", "ref_doc_id": "265d7b64-ddd6-4507-a17a-f8649b7bca3f"}}}